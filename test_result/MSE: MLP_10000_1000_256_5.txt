time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.89%, model saved.
Epoch: 0 Train: 4267.73584 Test: 3729.45752
Epoch 100: New minimal relative error: 76.23%, model saved.
Epoch: 100 Train: 101.52864 Test: 136.31729
Epoch 200: New minimal relative error: 20.98%, model saved.
Epoch: 200 Train: 25.78175 Test: 60.40298
Epoch: 300 Train: 13.73985 Test: 32.38416
Epoch: 400 Train: 7.32161 Test: 16.99363
Epoch: 500 Train: 3.67636 Test: 12.12698
Epoch: 600 Train: 4.78992 Test: 11.12010
Epoch 700: New minimal relative error: 20.57%, model saved.
Epoch: 700 Train: 5.42037 Test: 11.60263
Epoch: 800 Train: 13.50720 Test: 24.59255
Epoch 900: New minimal relative error: 19.27%, model saved.
Epoch: 900 Train: 1.62886 Test: 8.48574
Epoch 1000: New minimal relative error: 18.31%, model saved.
Epoch: 1000 Train: 1.05848 Test: 7.45300
Epoch 1100: New minimal relative error: 16.02%, model saved.
Epoch: 1100 Train: 0.88302 Test: 6.97595
Epoch: 1200 Train: 0.66315 Test: 7.59761
Epoch: 1300 Train: 0.68195 Test: 6.45963
Epoch: 1400 Train: 0.51262 Test: 6.08066
Epoch: 1500 Train: 0.33011 Test: 5.92993
Epoch: 1600 Train: 0.61034 Test: 5.88542
Epoch: 1700 Train: 1.18608 Test: 6.77559
Epoch: 1800 Train: 0.55045 Test: 6.04670
Epoch: 1900 Train: 0.28104 Test: 5.60041
Epoch: 2000 Train: 2.88437 Test: 7.43566
Epoch: 2100 Train: 2.65320 Test: 6.05643
Epoch: 2200 Train: 0.35111 Test: 5.62801
Epoch: 2300 Train: 16.36409 Test: 18.46170
Epoch: 2400 Train: 0.33428 Test: 5.62115
Epoch: 2500 Train: 1.16525 Test: 6.57100
Epoch: 2600 Train: 0.16518 Test: 5.39700
Epoch: 2700 Train: 0.12326 Test: 5.44706
Epoch: 2800 Train: 0.25797 Test: 5.60966
Epoch: 2900 Train: 3.52506 Test: 7.74202
Epoch: 3000 Train: 0.14394 Test: 5.50020
Epoch: 3100 Train: 2.21181 Test: 7.64008
Epoch: 3200 Train: 0.52351 Test: 5.94559
Epoch: 3300 Train: 0.26559 Test: 5.68033
Epoch 3400: New minimal relative error: 14.57%, model saved.
Epoch: 3400 Train: 0.08248 Test: 5.37866
Epoch: 3500 Train: 0.12426 Test: 5.68623
Epoch: 3600 Train: 0.64121 Test: 6.64971
Epoch 3700: New minimal relative error: 12.78%, model saved.
Epoch: 3700 Train: 1.33400 Test: 6.32083
Epoch: 3800 Train: 1.46447 Test: 6.62236
Epoch: 3900 Train: 0.57308 Test: 6.09900
Epoch: 4000 Train: 0.09077 Test: 5.50730
Epoch: 4100 Train: 0.06567 Test: 5.51954
Epoch: 4200 Train: 0.29341 Test: 5.83252
Epoch 4300: New minimal relative error: 12.69%, model saved.
Epoch: 4300 Train: 0.57072 Test: 5.92395
Epoch: 4400 Train: 0.86571 Test: 6.30622
Epoch: 4500 Train: 0.10910 Test: 5.72692
Epoch: 4600 Train: 0.07068 Test: 5.63740
Epoch: 4700 Train: 0.05113 Test: 5.72291
Epoch: 4800 Train: 0.05697 Test: 5.77517
Epoch: 4900 Train: 1.09234 Test: 6.40945
Epoch: 5000 Train: 0.86239 Test: 6.04204
Epoch: 5100 Train: 1.03691 Test: 7.16899
Epoch: 5200 Train: 0.04554 Test: 5.96349
Epoch: 5300 Train: 0.09885 Test: 6.18451
Epoch: 5400 Train: 0.09936 Test: 6.20415
Epoch: 5500 Train: 0.04440 Test: 6.22144
Epoch: 5600 Train: 2.65207 Test: 8.19268
Epoch: 5700 Train: 0.03900 Test: 6.31610
Epoch: 5800 Train: 0.48728 Test: 6.60143
Epoch: 5900 Train: 0.66318 Test: 7.00250
Epoch: 6000 Train: 1.87011 Test: 7.83846
Epoch: 6100 Train: 0.16150 Test: 6.55714
Epoch: 6200 Train: 0.19055 Test: 6.67884
Epoch: 6300 Train: 0.03378 Test: 6.50828
Epoch: 6400 Train: 0.03837 Test: 6.58085
Epoch: 6500 Train: 0.18090 Test: 6.71363
Epoch: 6600 Train: 0.05794 Test: 6.67321
Epoch 6700: New minimal relative error: 12.53%, model saved.
Epoch: 6700 Train: 0.16428 Test: 6.88399
Epoch 6800: New minimal relative error: 12.27%, model saved.
Epoch: 6800 Train: 0.03215 Test: 6.73274
Epoch: 6900 Train: 0.51166 Test: 7.02605
Epoch: 7000 Train: 0.03050 Test: 6.80091
Epoch: 7100 Train: 0.02924 Test: 6.87882
Epoch: 7200 Train: 0.06782 Test: 6.97688
Epoch: 7300 Train: 0.03106 Test: 6.95936
Epoch: 7400 Train: 0.20570 Test: 7.22729
Epoch: 7500 Train: 0.02680 Test: 7.08299
Epoch: 7600 Train: 0.03489 Test: 7.21226
Epoch: 7700 Train: 0.02577 Test: 7.17255
Epoch: 7800 Train: 0.02742 Test: 7.20439
Epoch: 7900 Train: 0.02464 Test: 7.24948
Epoch: 8000 Train: 0.02976 Test: 7.29525
Epoch: 8100 Train: 0.02423 Test: 7.34720
Epoch: 8200 Train: 0.03076 Test: 7.48038
Epoch: 8300 Train: 0.02363 Test: 7.37199
Epoch: 8400 Train: 0.04389 Test: 7.43576
Epoch: 8500 Train: 0.02265 Test: 7.47052
Epoch: 8600 Train: 0.02624 Test: 7.49156
Epoch: 8700 Train: 0.03538 Test: 7.57427
Epoch: 8800 Train: 0.02227 Test: 7.58626
Epoch: 8900 Train: 0.03121 Test: 7.59480
Epoch: 9000 Train: 0.25508 Test: 7.88704
Epoch: 9100 Train: 0.02084 Test: 7.69117
Epoch: 9200 Train: 0.02704 Test: 7.72825
Epoch: 9300 Train: 0.02812 Test: 7.76508
Epoch: 9400 Train: 0.28465 Test: 8.19583
Epoch: 9500 Train: 0.02002 Test: 7.82852
Epoch: 9600 Train: 0.01936 Test: 7.88889
Epoch: 9700 Train: 0.05725 Test: 7.94142
Epoch: 9800 Train: 0.01945 Test: 7.95526
Epoch: 9900 Train: 0.44370 Test: 8.29374
Epoch: 9999 Train: 0.01852 Test: 8.06155
Training Loss: tensor(0.0185)
Test Loss: tensor(8.0615)
Learned LE: [ 0.9618473  -0.05448098 -5.100895  ]
True LE: [ 8.6794728e-01  6.9274991e-03 -1.4544174e+01]
Relative Error: [ 5.743372   6.148195   6.5951815  7.0520535  7.5054607  7.958106
  8.417924   8.884083   9.338027   9.748174  10.08173   10.308336
 10.396319  10.30776    9.992438   9.379582   8.389344   7.001992
  5.4534845  4.6127625  5.565655   7.7140374  9.907639  11.457679
 12.174816  12.301059  12.193368  12.047852  11.847342  11.53786
 11.105137  10.555916   9.917106   9.233391   8.557138   7.937047
  7.4156017  7.053192   6.94072    7.138144   7.5461445  8.064728
  8.788033   9.635586  10.214015  10.314955   9.68787    8.226899
  6.7022524  5.8352184  5.668331   5.9045453  6.12115    5.9885015
  5.6764045  5.400286   5.1595254  4.9035254  4.637882   4.4461927
  4.419724   4.583899   4.9001827  5.3112626  5.768632   6.237025
  6.697774   7.1508603  7.6056747  8.066676   8.521519   8.941063
  9.29322    9.554561   9.707474   9.728552   9.574846   9.172857
  8.4257345  7.2496915  5.6929917  4.2812314  4.3307877  6.078084
  8.230415   9.877351  10.69193   10.872968  10.798269  10.753055
 10.748518  10.645422  10.374563   9.937067   9.3678665  8.714248
  8.025948   7.354187   6.739549   6.2042584  5.800603   5.6529894
  5.856688   6.279862   6.8104086  7.556814   8.413063   8.964026
  8.993238   8.079403   6.481646   5.2575903  4.8431506  5.023008
  5.3615017  5.3877263  5.1406083  4.8838673  4.6369247  4.345343
  4.0176077  3.761959   3.6883898  3.8210504  4.106418   4.4759455
  4.889558   5.329561   5.7788634  6.2257233  6.671258   7.1178555
  7.556589   7.9676614  8.326664   8.613609   8.819362   8.940443
  8.954987   8.796153   8.3533745  7.505789   6.1777678  4.537069
  3.5050333  4.384742   6.3731885  8.18528    9.210872   9.512405
  9.469779   9.451434   9.598008   9.72479    9.657087   9.369497
  8.906146   8.328991   7.688571   7.0216904  6.369835   5.774746
  5.2386804  4.7729945  4.523254   4.668358   5.075041   5.5849357
  6.3079476  7.164705   7.7407346  7.722653   6.571767   5.006111
  4.123239   4.0829673  4.4806733  4.759663   4.6332645  4.388905
  4.139924   3.835762   3.4776137  3.1876788  3.083364   3.1782627
  3.4099627  3.7053041  4.0232024  4.368519   4.757828   5.181076
  5.615841   6.048985   6.465729   6.847489   7.1851244  7.4762692
  7.715963   7.9051747  8.050017   8.117806   8.007191   7.5735235
  6.6789427  5.265442   3.6437645  3.020525   4.316002   6.226157
  7.6268015  8.208956   8.266737   8.21106    8.350482   8.654691
  8.84918    8.793965   8.497111   8.029714   7.472674   6.8789444
  6.2634625  5.6376266  5.0475755  4.5172997  4.0091662  3.5983448
  3.5758688  3.92376    4.379517   5.0242805  5.86919    6.5232196
  6.536595   5.27484    3.8018632  3.1996064  3.425457   3.9379742
  4.1135926  3.9146028  3.6695127  3.3830314  3.0231295  2.7082257
  2.574583   2.6333244  2.8024044  3.0146317  3.2383506  3.4649146
  3.7217953  4.051071   4.4506855  4.8764443  5.2855554  5.645557
  5.94293    6.1937275  6.4229727  6.640164   6.8511767  7.059892
  7.2169514  7.19795    6.8400784  5.9916115  4.61246    3.0771039
  2.576701   3.8949928  5.6428165  6.7705607  7.154593   7.1494913
  7.1445637  7.375783   7.776054   8.052425   8.03837    7.7466497
  7.2828016  6.7570357  6.2332964  5.716052   5.1688247  4.589293
  4.0327406  3.5097084  2.9806015  2.6489894  2.8332722  3.2047074
  3.7237577  4.4926476  5.2490582  5.463974   4.27576    2.8422694
  2.3724399  2.7535706  3.3441556  3.4532049  3.2227218  2.978236
  2.6593137  2.3214362  2.1255202  2.1332028  2.2607553  2.4048207
  2.5427747  2.6865673  2.8379328  3.0082736  3.2528293  3.6077
  4.017446   4.3936634  4.686482   4.899209   5.069562   5.2369585
  5.432326   5.67549    5.954733   6.2053328  6.3079095  6.1033416
  5.4306993  4.2538414  2.8719225  2.1178553  3.096385   4.717189
  5.79561    6.1934686  6.218006   6.2021008  6.4001107  6.8691387
  7.250656   7.293214   7.019095   6.5725408  6.0893006  5.638038
  5.225703   4.8156867  4.3468227  3.8109035  3.241926   2.6839685
  2.1156912  1.8675083  2.101257   2.4795825  3.0347002  3.8285654
  4.4138927  3.6439457  2.1570647  1.5673095  1.9710793  2.661447
  2.7926364  2.5795553  2.350507   2.045769   1.7767333  1.6647086
  1.7259468  1.8516296  1.9606895  2.0417738  2.1249416  2.2206979
  2.3172102  2.4427822  2.6858175  3.0471473  3.3941414  3.6372333
  3.7936952  3.9137628  4.024004   4.1592565  4.385573   4.7029996
  5.023099   5.2461586  5.2464514  4.8648176  4.050662   2.9875
  1.9748503  2.0433552  3.4480267  4.6821957]
