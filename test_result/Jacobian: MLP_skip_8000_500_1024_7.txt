time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.72%, model saved.
Epoch: 0 Train: 170161.45312 Test: 4016.16333
Epoch: 80 Train: 38966.48438 Test: 1768.84119
Epoch: 160 Train: 43724.56641 Test: 1926.45044
Epoch: 240 Train: 33369.38672 Test: 1627.44324
Epoch: 320 Train: 34764.66797 Test: 1041.01196
Epoch: 400 Train: 38236.37109 Test: 1346.67065
Epoch: 480 Train: 35873.46875 Test: 2401.81006
Epoch: 560 Train: 32833.41406 Test: 1259.29932
Epoch 640: New minimal relative error: 51.33%, model saved.
Epoch: 640 Train: 33975.27734 Test: 1126.43811
Epoch: 720 Train: 38767.08984 Test: 1594.72144
Epoch: 800 Train: 37277.35156 Test: 1580.61389
Epoch: 880 Train: 35972.56641 Test: 1483.34155
Epoch: 960 Train: 34341.86719 Test: 1313.66919
Epoch: 1040 Train: 33871.86719 Test: 1417.73389
Epoch: 1120 Train: 34733.59375 Test: 1466.85999
Epoch: 1200 Train: 32816.34375 Test: 1160.95813
Epoch: 1280 Train: 34948.75000 Test: 1345.17932
Epoch: 1360 Train: 34258.32422 Test: 1342.32861
Epoch: 1440 Train: 33266.71094 Test: 1088.03674
Epoch: 1520 Train: 31402.24805 Test: 1108.39148
Epoch: 1600 Train: 33845.00000 Test: 1317.70459
Epoch: 1680 Train: 30139.77930 Test: 1232.54919
Epoch: 1760 Train: 29357.14844 Test: 913.66718
Epoch: 1840 Train: 27975.41406 Test: 1354.18628
Epoch: 1920 Train: 24854.47266 Test: 690.99396
Epoch: 2000 Train: 16599.23242 Test: 433.88434
Epoch: 2080 Train: 21165.16016 Test: 466.00003
Epoch: 2160 Train: 15648.16211 Test: 271.69012
Epoch 2240: New minimal relative error: 32.02%, model saved.
Epoch: 2240 Train: 4738.68408 Test: 45.47528
Epoch: 2320 Train: 5695.43311 Test: 57.03791
Epoch: 2400 Train: 4073.15771 Test: 58.15770
Epoch 2480: New minimal relative error: 23.32%, model saved.
Epoch: 2480 Train: 1936.63147 Test: 14.96213
Epoch 2560: New minimal relative error: 12.06%, model saved.
Epoch: 2560 Train: 1466.20557 Test: 10.13713
Epoch 2640: New minimal relative error: 11.53%, model saved.
Epoch: 2640 Train: 1235.70300 Test: 4.96440
Epoch: 2720 Train: 1607.04968 Test: 15.38550
Epoch: 2800 Train: 1171.12256 Test: 5.56684
Epoch 2880: New minimal relative error: 4.53%, model saved.
Epoch: 2880 Train: 956.84521 Test: 3.67269
Epoch: 2960 Train: 1490.73389 Test: 13.46669
Epoch: 3040 Train: 1486.36426 Test: 9.28037
Epoch: 3120 Train: 766.25909 Test: 2.47126
Epoch: 3200 Train: 709.39307 Test: 7.41522
Epoch: 3280 Train: 690.73383 Test: 6.08748
Epoch: 3360 Train: 831.76276 Test: 5.35486
Epoch: 3440 Train: 1018.81171 Test: 8.48528
Epoch: 3520 Train: 681.56720 Test: 6.38069
Epoch: 3600 Train: 753.22186 Test: 3.77905
Epoch: 3680 Train: 686.51154 Test: 2.03312
Epoch: 3760 Train: 723.75464 Test: 9.07034
Epoch: 3840 Train: 1609.84070 Test: 17.80861
Epoch: 3920 Train: 788.84418 Test: 3.07221
Epoch: 4000 Train: 623.70203 Test: 1.82090
Epoch: 4080 Train: 615.88654 Test: 2.23001
Epoch: 4160 Train: 651.00885 Test: 2.03810
Epoch: 4240 Train: 414.77164 Test: 3.26274
Epoch: 4320 Train: 511.73276 Test: 10.25522
Epoch: 4400 Train: 734.07715 Test: 5.97739
Epoch: 4480 Train: 617.24463 Test: 2.40574
Epoch: 4560 Train: 751.09772 Test: 9.00642
Epoch: 4640 Train: 563.74683 Test: 2.63975
Epoch: 4720 Train: 564.05878 Test: 2.27796
Epoch: 4800 Train: 519.29657 Test: 3.14544
Epoch: 4880 Train: 457.79395 Test: 5.14425
Epoch: 4960 Train: 478.99606 Test: 2.26208
Epoch: 5040 Train: 369.51529 Test: 4.43697
Epoch: 5120 Train: 353.66699 Test: 4.95814
Epoch: 5200 Train: 404.25989 Test: 1.19431
Epoch 5280: New minimal relative error: 2.78%, model saved.
Epoch: 5280 Train: 396.52377 Test: 0.67701
Epoch: 5360 Train: 341.83853 Test: 2.46497
Epoch: 5440 Train: 287.34399 Test: 1.39653
Epoch: 5520 Train: 288.20432 Test: 0.46591
Epoch: 5600 Train: 311.87216 Test: 1.12835
Epoch: 5680 Train: 273.54114 Test: 0.39080
Epoch: 5760 Train: 266.10229 Test: 0.50943
Epoch: 5840 Train: 336.96957 Test: 0.95024
Epoch: 5920 Train: 298.36453 Test: 0.64212
Epoch: 6000 Train: 276.82236 Test: 1.69287
Epoch: 6080 Train: 328.69437 Test: 1.52324
Epoch: 6160 Train: 411.54440 Test: 1.53690
Epoch: 6240 Train: 356.08981 Test: 1.29098
Epoch: 6320 Train: 277.49124 Test: 1.05447
Epoch: 6400 Train: 276.37027 Test: 0.44892
Epoch 6480: New minimal relative error: 2.73%, model saved.
Epoch: 6480 Train: 259.61298 Test: 0.44102
Epoch: 6560 Train: 370.89066 Test: 3.56810
Epoch: 6640 Train: 319.77591 Test: 3.75076
Epoch: 6720 Train: 270.87708 Test: 1.08955
Epoch: 6800 Train: 252.96870 Test: 1.16006
Epoch: 6880 Train: 266.01224 Test: 0.69015
Epoch: 6960 Train: 328.35260 Test: 1.52112
Epoch: 7040 Train: 314.81006 Test: 1.69760
Epoch: 7120 Train: 428.28836 Test: 2.03934
Epoch: 7200 Train: 372.78094 Test: 2.33738
Epoch: 7280 Train: 374.74051 Test: 1.00717
Epoch: 7360 Train: 439.60349 Test: 4.63184
Epoch: 7440 Train: 2021.18225 Test: 14.58639
Epoch: 7520 Train: 563.66626 Test: 2.07324
Epoch: 7600 Train: 373.82379 Test: 0.68213
Epoch: 7680 Train: 324.77014 Test: 0.54663
Epoch: 7760 Train: 265.91531 Test: 0.35374
Epoch: 7840 Train: 311.32788 Test: 2.84641
Epoch: 7920 Train: 238.17490 Test: 0.83525
Epoch: 7999 Train: 234.33528 Test: 0.47466
Training Loss: tensor(234.3353)
Test Loss: tensor(0.4747)
Learned LE: [  0.9334761   -0.06227411 -14.550055  ]
True LE: [ 8.7617987e-01 -3.2290602e-03 -1.4550492e+01]
Relative Error: [3.3985543  3.4569302  3.2371705  2.9414618  2.8348396  2.6256096
 2.1755033  1.7621375  1.2358612  0.7723467  0.5784554  0.7587453
 1.232222   1.4002441  1.3818922  1.5544251  1.7695434  1.7294042
 1.6082684  1.3561804  1.1876911  1.1776862  1.3166364  1.4638138
 1.6934514  2.0434113  2.559583   2.8964932  3.3117604  3.7472925
 3.835037   4.167301   4.1417847  3.8184676  3.4960628  2.9491463
 1.9813355  0.8771971  0.3277497  0.48164517 0.75811857 0.92577535
 1.0020648  1.1479644  1.3694922  1.5088173  1.388814   1.0406353
 0.59760517 0.22266972 0.59312123 0.97419953 1.4103074  1.7510971
 2.102395   2.3637981  2.4454572  2.573946   2.785965   2.8572295
 2.93791    2.9957433  3.031262   3.018599   2.8863337  2.6453385
 2.5148287  2.3541028  1.9848274  1.5673853  1.1056396  0.67113817
 0.5015392  0.82722676 1.2218196  1.3084515  1.2814775  1.5086637
 1.7811123  1.7485058  1.6025679  1.3415171  1.149018   1.1365993
 1.2990276  1.4391445  1.6378286  1.9160141  2.3926947  2.672009
 3.050803   3.5076656  3.6262703  3.9713182  3.9704926  3.671696
 3.2667153  2.6584945  1.6611332  0.5926353  0.25455615 0.3594231
 0.68486357 0.93141955 0.9847909  1.2363814  1.4507725  1.5633734
 1.3798622  0.988816   0.5327425  0.19816992 0.612811   1.0612917
 1.3914306  1.6978846  1.901814   2.0398524  2.1267543  2.2412295
 2.4170039  2.5053961  2.5796266  2.6603096  2.6879747  2.6978056
 2.538445   2.363043   2.2346334  2.0718644  1.72571    1.392056
 1.0173166  0.6845956  0.53409404 0.962484   1.2105826  1.3074912
 1.2636664  1.495355   1.7981949  1.7797987  1.6035665  1.3160151
 1.1202917  1.1755097  1.3218622  1.411607   1.5515817  1.7967652
 2.158245   2.4217012  2.7632005  3.2252166  3.3943574  3.7384543
 3.8014512  3.4716804  3.06466    2.3734088  1.3652682  0.5397626
 0.2789728  0.22793072 0.6220751  0.9118067  0.9743926  1.2528771
 1.5061427  1.5550077  1.3007001  0.8561624  0.40841484 0.23661186
 0.7255392  1.0753038  1.3249046  1.494548   1.6682364  1.7721567
 1.82513    1.9866954  2.1631494  2.3514209  2.4103186  2.487365
 2.467558   2.4200666  2.3990586  2.2852988  2.1147437  1.8021429
 1.5263562  1.2232488  1.0165834  0.7826148  0.70205843 0.97608966
 1.2003856  1.3314902  1.2504753  1.4533639  1.7855417  1.8068066
 1.6179333  1.2981452  1.0641152  1.1930807  1.3269726  1.3661594
 1.3804913  1.6449611  1.885746   2.1264758  2.4812448  2.9059057
 3.146533   3.5110145  3.648616   3.2641969  2.8455727  2.1288254
 1.1984912  0.62571114 0.3271534  0.15970449 0.5314975  0.86295605
 0.9409977  1.178066   1.4069027  1.4165378  1.2142818  0.83168286
 0.43271574 0.3206607  0.7913911  1.0404642  1.2353376  1.2842557
 1.4749215  1.5377398  1.5835481  1.7791873  1.9957887  2.2310731
 2.2328525  2.2460792  2.2802682  2.2779896  2.2169254  2.1712027
 2.0413618  1.6832594  1.3900695  1.1679646  0.98299396 0.90852135
 0.86466736 1.0735008  1.2090336  1.2865105  1.2345724  1.3824525
 1.7391883  1.765193   1.5760113  1.2829276  1.058368   1.1172067
 1.2581803  1.3058317  1.2480363  1.3946632  1.6277887  1.8065518
 2.1236613  2.5714939  2.955266   3.341968   3.5144699  3.1318028
 2.701089   2.0255413  1.1314173  0.7163788  0.39564645 0.19260578
 0.40798923 0.72387385 0.8637657  1.0247997  1.2805175  1.3810577
 1.2119771  0.8383554  0.4564444  0.35694057 0.74314505 0.86724645
 0.9001581  1.0694991  1.2928455  1.3635995  1.4820573  1.6829411
 1.8050883  1.978934   2.0481946  2.0842454  2.1841638  2.0860105
 1.9403812  1.8517935  1.7876725  1.5723838  1.4085338  1.186642
 0.9657883  0.96580833 0.9241282  1.0802581  1.2601148  1.2139733
 1.2033527  1.2538979  1.6425891  1.7310076  1.5722643  1.2548504
 1.0348586  0.9907231  1.1002884  1.1460496  1.1005646  1.1729819
 1.3197492  1.452549   1.7611344  2.2564037  2.7411861  3.122899
 3.3449926  3.0088537  2.5898793  2.0385802  1.239105   0.84282756
 0.43721396 0.25050277 0.31350917 0.53298986 0.6710596  0.8752854
 1.1772373  1.2496397  1.1502192  0.8131194  0.4517643  0.40346044
 0.7166083  0.77913326 0.6898357  0.7799501  1.0796148  1.276467
 1.47759    1.5637319  1.6230348  1.7719123  1.8697152  1.8798838
 1.9549099  1.8653611  1.695659   1.5038935  1.3566887  1.214834
 1.2786952  1.2558702  1.0201886  1.0044708  0.9026983  0.9890116
 1.2587556  1.2841042  1.140665   1.1586589  1.5574052  1.7796865
 1.669802   1.3019054  1.013963   0.9223167  0.9517056  0.93979466
 0.8854502  0.91160834 0.9698509  1.0842936 ]
