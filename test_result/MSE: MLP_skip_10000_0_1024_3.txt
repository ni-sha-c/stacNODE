time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.73%, model saved.
Epoch: 0 Train: 3712.73730 Test: 3747.56958
Epoch 100: New minimal relative error: 37.13%, model saved.
Epoch: 100 Train: 95.36548 Test: 60.52222
Epoch 200: New minimal relative error: 11.61%, model saved.
Epoch: 200 Train: 12.29485 Test: 11.76798
Epoch: 300 Train: 7.28764 Test: 7.65865
Epoch 400: New minimal relative error: 9.53%, model saved.
Epoch: 400 Train: 7.01480 Test: 4.87966
Epoch: 500 Train: 4.00688 Test: 3.72966
Epoch: 600 Train: 3.48796 Test: 3.25079
Epoch: 700 Train: 2.89633 Test: 2.64865
Epoch: 800 Train: 7.30883 Test: 7.63698
Epoch: 900 Train: 2.00933 Test: 1.69466
Epoch: 1000 Train: 3.80058 Test: 4.31811
Epoch 1100: New minimal relative error: 9.17%, model saved.
Epoch: 1100 Train: 2.43780 Test: 1.76247
Epoch: 1200 Train: 1.34672 Test: 1.09212
Epoch: 1300 Train: 2.28825 Test: 2.03689
Epoch: 1400 Train: 11.98379 Test: 11.25835
Epoch: 1500 Train: 0.82587 Test: 0.72139
Epoch: 1600 Train: 3.01018 Test: 3.51126
Epoch 1700: New minimal relative error: 6.70%, model saved.
Epoch: 1700 Train: 1.07021 Test: 0.90101
Epoch: 1800 Train: 1.01136 Test: 0.85387
Epoch: 1900 Train: 4.91645 Test: 3.72417
Epoch: 2000 Train: 0.93158 Test: 0.72818
Epoch: 2100 Train: 2.46545 Test: 2.67667
Epoch: 2200 Train: 1.15957 Test: 1.21032
Epoch: 2300 Train: 0.53791 Test: 0.42061
Epoch: 2400 Train: 0.34478 Test: 0.29564
Epoch: 2500 Train: 3.44720 Test: 4.90166
Epoch 2600: New minimal relative error: 3.14%, model saved.
Epoch: 2600 Train: 0.71892 Test: 0.41471
Epoch: 2700 Train: 0.65185 Test: 0.46850
Epoch: 2800 Train: 0.90062 Test: 0.63240
Epoch: 2900 Train: 0.82488 Test: 0.96264
Epoch: 3000 Train: 0.61270 Test: 0.52105
Epoch: 3100 Train: 0.21951 Test: 0.19488
Epoch: 3200 Train: 0.77970 Test: 0.93839
Epoch: 3300 Train: 0.34504 Test: 0.33238
Epoch: 3400 Train: 0.49503 Test: 0.46013
Epoch: 3500 Train: 3.44387 Test: 3.39424
Epoch: 3600 Train: 0.34970 Test: 0.24337
Epoch: 3700 Train: 3.01090 Test: 2.23669
Epoch: 3800 Train: 0.70629 Test: 0.19324
Epoch: 3900 Train: 0.32524 Test: 0.29930
Epoch: 4000 Train: 1.07013 Test: 0.88541
Epoch: 4100 Train: 1.50842 Test: 1.47005
Epoch: 4200 Train: 0.17533 Test: 0.16970
Epoch: 4300 Train: 0.12706 Test: 0.13344
Epoch: 4400 Train: 0.34164 Test: 0.34498
Epoch: 4500 Train: 1.37090 Test: 1.53184
Epoch: 4600 Train: 0.52903 Test: 0.64401
Epoch: 4700 Train: 0.35552 Test: 0.33548
Epoch: 4800 Train: 0.18164 Test: 0.19354
Epoch: 4900 Train: 0.50089 Test: 0.55198
Epoch: 5000 Train: 0.20064 Test: 0.22748
Epoch: 5100 Train: 0.09613 Test: 0.11047
Epoch: 5200 Train: 0.11222 Test: 0.13044
Epoch: 5300 Train: 0.28403 Test: 0.33400
Epoch: 5400 Train: 0.20574 Test: 0.16846
Epoch: 5500 Train: 0.60408 Test: 0.78526
Epoch: 5600 Train: 0.16057 Test: 0.19073
Epoch: 5700 Train: 0.24117 Test: 0.26213
Epoch: 5800 Train: 0.55675 Test: 0.67389
Epoch: 5900 Train: 0.13626 Test: 0.15968
Epoch: 6000 Train: 0.94029 Test: 1.02754
Epoch: 6100 Train: 1.34363 Test: 1.35533
Epoch 6200: New minimal relative error: 2.67%, model saved.
Epoch: 6200 Train: 0.06947 Test: 0.09381
Epoch: 6300 Train: 0.22409 Test: 0.17258
Epoch: 6400 Train: 0.12849 Test: 0.18014
Epoch: 6500 Train: 0.08999 Test: 0.10874
Epoch: 6600 Train: 0.35281 Test: 0.15562
Epoch: 6700 Train: 0.06178 Test: 0.08828
Epoch: 6800 Train: 0.06823 Test: 0.09186
Epoch: 6900 Train: 0.06321 Test: 0.09230
Epoch: 7000 Train: 0.06180 Test: 0.09116
Epoch: 7100 Train: 0.24528 Test: 0.19031
Epoch: 7200 Train: 0.06021 Test: 0.09411
Epoch: 7300 Train: 0.05963 Test: 0.08517
Epoch: 7400 Train: 0.12339 Test: 0.13606
Epoch: 7500 Train: 0.05239 Test: 0.08123
Epoch: 7600 Train: 0.05424 Test: 0.08258
Epoch: 7700 Train: 0.07625 Test: 0.11856
Epoch: 7800 Train: 0.06348 Test: 0.09935
Epoch: 7900 Train: 0.04878 Test: 0.07847
Epoch: 8000 Train: 0.05035 Test: 0.08259
Epoch: 8100 Train: 0.06632 Test: 0.11766
Epoch: 8200 Train: 0.04645 Test: 0.07655
Epoch: 8300 Train: 0.04671 Test: 0.07645
Epoch: 8400 Train: 0.04590 Test: 0.07583
Epoch: 8500 Train: 0.06111 Test: 0.08473
Epoch: 8600 Train: 0.22780 Test: 0.32272
Epoch: 8700 Train: 0.06473 Test: 0.10059
Epoch: 8800 Train: 0.23825 Test: 0.33249
Epoch: 8900 Train: 0.05638 Test: 0.09410
Epoch: 9000 Train: 0.06782 Test: 0.10428
Epoch: 9100 Train: 0.04324 Test: 0.07428
Epoch: 9200 Train: 0.04078 Test: 0.07262
Epoch: 9300 Train: 0.04132 Test: 0.07269
Epoch: 9400 Train: 0.03849 Test: 0.06997
Epoch: 9500 Train: 0.03986 Test: 0.07161
Epoch 9600: New minimal relative error: 2.34%, model saved.
Epoch: 9600 Train: 0.04178 Test: 0.07846
Epoch: 9700 Train: 0.04334 Test: 0.07623
Epoch: 9800 Train: 0.03640 Test: 0.06785
Epoch: 9900 Train: 0.03689 Test: 0.06872
Epoch: 9999 Train: 0.03861 Test: 0.07396
Training Loss: tensor(0.0386)
Test Loss: tensor(0.0740)
Learned LE: [ 8.6443543e-01  2.0762412e-03 -5.3096514e+00]
True LE: [ 8.66705835e-01  3.41823534e-03 -1.45422535e+01]
Relative Error: [1.4591358  1.6847575  1.8937846  2.0020761  2.122586   2.0026934
 2.2201838  2.156895   1.9235499  1.4758339  1.311176   1.4827017
 1.8386757  2.433616   3.6002688  4.7351403  5.8939214  5.787293
 5.589428   5.629908   5.7798476  5.862447   5.518261   4.8740187
 4.0852447  3.5643895  3.0412786  3.2930288  3.0858667  2.8948286
 2.9297204  2.4453096  1.9346114  1.9075042  2.0967004  2.0892744
 1.8514501  1.863785   1.6855876  1.3153094  0.989024   0.61741024
 0.48855707 0.50845206 0.5838373  0.65953594 0.9063949  1.3424305
 1.6357338  1.7728958  1.6030467  1.5843093  1.6346296  1.6361495
 1.5978832  1.6297771  1.5522848  1.4492701  1.4751359  1.3925327
 1.2227646  1.0861522  1.072224   1.3031763  1.6815834  1.6632806
 1.6968393  1.6986713  1.7036415  2.002462   1.8763429  1.5654597
 1.0462376  0.8983284  1.2309514  1.8874893  2.7908628  3.7134871
 4.532808   5.4215336  5.360392   4.937015   4.854496   5.0110636
 4.9428177  4.7646937  4.4240317  3.838098   3.2674794  2.7918093
 2.7544944  2.655412   2.6028163  2.5384166  2.0196152  1.4664916
 1.6040232  1.9035218  1.7037587  1.54283    1.4890461  1.2577008
 0.90697527 0.70882916 0.50175756 0.35346916 0.39141583 0.35217527
 0.36519268 0.6515595  0.93095154 1.1913588  1.1007688  1.0458326
 1.0455283  1.0511348  1.0598725  1.0790938  1.0726805  1.0696045
 0.98824865 0.9393651  0.89624393 0.8003846  0.79173493 0.95553344
 1.3906584  1.4677365  1.3814563  1.3352579  1.25577    1.4251735
 1.7066813  1.5349766  1.2143279  0.6762702  0.5023724  1.1630981
 1.8306676  2.5986004  3.4954567  3.9797273  4.2137594  4.452361
 4.0220113  3.9247246  3.8676481  3.7839673  3.654692   3.464069
 3.4648657  2.9197628  2.4428277  2.1205804  2.1138306  1.9863075
 2.0738487  1.554744   1.0260342  1.225642   1.4847939  1.2714611
 1.1773621  1.0346698  0.7944014  0.5903905  0.5000061  0.4115478
 0.30515236 0.3592764  0.2922075  0.29193413 0.2836351  0.472112
 0.6002477  0.5681299  0.6050381  0.56047875 0.6065159  0.6216674
 0.6199336  0.7153146  0.7672486  0.68404984 0.63819873 0.5420801
 0.5473261  0.6715717  1.0535171  1.2998486  1.0702726  1.0467141
 0.949748   0.8334991  1.0813665  1.306871   1.1334281  0.8757669
 0.45603397 0.29197362 0.8657692  1.4214152  2.36296    2.9016933
 2.9261925  2.874633   3.1632695  3.028499   2.553332   2.5095031
 2.5368063  2.4070609  2.3312688  2.4886305  2.3349376  1.9175901
 1.5173246  1.4015319  1.2270069  1.3979177  1.0405604  0.62350905
 0.6951814  0.9299497  0.7826072  0.7083607  0.6082322  0.28974187
 0.26859045 0.2693521  0.34981608 0.22616464 0.2853595  0.22226784
 0.17270255 0.10069514 0.26402754 0.3599692  0.25001818 0.18777186
 0.19938028 0.28533152 0.3344246  0.37300518 0.501852   0.52344847
 0.535042   0.48539472 0.4021431  0.31624717 0.48806584 0.9817941
 0.8744614  0.67511636 0.7829312  0.67086756 0.52977335 0.65764064
 0.813409   0.80250865 0.5990838  0.2512114  0.3998253  0.5228456
 1.2809045  1.9412255  2.0569227  2.0438685  1.9264023  1.9020634
 2.2745385  1.796239   1.6371028  1.5515423  1.3726879  1.2012888
 1.2545003  1.3964651  1.3016325  1.0935138  0.9131937  0.79962295
 0.712421   0.7758399  0.39888084 0.49032095 0.5310339  0.5446075
 0.41555953 0.38357586 0.03479943 0.17793697 0.21189372 0.22032091
 0.14659521 0.2275313  0.20503435 0.23060094 0.19745095 0.1856971
 0.18413542 0.22996578 0.17224963 0.1904301  0.34502026 0.42130798
 0.46035853 0.3834295  0.38285083 0.4519334  0.40900457 0.37799662
 0.25470927 0.22631247 0.7406785  0.6061345  0.4336332  0.51872116
 0.5494074  0.51245934 0.5120604  0.36014685 0.4199034  0.47071442
 0.30194262 0.56267476 0.63946563 1.1433582  1.8093357  1.587017
 1.2587522  1.2252709  1.2097067  1.4554613  1.990207   1.4582324
 1.3703895  1.1344665  0.8090999  0.5539994  0.58215207 0.5872221
 0.7115516  0.6155922  0.5008799  0.48565316 0.3791157  0.43601102
 0.35848323 0.45313323 0.32731086 0.28232494 0.25747114 0.128267
 0.23697552 0.23434609 0.18947576 0.13587308 0.28337234 0.34477174
 0.3319328  0.23871541 0.21374623 0.24750847 0.21881992 0.12388743
 0.11134101 0.22893606 0.33034414 0.41212636 0.3752237  0.32182163
 0.31397238 0.3027368  0.32086888 0.24364214 0.3481586  0.47425961
 0.44210306 0.33378747 0.2705798  0.33720398 0.33758515 0.6577159
 0.45638606 0.12884657 0.19534698 0.39694294 0.7065685  1.0306166
 1.3576857  1.587338   1.5683986  1.2509197  1.0341702  1.0351452
 1.0003513  0.65732056 1.0491893  0.5158968 ]
