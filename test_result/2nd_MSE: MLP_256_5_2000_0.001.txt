time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.86%, model saved.
Epoch: 0 Train: 3832.81055 Test: 4259.23682
Epoch 100: New minimal relative error: 90.28%, model saved.
Epoch: 100 Train: 75.96404 Test: 83.41349
Epoch: 200 Train: 14.33498 Test: 24.10995
Epoch 300: New minimal relative error: 28.19%, model saved.
Epoch: 300 Train: 15.96057 Test: 16.52420
Epoch 400: New minimal relative error: 13.63%, model saved.
Epoch: 400 Train: 3.94219 Test: 3.92679
Epoch: 500 Train: 12.64550 Test: 10.11787
Epoch: 600 Train: 2.90357 Test: 1.59779
Epoch: 700 Train: 2.40445 Test: 1.68262
Epoch: 800 Train: 2.18960 Test: 1.47627
Epoch 900: New minimal relative error: 11.07%, model saved.
Epoch: 900 Train: 1.75222 Test: 1.29674
Epoch: 1000 Train: 1.02711 Test: 0.74496
Epoch: 1100 Train: 2.77374 Test: 0.97705
Epoch: 1200 Train: 0.65143 Test: 0.43563
Epoch: 1300 Train: 1.58199 Test: 2.22378
Epoch: 1400 Train: 0.91854 Test: 0.51060
Epoch: 1500 Train: 0.83427 Test: 0.70882
Epoch 1600: New minimal relative error: 9.06%, model saved.
Epoch: 1600 Train: 0.68228 Test: 0.51169
Epoch: 1700 Train: 5.41832 Test: 2.24597
Epoch: 1800 Train: 0.79158 Test: 0.53894
Epoch: 1900 Train: 0.29313 Test: 0.20039
Epoch: 2000 Train: 4.27633 Test: 5.85150
Epoch: 2100 Train: 0.29756 Test: 0.33125
Epoch: 2200 Train: 0.22957 Test: 0.14780
Epoch: 2300 Train: 0.42441 Test: 0.34507
Epoch: 2400 Train: 2.44720 Test: 2.48908
Epoch 2500: New minimal relative error: 8.66%, model saved.
Epoch: 2500 Train: 0.20844 Test: 0.12885
Epoch: 2600 Train: 0.42759 Test: 0.43986
Epoch: 2700 Train: 1.24583 Test: 0.96867
Epoch: 2800 Train: 5.29574 Test: 4.42688
Epoch: 2900 Train: 12.12240 Test: 10.32217
Epoch: 3000 Train: 0.13644 Test: 0.10461
Epoch: 3100 Train: 0.13215 Test: 0.09636
Epoch: 3200 Train: 0.15398 Test: 0.11066
Epoch: 3300 Train: 0.14383 Test: 0.11620
Epoch: 3400 Train: 0.74292 Test: 0.63615
Epoch: 3500 Train: 0.10344 Test: 0.08269
Epoch: 3600 Train: 0.10733 Test: 0.08154
Epoch: 3700 Train: 0.12695 Test: 0.10426
Epoch: 3800 Train: 0.89096 Test: 0.82264
Epoch 3900: New minimal relative error: 8.35%, model saved.
Epoch: 3900 Train: 0.09386 Test: 0.07997
Epoch: 4000 Train: 0.08943 Test: 0.06974
Epoch: 4100 Train: 0.93774 Test: 0.11138
Epoch: 4200 Train: 0.07919 Test: 0.06382
Epoch: 4300 Train: 0.09042 Test: 0.07594
Epoch: 4400 Train: 0.07092 Test: 0.05907
Epoch: 4500 Train: 0.08198 Test: 0.06867
Epoch: 4600 Train: 0.06600 Test: 0.05608
Epoch: 4700 Train: 0.07168 Test: 0.05919
Epoch: 4800 Train: 0.06643 Test: 0.05545
Epoch: 4900 Train: 0.19548 Test: 0.12523
Epoch: 5000 Train: 0.06379 Test: 0.05818
Epoch: 5100 Train: 0.06230 Test: 0.05262
Epoch: 5200 Train: 0.13588 Test: 0.08139
Epoch: 5300 Train: 0.06342 Test: 0.05105
Epoch: 5400 Train: 1.75228 Test: 2.24982
Epoch: 5500 Train: 0.05265 Test: 0.04693
Epoch: 5600 Train: 0.13370 Test: 0.06289
Epoch: 5700 Train: 0.05065 Test: 0.04554
Epoch: 5800 Train: 0.05160 Test: 0.04791
Epoch: 5900 Train: 0.07065 Test: 0.09238
Epoch: 6000 Train: 0.04537 Test: 0.04197
Epoch: 6100 Train: 0.44817 Test: 0.75536
Epoch: 6200 Train: 0.04564 Test: 0.04196
Epoch: 6300 Train: 1.43100 Test: 2.19081
Epoch: 6400 Train: 0.04074 Test: 0.03851
Epoch: 6500 Train: 0.05286 Test: 0.04897
Epoch: 6600 Train: 0.03909 Test: 0.03751
Epoch: 6700 Train: 0.05378 Test: 0.16735
Epoch: 6800 Train: 0.03872 Test: 0.03718
Epoch: 6900 Train: 0.03587 Test: 0.03525
Epoch: 7000 Train: 0.03608 Test: 0.03647
Epoch: 7100 Train: 0.03665 Test: 0.03541
Epoch: 7200 Train: 0.03408 Test: 0.03421
Epoch: 7300 Train: 0.03561 Test: 0.03455
Epoch: 7400 Train: 0.03258 Test: 0.03278
Epoch 7500: New minimal relative error: 5.15%, model saved.
Epoch: 7500 Train: 0.06397 Test: 0.03990
Epoch: 7600 Train: 0.03193 Test: 0.03224
Epoch: 7700 Train: 0.11493 Test: 0.10573
Epoch: 7800 Train: 0.37128 Test: 0.63019
Epoch: 7900 Train: 0.03042 Test: 0.03107
Epoch: 8000 Train: 0.02881 Test: 0.02991
Epoch: 8100 Train: 0.03152 Test: 0.04264
Epoch: 8200 Train: 0.02874 Test: 0.02979
Epoch: 8300 Train: 0.02923 Test: 0.03210
Epoch: 8400 Train: 0.02692 Test: 0.02862
Epoch: 8500 Train: 0.02726 Test: 0.02888
Epoch: 8600 Train: 0.02597 Test: 0.02767
Epoch: 8700 Train: 0.08603 Test: 0.08411
Epoch: 8800 Train: 0.02547 Test: 0.02726
Epoch: 8900 Train: 0.04560 Test: 0.02793
Epoch: 9000 Train: 0.02751 Test: 0.05517
Epoch: 9100 Train: 0.02408 Test: 0.02614
Epoch: 9200 Train: 1.79086 Test: 1.61197
Epoch: 9300 Train: 0.02363 Test: 0.02576
Epoch: 9400 Train: 0.02278 Test: 0.02506
Epoch: 9500 Train: 0.06601 Test: 0.06718
Epoch: 9600 Train: 0.09192 Test: 0.08365
Epoch: 9700 Train: 0.17004 Test: 0.17724
Epoch: 9800 Train: 0.02162 Test: 0.02422
Epoch: 9900 Train: 0.02145 Test: 0.02389
Epoch: 9999 Train: 0.04506 Test: 0.05649
Training Loss: tensor(0.0451)
Test Loss: tensor(0.0565)
Learned LE: [ 0.89140534 -0.00852981 -5.581487  ]
True LE: [ 8.7312692e-01 -2.5514518e-03 -1.4553945e+01]
Relative Error: [3.990525   3.9349034  3.8433597  3.718305   3.552501   3.3813405
 3.2995377  3.3584583  3.4560018  3.4108095  3.1224542  2.6059892
 1.9384096  1.2092167  0.6385687  0.8578261  1.5088116  2.1641772
 2.7317247  3.1805277  3.5083249  3.7193422  3.8116739  3.7863233
 3.6617641  3.474286   3.2605252  3.046159   2.8571668  2.7015367
 2.4818766  2.093595   1.6257958  1.1739877  0.8893378  0.7829585
 0.87410307 1.0209397  1.1701566  1.4119596  1.5568204  1.3627795
 1.072688   0.9592472  1.0222706  1.2440735  1.5961661  1.9860989
 2.3538682  2.670683   2.8879306  2.9741156  2.9487572  2.8702757
 2.8022864  2.7879057  2.8420827  2.9555511  3.1040354  3.2590382
 3.395848   3.4940052  3.5373993  3.5212317  3.458215   3.3628159
 3.2320418  3.0815406  2.9963593  3.0645566  3.2206383  3.2655277
 3.0627143  2.606498   1.9746377  1.2683427  0.619091   0.6251453
 1.2882173  2.0149004  2.6688707  3.1957717  3.5867944  3.8510735
 3.9926891  4.011315   3.9140344  3.724546   3.4785051  3.2099268
 2.94897    2.7364345  2.5728247  2.293076   1.828604   1.3256292
 0.92515576 0.72179043 0.67874926 0.8064136  0.9366785  1.1617774
 1.3841031  1.2979065  0.98776835 0.78974366 0.7934001  0.98772806
 1.333461   1.7191348  2.0820448  2.3898299  2.585212   2.632708
 2.5540378  2.4160597  2.2913597  2.2267447  2.237442   2.317267
 2.4479682  2.605162   2.7644722  2.9038484  3.0024772  3.0435693
 3.0252402  2.9638388  2.8701868  2.7485664  2.655195   2.7005136
 2.8964107  3.052888   2.9744198  2.6154664  2.0416536  1.3785132
 0.7250613  0.37668213 0.95869815 1.729842   2.4627557  3.0744221
 3.5368588  3.8522243  4.029873   4.083408   4.032524   3.8955166
 3.688693   3.4258697  3.1228712  2.8213673  2.5915585  2.42915
 2.0945938  1.5744176  1.0589441  0.7384095  0.56728435 0.5960192
 0.70727587 0.8866217  1.1696043  1.2231256  0.9830054  0.70010376
 0.6008548  0.72451025 1.0454801  1.4310707  1.7973702  2.0975885
 2.2711143  2.286135   2.1651978  1.9793695  1.8089267  1.7032574
 1.6717308  1.703272   1.7859582  1.9099077  2.061715   2.2229648
 2.370677   2.4793365  2.5283139  2.5169692  2.4639127  2.3792143
 2.2875364  2.2834027  2.4622817  2.7262208  2.8182323  2.6084597
 2.1284132  1.511096   0.90850115 0.36734545 0.5605941  1.3034052
 2.0876832  2.80343    3.3814747  3.7864559  4.012085   4.076668
 4.0241127  3.9040484  3.7428513  3.5481527  3.3130522  3.0063086
 2.6725602  2.4285257  2.2843943  1.9246478  1.367732   0.84903103
 0.57847553 0.4444177  0.50198156 0.60297984 0.8754306  1.0773892
 0.9928106  0.7193607  0.50526035 0.49340543 0.7319668  1.1010371
 1.476509   1.7861171  1.9517176  1.9464692  1.7945753  1.5694906
 1.3629541  1.2329547  1.1824847  1.1818082  1.2075456  1.2609757
 1.3543581  1.4903076  1.654004   1.817687   1.9475398  2.0143437
 2.0150232  1.9731535  1.9044555  1.8556912  1.944987   2.2321548
 2.519893   2.5354679  2.218338   1.6663253  1.0819488  0.5845674
 0.26280576 0.78613627 1.5398148  2.3423178  3.081839   3.6466212
 3.9790142  4.085855   4.023144   3.8689647  3.6850526  3.4929032
 3.2920487  3.0991457  2.8459396  2.5033677  2.2428925  2.1359775
 1.8171417  1.2512673  0.70340765 0.4471433  0.35956818 0.38997555
 0.5091727  0.81655663 0.88957727 0.7807027  0.52775866 0.3803432
 0.43760458 0.7278169  1.0857819  1.4232477  1.6282821  1.642622
 1.4792435  1.2146995  0.96220493 0.8066338  0.75942653 0.77175355
 0.785759   0.78128016 0.78292775 0.8303366  0.94207406 1.1033064
 1.2819998  1.4396544  1.5336349  1.5465039  1.5098263  1.4571204
 1.4444492  1.6035885  1.9706587  2.277785   2.2454095  1.8636113
 1.283003   0.7554302  0.3642327  0.33793327 0.90208036 1.6602337
 2.5110266  3.271678   3.778626   3.9874747  3.9561856  3.7866282
 3.5677042  3.3489614  3.1483784  2.946241   2.747865   2.5872784
 2.310884   2.0274365  1.9411554  1.7649757  1.2624596  0.66155046
 0.35083833 0.2938002  0.29003227 0.39180598 0.66920364 0.67914283
 0.5979756  0.3898897  0.30219007 0.37255567 0.64462155 0.9666804
 1.2377481  1.3546644  1.2597873  0.98499626 0.66485703 0.44982466
 0.38891864 0.43314165 0.4994545  0.5206169  0.48780608 0.4424558
 0.44226646 0.5196542  0.6572912  0.8243731  0.9946988  1.123359
 1.1572866  1.1077231  1.0494084  1.0453596  1.2254641  1.6450087
 2.0015175  1.9891391  1.6066571  1.0156627  0.49427924 0.21880767
 0.39386758 0.9123966  1.6496886  2.4896526  3.194594   3.5996227
 3.6938484  3.5711224  3.3567197  3.1260953 ]
