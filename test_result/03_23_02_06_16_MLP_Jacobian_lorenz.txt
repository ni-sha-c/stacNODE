time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 2000
num_train: 8000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 128
n_layers: 8
reg_param: 800
optim_name: AdamW
Learned LE: [ 8.4577686e-01  1.0290093e-02 -1.4530575e+01]
True LE: [ 8.6606967e-01  7.3760361e-03 -1.4547889e+01]
