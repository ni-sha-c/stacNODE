time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.58%, model saved.
Epoch: 0 Train: 31831.66406 Test: 4016.88062
Epoch: 80 Train: 9307.54785 Test: 1620.59143
Epoch 160: New minimal relative error: 92.84%, model saved.
Epoch: 160 Train: 8342.34375 Test: 1374.18176
Epoch 240: New minimal relative error: 87.90%, model saved.
Epoch: 240 Train: 8830.13184 Test: 1157.47302
Epoch 320: New minimal relative error: 65.59%, model saved.
Epoch: 320 Train: 7069.61133 Test: 1105.82764
Epoch: 400 Train: 6024.94727 Test: 744.42468
Epoch: 480 Train: 5051.92773 Test: 540.86304
Epoch 560: New minimal relative error: 48.33%, model saved.
Epoch: 560 Train: 3882.98804 Test: 381.86273
Epoch: 640 Train: 3015.37256 Test: 288.74173
Epoch: 720 Train: 1098.38611 Test: 297.32849
Epoch: 800 Train: 499.46301 Test: 10.70160
Epoch 880: New minimal relative error: 46.77%, model saved.
Epoch: 880 Train: 397.80881 Test: 40.04416
Epoch 960: New minimal relative error: 9.75%, model saved.
Epoch: 960 Train: 301.34320 Test: 5.91562
Epoch: 1040 Train: 313.37781 Test: 6.10780
Epoch: 1120 Train: 251.96997 Test: 4.18164
Epoch: 1200 Train: 216.02734 Test: 3.38201
Epoch: 1280 Train: 204.57298 Test: 3.17640
Epoch: 1360 Train: 195.97871 Test: 3.95828
Epoch: 1440 Train: 164.54666 Test: 3.01211
Epoch 1520: New minimal relative error: 4.01%, model saved.
Epoch: 1520 Train: 159.77824 Test: 1.71740
Epoch: 1600 Train: 145.05173 Test: 2.70637
Epoch: 1680 Train: 130.94041 Test: 1.39776
Epoch: 1760 Train: 130.19957 Test: 3.88384
Epoch: 1840 Train: 107.93678 Test: 1.22547
Epoch: 1920 Train: 98.40924 Test: 0.78303
Epoch: 2000 Train: 104.62717 Test: 1.70199
Epoch: 2080 Train: 94.69719 Test: 1.28582
Epoch 2160: New minimal relative error: 3.25%, model saved.
Epoch: 2160 Train: 83.47256 Test: 0.77638
Epoch: 2240 Train: 88.68652 Test: 6.42501
Epoch: 2320 Train: 76.79171 Test: 1.26913
Epoch: 2400 Train: 79.50895 Test: 2.33624
Epoch: 2480 Train: 71.28226 Test: 0.40527
Epoch: 2560 Train: 85.91582 Test: 6.04157
Epoch: 2640 Train: 71.43681 Test: 0.75664
Epoch: 2720 Train: 74.57985 Test: 1.01159
Epoch: 2800 Train: 78.48601 Test: 9.15385
Epoch 2880: New minimal relative error: 1.99%, model saved.
Epoch: 2880 Train: 64.24398 Test: 0.41125
Epoch: 2960 Train: 77.51336 Test: 1.58987
Epoch: 3040 Train: 68.79791 Test: 0.68704
Epoch: 3120 Train: 72.95946 Test: 2.07133
Epoch: 3200 Train: 91.19445 Test: 3.24249
Epoch: 3280 Train: 76.82209 Test: 0.95561
Epoch: 3360 Train: 77.70909 Test: 1.20046
Epoch: 3440 Train: 69.26808 Test: 0.38777
Epoch: 3520 Train: 74.36563 Test: 4.08312
Epoch: 3600 Train: 67.92271 Test: 2.02146
Epoch 3680: New minimal relative error: 1.59%, model saved.
Epoch: 3680 Train: 60.53151 Test: 0.26337
Epoch: 3760 Train: 72.19310 Test: 3.06298
Epoch: 3840 Train: 57.57753 Test: 0.39946
Epoch: 3920 Train: 55.84384 Test: 1.25637
Epoch: 4000 Train: 53.28860 Test: 0.41961
Epoch: 4080 Train: 53.47411 Test: 0.67558
Epoch: 4160 Train: 52.85630 Test: 0.25054
Epoch: 4240 Train: 51.74860 Test: 0.42904
Epoch: 4320 Train: 56.16080 Test: 1.62530
Epoch: 4400 Train: 48.90975 Test: 0.22905
Epoch: 4480 Train: 46.96660 Test: 0.25972
Epoch: 4560 Train: 45.52072 Test: 0.26035
Epoch: 4640 Train: 46.26732 Test: 0.33612
Epoch: 4720 Train: 50.93832 Test: 0.58793
Epoch: 4800 Train: 44.16675 Test: 0.16399
Epoch: 4880 Train: 42.81094 Test: 0.20590
Epoch: 4960 Train: 43.92392 Test: 0.49728
Epoch: 5040 Train: 43.13330 Test: 0.56389
Epoch: 5120 Train: 39.99466 Test: 0.16918
Epoch: 5200 Train: 41.12012 Test: 1.12546
Epoch: 5280 Train: 37.71481 Test: 0.12148
Epoch: 5360 Train: 42.81850 Test: 0.62369
Epoch: 5440 Train: 38.83282 Test: 1.73866
Epoch: 5520 Train: 44.08322 Test: 2.94084
Epoch: 5600 Train: 34.09653 Test: 0.09833
Epoch: 5680 Train: 41.48346 Test: 2.02054
Epoch: 5760 Train: 35.13174 Test: 0.09387
Epoch: 5840 Train: 34.42253 Test: 0.11009
Epoch: 5920 Train: 37.04502 Test: 0.93990
Epoch: 6000 Train: 35.05720 Test: 0.15374
Epoch: 6080 Train: 33.73117 Test: 0.12666
Epoch: 6160 Train: 32.63754 Test: 0.12926
Epoch: 6240 Train: 31.88454 Test: 0.10150
Epoch 6320: New minimal relative error: 1.52%, model saved.
Epoch: 6320 Train: 31.68271 Test: 0.08406
Epoch: 6400 Train: 33.14748 Test: 0.79227
Epoch: 6480 Train: 31.18333 Test: 0.19616
Epoch: 6560 Train: 31.99300 Test: 0.07825
Epoch: 6640 Train: 33.28490 Test: 0.21397
Epoch 6720: New minimal relative error: 1.12%, model saved.
Epoch: 6720 Train: 32.48364 Test: 0.08821
Epoch: 6800 Train: 34.02940 Test: 0.61897
Epoch: 6880 Train: 32.73735 Test: 0.08952
Epoch: 6960 Train: 33.19674 Test: 0.12828
Epoch: 7040 Train: 31.98664 Test: 0.08612
Epoch: 7120 Train: 31.43748 Test: 0.18173
Epoch: 7200 Train: 33.20481 Test: 0.08741
Epoch 7280: New minimal relative error: 0.90%, model saved.
Epoch: 7280 Train: 32.76287 Test: 0.07992
Epoch: 7360 Train: 32.52680 Test: 0.09309
Epoch: 7440 Train: 35.01474 Test: 0.14756
Epoch: 7520 Train: 35.28064 Test: 0.14177
Epoch: 7600 Train: 32.85301 Test: 0.74158
Epoch: 7680 Train: 30.75447 Test: 0.14438
Epoch: 7760 Train: 29.61636 Test: 0.06818
Epoch: 7840 Train: 29.78672 Test: 0.11459
Epoch: 7920 Train: 30.15544 Test: 0.31226
Epoch: 7999 Train: 30.33600 Test: 0.08141
Training Loss: tensor(30.3360)
Test Loss: tensor(0.0814)
Learned LE: [  0.82057005   0.01930348 -14.523175  ]
True LE: [ 8.5023159e-01 -2.0245370e-03 -1.4525001e+01]
Relative Error: [0.81832606 0.9446492  0.90102684 0.9114122  0.88764346 0.88953745
 1.1374029  1.2856795  1.2523551  1.179878   1.1615964  1.3368694
 1.6241007  1.9449463  2.2433906  2.415123   2.5076723  2.4357014
 2.2798965  2.2098076  2.3729682  2.5231812  2.6475549  2.7355742
 2.6706793  2.5758195  2.4765666  2.4047601  2.3639493  2.1880682
 1.9755344  1.7731405  1.512063   1.2338636  1.0663732  0.977847
 0.9256363  0.97742677 0.98519    0.9055282  0.7054975  0.65835625
 0.79634523 1.1484773  1.7006191  2.2695346  2.2360263  2.0193605
 1.9203428  1.931769   1.9283825  1.7877569  1.6709785  1.2735223
 0.8334332  0.5091536  0.58942276 0.6195703  0.5853118  0.7648906
 0.8708176  0.9341876  1.0173234  1.1211904  1.0857929  1.0035658
 0.9632071  0.8423982  1.0826927  1.1421078  1.1279343  1.0277325
 1.0471034  1.2306943  1.5514326  1.9172449  2.1573644  2.3032
 2.3547964  2.284457   2.0780966  2.1657999  2.2934527  2.3892312
 2.4805286  2.55694    2.4955359  2.3919723  2.2730253  2.2733498
 2.2531385  2.0405495  1.7766254  1.5762703  1.3342918  1.046953
 0.9168489  0.87242687 0.8321829  0.92338586 0.8007219  0.61107206
 0.48828697 0.46523046 0.63080883 1.0435498  1.6431872  2.104632
 1.8197908  1.6730183  1.6618212  1.6904792  1.5641338  1.4872307
 1.2492106  0.9041384  0.66544664 0.6751783  0.85017765 0.86980945
 0.8401505  0.865285   0.998521   1.0819578  1.1841846  1.2438219
 1.2415051  1.1786511  0.9558408  0.82893753 0.9545598  1.0003642
 0.9805378  0.9017891  0.96582454 1.1554819  1.5287728  1.8642433
 2.0848274  2.1967332  2.1933908  2.1013374  1.8956258  2.070931
 2.1751006  2.2144186  2.2644134  2.3358572  2.3157833  2.189189
 2.0392     2.102051   2.1240265  1.8769093  1.5813526  1.3972957
 1.1474596  0.90951556 0.8021607  0.7566051  0.7501917  0.71624357
 0.56071895 0.45735464 0.38789308 0.35052857 0.54872453 1.0104897
 1.6296685  1.7744386  1.5102717  1.3844806  1.3128047  1.3833176
 1.2985559  1.2228229  0.9202073  0.74031067 0.74124557 0.95472264
 1.1653992  1.0824487  1.0614105  1.000371   1.0691129  1.1785508
 1.3078904  1.3294408  1.3685582  1.2546743  1.0330434  0.7584582
 0.79321367 0.8782865  0.82907426 0.83388865 0.91781926 1.1039305
 1.4802225  1.817835   2.010428   2.0735054  2.020883   1.8724514
 1.8015864  1.9207778  2.003992   2.070086   2.1779966  2.262511
 2.2596834  2.1334045  1.9795725  2.0250194  1.9850641  1.6868193
 1.3746173  1.2168263  0.9727207  0.7903445  0.6957397  0.6729316
 0.64352584 0.53539306 0.39356294 0.38960025 0.35059693 0.27694064
 0.4918442  0.97007614 1.4478124  1.5027206  1.3091092  1.0730653
 0.9956125  1.1345973  1.0541555  0.93291485 0.78225666 0.76775944
 0.9027269  1.1896666  1.4213828  1.2713221  1.2190336  1.1536506
 1.0846434  1.2198945  1.3522133  1.3686936  1.3989938  1.2945971
 1.0693178  0.73623484 0.6650218  0.7411592  0.73755735 0.8197347
 0.895246   1.0869225  1.4603919  1.7718122  1.929798   1.9504994
 1.8435736  1.6379125  1.6822888  1.7486162  1.9195596  2.0054705
 2.077668   2.1394165  2.151393   2.0233016  1.9051853  1.9760952
 1.9296862  1.6549381  1.2425611  1.0101018  0.83840454 0.6605757
 0.5817664  0.60294616 0.55011606 0.40202206 0.3326576  0.4383039
 0.38929865 0.23098353 0.41097915 0.9060802  1.1963034  1.29083
 0.9952863  0.83957165 0.8807113  0.9338593  0.8886147  0.7764522
 0.79014164 0.87633187 1.03974    1.3474438  1.5808976  1.3967366
 1.3112047  1.2411714  1.1043538  1.2010483  1.3210524  1.3561262
 1.3786465  1.3053957  1.0712451  0.7791457  0.57472676 0.61245954
 0.6671672  0.8130931  0.8838942  1.0680747  1.4684242  1.7458919
 1.855984   1.835924   1.6814675  1.4177631  1.5081319  1.6953806
 1.8391477  1.9196951  1.9415832  1.9632121  1.9816816  1.8580096
 1.7725998  1.8657427  1.8243378  1.6251551  1.2537487  0.88229656
 0.7126839  0.5398965  0.46766293 0.53231686 0.422389   0.3852498
 0.4274474  0.54513276 0.44569713 0.23477827 0.40006992 0.80123425
 1.0651027  1.124599   0.8126812  0.7291437  0.8360523  0.84808797
 0.80397725 0.75582385 0.8384293  0.9512594  1.1357235  1.4056455
 1.6535822  1.4387788  1.3633308  1.2580537  1.1402396  1.127162
 1.2362515  1.2316241  1.3100394  1.2728466  1.1057737  0.83333457
 0.48149037 0.5009266  0.61006474 0.8053907  0.8851295  1.100034
 1.4687191  1.7125024  1.7435424  1.6415156  1.4285861  1.1693556
 1.4112132  1.6784953  1.7655061  1.8048309  1.7792752  1.7479857
 1.7314773  1.6374301  1.5807314  1.7001073 ]
