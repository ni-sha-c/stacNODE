time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.68%, model saved.
Epoch: 0 Train: 3935.97485 Test: 3813.04712
Epoch 80: New minimal relative error: 60.72%, model saved.
Epoch: 80 Train: 70.26631 Test: 92.16081
Epoch 160: New minimal relative error: 29.10%, model saved.
Epoch: 160 Train: 13.99798 Test: 20.38250
Epoch 240: New minimal relative error: 16.81%, model saved.
Epoch: 240 Train: 7.27545 Test: 11.01878
Epoch: 320 Train: 41.35852 Test: 46.34412
Epoch: 400 Train: 14.22591 Test: 17.71531
Epoch: 480 Train: 3.89880 Test: 6.31056
Epoch: 560 Train: 2.87303 Test: 4.29740
Epoch: 640 Train: 4.47041 Test: 5.66809
Epoch: 720 Train: 8.25261 Test: 9.77055
Epoch: 800 Train: 1.98251 Test: 3.19605
Epoch: 880 Train: 2.01650 Test: 3.20365
Epoch 960: New minimal relative error: 15.27%, model saved.
Epoch: 960 Train: 1.79355 Test: 2.81987
Epoch 1040: New minimal relative error: 11.17%, model saved.
Epoch: 1040 Train: 1.43849 Test: 2.67249
Epoch: 1120 Train: 3.34563 Test: 6.12145
Epoch: 1200 Train: 10.04325 Test: 3.22424
Epoch: 1280 Train: 12.85643 Test: 8.65676
Epoch: 1360 Train: 0.91092 Test: 1.70902
Epoch: 1440 Train: 0.88370 Test: 1.59126
Epoch: 1520 Train: 3.34822 Test: 2.27811
Epoch: 1600 Train: 0.67852 Test: 1.36863
Epoch: 1680 Train: 0.88353 Test: 1.49095
Epoch: 1760 Train: 0.59565 Test: 1.20349
Epoch: 1840 Train: 1.07487 Test: 1.58063
Epoch: 1920 Train: 0.64634 Test: 1.23180
Epoch: 2000 Train: 0.50439 Test: 1.06520
Epoch: 2080 Train: 0.63534 Test: 1.25079
Epoch 2160: New minimal relative error: 8.06%, model saved.
Epoch: 2160 Train: 0.56922 Test: 1.15430
Epoch: 2240 Train: 1.61011 Test: 2.59607
Epoch: 2320 Train: 3.19958 Test: 3.95057
Epoch: 2400 Train: 0.69399 Test: 1.05940
Epoch: 2480 Train: 0.32335 Test: 0.77793
Epoch: 2560 Train: 0.51609 Test: 0.99993
Epoch 2640: New minimal relative error: 7.58%, model saved.
Epoch: 2640 Train: 0.28855 Test: 0.72227
Epoch: 2720 Train: 0.27687 Test: 0.65896
Epoch: 2800 Train: 2.08452 Test: 2.69046
Epoch: 2880 Train: 2.24577 Test: 3.55419
Epoch: 2960 Train: 0.32194 Test: 0.75211
Epoch: 3040 Train: 1.71151 Test: 2.19459
Epoch: 3120 Train: 0.93370 Test: 1.54894
Epoch: 3200 Train: 1.13532 Test: 1.32432
Epoch: 3280 Train: 0.21712 Test: 0.52076
Epoch: 3360 Train: 0.33820 Test: 0.66215
Epoch: 3440 Train: 0.38739 Test: 0.68937
Epoch: 3520 Train: 0.62958 Test: 0.85857
Epoch: 3600 Train: 0.33020 Test: 0.59669
Epoch: 3680 Train: 0.67203 Test: 1.00855
Epoch: 3760 Train: 0.26474 Test: 0.56109
Epoch: 3840 Train: 0.18402 Test: 0.48423
Epoch 3920: New minimal relative error: 7.45%, model saved.
Epoch: 3920 Train: 0.17818 Test: 0.42709
Epoch: 4000 Train: 0.15251 Test: 0.43540
Epoch: 4080 Train: 0.16760 Test: 0.41619
Epoch: 4160 Train: 0.13581 Test: 0.38135
Epoch: 4240 Train: 0.28946 Test: 0.49680
Epoch: 4320 Train: 0.44929 Test: 0.73258
Epoch 4400: New minimal relative error: 5.44%, model saved.
Epoch: 4400 Train: 0.29353 Test: 0.50970
Epoch: 4480 Train: 1.79808 Test: 2.46531
Epoch: 4560 Train: 0.36971 Test: 0.74489
Epoch: 4640 Train: 0.98651 Test: 1.45923
Epoch: 4720 Train: 0.17841 Test: 0.37502
Epoch: 4800 Train: 0.16744 Test: 0.35034
Epoch: 4880 Train: 0.18282 Test: 0.39203
Epoch: 4960 Train: 0.11873 Test: 0.30679
Epoch: 5040 Train: 0.90232 Test: 0.88902
Epoch: 5120 Train: 0.63049 Test: 0.95142
Epoch: 5200 Train: 0.43381 Test: 0.61253
Epoch: 5280 Train: 0.46751 Test: 0.64027
Epoch: 5360 Train: 0.13456 Test: 0.36336
Epoch: 5440 Train: 0.17280 Test: 0.39465
Epoch 5520: New minimal relative error: 5.01%, model saved.
Epoch: 5520 Train: 0.23078 Test: 0.37702
Epoch: 5600 Train: 0.12687 Test: 0.28175
Epoch 5680: New minimal relative error: 4.38%, model saved.
Epoch: 5680 Train: 0.08597 Test: 0.25768
Epoch: 5760 Train: 0.12476 Test: 0.29499
Epoch: 5840 Train: 0.14572 Test: 0.28196
Epoch: 5920 Train: 0.08678 Test: 0.25375
Epoch: 6000 Train: 0.08168 Test: 0.25339
Epoch: 6080 Train: 0.07515 Test: 0.23497
Epoch: 6160 Train: 0.14354 Test: 0.36955
Epoch: 6240 Train: 0.12230 Test: 0.25296
Epoch: 6320 Train: 0.12419 Test: 0.31310
Epoch: 6400 Train: 0.10524 Test: 0.27874
Epoch: 6480 Train: 0.19944 Test: 0.39581
Epoch: 6560 Train: 0.16894 Test: 0.27397
Epoch: 6640 Train: 0.53521 Test: 0.74633
Epoch: 6720 Train: 0.08382 Test: 0.22107
Epoch: 6800 Train: 0.06932 Test: 0.20555
Epoch: 6880 Train: 0.06562 Test: 0.20228
Epoch: 6960 Train: 0.06693 Test: 0.20726
Epoch: 7040 Train: 0.68462 Test: 0.94988
Epoch: 7120 Train: 0.06662 Test: 0.20145
Epoch: 7200 Train: 0.06355 Test: 0.20320
Epoch: 7280 Train: 0.05981 Test: 0.19247
Epoch: 7360 Train: 0.06517 Test: 0.20038
Epoch: 7440 Train: 0.12528 Test: 0.22177
Epoch: 7520 Train: 0.06334 Test: 0.19664
Epoch: 7600 Train: 0.06952 Test: 0.19588
Epoch: 7680 Train: 0.06937 Test: 0.18688
Epoch: 7760 Train: 0.05432 Test: 0.17838
Epoch: 7840 Train: 0.05537 Test: 0.18723
Epoch: 7920 Train: 0.05297 Test: 0.17520
Epoch: 7999 Train: 0.05641 Test: 0.18704
Training Loss: tensor(0.0564)
Test Loss: tensor(0.1870)
Learned LE: [ 0.79716676  0.04201152 -3.512944  ]
True LE: [ 8.5901374e-01  3.1910636e-03 -1.4533745e+01]
Relative Error: [14.341505  14.888681  15.384872  15.764245  15.923595  15.905868
 15.697995  15.220698  14.543918  13.892367  13.285692  12.735352
 12.10735   11.501359  10.965551  10.479207  10.054254   9.685834
  9.341931   9.135191   9.043112   9.017681   9.121616   9.346613
  9.600941   9.839151   9.984968  10.159617  10.289397  10.567626
 10.947154  11.291943  11.46697   11.683155  11.851335  12.366417
 12.810702  13.150406  13.292798  13.211246  13.08229   12.847071
 12.579825  12.39283   12.2009325 11.90835   11.540623  11.220772
 10.986551  10.811674  10.656546  10.591368  10.612903  10.708828
 10.913432  11.191506  11.522808  11.848022  12.2377825 12.651674
 12.991624  13.454434  13.860217  14.356158  14.786636  15.069166
 15.198262  15.132024  14.861157  14.276087  13.560168  12.950137
 12.384729  11.804943  11.150852  10.572946  10.059639   9.56787
  9.123138   8.747381   8.486295   8.356974   8.292101   8.243612
  8.354329   8.565358   8.766207   8.8379755  9.004312   9.203044
  9.481435   9.856849  10.2417965 10.586745  10.750367  10.975659
 11.334747  11.815806  12.291071  12.563578  12.566084  12.452979
 12.287123  11.930242  11.712403  11.60365   11.49891   11.1747465
 10.812231  10.518414  10.32803   10.164722  10.079341  10.0879545
 10.183243  10.350978  10.546906  10.780971  11.040308  11.36525
 11.741795  12.124867  12.473806  13.013101  13.441779  13.864115
 14.225192  14.415282  14.484069  14.385674  14.026494  13.352119
 12.650846  12.076809  11.526518  10.870931  10.251227   9.70643
  9.090252   8.595077   8.219121   7.920109   7.7170925  7.5839148
  7.511787   7.5451045  7.646712   7.6919737  7.7022195  7.750262
  7.9071217  8.146908   8.592751   9.054562   9.463512   9.8546295
 10.077979  10.346904  10.846438  11.280294  11.793316  11.947985
 11.8644905 11.700752  11.406921  11.078865  10.941517  10.930433
 10.813659  10.472952  10.109434   9.870564   9.722851   9.618352
  9.605174   9.666189   9.796238   9.971194  10.152228  10.262911
 10.526312  10.894062  11.2792225 11.637152  11.959513  12.479713
 12.983577  13.403175  13.696182  13.795139  13.805585  13.657067
 13.213704  12.486834  11.804099  11.246199  10.623726   9.990142
  9.353531   8.703237   8.169651   7.7684813  7.475066   7.239706
  7.0561624  6.9092474  6.835831   6.784981   6.7863398  6.755267
  6.685077   6.700656   6.840822   7.1260967  7.7159486  8.335657
  8.710243   9.116055   9.429923   9.85039   10.372831  10.828586
 11.296241  11.347325  11.185033  10.9591465 10.559582  10.303332
 10.252495  10.29635   10.21047    9.768695   9.446769   9.293329
  9.1852865  9.133183   9.162354   9.246034   9.396033   9.5338545
  9.677518   9.743899   9.915866  10.285137  10.709802  11.100661
 11.421217  11.943034  12.452171  12.837052  13.119085  13.152708
 13.118683  12.9474    12.444109  11.681949  10.997802  10.432879
  9.770772   9.102879   8.413102   7.8345075  7.376709   7.048077
  6.794311   6.596903   6.4238973  6.2822447  6.1481056  6.0279155
  5.94566    5.8208284  5.7704186  5.781791   5.882675   6.2099004
  6.895631   7.577611   8.025146   8.422148   8.836295   9.397621
  9.924303  10.425728  10.759702  10.704658  10.501316  10.16283
  9.777504   9.596973   9.6269     9.748416   9.501786   9.108855
  8.882516   8.755441   8.637268   8.644027   8.714614   8.839909
  8.917912   8.921915   8.952388   9.036298   9.15344    9.530415
  9.966583  10.383312  10.740914  11.287084  11.804361  12.235923
 12.497346  12.543532  12.458458  12.204889  11.68921   10.90872
 10.230335   9.591794   8.888729   8.162782   7.547372   7.041
  6.6582494  6.3995137  6.196015   6.011921   5.849759   5.67039
  5.4853015  5.302242   5.1757126  5.054345   5.0149093  5.003654
  5.091487   5.460118   6.1633573  6.839954   7.3431134  7.7857122
  8.2349     8.948426   9.559078  10.049917  10.200121  10.079667
  9.813174   9.385913   9.061356   8.975654   9.064226   9.109069
  8.825776   8.535859   8.33954    8.160816   8.128004   8.184262
  8.306124   8.293048   8.1669855  8.098085   8.112408   8.192649
  8.332827   8.768196   9.275475   9.657919   9.980605  10.525205
 11.204827  11.567027  11.89146   11.969456  11.842635  11.52662
 10.911848  10.1546     9.46555    8.754821   7.9989777  7.3457413
  6.767677   6.313219   6.0254645  5.8373156  5.6666884  5.497505
  5.3023524  5.1035795  4.881484   4.697429   4.567713   4.444261
  4.402066   4.3679695  4.4297404  4.8240905]
