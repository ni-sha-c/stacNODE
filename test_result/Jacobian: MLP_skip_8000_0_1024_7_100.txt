time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.56%, model saved.
Epoch: 0 Train: 9381.77441 Test: 4375.33984
Epoch: 80 Train: 2653.70410 Test: 1226.82812
Epoch: 160 Train: 2518.02002 Test: 1199.73254
Epoch: 240 Train: 2682.02588 Test: 1188.15649
Epoch: 320 Train: 1725.04126 Test: 670.76550
Epoch 400: New minimal relative error: 75.42%, model saved.
Epoch: 400 Train: 898.35938 Test: 228.20607
Epoch: 480 Train: 425.60419 Test: 86.45133
Epoch 560: New minimal relative error: 54.27%, model saved.
Epoch: 560 Train: 292.19025 Test: 47.44005
Epoch: 640 Train: 160.82120 Test: 90.51962
Epoch 720: New minimal relative error: 9.68%, model saved.
Epoch: 720 Train: 100.40028 Test: 10.29765
Epoch: 800 Train: 72.11372 Test: 14.84097
Epoch: 880 Train: 60.27397 Test: 18.94460
Epoch: 960 Train: 50.55376 Test: 18.60101
Epoch: 1040 Train: 36.00563 Test: 0.85172
Epoch 1120: New minimal relative error: 8.48%, model saved.
Epoch: 1120 Train: 33.48086 Test: 3.45799
Epoch: 1200 Train: 44.27728 Test: 16.31102
Epoch: 1280 Train: 25.74745 Test: 0.75137
Epoch 1360: New minimal relative error: 4.58%, model saved.
Epoch: 1360 Train: 47.24636 Test: 23.52487
Epoch: 1440 Train: 27.95260 Test: 4.94116
Epoch: 1520 Train: 26.88119 Test: 4.79056
Epoch: 1600 Train: 24.57120 Test: 3.10962
Epoch: 1680 Train: 22.98443 Test: 3.07243
Epoch: 1760 Train: 21.96804 Test: 3.74587
Epoch: 1840 Train: 20.61836 Test: 5.20594
Epoch: 1920 Train: 15.39647 Test: 2.21984
Epoch: 2000 Train: 24.91638 Test: 8.41260
Epoch 2080: New minimal relative error: 1.82%, model saved.
Epoch: 2080 Train: 14.25533 Test: 0.66305
Epoch: 2160 Train: 17.07047 Test: 4.13861
Epoch: 2240 Train: 14.81327 Test: 2.06509
Epoch: 2320 Train: 19.28689 Test: 6.47026
Epoch: 2400 Train: 17.94644 Test: 6.33449
Epoch: 2480 Train: 12.94387 Test: 3.63173
Epoch: 2560 Train: 10.37228 Test: 0.57257
Epoch: 2640 Train: 11.34862 Test: 0.38507
Epoch: 2720 Train: 14.06327 Test: 3.49125
Epoch: 2800 Train: 21.57346 Test: 7.99322
Epoch: 2880 Train: 11.64618 Test: 0.63099
Epoch: 2960 Train: 17.15108 Test: 5.90901
Epoch: 3040 Train: 9.82088 Test: 0.88800
Epoch: 3120 Train: 15.01070 Test: 4.85732
Epoch: 3200 Train: 8.98796 Test: 1.09684
Epoch: 3280 Train: 14.02839 Test: 6.33835
Epoch: 3360 Train: 9.48426 Test: 2.39817
Epoch: 3440 Train: 12.88022 Test: 6.18038
Epoch: 3520 Train: 12.79756 Test: 5.17447
Epoch: 3600 Train: 8.90349 Test: 2.46910
Epoch: 3680 Train: 13.23533 Test: 4.88748
Epoch: 3760 Train: 7.39575 Test: 1.63173
Epoch: 3840 Train: 8.52895 Test: 0.81022
Epoch: 3920 Train: 10.17801 Test: 2.26521
Epoch: 4000 Train: 7.24806 Test: 1.02493
Epoch: 4080 Train: 7.38670 Test: 1.09503
Epoch: 4160 Train: 7.18189 Test: 0.18517
Epoch 4240: New minimal relative error: 0.92%, model saved.
Epoch: 4240 Train: 6.35569 Test: 0.06614
Epoch: 4320 Train: 11.03327 Test: 4.08301
Epoch: 4400 Train: 10.87912 Test: 3.73018
Epoch: 4480 Train: 9.63161 Test: 2.83042
Epoch: 4560 Train: 12.49409 Test: 6.61599
Epoch: 4640 Train: 10.89235 Test: 5.19092
Epoch: 4720 Train: 9.79164 Test: 2.91137
Epoch: 4800 Train: 8.15781 Test: 1.06457
Epoch: 4880 Train: 11.97417 Test: 8.27177
Epoch: 4960 Train: 6.52440 Test: 1.08355
Epoch: 5040 Train: 6.59037 Test: 0.42382
Epoch: 5120 Train: 5.58233 Test: 0.11414
Epoch: 5200 Train: 5.68377 Test: 0.10112
Epoch: 5280 Train: 7.32561 Test: 0.97580
Epoch: 5360 Train: 5.75071 Test: 0.26157
Epoch: 5440 Train: 5.56815 Test: 0.40662
Epoch: 5520 Train: 4.96842 Test: 0.07395
Epoch: 5600 Train: 5.87618 Test: 0.31703
Epoch: 5680 Train: 5.61485 Test: 1.02329
Epoch: 5760 Train: 5.42063 Test: 0.50814
Epoch: 5840 Train: 5.80785 Test: 0.32339
Epoch: 5920 Train: 4.93578 Test: 0.12741
Epoch: 6000 Train: 5.15775 Test: 0.45615
Epoch: 6080 Train: 4.60879 Test: 0.10821
Epoch: 6160 Train: 4.60062 Test: 0.03410
Epoch: 6240 Train: 4.71633 Test: 0.38603
Epoch: 6320 Train: 4.69216 Test: 0.27740
Epoch 6400: New minimal relative error: 0.59%, model saved.
Epoch: 6400 Train: 4.21432 Test: 0.02966
Epoch: 6480 Train: 4.10140 Test: 0.13649
Epoch: 6560 Train: 3.95186 Test: 0.10761
Epoch: 6640 Train: 6.30114 Test: 1.14023
Epoch: 6720 Train: 6.31018 Test: 1.39465
Epoch: 6800 Train: 4.47075 Test: 0.52085
Epoch: 6880 Train: 5.78630 Test: 1.08193
Epoch: 6960 Train: 3.67931 Test: 0.08570
Epoch: 7040 Train: 3.52608 Test: 0.05273
Epoch: 7120 Train: 3.43514 Test: 0.04089
Epoch: 7200 Train: 3.87284 Test: 0.26006
Epoch: 7280 Train: 3.70919 Test: 0.14034
Epoch: 7360 Train: 4.94105 Test: 0.75455
Epoch: 7440 Train: 3.48839 Test: 0.10658
Epoch: 7520 Train: 5.79947 Test: 1.14780
Epoch: 7600 Train: 3.29598 Test: 0.07666
Epoch: 7680 Train: 3.25433 Test: 0.16281
Epoch: 7760 Train: 3.16107 Test: 0.08064
Epoch: 7840 Train: 3.44686 Test: 0.19136
Epoch: 7920 Train: 3.34996 Test: 0.04558
Epoch: 7999 Train: 3.81487 Test: 0.15635
Training Loss: tensor(3.8149)
Test Loss: tensor(0.1563)
Learned LE: [  0.810415     0.03170023 -14.520047  ]
True LE: [ 8.4911156e-01 -2.8352875e-03 -1.4525710e+01]
Relative Error: [4.2137895  4.2300196  4.567583   4.8535986  4.6248426  4.1381035
 3.6562815  3.030903   1.7913606  0.891656   0.29922238 0.5609305
 1.2228241  1.8238432  2.3753302  2.864058   3.0311613  2.9302428
 2.5713093  1.9024273  1.193514   1.3733563  1.7066119  2.0543127
 2.3449469  2.6678586  3.197225   3.2703362  2.8577044  2.320064
 2.0500078  1.9950575  2.153724   2.3996413  2.4125896  2.2457447
 2.13999    2.1937     2.1695168  2.1979809  2.184718   2.3255932
 2.2243736  1.7798761  1.5260358  1.5622762  1.4112375  1.1729842
 0.8908209  0.77956283 1.3770347  2.097878   3.0428486  4.0288663
 4.3915396  4.765969   5.255783   5.6417575  5.7234635  5.4301577
 4.807389   4.209376   3.7874095  3.6521957  4.0562873  4.545785
 4.49772    3.9596152  3.3930752  2.7587955  1.8146502  0.78486735
 0.315776   0.45653418 1.0814495  1.8384844  2.3468244  2.9611702
 3.2873273  3.2984586  2.9210627  2.3146324  1.4626151  1.0301337
 1.2977605  1.6280257  1.9272095  2.2691746  2.86471    3.1752925
 2.9363153  2.2417135  1.8392768  1.7056313  1.9107242  2.2104437
 2.2175016  1.9819889  1.9504713  2.1070313  2.1619954  2.171511
 2.214796   2.312133   2.2772508  2.024404   1.5911095  1.5764736
 1.5214963  1.2676692  0.8316037  0.6857671  1.1845576  1.918406
 2.695187   3.4629996  3.7674472  3.8204365  4.2627535  4.778385
 4.9446282  4.776283   4.539921   3.9495118  3.4006536  3.1929493
 3.5951881  4.0291266  4.173775   3.8265758  3.2811701  2.645966
 1.7630371  0.86665684 0.34015316 0.5269854  1.0051348  1.7521938
 2.3586502  2.9044912  3.3632615  3.5117645  3.3360178  2.7435632
 1.8964185  1.0981725  0.9647399  1.2366978  1.4867474  1.8161112
 2.487619   2.9376743  2.8512206  2.370925   1.7700275  1.4172298
 1.5741478  1.9328992  2.0590153  1.8784449  1.6752415  1.9110417
 2.0571737  2.1442575  2.2867672  2.2749677  2.2145107  2.0757637
 1.6954968  1.5576544  1.5635034  1.3490576  0.94671696 0.70450425
 1.0972583  1.8957655  2.6490433  2.9983075  3.2470007  3.2436743
 3.251975   3.7172832  4.113982   4.1181383  3.9889317  3.7732263
 3.1723673  2.8113952  3.0918374  3.5466545  3.8285184  3.6019082
 3.211338   2.6975307  1.8142838  1.0673014  0.22586358 0.54100573
 0.8784922  1.5346668  2.1844597  2.6910393  3.2113776  3.4589448
 3.4673295  3.1074963  2.339996   1.52329    1.0347854  0.8667878
 1.1201355  1.3331004  1.9696463  2.6155877  2.9066243  2.5293474
 1.8325487  1.2763993  1.290428   1.663428   1.8165582  1.7587754
 1.5346985  1.6576364  1.8311384  1.9664481  2.2155485  2.1828883
 2.0361547  1.8307574  1.6805573  1.362078   1.411988   1.3505996
 1.0613184  0.7558684  0.9431053  1.5603898  2.3354952  2.7417612
 2.7130647  2.6835089  2.6545386  2.5970874  3.1673782  3.5624127
 3.5268328  3.3913362  3.003264   2.5922682  2.5593956  2.9643254
 3.382724   3.393641   3.141075   2.7834315  2.090804   1.3434578
 0.6028967  0.30766517 0.8769352  1.1379607  1.7519071  2.310348
 2.8856869  3.2441385  3.3222091  3.1460583  2.6586325  1.8803654
 1.367075   0.9673142  0.7771073  0.9822137  1.294401   2.217279
 2.5236404  2.5212722  2.1652849  1.2961495  1.0456892  1.3143281
 1.6570238  1.6714818  1.2516804  1.4868926  1.5832391  1.7761859
 2.063737   2.044451   1.726301   1.5281001  1.4418846  1.2588409
 1.1754545  1.3121982  1.1624389  0.8607801  0.7351807  1.2195548
 1.8373903  2.2410572  2.2806396  2.0869486  2.0199463  2.0719495
 2.1423786  2.7895436  3.1100168  3.0117064  2.8323045  2.48865
 2.0791264  2.1744902  2.623286   2.8591015  2.7969797  2.6483228
 2.5095177  1.73252    1.0316613  0.4096914  0.5407172  0.9289278
 1.1400717  1.655913   2.2092872  2.7645283  2.9793444  2.9427984
 2.7049649  2.1154852  1.4008856  1.2141685  1.0528939  0.7377027
 0.7965775  1.3057077  2.0082166  2.0095143  1.9610438  1.6408037
 0.927635   0.9448369  1.2695923  1.4699374  1.3084788  1.2737277
 1.3901227  1.4951265  1.7176039  1.7947041  1.4955065  1.2851741
 1.1657835  1.1604949  1.1166916  0.99665046 1.048755   0.93351287
 0.7098126  0.9369849  1.3972088  1.7753124  2.1271307  1.7844305
 1.6280326  1.4506664  1.345696   1.4351307  2.0305407  2.3194354
 2.2609937  2.338458   2.2093356  1.7123901  1.8230911  1.9548664
 1.9827836  2.1912858  2.2027774  2.1640048  1.5372542  0.79847014
 0.43957728 0.5413447  0.6621803  0.7847557  1.2343571  1.7767704
 2.2024548  2.48275    2.4344332  2.1273403  1.5980175  1.1056906
 1.1622155  1.2838408  0.8989043  0.6618675 ]
