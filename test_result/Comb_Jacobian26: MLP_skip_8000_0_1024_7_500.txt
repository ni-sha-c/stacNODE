time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.77%, model saved.
Epoch: 0 Train: 31907.71875 Test: 4054.10767
Epoch: 80 Train: 8415.06250 Test: 1784.61304
Epoch 160: New minimal relative error: 101.50%, model saved.
Epoch: 160 Train: 7389.85547 Test: 1920.20508
Epoch: 240 Train: 6936.84033 Test: 1319.61230
Epoch 320: New minimal relative error: 71.32%, model saved.
Epoch: 320 Train: 6424.39648 Test: 1304.07983
Epoch: 400 Train: 6528.69189 Test: 1294.64490
Epoch: 480 Train: 5366.43652 Test: 806.61462
Epoch: 560 Train: 7088.68164 Test: 1467.04016
Epoch 640: New minimal relative error: 39.09%, model saved.
Epoch: 640 Train: 3991.28394 Test: 588.88922
Epoch: 720 Train: 2975.83276 Test: 278.28906
Epoch: 800 Train: 2446.53003 Test: 501.94424
Epoch: 880 Train: 912.49170 Test: 104.55970
Epoch 960: New minimal relative error: 20.51%, model saved.
Epoch: 960 Train: 343.66898 Test: 9.27228
Epoch 1040: New minimal relative error: 16.74%, model saved.
Epoch: 1040 Train: 242.64459 Test: 16.31581
Epoch 1120: New minimal relative error: 14.90%, model saved.
Epoch: 1120 Train: 184.98109 Test: 21.00121
Epoch: 1200 Train: 126.00089 Test: 3.62298
Epoch: 1280 Train: 123.40285 Test: 2.94446
Epoch 1360: New minimal relative error: 12.68%, model saved.
Epoch: 1360 Train: 104.68407 Test: 4.94621
Epoch: 1440 Train: 108.19949 Test: 10.37314
Epoch 1520: New minimal relative error: 5.60%, model saved.
Epoch: 1520 Train: 94.16061 Test: 1.51818
Epoch: 1600 Train: 92.72836 Test: 1.89960
Epoch: 1680 Train: 96.28359 Test: 7.37577
Epoch: 1760 Train: 137.97038 Test: 10.01468
Epoch: 1840 Train: 83.52962 Test: 2.48775
Epoch: 1920 Train: 81.05013 Test: 0.83051
Epoch: 2000 Train: 85.77569 Test: 2.84593
Epoch: 2080 Train: 73.10079 Test: 5.74781
Epoch: 2160 Train: 66.82887 Test: 0.69785
Epoch: 2240 Train: 55.87683 Test: 0.74946
Epoch: 2320 Train: 87.12150 Test: 4.22437
Epoch 2400: New minimal relative error: 2.14%, model saved.
Epoch: 2400 Train: 76.24197 Test: 0.84293
Epoch: 2480 Train: 62.85600 Test: 1.01512
Epoch: 2560 Train: 71.10667 Test: 2.68703
Epoch: 2640 Train: 59.24495 Test: 3.66587
Epoch: 2720 Train: 74.72038 Test: 7.27966
Epoch 2800: New minimal relative error: 1.41%, model saved.
Epoch: 2800 Train: 55.67892 Test: 0.63906
Epoch: 2880 Train: 52.47355 Test: 0.96295
Epoch: 2960 Train: 52.50474 Test: 0.77664
Epoch: 3040 Train: 50.94067 Test: 1.78762
Epoch: 3120 Train: 48.54854 Test: 2.39367
Epoch: 3200 Train: 41.28355 Test: 0.26908
Epoch: 3280 Train: 82.05593 Test: 8.24670
Epoch: 3360 Train: 42.00490 Test: 0.27563
Epoch: 3440 Train: 47.64277 Test: 1.57510
Epoch: 3520 Train: 43.23233 Test: 2.39440
Epoch: 3600 Train: 37.89562 Test: 0.24240
Epoch: 3680 Train: 37.68382 Test: 0.96553
Epoch: 3760 Train: 38.06776 Test: 1.55316
Epoch: 3840 Train: 41.33264 Test: 0.30110
Epoch: 3920 Train: 37.32793 Test: 0.63787
Epoch: 4000 Train: 36.60981 Test: 0.22785
Epoch: 4080 Train: 34.74091 Test: 0.21388
Epoch: 4160 Train: 38.84801 Test: 0.42587
Epoch: 4240 Train: 35.59993 Test: 2.16214
Epoch: 4320 Train: 45.54733 Test: 2.74363
Epoch: 4400 Train: 35.52655 Test: 0.64292
Epoch: 4480 Train: 32.52542 Test: 1.78573
Epoch: 4560 Train: 32.39275 Test: 0.16706
Epoch: 4640 Train: 32.24753 Test: 0.48153
Epoch: 4720 Train: 32.04239 Test: 0.18166
Epoch: 4800 Train: 33.40976 Test: 0.31430
Epoch: 4880 Train: 34.05317 Test: 1.14316
Epoch: 4960 Train: 31.22150 Test: 0.17724
Epoch: 5040 Train: 32.93399 Test: 2.01341
Epoch: 5120 Train: 28.12635 Test: 0.33611
Epoch: 5200 Train: 32.45322 Test: 1.11711
Epoch: 5280 Train: 30.59199 Test: 0.13274
Epoch: 5360 Train: 30.19375 Test: 0.93255
Epoch: 5440 Train: 28.59657 Test: 0.16762
Epoch: 5520 Train: 27.68756 Test: 0.20247
Epoch: 5600 Train: 30.69617 Test: 1.93195
Epoch: 5680 Train: 27.61007 Test: 0.78627
Epoch: 5760 Train: 24.94592 Test: 0.17976
Epoch: 5840 Train: 25.11115 Test: 0.16775
Epoch: 5920 Train: 29.56432 Test: 1.51342
Epoch 6000: New minimal relative error: 1.14%, model saved.
Epoch: 6000 Train: 28.43394 Test: 0.13804
Epoch: 6080 Train: 28.29645 Test: 0.26755
Epoch: 6160 Train: 27.49843 Test: 0.22463
Epoch: 6240 Train: 25.14090 Test: 0.29704
Epoch: 6320 Train: 24.42104 Test: 0.73192
Epoch: 6400 Train: 26.84072 Test: 0.14911
Epoch: 6480 Train: 28.08901 Test: 0.40552
Epoch 6560: New minimal relative error: 0.96%, model saved.
Epoch: 6560 Train: 25.08911 Test: 0.06281
Epoch: 6640 Train: 25.67020 Test: 0.20225
Epoch: 6720 Train: 27.73744 Test: 0.96693
Epoch: 6800 Train: 25.28850 Test: 0.05988
Epoch: 6880 Train: 23.63422 Test: 0.17247
Epoch: 6960 Train: 25.71652 Test: 0.28012
Epoch: 7040 Train: 26.34401 Test: 0.20635
Epoch: 7120 Train: 25.13057 Test: 0.20348
Epoch: 7200 Train: 26.81845 Test: 0.18524
Epoch: 7280 Train: 24.57935 Test: 0.07690
Epoch: 7360 Train: 23.73094 Test: 0.09420
Epoch 7440: New minimal relative error: 0.94%, model saved.
Epoch: 7440 Train: 23.67393 Test: 0.10045
Epoch: 7520 Train: 23.55301 Test: 0.11617
Epoch: 7600 Train: 24.21729 Test: 0.32225
Epoch: 7680 Train: 22.26991 Test: 0.05623
Epoch: 7760 Train: 23.24671 Test: 0.23912
Epoch: 7840 Train: 21.92358 Test: 0.35447
Epoch: 7920 Train: 24.43020 Test: 0.96497
Epoch: 7999 Train: 23.13832 Test: 0.06057
Training Loss: tensor(23.1383)
Test Loss: tensor(0.0606)
Learned LE: [ 8.7372118e-01 -6.9584306e-03 -1.4532627e+01]
True LE: [ 8.68660569e-01 -1.12245085e-04 -1.45381613e+01]
Relative Error: [0.94918936 0.95791316 0.8884142  0.85044056 0.6597758  0.5729414
 0.42436156 0.4300612  0.55622303 0.66247755 0.6277091  0.39339495
 0.3944382  0.20284985 0.1533317  0.14475176 0.12049011 0.2217961
 0.53042567 0.78973293 0.77852833 0.45639378 0.2797541  0.18698847
 0.3369508  0.20482254 0.26580393 0.27515084 0.25560945 0.43951082
 0.7294777  0.9860225  0.93650883 0.9818666  0.9072799  0.7169962
 0.49367374 0.40239048 0.18396567 0.20029698 0.16267458 0.22164036
 0.29870355 0.3476295  0.4213065  0.48581815 0.38213947 0.5238309
 0.5725744  0.72022647 0.8811377  1.0485332  1.1577783  0.958202
 0.7265375  0.60229725 0.4986676  0.44923303 0.5356025  0.60046434
 0.6662683  0.75780237 0.8982352  1.0299711  1.00439    0.784866
 0.8383837  0.6726894  0.6097215  0.5319606  0.4843822  0.6375163
 0.7477212  0.6887351  0.5134062  0.42171165 0.33946383 0.291948
 0.21179444 0.22228198 0.23925474 0.46265736 0.63113827 0.8161169
 0.52553445 0.34643477 0.30335653 0.33604413 0.17222638 0.15188165
 0.43701926 0.52625805 0.57267237 0.7634022  0.9463439  1.024287
 1.0763648  1.0159037  1.0755478  0.82351494 0.38486767 0.20815425
 0.10659552 0.09682509 0.14845262 0.263919   0.31847548 0.50133634
 0.553782   0.43423754 0.44665852 0.6067622  0.7146402  0.869036
 1.0117737  1.0465283  0.7991396  0.53570354 0.4947626  0.6206174
 0.6507372  0.6162516  0.59840864 0.71755147 0.8380426  0.9081405
 1.0196714  1.1686748  0.9902391  0.79620594 0.705909   0.5617532
 0.527627   0.50749767 0.6306754  0.6684443  0.7512915  0.6482453
 0.4306722  0.27264443 0.3041351  0.22694184 0.26430044 0.22592884
 0.27710947 0.5507492  0.70554954 0.6229506  0.4839801  0.41348547
 0.34242642 0.29101792 0.17607577 0.5191349  0.59427863 0.2704212
 0.3972435  0.65760237 0.70935565 0.9445823  1.0612048  1.0808419
 1.1614455  0.82292837 0.40170386 0.07223675 0.05353535 0.11339583
 0.13537556 0.3288108  0.49407476 0.68536794 0.50293344 0.47230408
 0.5241043  0.5720666  0.6972332  0.75581616 0.84253985 0.7752981
 0.4727034  0.33138394 0.4863556  0.59361976 0.64249873 0.6321425
 0.5637887  0.7343599  0.91239345 0.9414234  1.0398482  1.1054341
 1.0359997  0.9442846  0.8149742  0.6526157  0.5198957  0.54523486
 0.5445719  0.58080107 0.70485646 0.70485383 0.33807632 0.11987136
 0.12885734 0.12834537 0.22297755 0.09227415 0.23269114 0.332179
 0.6832291  0.53796524 0.44630548 0.4689806  0.36331558 0.2629994
 0.24567536 0.3210102  0.27546087 0.06481591 0.1719953  0.5534255
 0.54601455 0.63256395 0.8903906  1.0269803  1.0471985  0.8481223
 0.588169   0.24139202 0.07152665 0.2245131  0.10171106 0.4217479
 0.62300014 0.7087194  0.6464872  0.54827386 0.47426406 0.6095747
 0.59393585 0.644051   0.7120831  0.5241682  0.3681215  0.2804077
 0.39613593 0.52098644 0.5759007  0.55298024 0.48884395 0.54845536
 0.7522003  0.8767633  0.9108184  0.8579264  0.7441741  0.589077
 0.68385327 0.6955415  0.5995803  0.5611727  0.54743385 0.52836466
 0.510141   0.5011163  0.2466771  0.1445118  0.16501904 0.10570624
 0.10469279 0.04154661 0.21654822 0.19005635 0.33211637 0.49993944
 0.48517314 0.44002366 0.43872273 0.3837149  0.43894762 0.24836999
 0.23261301 0.07065465 0.0958555  0.19114019 0.43807676 0.50454664
 0.56330734 0.7454754  0.87019    0.8727448  0.6340332  0.5341948
 0.2564843  0.3117757  0.09569171 0.3299427  0.51931715 0.59047645
 0.67460555 0.5489805  0.38594237 0.48253685 0.5968504  0.74507076
 0.6867567  0.5134722  0.24757423 0.25142473 0.3471008  0.38908824
 0.42426088 0.4682253  0.533418   0.5509936  0.6228901  0.72072226
 0.7472216  0.69621557 0.6875788  0.6403381  0.5941594  0.64217055
 0.72734386 0.6913402  0.7690792  0.74678063 0.541698   0.40395418
 0.37892127 0.17554413 0.12694967 0.12844893 0.06227594 0.09625957
 0.1296165  0.14044254 0.11043184 0.31358457 0.4779968  0.4478573
 0.33496496 0.36625862 0.4517145  0.46999577 0.34054768 0.44070706
 0.17850916 0.15363365 0.34303266 0.3852152  0.5158319  0.47286284
 0.55649996 0.6054884  0.619074   0.6117217  0.5644205  0.37618443
 0.45061758 0.19498827 0.2107979  0.3814888  0.5094146  0.5077348
 0.31629157 0.3289292  0.52066827 0.71985364 0.8016583  0.71638703
 0.63693327 0.6317588  0.5384443  0.4855011  0.3468696  0.3228438
 0.38935298 0.44794893 0.58022344 0.5922545  0.4355598  0.45302105
 0.48018312 0.53626925 0.63352627 0.6199845  0.57283247 0.58004445
 0.7716393  0.9021475  0.8523627  0.7381196  0.72047496 0.52348197
 0.38045117 0.33875465 0.25327322 0.15415654]
