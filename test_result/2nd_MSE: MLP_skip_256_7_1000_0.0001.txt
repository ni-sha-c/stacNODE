time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 105.31%, model saved.
Epoch: 0 Train: 4483.67871 Test: 4087.88330
Epoch 100: New minimal relative error: 93.41%, model saved.
Epoch: 100 Train: 156.56819 Test: 165.06744
Epoch 200: New minimal relative error: 33.58%, model saved.
Epoch: 200 Train: 37.47162 Test: 59.13000
Epoch: 300 Train: 20.25271 Test: 26.37724
Epoch 400: New minimal relative error: 31.23%, model saved.
Epoch: 400 Train: 8.92485 Test: 15.14066
Epoch: 500 Train: 11.28732 Test: 12.56534
Epoch 600: New minimal relative error: 29.50%, model saved.
Epoch: 600 Train: 5.97832 Test: 12.37642
Epoch: 700 Train: 14.87722 Test: 14.35741
Epoch: 800 Train: 6.63168 Test: 10.00163
Epoch 900: New minimal relative error: 26.21%, model saved.
Epoch: 900 Train: 4.87261 Test: 8.64576
Epoch: 1000 Train: 4.88515 Test: 14.35905
Epoch 1100: New minimal relative error: 25.78%, model saved.
Epoch: 1100 Train: 6.37586 Test: 11.82476
Epoch: 1200 Train: 2.30836 Test: 5.24846
Epoch: 1300 Train: 3.53363 Test: 5.43297
Epoch: 1400 Train: 27.39141 Test: 20.65104
Epoch: 1500 Train: 1.23985 Test: 3.79207
Epoch: 1600 Train: 2.88544 Test: 4.80028
Epoch: 1700 Train: 4.52666 Test: 6.13100
Epoch: 1800 Train: 6.20367 Test: 7.28212
Epoch 1900: New minimal relative error: 19.84%, model saved.
Epoch: 1900 Train: 1.21718 Test: 3.40083
Epoch: 2000 Train: 1.63989 Test: 3.91702
Epoch: 2100 Train: 2.23490 Test: 4.47318
Epoch 2200: New minimal relative error: 18.83%, model saved.
Epoch: 2200 Train: 0.83243 Test: 2.60953
Epoch 2300: New minimal relative error: 13.66%, model saved.
Epoch: 2300 Train: 0.57609 Test: 2.28045
Epoch: 2400 Train: 0.57185 Test: 2.27604
Epoch 2500: New minimal relative error: 11.13%, model saved.
Epoch: 2500 Train: 1.79146 Test: 3.12162
Epoch: 2600 Train: 5.56015 Test: 4.64249
Epoch: 2700 Train: 0.72217 Test: 2.32131
Epoch: 2800 Train: 0.73551 Test: 2.25667
Epoch: 2900 Train: 1.15219 Test: 2.60308
Epoch: 3000 Train: 0.48169 Test: 1.85992
Epoch: 3100 Train: 3.97865 Test: 4.83914
Epoch: 3200 Train: 0.37026 Test: 1.80933
Epoch: 3300 Train: 0.36898 Test: 1.71557
Epoch: 3400 Train: 0.30534 Test: 1.65802
Epoch: 3500 Train: 1.11339 Test: 2.67722
Epoch: 3600 Train: 3.67034 Test: 5.57285
Epoch: 3700 Train: 0.47575 Test: 1.94435
Epoch: 3800 Train: 0.35729 Test: 1.53830
Epoch: 3900 Train: 1.82926 Test: 3.46659
Epoch: 4000 Train: 0.71485 Test: 2.65532
Epoch: 4100 Train: 0.26993 Test: 1.45975
Epoch: 4200 Train: 0.20608 Test: 1.38279
Epoch: 4300 Train: 0.20394 Test: 1.40363
Epoch: 4400 Train: 0.31150 Test: 1.46998
Epoch: 4500 Train: 0.21211 Test: 1.38506
Epoch: 4600 Train: 0.50981 Test: 1.61899
Epoch: 4700 Train: 3.00424 Test: 4.08518
Epoch: 4800 Train: 6.54264 Test: 5.95520
Epoch: 4900 Train: 2.30588 Test: 2.09207
Epoch: 5000 Train: 1.80764 Test: 2.58258
Epoch: 5100 Train: 0.49802 Test: 1.62722
Epoch: 5200 Train: 0.23190 Test: 1.29242
Epoch: 5300 Train: 0.35424 Test: 1.57707
Epoch: 5400 Train: 2.61427 Test: 3.91851
Epoch: 5500 Train: 0.24472 Test: 1.38177
Epoch: 5600 Train: 0.87085 Test: 1.72465
Epoch: 5700 Train: 1.58083 Test: 1.76420
Epoch: 5800 Train: 1.93324 Test: 2.23595
Epoch: 5900 Train: 0.12199 Test: 1.03179
Epoch: 6000 Train: 0.38183 Test: 1.34565
Epoch: 6100 Train: 0.55904 Test: 1.21423
Epoch: 6200 Train: 1.09428 Test: 1.98836
Epoch: 6300 Train: 0.11282 Test: 0.98371
Epoch: 6400 Train: 0.11494 Test: 1.00848
Epoch: 6500 Train: 0.11941 Test: 1.01424
Epoch 6600: New minimal relative error: 9.25%, model saved.
Epoch: 6600 Train: 0.11306 Test: 0.99284
Epoch: 6700 Train: 0.11446 Test: 0.99746
Epoch: 6800 Train: 0.11462 Test: 1.00931
Epoch: 6900 Train: 0.11366 Test: 0.96929
Epoch: 7000 Train: 0.13618 Test: 1.04408
Epoch: 7100 Train: 0.09717 Test: 0.91441
Epoch: 7200 Train: 0.18897 Test: 1.04350
Epoch: 7300 Train: 0.40341 Test: 1.09427
Epoch: 7400 Train: 0.09839 Test: 0.89598
Epoch: 7500 Train: 0.09258 Test: 0.90434
Epoch: 7600 Train: 0.13969 Test: 0.93640
Epoch: 7700 Train: 0.11171 Test: 0.90578
Epoch: 7800 Train: 0.08812 Test: 0.87616
Epoch: 7900 Train: 0.08895 Test: 0.89039
Epoch: 8000 Train: 0.24403 Test: 1.08524
Epoch: 8100 Train: 0.08984 Test: 0.90067
Epoch: 8200 Train: 0.10626 Test: 0.87207
Epoch: 8300 Train: 0.08237 Test: 0.85273
Epoch: 8400 Train: 0.24698 Test: 1.36945
Epoch: 8500 Train: 0.08014 Test: 0.83375
Epoch: 8600 Train: 0.07984 Test: 0.82787
Epoch: 8700 Train: 0.07828 Test: 0.83475
Epoch: 8800 Train: 0.36054 Test: 1.27845
Epoch: 8900 Train: 0.07570 Test: 0.80747
Epoch: 9000 Train: 1.16334 Test: 1.96220
Epoch: 9100 Train: 0.07394 Test: 0.79812
Epoch: 9200 Train: 0.07519 Test: 0.80354
Epoch: 9300 Train: 0.76110 Test: 1.50517
Epoch: 9400 Train: 0.31006 Test: 1.07862
Epoch: 9500 Train: 0.07072 Test: 0.77699
Epoch: 9600 Train: 0.07163 Test: 0.78288
Epoch 9700: New minimal relative error: 7.22%, model saved.
Epoch: 9700 Train: 0.24960 Test: 0.90282
Epoch: 9800 Train: 0.07582 Test: 0.77839
Epoch: 9900 Train: 0.08316 Test: 0.78127
Epoch: 9999 Train: 0.30238 Test: 0.98185
Training Loss: tensor(0.3024)
Test Loss: tensor(0.9819)
Learned LE: [ 0.7800721  -0.04389046 -3.1844842 ]
True LE: [ 8.6608213e-01 -1.2018776e-03 -1.4537391e+01]
Relative Error: [ 9.438741   9.346354   9.553355   9.816358   9.474841   9.109583
  8.988768   9.083037   9.224406   9.391121   9.410608   9.313027
  9.189209   8.818259   8.290045   7.321982   6.7488303  8.065001
  7.686382   6.6129327  5.5634356  4.826744   4.7091966  4.680345
  4.8648605  5.193888   5.558513   5.85293    6.3828735  7.1726346
  8.097504   9.079305  10.5065365 11.907153  12.667712  12.729542
 12.103849  11.481733  10.242527   8.546993   7.2649374  6.978707
  6.548356   5.709082   5.048507   4.9992266  5.441108   5.972703
  6.293682   6.479608   6.3722806  6.6118574  6.7992077  6.8706183
  6.996209   6.9486036  6.8898587  6.9122424  7.0689654  7.551225
  8.270829   8.792701   8.614848   8.484808   8.51346    8.683087
  8.736915   8.275462   7.996563   8.079073   8.278726   8.44115
  8.583856   8.531754   8.481917   8.045293   7.823559   6.628508
  6.718331   7.936442   7.6820793  6.7886376  5.698787   5.02022
  4.704689   4.5432634  4.5280404  4.748042   5.0324264  5.2874103
  5.6444774  6.2917166  7.172648   8.189886   9.439437  11.242498
 12.346696  12.845211  12.47895   11.86046   10.814778   8.984337
  7.2737823  6.7287045  6.171939   5.148478   4.476153   4.562347
  5.245485   6.0358324  6.491168   6.5698004  6.465679   6.729592
  6.8313756  6.8220797  6.8200936  6.689192   6.567578   6.604289
  6.715042   7.099615   7.7379885  7.9950676  7.827624   7.665262
  7.488455   7.6161265  7.8169465  7.613393   7.241223   7.102572
  7.2267265  7.453326   7.825369   7.912037   7.8879566  7.446836
  7.169232   6.1203265  6.3267074  7.706886   7.553845   7.192342
  6.2440653  5.198544   4.588217   4.252036   4.1417813  4.357753
  4.5833936  4.758784   4.9708204  5.2581043  5.9239316  6.8676324
  7.91451    9.369171  11.521008  12.376016  12.614921  12.09689
 11.367133   9.656117   7.6511087  6.6950603  6.003585   4.744367
  3.9776022  4.0973043  5.0419064  6.1297956  6.8171263  6.782288
  6.6757693  6.721446   6.8454595  6.7806506  6.700711   6.450829
  6.210145   6.123086   6.378689   6.6784806  7.1963377  7.252985
  6.995478   6.815126   6.7284904  6.672229   6.842158   7.057637
  6.6765623  6.3440175  6.3230095  6.493077   6.8004155  7.428687
  7.3568     7.0140266  6.61473    5.733753   5.6906657  6.921625
  7.02888    7.0265555  6.4629426  5.280471   4.5843353  4.0436134
  3.8896832  3.9978285  4.3513913  4.5523996  4.5776596  4.645025
  4.8918157  5.587616   6.4579496  7.4244657  8.965848  10.933197
 12.055657  12.158775  11.634071  10.397882   8.218448   6.695427
  5.9813933  4.6249723  3.5465226  3.6321514  4.704634   6.1380277
  7.134361   7.2171607  6.9633474  6.854036   7.0167346  6.945842
  6.783096   6.437815   6.060141   5.7551265  5.8961043  6.188693
  6.626019   6.5195866  6.2011557  6.023265   5.95809    5.956851
  5.9969673  6.1617513  6.426528   5.8699865  5.6252556  5.7018394
  5.908901   6.5805445  6.789654   6.7592597  6.244452   5.542688
  5.4051414  5.997126   6.449021   6.1312423  5.7304673  5.3895893
  4.83032    4.090361   3.6453586  3.6572113  4.0829787  4.444416
  4.5068717  4.417237   4.3043637  4.4917135  5.0870457  5.852924
  6.673129   8.289847  10.033531  11.316151  11.480003  10.682716
  9.157826   7.097577   5.9520855  4.798174   3.3238072  3.1059127
  4.194842   5.899267   7.3087564  7.641945   7.5458474  7.3093295
  7.258618   7.2181225  7.019904   6.6449428  6.178276   5.669903
  5.5141025  5.6527343  5.9721045  5.7544208  5.4730144  5.2830906
  5.242378   5.261602   5.291786   5.3760247  5.504035   5.7550464
  5.17952    5.068703   5.2352624  5.5818114  6.2452445  6.282935
  6.084777   5.557242   4.9704475  5.765612   5.8503065  5.4217086
  4.995548   4.8461795  4.6709447  4.1822767  3.5986297  3.6984835
  4.149845   4.5939183  4.4778647  4.441934   4.36449    4.2596354
  4.2512527  4.3867598  4.9253235  5.7926044  7.3303895  8.840147
 10.201967  10.860151   9.668331   7.862381   6.1464343  5.2338815
  3.5513704  2.6767442  3.527474   5.2857013  7.108793   7.9728374
  8.114238   7.8500996  7.304814   7.230457   7.164588   6.918319
  6.512139   5.8870687  5.356236   5.21503    5.332035   5.0840545
  4.734725   4.495986   4.3660855  4.3595424  4.43066    4.512261
  4.6453514  4.809028   5.1452823  4.57643    4.4389005  4.6292057
  5.515005   5.8306584  5.8393254  5.5550194  4.8976545  5.3874087
  6.2738776  4.862272   4.369505   4.209908   4.0401926  3.8701348
  3.7790477  3.9887726  4.3172674  4.796381 ]
