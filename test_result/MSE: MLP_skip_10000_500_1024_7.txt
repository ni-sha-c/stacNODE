time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.06%, model saved.
Epoch: 0 Train: 3845.39331 Test: 4157.61328
Epoch 100: New minimal relative error: 56.55%, model saved.
Epoch: 100 Train: 107.34005 Test: 85.99426
Epoch 200: New minimal relative error: 34.01%, model saved.
Epoch: 200 Train: 6.81548 Test: 8.32699
Epoch 300: New minimal relative error: 21.25%, model saved.
Epoch: 300 Train: 12.08255 Test: 4.21522
Epoch 400: New minimal relative error: 9.79%, model saved.
Epoch: 400 Train: 7.22176 Test: 11.10254
Epoch 500: New minimal relative error: 7.78%, model saved.
Epoch: 500 Train: 1.78168 Test: 2.62400
Epoch: 600 Train: 7.81812 Test: 10.95055
Epoch: 700 Train: 9.99528 Test: 8.76355
Epoch: 800 Train: 3.45052 Test: 4.80302
Epoch: 900 Train: 1.59933 Test: 1.19489
Epoch: 1000 Train: 1.16831 Test: 2.06485
Epoch: 1100 Train: 0.66362 Test: 1.70314
Epoch: 1200 Train: 2.78863 Test: 3.54951
Epoch: 1300 Train: 17.24152 Test: 7.24308
Epoch: 1400 Train: 0.26623 Test: 0.32452
Epoch: 1500 Train: 1.58734 Test: 2.69827
Epoch: 1600 Train: 8.44012 Test: 10.28312
Epoch: 1700 Train: 1.30909 Test: 1.68010
Epoch: 1800 Train: 1.60665 Test: 2.19216
Epoch: 1900 Train: 5.39880 Test: 7.25883
Epoch: 2000 Train: 3.00923 Test: 3.49795
Epoch: 2100 Train: 2.76967 Test: 3.71345
Epoch: 2200 Train: 2.56616 Test: 2.32178
Epoch: 2300 Train: 0.82081 Test: 1.06963
Epoch: 2400 Train: 1.70624 Test: 1.61984
Epoch: 2500 Train: 0.48774 Test: 0.44334
Epoch: 2600 Train: 6.67966 Test: 12.01910
Epoch 2700: New minimal relative error: 6.20%, model saved.
Epoch: 2700 Train: 0.18256 Test: 0.21553
Epoch: 2800 Train: 0.28834 Test: 0.58126
Epoch: 2900 Train: 0.54918 Test: 0.87724
Epoch: 3000 Train: 0.84515 Test: 0.80322
Epoch: 3100 Train: 0.28085 Test: 0.27682
Epoch 3200: New minimal relative error: 4.83%, model saved.
Epoch: 3200 Train: 0.73624 Test: 0.94993
Epoch: 3300 Train: 2.64387 Test: 3.52995
Epoch: 3400 Train: 1.13451 Test: 1.41206
Epoch: 3500 Train: 0.38318 Test: 0.52964
Epoch: 3600 Train: 0.30768 Test: 0.49575
Epoch: 3700 Train: 1.20653 Test: 1.14251
Epoch: 3800 Train: 0.45191 Test: 0.53768
Epoch: 3900 Train: 0.63125 Test: 0.66211
Epoch: 4000 Train: 0.09377 Test: 0.11446
Epoch: 4100 Train: 0.73323 Test: 0.95569
Epoch: 4200 Train: 0.20736 Test: 0.21202
Epoch: 4300 Train: 0.30404 Test: 0.38510
Epoch: 4400 Train: 3.27102 Test: 4.83497
Epoch: 4500 Train: 2.29040 Test: 2.66361
Epoch: 4600 Train: 1.10625 Test: 0.74692
Epoch: 4700 Train: 0.76079 Test: 0.86887
Epoch: 4800 Train: 0.60060 Test: 0.76783
Epoch: 4900 Train: 0.20810 Test: 0.18160
Epoch: 5000 Train: 0.83397 Test: 1.12221
Epoch: 5100 Train: 0.19774 Test: 0.23667
Epoch: 5200 Train: 0.36983 Test: 0.30524
Epoch: 5300 Train: 0.65173 Test: 0.66587
Epoch: 5400 Train: 0.14700 Test: 0.11360
Epoch: 5500 Train: 0.19311 Test: 0.23621
Epoch: 5600 Train: 1.78827 Test: 1.60544
Epoch: 5700 Train: 0.11363 Test: 0.11474
Epoch: 5800 Train: 0.26114 Test: 0.23294
Epoch: 5900 Train: 0.06958 Test: 0.07724
Epoch: 6000 Train: 0.15066 Test: 0.13775
Epoch: 6100 Train: 0.18500 Test: 0.21462
Epoch: 6200 Train: 1.59182 Test: 1.70772
Epoch: 6300 Train: 1.02293 Test: 1.09601
Epoch: 6400 Train: 0.42170 Test: 0.51170
Epoch: 6500 Train: 0.10424 Test: 0.16203
Epoch: 6600 Train: 0.05950 Test: 0.10000
Epoch: 6700 Train: 0.05810 Test: 0.06587
Epoch: 6800 Train: 0.76181 Test: 0.70497
Epoch: 6900 Train: 0.32589 Test: 0.17366
Epoch: 7000 Train: 0.04509 Test: 0.06904
Epoch: 7100 Train: 0.05051 Test: 0.07383
Epoch: 7200 Train: 0.03928 Test: 0.05267
Epoch: 7300 Train: 0.05531 Test: 0.07176
Epoch: 7400 Train: 0.02861 Test: 0.05054
Epoch: 7500 Train: 0.26390 Test: 0.23218
Epoch: 7600 Train: 0.09641 Test: 0.11034
Epoch: 7700 Train: 0.09793 Test: 0.15559
Epoch: 7800 Train: 0.13434 Test: 0.13451
Epoch: 7900 Train: 0.04326 Test: 0.06190
Epoch: 8000 Train: 0.21696 Test: 0.26423
Epoch: 8100 Train: 0.79012 Test: 0.88648
Epoch: 8200 Train: 0.44391 Test: 0.66827
Epoch: 8300 Train: 0.04167 Test: 0.06452
Epoch: 8400 Train: 0.02260 Test: 0.03967
Epoch: 8500 Train: 0.03527 Test: 0.04985
Epoch: 8600 Train: 0.07413 Test: 0.06523
Epoch: 8700 Train: 0.03258 Test: 0.04168
Epoch: 8800 Train: 0.04170 Test: 0.06907
Epoch: 8900 Train: 0.02258 Test: 0.04300
Epoch: 9000 Train: 0.04491 Test: 0.05661
Epoch: 9100 Train: 0.24430 Test: 0.31630
Epoch 9200: New minimal relative error: 3.37%, model saved.
Epoch: 9200 Train: 0.03090 Test: 0.04747
Epoch: 9300 Train: 0.06606 Test: 0.04187
Epoch: 9400 Train: 0.02049 Test: 0.03465
Epoch: 9500 Train: 0.32314 Test: 0.39256
Epoch: 9600 Train: 0.04349 Test: 0.08221
Epoch: 9700 Train: 0.01964 Test: 0.03379
Epoch: 9800 Train: 0.03470 Test: 0.04857
Epoch: 9900 Train: 0.06131 Test: 0.09074
Epoch: 9999 Train: 0.02240 Test: 0.04063
Training Loss: tensor(0.0224)
Test Loss: tensor(0.0406)
Learned LE: [ 0.8256409   0.05736325 -3.8895085 ]
True LE: [ 8.6386168e-01  1.0086795e-03 -1.4533549e+01]
Relative Error: [0.46368903 0.7179098  0.5360665  0.18635975 0.3192487  0.58957607
 0.7964695  0.7593693  0.9271577  0.77347165 0.27341658 0.28778496
 0.24505916 0.19817558 0.32725057 0.6598148  0.6453643  0.5118538
 0.4970352  0.56954724 0.58968157 0.16125919 0.2961458  0.5508291
 0.7248942  0.5197292  0.6796333  0.49687225 0.22141388 0.27335206
 0.22183019 0.15722485 0.36705914 0.27490067 0.20188436 0.2897788
 0.27208754 0.31101197 0.2954282  0.5086483  0.3408415  0.4158842
 0.28895313 0.18407129 0.23032592 0.1518029  0.15800965 0.3856719
 0.32278958 0.36410883 0.2362998  0.08258742 0.11766425 0.20400698
 0.3392406  0.18803044 0.12133684 0.2669165  0.39925146 0.52309006
 0.3050991  0.26880354 0.31148037 0.36599362 0.5652873  0.33483198
 0.2631936  0.47700006 0.7169884  0.44517508 0.40291646 0.5825444
 0.43558916 0.14293481 0.27850872 0.11324377 0.30947185 0.60329574
 0.8465557  0.54686266 0.43433294 0.48156047 0.4361229  0.25675127
 0.1442193  0.56952417 0.653594   0.2794987  0.42429185 0.4761541
 0.25841227 0.21700905 0.25374362 0.19263682 0.3811283  0.572498
 0.43060887 0.24136896 0.4558021  0.21505773 0.23688409 0.458002
 0.6371037  0.46009436 0.51581156 0.2212829  0.34384593 0.41461965
 0.2452903  0.31035313 0.25288907 0.27206498 0.18805341 0.11191875
 0.10691684 0.22628896 0.20864278 0.12901035 0.09591459 0.22665705
 0.26143685 0.31710404 0.3016711  0.20192939 0.31017357 0.20466992
 0.39170682 0.3469119  0.4001736  0.46772873 0.7660874  0.39219832
 0.3396222  0.41770083 0.43227887 0.3314437  0.4100733  0.46137166
 0.13562682 0.47106495 0.76579154 0.4685686  0.32925695 0.33794835
 0.43989038 0.39785448 0.33607948 0.44226208 0.516895   0.2790195
 0.17350845 0.34215382 0.38141528 0.2170133  0.09045707 0.30350962
 0.35139874 0.5235798  0.5266744  0.52145755 0.5154272  0.21382855
 0.37128323 0.49369243 0.33035454 0.51537764 0.4726053  0.49300975
 0.20440179 0.39938226 0.38079137 0.26059225 0.31651494 0.22551815
 0.27626848 0.22313884 0.18605709 0.19514039 0.45576814 0.3850668
 0.30131355 0.17483817 0.22504923 0.20847851 0.25584608 0.2516167
 0.21751723 0.33042946 0.26993346 0.24129798 0.3842495  0.48530036
 0.6101246  0.47930068 0.40272352 0.4795902  0.2998087  0.11551353
 0.38915282 0.5236575  0.3286936  0.4034477  0.6430096  0.38070357
 0.26642993 0.1814892  0.47610036 0.5960832  0.6019366  0.67079467
 0.48191148 0.37436736 0.25711745 0.26541504 0.48237085 0.44475338
 0.36126253 0.11871577 0.37702715 0.4128447  0.39222747 0.5242459
 0.53763074 0.38350007 0.10054395 0.5744862  0.3501576  0.21371494
 0.4491069  0.5092664  0.32653934 0.36724406 0.2233719  0.22127198
 0.16294551 0.4247384  0.24473622 0.17305355 0.2208273  0.17057526
 0.32030252 0.52092403 0.46798488 0.23129731 0.28848004 0.20444402
 0.1482119  0.27486846 0.28774944 0.12639143 0.24024767 0.37023038
 0.2681562  0.38019884 0.37308168 0.3551374  0.39087716 0.397432
 0.15072845 0.12519884 0.07655857 0.5689655  0.34393427 0.24861276
 0.54161304 0.6126952  0.47624776 0.31074566 0.2824733  0.359238
 0.5270905  0.65998816 0.6454075  0.36132875 0.291853   0.08796939
 0.09682734 0.3723098  0.59589875 0.6341518  0.33460325 0.3821838
 0.3706065  0.42748308 0.4927737  0.5210943  0.3173228  0.1912936
 0.61590034 0.2676695  0.29782072 0.2386746  0.59477204 0.31106448
 0.4549789  0.26770246 0.06329121 0.42927703 0.45670182 0.19464171
 0.20879403 0.26410297 0.24321784 0.29615822 0.3296514  0.3406526
 0.22823377 0.3195169  0.22050382 0.12512143 0.29422086 0.2189609
 0.06754982 0.20871463 0.33394083 0.35624006 0.3666031  0.191763
 0.3255531  0.43207282 0.24033281 0.28932554 0.11533463 0.17249173
 0.50469875 0.39406955 0.2835573  0.6946662  0.5665232  0.5526987
 0.46958098 0.34103182 0.33940774 0.33602864 0.5542814  0.29786023
 0.29400742 0.27112246 0.14066981 0.08115251 0.21077327 0.4741715
 0.6980938  0.6633279  0.41608405 0.3584099  0.49617806 0.47143906
 0.42994338 0.33647543 0.27183452 0.6288006  0.40820587 0.31001845
 0.27949417 0.64884734 0.15207069 0.46830574 0.2660127  0.18478964
 0.4763969  0.40627703 0.24066547 0.11190102 0.19479476 0.32346135
 0.20460442 0.08337872 0.13400324 0.2241376  0.3490282  0.26175874
 0.1032524  0.24858609 0.27989185 0.14534117 0.12914257 0.24873991
 0.4161253  0.26367712 0.11431531 0.35537305 0.37719527 0.08066124
 0.17981075 0.11431973 0.20566322 0.50943244 0.28763768 0.21796417
 0.347986   0.39390996 0.49097916 0.5681993  0.37329054 0.41872767
 0.3349743  0.50787663 0.2079094  0.35000557]
