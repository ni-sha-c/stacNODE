time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.03%, model saved.
Epoch: 0 Train: 3860.07983 Test: 4261.06592
Epoch 100: New minimal relative error: 33.84%, model saved.
Epoch: 100 Train: 49.54046 Test: 69.09487
Epoch 200: New minimal relative error: 31.60%, model saved.
Epoch: 200 Train: 8.96672 Test: 7.70752
Epoch 300: New minimal relative error: 19.59%, model saved.
Epoch: 300 Train: 7.74681 Test: 9.86720
Epoch: 400 Train: 39.49626 Test: 40.45139
Epoch: 500 Train: 8.17003 Test: 19.69664
Epoch: 600 Train: 17.87754 Test: 21.40271
Epoch: 700 Train: 19.46186 Test: 5.51651
Epoch: 800 Train: 2.00577 Test: 1.41732
Epoch 900: New minimal relative error: 13.03%, model saved.
Epoch: 900 Train: 0.92482 Test: 1.15595
Epoch 1000: New minimal relative error: 11.27%, model saved.
Epoch: 1000 Train: 3.69682 Test: 3.72250
Epoch: 1100 Train: 15.45892 Test: 20.31034
Epoch: 1200 Train: 2.11563 Test: 3.09311
Epoch: 1300 Train: 8.48323 Test: 4.98629
Epoch: 1400 Train: 0.88396 Test: 1.01616
Epoch: 1500 Train: 2.70092 Test: 2.56127
Epoch: 1600 Train: 2.59413 Test: 1.49285
Epoch: 1700 Train: 2.04424 Test: 1.71043
Epoch: 1800 Train: 3.49320 Test: 4.73183
Epoch 1900: New minimal relative error: 8.97%, model saved.
Epoch: 1900 Train: 1.21101 Test: 1.27037
Epoch: 2000 Train: 2.75312 Test: 2.43298
Epoch: 2100 Train: 0.27394 Test: 0.21626
Epoch: 2200 Train: 0.79531 Test: 1.46674
Epoch: 2300 Train: 0.99364 Test: 2.97268
Epoch: 2400 Train: 0.40323 Test: 0.33168
Epoch: 2500 Train: 0.49748 Test: 0.47869
Epoch: 2600 Train: 1.17425 Test: 1.79288
Epoch: 2700 Train: 1.80411 Test: 1.35774
Epoch: 2800 Train: 0.28760 Test: 0.44586
Epoch: 2900 Train: 0.65348 Test: 0.69633
Epoch 3000: New minimal relative error: 8.40%, model saved.
Epoch: 3000 Train: 0.32987 Test: 0.63288
Epoch: 3100 Train: 0.50516 Test: 0.50728
Epoch: 3200 Train: 0.18742 Test: 0.17311
Epoch: 3300 Train: 0.48170 Test: 0.68861
Epoch: 3400 Train: 0.84084 Test: 0.74737
Epoch: 3500 Train: 0.16539 Test: 0.16435
Epoch: 3600 Train: 1.67369 Test: 1.20951
Epoch: 3700 Train: 0.22886 Test: 0.24897
Epoch: 3800 Train: 2.59277 Test: 3.24215
Epoch: 3900 Train: 0.22737 Test: 0.36028
Epoch: 4000 Train: 0.36350 Test: 0.40622
Epoch: 4100 Train: 0.64446 Test: 0.68317
Epoch: 4200 Train: 0.78398 Test: 0.78760
Epoch: 4300 Train: 0.24104 Test: 0.22694
Epoch: 4400 Train: 0.21135 Test: 0.19778
Epoch 4500: New minimal relative error: 7.15%, model saved.
Epoch: 4500 Train: 1.14561 Test: 1.62699
Epoch: 4600 Train: 2.97332 Test: 1.72329
Epoch: 4700 Train: 0.61066 Test: 0.54493
Epoch: 4800 Train: 0.09470 Test: 0.09532
Epoch: 4900 Train: 1.23200 Test: 0.98863
Epoch: 5000 Train: 0.40742 Test: 0.61629
Epoch: 5100 Train: 0.34924 Test: 0.13940
Epoch: 5200 Train: 0.14806 Test: 0.19911
Epoch: 5300 Train: 0.18695 Test: 0.19725
Epoch: 5400 Train: 2.11526 Test: 2.57416
Epoch: 5500 Train: 0.84340 Test: 0.82147
Epoch: 5600 Train: 0.28598 Test: 0.25612
Epoch: 5700 Train: 0.75439 Test: 0.71971
Epoch: 5800 Train: 0.25707 Test: 0.20887
Epoch: 5900 Train: 0.97658 Test: 0.95843
Epoch: 6000 Train: 1.34789 Test: 1.59468
Epoch: 6100 Train: 0.13214 Test: 0.22011
Epoch: 6200 Train: 0.23216 Test: 0.26036
Epoch 6300: New minimal relative error: 6.48%, model saved.
Epoch: 6300 Train: 0.16116 Test: 0.19074
Epoch: 6400 Train: 0.16841 Test: 0.17032
Epoch: 6500 Train: 0.15585 Test: 0.12167
Epoch: 6600 Train: 1.49400 Test: 0.99056
Epoch: 6700 Train: 0.11024 Test: 0.13325
Epoch: 6800 Train: 0.07167 Test: 0.08409
Epoch: 6900 Train: 0.32346 Test: 0.42084
Epoch: 7000 Train: 0.26309 Test: 0.22960
Epoch: 7100 Train: 0.10809 Test: 0.13387
Epoch: 7200 Train: 0.24717 Test: 0.22198
Epoch: 7300 Train: 0.47702 Test: 0.47470
Epoch: 7400 Train: 0.11436 Test: 0.18100
Epoch: 7500 Train: 0.12003 Test: 0.10973
Epoch: 7600 Train: 0.04982 Test: 0.06471
Epoch: 7700 Train: 0.35640 Test: 0.43178
Epoch: 7800 Train: 0.10678 Test: 0.10560
Epoch: 7900 Train: 0.42771 Test: 0.39352
Epoch: 8000 Train: 0.10335 Test: 0.14588
Epoch: 8100 Train: 0.18155 Test: 0.20463
Epoch: 8200 Train: 0.04204 Test: 0.06648
Epoch: 8300 Train: 0.15661 Test: 0.14873
Epoch: 8400 Train: 0.22683 Test: 0.27558
Epoch: 8500 Train: 0.13625 Test: 0.17436
Epoch: 8600 Train: 0.07292 Test: 0.07635
Epoch: 8700 Train: 0.07637 Test: 0.09661
Epoch: 8800 Train: 0.07328 Test: 0.09103
Epoch: 8900 Train: 0.28844 Test: 0.26087
Epoch: 9000 Train: 0.09236 Test: 0.13540
Epoch 9100: New minimal relative error: 4.36%, model saved.
Epoch: 9100 Train: 0.02754 Test: 0.03853
Epoch: 9200 Train: 0.03843 Test: 0.04890
Epoch: 9300 Train: 0.11903 Test: 0.11516
Epoch: 9400 Train: 0.03672 Test: 0.06201
Epoch: 9500 Train: 0.07094 Test: 0.09615
Epoch: 9600 Train: 0.09423 Test: 0.12665
Epoch: 9700 Train: 0.09510 Test: 0.08298
Epoch: 9800 Train: 0.13212 Test: 0.17320
Epoch: 9900 Train: 0.07416 Test: 0.11185
Epoch: 9999 Train: 0.02663 Test: 0.03769
Training Loss: tensor(0.0266)
Test Loss: tensor(0.0377)
Learned LE: [ 0.7799552   0.03490911 -3.7037735 ]
True LE: [ 8.766197e-01 -5.679785e-03 -1.454551e+01]
Relative Error: [10.191024  10.445105  10.47893   10.460136  10.301683   9.746591
  8.747222   7.6296234  6.911863   6.3319554  5.8080287  5.3539677
  5.02152    4.902127   5.1563463  5.5379148  6.139283   6.844346
  7.651244   8.579082   9.65446   10.820473  12.036149  13.17513
 14.188239  15.138492  16.171316  17.027327  17.592121  17.997087
 18.381731  18.532618  18.483442  18.2503    17.868715  17.429253
 16.949106  16.363447  15.657205  14.992794  14.350362  13.714485
 13.10769   12.501885  11.842496  11.086001  10.404582   9.787026
  9.1071     8.4924     7.9194636  7.5119944  7.154355   6.9668627
  7.003486   7.0690913  7.220713   7.457291   7.7564044  8.218405
  8.778496   9.391754   9.848014   9.974222  10.020102   9.959024
  9.660047   8.69276    7.4679523  6.551323   5.9479747  5.4816303
  4.990867   4.5632424  4.2869782  4.30179    4.491289   4.979979
  5.678449   6.3838177  7.2132125  8.17096    9.291637  10.500836
 11.8040905 12.911277  13.905823  14.800251  15.847945  16.639774
 17.220354  17.640484  17.889013  18.021873  17.98974   17.72676
 17.31724   16.827436  16.299397  15.687591  14.963444  14.300045
 13.681584  13.084703  12.471507  11.839903  11.129915  10.377719
  9.71552    9.071389   8.393194   7.777949   7.211884   6.82799
  6.462595   6.341468   6.396083   6.4188943  6.5535574  6.8872237
  7.2779713  7.8668795  8.453363   9.108751   9.493213   9.569259
  9.598136   9.166772   8.379153   7.223046   6.100809   5.3500814
  4.7283745  4.3976583  4.2006626  3.8197353  3.6765597  3.6285985
  3.93429    4.456141   5.2802525  6.036815   6.8646936  7.8576217
  8.981308  10.204354  11.507598  12.612666  13.611153  14.590528
 15.463452  16.27517   16.851505  17.184881  17.36947   17.515806
 17.488794  17.168106  16.677406  16.179302  15.577775  14.943064
 14.244266  13.5895815 13.022966  12.422772  11.808959  11.148497
 10.423177   9.716273   9.073736   8.39505    7.685274   7.0917172
  6.587042   6.177762   5.7976623  5.749385   5.760194   5.754958
  5.9273934  6.373734   6.8870807  7.502962   8.182684   8.879336
  9.1197605  9.09727    8.603402   7.9841824  7.0049787  5.860095
  4.93974    4.2404904  3.6310725  3.3800836  3.1808083  3.2510765
  3.0087416  3.1198657  3.463222   4.090088   4.937164   5.687192
  6.564197   7.5732894  8.689552   9.9471    11.223652  12.285213
 13.363431  14.275554  15.091695  15.887501  16.438055  16.768883
 16.856512  17.03948   16.94423   16.63447   16.09166   15.544656
 14.859289  14.144048  13.492632  12.908743  12.350669  11.7391
 11.138784  10.482783   9.750154   9.090172   8.481257   7.767206
  7.04862    6.457937   5.9922485  5.5508943  5.194274   5.158096
  5.0661974  5.078698   5.3459134  5.8401575  6.5283165  7.2321544
  8.0056095  8.604416   8.627279   8.06038    7.581239   6.8079033
  5.7660794  4.703649   3.9292254  3.2172446  2.696494   2.422138
  2.4459584  2.480148   2.4379256  2.6301372  3.0864666  3.7671375
  4.6005383  5.3807893  6.2887697  7.293888   8.406879   9.677003
 10.900741  11.966045  13.128896  14.013943  14.770018  15.489362
 16.066517  16.343285  16.363194  16.497694  16.425194  15.994628
 15.42071   14.832287  14.114404  13.444851  12.785056  12.134802
 11.5758915 10.993997  10.419503   9.811299   9.117899   8.503057
  7.8919926  7.1723595  6.476067   5.8826265  5.4155846  4.9617395
  4.63968    4.5281677  4.3562813  4.44499    4.8312736  5.37109
  6.188071   7.001518   7.8028345  8.189903   7.642717   7.2009683
  6.5982556  5.6800976  4.63988    3.689963   2.9582658  2.3158007
  1.8862884  1.7347134  1.7701212  1.7920636  1.9580686  2.2059019
  2.7322154  3.4768667  4.297532   5.1756067  6.072631   7.085257
  8.225088   9.426516  10.568454  11.672911  12.822654  13.729158
 14.392445  15.112225  15.692912  15.952044  15.897852  16.04144
 15.88326   15.397808  14.784932  14.183357  13.473808  12.7784195
 12.060156  11.3145275 10.779475  10.209784   9.704531   9.139948
  8.513282   7.9537063  7.3219314  6.6047177  5.968223   5.3715787
  4.8490543  4.391749   4.0753775  3.8543012  3.6520123  3.8708444
  4.355605   5.0370135  5.8828006  6.810003   7.579524   7.1893034
  6.7420177  6.3615775  5.533894   4.557469   3.55037    2.7474551
  2.0806227  1.5564768  1.2654133  1.2802804  1.2703611  1.3804884
  1.5057697  1.8604707  2.4653778  3.2389755  4.14664    5.013678
  5.8558383  6.9056435  8.015095   9.125064  10.242527  11.430613
 12.515993  13.418074  14.016861  14.71534  ]
