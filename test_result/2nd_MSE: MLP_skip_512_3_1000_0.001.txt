time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 98.90%, model saved.
Epoch: 0 Train: 4176.19385 Test: 3512.22241
Epoch 100: New minimal relative error: 32.41%, model saved.
Epoch: 100 Train: 58.29786 Test: 78.13145
Epoch 200: New minimal relative error: 32.03%, model saved.
Epoch: 200 Train: 13.26673 Test: 23.66982
Epoch 300: New minimal relative error: 18.54%, model saved.
Epoch: 300 Train: 7.17120 Test: 14.28529
Epoch 400: New minimal relative error: 17.69%, model saved.
Epoch: 400 Train: 5.16741 Test: 11.54598
Epoch 500: New minimal relative error: 11.56%, model saved.
Epoch: 500 Train: 8.79230 Test: 10.82263
Epoch: 600 Train: 3.74966 Test: 9.28498
Epoch: 700 Train: 2.78687 Test: 7.22270
Epoch: 800 Train: 2.38944 Test: 6.42283
Epoch: 900 Train: 2.36781 Test: 6.21309
Epoch: 1000 Train: 2.70695 Test: 6.44599
Epoch: 1100 Train: 1.87972 Test: 5.02442
Epoch 1200: New minimal relative error: 7.34%, model saved.
Epoch: 1200 Train: 1.58070 Test: 4.68105
Epoch: 1300 Train: 1.39281 Test: 4.21667
Epoch: 1400 Train: 1.20949 Test: 3.94025
Epoch: 1500 Train: 1.26892 Test: 3.74423
Epoch: 1600 Train: 1.03732 Test: 3.92535
Epoch: 1700 Train: 2.01816 Test: 4.60218
Epoch: 1800 Train: 3.16292 Test: 5.61905
Epoch: 1900 Train: 3.48589 Test: 5.04388
Epoch: 2000 Train: 1.85838 Test: 3.88983
Epoch: 2100 Train: 1.08201 Test: 3.06376
Epoch: 2200 Train: 1.43545 Test: 3.38321
Epoch: 2300 Train: 0.57729 Test: 2.36631
Epoch: 2400 Train: 0.54975 Test: 2.36154
Epoch: 2500 Train: 0.66198 Test: 2.31602
Epoch: 2600 Train: 0.95539 Test: 2.45654
Epoch: 2700 Train: 0.77681 Test: 2.21503
Epoch: 2800 Train: 0.58041 Test: 1.98620
Epoch 2900: New minimal relative error: 6.73%, model saved.
Epoch: 2900 Train: 0.91443 Test: 2.02250
Epoch: 3000 Train: 0.63033 Test: 1.96215
Epoch: 3100 Train: 0.45996 Test: 1.74129
Epoch: 3200 Train: 0.51096 Test: 1.78097
Epoch: 3300 Train: 0.31112 Test: 1.55379
Epoch: 3400 Train: 0.54248 Test: 1.75175
Epoch: 3500 Train: 0.73784 Test: 1.78212
Epoch: 3600 Train: 0.79053 Test: 1.69623
Epoch: 3700 Train: 1.14004 Test: 2.30351
Epoch: 3800 Train: 1.60608 Test: 2.45371
Epoch: 3900 Train: 0.40704 Test: 1.44535
Epoch: 4000 Train: 0.55444 Test: 1.49552
Epoch: 4100 Train: 0.22931 Test: 1.20269
Epoch: 4200 Train: 0.24823 Test: 1.19283
Epoch: 4300 Train: 2.95711 Test: 3.24782
Epoch: 4400 Train: 0.25461 Test: 1.17945
Epoch: 4500 Train: 0.19795 Test: 1.06515
Epoch: 4600 Train: 0.41686 Test: 1.36343
Epoch: 4700 Train: 0.22600 Test: 1.18806
Epoch: 4800 Train: 0.30061 Test: 1.13418
Epoch: 4900 Train: 0.17327 Test: 0.97453
Epoch: 5000 Train: 0.86891 Test: 1.83404
Epoch: 5100 Train: 0.34224 Test: 1.04032
Epoch: 5200 Train: 0.15926 Test: 0.92827
Epoch: 5300 Train: 0.57148 Test: 1.20445
Epoch: 5400 Train: 0.17538 Test: 0.89018
Epoch: 5500 Train: 0.18465 Test: 0.91284
Epoch: 5600 Train: 0.14067 Test: 0.83845
Epoch: 5700 Train: 0.13768 Test: 0.83615
Epoch: 5800 Train: 0.13877 Test: 0.81061
Epoch: 5900 Train: 0.13471 Test: 0.79872
Epoch: 6000 Train: 0.12743 Test: 0.77967
Epoch: 6100 Train: 0.12859 Test: 0.77944
Epoch: 6200 Train: 0.12290 Test: 0.75593
Epoch: 6300 Train: 0.12877 Test: 0.74160
Epoch: 6400 Train: 0.18636 Test: 0.80370
Epoch: 6500 Train: 0.23265 Test: 0.81513
Epoch: 6600 Train: 0.11309 Test: 0.70640
Epoch: 6700 Train: 0.11277 Test: 0.69950
Epoch: 6800 Train: 0.27122 Test: 0.85496
Epoch: 6900 Train: 0.26158 Test: 0.81588
Epoch 7000: New minimal relative error: 5.57%, model saved.
Epoch: 7000 Train: 0.10513 Test: 0.66312
Epoch: 7100 Train: 0.10931 Test: 0.66438
Epoch: 7200 Train: 0.12418 Test: 0.66197
Epoch: 7300 Train: 0.12907 Test: 0.65391
Epoch: 7400 Train: 0.11367 Test: 0.66210
Epoch: 7500 Train: 0.09655 Test: 0.62175
Epoch: 7600 Train: 0.09572 Test: 0.61626
Epoch: 7700 Train: 0.10275 Test: 0.61085
Epoch: 7800 Train: 0.09272 Test: 0.60235
Epoch: 7900 Train: 0.11998 Test: 0.59505
Epoch: 8000 Train: 0.09497 Test: 0.58345
Epoch: 8100 Train: 0.11139 Test: 0.60743
Epoch: 8200 Train: 0.12138 Test: 0.59142
Epoch: 8300 Train: 0.10042 Test: 0.58900
Epoch: 8400 Train: 0.09545 Test: 0.57011
Epoch: 8500 Train: 0.08463 Test: 0.55484
Epoch: 8600 Train: 0.08228 Test: 0.54445
Epoch: 8700 Train: 0.08248 Test: 0.53268
Epoch: 8800 Train: 0.07989 Test: 0.53145
Epoch: 8900 Train: 0.07863 Test: 0.52403
Epoch: 9000 Train: 0.08088 Test: 0.52703
Epoch: 9100 Train: 0.07956 Test: 0.51888
Epoch: 9200 Train: 0.07571 Test: 0.50780
Epoch: 9300 Train: 0.12522 Test: 0.57536
Epoch: 9400 Train: 0.07410 Test: 0.49781
Epoch: 9500 Train: 0.07290 Test: 0.49362
Epoch: 9600 Train: 0.07471 Test: 0.49291
Epoch: 9700 Train: 0.21787 Test: 0.64917
Epoch: 9800 Train: 0.07038 Test: 0.48009
Epoch: 9900 Train: 0.14559 Test: 0.52431
Epoch: 9999 Train: 0.07066 Test: 0.48853
Training Loss: tensor(0.0707)
Test Loss: tensor(0.4885)
Learned LE: [ 0.834324   0.0282005 -3.9309103]
True LE: [ 8.6013180e-01  1.3138057e-03 -1.4541083e+01]
Relative Error: [7.7596297  8.662764   9.446828   9.781708   9.492523   8.957742
 8.252401   7.519383   6.664524   5.7296567  4.9203706  4.2994604
 4.052222   3.9576125  3.7511952  3.5863116  3.4830678  3.4226425
 3.4850292  3.5081923  3.449985   3.346097   3.3075159  3.3031728
 3.2773328  3.1892262  2.74421    2.2547016  2.0837977  2.1748827
 2.353976   2.6969633  2.7341506  2.5021966  2.1161926  1.7497338
 2.022594   2.6721196  3.5509586  4.184588   4.5747137  4.051962
 3.4134529  2.7802093  2.4431095  2.2277389  2.2256544  2.5848947
 2.683886   2.840241   3.113452   3.3970435  3.746809   3.9524133
 4.1982107  4.5683765  4.67854    4.763749   4.9016094  5.1231375
 5.392638   6.0095367  6.9326     7.6984534  8.471662   9.060003
 8.944046   8.437928   7.6881633  6.916772   6.034239   5.1317444
 4.414383   3.8214583  3.664003   3.5066571  3.264918   3.1344912
 3.1645565  3.132685   3.2068393  3.3133705  3.298449   3.206673
 3.1472697  3.0129852  2.9112952  2.819661   2.6645877  2.0694287
 1.6324062  1.6765188  1.9701732  2.266032   2.3638842  2.1468701
 1.793203   1.3732667  1.6965067  2.405243   3.2540045  3.943546
 4.429971   4.144356   3.3739316  2.6029177  2.1294835  1.7470319
 1.6894033  1.9200083  2.039489   2.2539825  2.5693421  2.8752801
 3.1728215  3.4186158  3.7310865  3.8527925  3.9649282  4.065174
 4.1557927  4.31224    4.5561967  4.9679794  5.728068   6.640491
 7.397563   8.0396805  8.370717   7.9822836  7.160049   6.24559
 5.4374857  4.6503773  3.9322839  3.4134474  3.2561622  3.02527
 2.8054042  2.832571   2.8271296  2.7467892  2.9166932  3.114794
 3.2449465  3.1877189  3.1423256  2.9346035  2.6554518  2.4633248
 2.2866838  1.9906144  1.3233428  1.1961244  1.4290153  1.7284516
 1.8875804  1.7688831  1.518925   1.0280979  1.2597888  1.8832183
 2.6864722  3.4778113  3.8501937  3.8005702  3.2999768  2.5185308
 1.8847042  1.3951176  1.2686151  1.3581083  1.5219238  1.7859036
 2.0635633  2.3621325  2.6499863  2.9527044  3.2136316  3.237819
 3.2721639  3.4744382  3.6025698  3.6779873  3.7880905  3.972872
 4.5714593  5.415263   6.240797   7.0532427  7.5616574  7.540863
 6.851836   5.7876854  4.844307   4.1119056  3.471083   3.0435998
 2.801865   2.5632641  2.4892025  2.5660398  2.384653   2.3027704
 2.4832957  2.7091122  2.8814647  2.9725604  3.1426792  2.9693966
 2.6236112  2.2352462  1.897798   1.6520362  1.2377021  0.7828072
 0.94140947 1.1802095  1.4310738  1.5455412  1.2575465  0.8924623
 0.8570198  1.2226527  2.0490048  2.8957474  3.3601177  3.3268824
 3.040911   2.3288634  1.8082222  1.2547569  0.8739995  0.8061802
 0.9970768  1.2423633  1.6033454  1.9255784  2.248335   2.5077417
 2.7930317  2.7689328  2.775672   2.9010112  3.124234   3.21454
 3.183649   3.2168422  3.6275342  4.2299333  5.1130657  5.855437
 6.593683   6.9932632  6.638992   5.6074224  4.480165   3.6757917
 2.9865162  2.7228613  2.354478   2.1419816  2.1659825  2.2115097
 2.0584695  1.9100391  2.0697725  2.3743443  2.63862    2.7398417
 2.8764524  2.8522353  2.7000241  2.2600713  1.7598705  1.3103192
 1.1183527  0.64273274 0.5574434  0.71856624 1.036243   1.2295727
 1.1614223  0.7302984  0.65351737 0.75601816 1.3651406  2.1842349
 2.8023674  2.8957584  2.6625607  2.267095   1.823454   1.3349582
 0.87380195 0.52384806 0.44394457 0.70125633 1.145617   1.4922969
 1.8797007  2.1397386  2.3872643  2.3917847  2.4349072  2.466315
 2.6496859  2.7887156  2.8196201  2.724814   2.7666478  3.2962217
 3.9734716  4.824283   5.4746876  6.1294136  6.387709   5.6089783
 4.4365773  3.3195152  2.6204126  2.2826676  1.95367    1.7821543
 1.841706   1.8380411  1.7782857  1.5741533  1.6787816  1.9857409
 2.269346   2.4144242  2.5576766  2.58212    2.518541   2.435582
 1.8734065  1.2539015  0.83718926 0.687062   0.3829159  0.32523805
 0.5495141  1.0488033  1.0381973  0.7314502  0.49894997 0.5842935
 0.8274654  1.451967   2.1218746  2.3971338  2.4990392  2.172931
 1.8029491  1.3727083  1.1443334  0.65918195 0.3800594  0.33171925
 0.58331937 0.93517154 1.2915988  1.6481746  1.9902669  2.0195675
 2.1078327  2.2141063  2.2233107  2.2903583  2.3610797  2.3337953
 2.1977615  2.3577118  2.8581448  3.5834935  4.398851   5.0620904
 5.6042066  5.769557   4.642554   3.3643696  2.372848   1.9440413
 1.608216   1.482647   1.50804    1.4964888  1.3798776  1.3485624
 1.3098655  1.5216259  1.7835398  1.9709871  2.1017654  2.1752114
 2.1536617  2.1225998  1.9671779  1.4926149 ]
