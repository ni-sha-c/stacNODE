time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.47%, model saved.
Epoch: 0 Train: 32399.63672 Test: 3874.70337
Epoch: 100 Train: 9246.15332 Test: 1407.06335
Epoch 200: New minimal relative error: 92.98%, model saved.
Epoch: 200 Train: 8513.74707 Test: 1202.30298
Epoch: 300 Train: 8088.05469 Test: 1428.31042
Epoch: 400 Train: 7605.18652 Test: 1040.61816
Epoch: 500 Train: 7606.98242 Test: 1020.72321
Epoch: 600 Train: 7390.52344 Test: 975.07776
Epoch: 700 Train: 6613.15479 Test: 786.19415
Epoch: 800 Train: 5233.33789 Test: 486.33090
Epoch: 900 Train: 4293.72021 Test: 343.77783
Epoch: 1000 Train: 2185.82617 Test: 121.77995
Epoch: 1100 Train: 1133.88062 Test: 78.65609
Epoch 1200: New minimal relative error: 51.64%, model saved.
Epoch: 1200 Train: 521.11694 Test: 40.28915
Epoch 1300: New minimal relative error: 13.71%, model saved.
Epoch: 1300 Train: 407.58060 Test: 11.82817
Epoch: 1400 Train: 343.66699 Test: 8.74026
Epoch: 1500 Train: 310.08673 Test: 7.68253
Epoch: 1600 Train: 252.36270 Test: 3.56426
Epoch: 1700 Train: 229.00601 Test: 2.98430
Epoch: 1800 Train: 213.34152 Test: 3.30656
Epoch: 1900 Train: 218.30693 Test: 4.00320
Epoch 2000: New minimal relative error: 9.32%, model saved.
Epoch: 2000 Train: 189.06580 Test: 2.72855
Epoch: 2100 Train: 178.71840 Test: 2.61306
Epoch: 2200 Train: 163.89259 Test: 2.42127
Epoch: 2300 Train: 147.87573 Test: 1.88230
Epoch 2400: New minimal relative error: 8.78%, model saved.
Epoch: 2400 Train: 143.07794 Test: 1.62010
Epoch 2500: New minimal relative error: 6.73%, model saved.
Epoch: 2500 Train: 138.05080 Test: 3.09871
Epoch: 2600 Train: 136.71249 Test: 5.26202
Epoch: 2700 Train: 137.36760 Test: 4.56064
Epoch: 2800 Train: 113.25720 Test: 1.14241
Epoch: 2900 Train: 107.78585 Test: 2.05650
Epoch: 3000 Train: 103.68047 Test: 1.48635
Epoch: 3100 Train: 104.13324 Test: 1.74654
Epoch 3200: New minimal relative error: 6.00%, model saved.
Epoch: 3200 Train: 101.13000 Test: 1.05999
Epoch: 3300 Train: 98.83955 Test: 1.28031
Epoch: 3400 Train: 94.11233 Test: 0.95730
Epoch: 3500 Train: 117.32793 Test: 1.96281
Epoch: 3600 Train: 85.57122 Test: 0.72856
Epoch: 3700 Train: 79.32535 Test: 1.50660
Epoch: 3800 Train: 74.95814 Test: 0.75750
Epoch 3900: New minimal relative error: 3.27%, model saved.
Epoch: 3900 Train: 72.87225 Test: 0.70406
Epoch: 4000 Train: 70.77332 Test: 0.78011
Epoch: 4100 Train: 67.19022 Test: 0.64244
Epoch: 4200 Train: 72.13098 Test: 0.60203
Epoch: 4300 Train: 64.75495 Test: 0.45546
Epoch: 4400 Train: 61.67563 Test: 0.46704
Epoch: 4500 Train: 60.34752 Test: 0.48144
Epoch: 4600 Train: 58.61434 Test: 0.42231
Epoch: 4700 Train: 59.77494 Test: 0.97978
Epoch: 4800 Train: 53.79039 Test: 0.38699
Epoch: 4900 Train: 51.56861 Test: 0.31722
Epoch: 5000 Train: 52.64381 Test: 0.57503
Epoch: 5100 Train: 50.46531 Test: 1.43102
Epoch: 5200 Train: 50.73577 Test: 0.31313
Epoch: 5300 Train: 61.11818 Test: 7.12827
Epoch: 5400 Train: 46.86069 Test: 0.25958
Epoch: 5500 Train: 43.89734 Test: 0.50896
Epoch: 5600 Train: 47.54549 Test: 2.69646
Epoch: 5700 Train: 41.04113 Test: 0.32270
Epoch: 5800 Train: 38.31807 Test: 0.23507
Epoch 5900: New minimal relative error: 3.10%, model saved.
Epoch: 5900 Train: 38.06546 Test: 0.34343
Epoch: 6000 Train: 38.30020 Test: 0.18496
Epoch: 6100 Train: 38.62209 Test: 0.26248
Epoch: 6200 Train: 38.88107 Test: 0.69152
Epoch: 6300 Train: 37.64381 Test: 0.22657
Epoch: 6400 Train: 37.47356 Test: 0.37060
Epoch: 6500 Train: 35.53955 Test: 0.20824
Epoch: 6600 Train: 38.37462 Test: 0.26900
Epoch: 6700 Train: 37.76121 Test: 0.33200
Epoch: 6800 Train: 45.72928 Test: 5.92009
Epoch: 6900 Train: 40.87920 Test: 0.51509
Epoch: 7000 Train: 44.41220 Test: 1.36174
Epoch: 7100 Train: 41.84160 Test: 0.25444
Epoch: 7200 Train: 39.52057 Test: 0.38293
Epoch: 7300 Train: 41.40192 Test: 0.68455
Epoch: 7400 Train: 40.54144 Test: 0.26648
Epoch: 7500 Train: 40.35830 Test: 0.36599
Epoch 7600: New minimal relative error: 2.71%, model saved.
Epoch: 7600 Train: 41.19587 Test: 0.25321
Epoch: 7700 Train: 42.06810 Test: 0.96959
Epoch: 7800 Train: 38.73568 Test: 0.37318
Epoch: 7900 Train: 37.13316 Test: 0.19836
Epoch: 8000 Train: 35.67573 Test: 0.18701
Epoch: 8100 Train: 35.28678 Test: 0.21084
Epoch: 8200 Train: 35.50781 Test: 0.35758
Epoch: 8300 Train: 33.50907 Test: 0.32395
Epoch: 8400 Train: 34.66055 Test: 0.65762
Epoch: 8500 Train: 30.58899 Test: 0.13938
Epoch: 8600 Train: 31.20494 Test: 0.33900
Epoch: 8700 Train: 31.34198 Test: 0.19829
Epoch: 8800 Train: 31.09486 Test: 0.15147
Epoch: 8900 Train: 29.36137 Test: 0.13437
Epoch: 9000 Train: 29.30115 Test: 0.15801
Epoch: 9100 Train: 31.10735 Test: 0.43474
Epoch: 9200 Train: 30.20690 Test: 0.14830
Epoch: 9300 Train: 30.91635 Test: 0.18686
Epoch: 9400 Train: 30.45385 Test: 0.15620
Epoch 9500: New minimal relative error: 1.89%, model saved.
Epoch: 9500 Train: 29.68012 Test: 0.20206
Epoch: 9600 Train: 30.70039 Test: 0.19246
Epoch: 9700 Train: 31.43732 Test: 0.36752
Epoch: 9800 Train: 32.83717 Test: 0.17917
Epoch: 9900 Train: 33.65405 Test: 0.16936
Epoch: 9999 Train: 31.58188 Test: 0.16204
Training Loss: tensor(31.5819)
Test Loss: tensor(0.1620)
Learned LE: [ 8.5111248e-01  1.1422929e-02 -1.4500281e+01]
True LE: [ 8.5023159e-01 -2.0245370e-03 -1.4525001e+01]
Relative Error: [3.7538893  3.1162202  2.4096928  1.6970264  1.1998233  1.3152243
 1.8205937  2.460788   3.0362477  3.4755368  3.7866545  3.853559
 3.6556232  3.2674341  2.8802764  2.7389095  2.7012022  2.6633544
 2.6511385  2.5926135  2.492491   2.3772976  2.0756698  1.7461264
 1.4747622  1.1672604  1.2233438  1.3897072  1.528507   1.6621438
 1.7621102  1.8011682  1.6266415  1.5318401  1.4149034  1.4181563
 1.6075764  1.8365569  1.999049   2.240907   2.5455794  2.7538266
 2.996985   3.2802315  3.5068772  3.6616006  3.5649426  3.2456503
 3.1373944  3.176564   3.3468795  3.5829968  3.8594015  4.171767
 4.3750963  4.512452   4.656065   4.788982   4.908561   4.893237
 4.510962   3.997678   3.4588528  2.8355474  2.1529357  1.4759978
 1.0779936  1.2696468  1.8103997  2.4540987  2.9383595  3.2717512
 3.588624   3.526511   3.2652478  2.9337587  2.682924   2.6584868
 2.6672876  2.6778402  2.6526096  2.516394   2.3443863  2.1970334
 1.959157   1.6661974  1.4248317  1.3239185  1.3770031  1.5106664
 1.6321875  1.7442232  1.8173777  1.8030114  1.5834378  1.4502394
 1.3953998  1.4100758  1.5651636  1.7129948  1.8547128  2.0497699
 2.3482578  2.5142488  2.7561767  3.0827565  3.3391953  3.4541547
 3.1682525  2.8939688  2.8258848  2.8915555  3.058034   3.305239
 3.5938156  3.917617   4.093487   4.2302446  4.3896546  4.548078
 4.694134   4.5875974  4.2081904  3.6775894  3.1494248  2.5614421
 1.9096599  1.2944977  0.98908144 1.1395878  1.7813983  2.4207444
 2.813881   3.0680897  3.3654864  3.2438557  2.952799   2.6971931
 2.5607736  2.6137185  2.6454113  2.6265223  2.5138857  2.323112
 2.1085205  1.9179354  1.6865588  1.4809425  1.4836888  1.5215671
 1.5503389  1.6497989  1.7387828  1.8411629  1.8772403  1.8140327
 1.5464561  1.4009589  1.405169   1.3838643  1.4958545  1.5648842
 1.6507844  1.8633671  2.1567464  2.2621024  2.4999917  2.856519
 3.09357    3.2262032  2.773585   2.5349722  2.4901066  2.5788786
 2.7454906  2.9871364  3.2819676  3.605362   3.7615843  3.9039395
 4.086544   4.2786164  4.4162602  4.241154   3.9042706  3.4059308
 2.8808868  2.332027   1.7151319  1.1831967  0.8680094  1.020701
 1.6992867  2.3248997  2.6370912  2.8386455  3.1689248  2.9695013
 2.685095   2.457397   2.396593   2.527609   2.4694839  2.4051661
 2.3111875  2.1633093  1.9086245  1.673754   1.4627353  1.3258605
 1.469714   1.6987871  1.7528093  1.8205918  1.8743753  1.9351668
 1.9414129  1.8378865  1.5386114  1.4107759  1.4061779  1.3509144
 1.4185183  1.3715434  1.4603379  1.6601275  1.9036217  2.0047598
 2.2368984  2.554443   2.811071   2.7985287  2.3619854  2.1410422
 2.1108472  2.2181582  2.384992   2.6256862  2.9178965  3.2172163
 3.3762302  3.5300515  3.742024   3.9576921  4.08913    3.9043005
 3.6149342  3.1631231  2.6620111  2.1548254  1.5904635  1.1230997
 0.76040274 0.9193896  1.6036879  2.21869    2.439106   2.6259851
 2.978566   2.681241   2.4387417  2.264923   2.2694278  2.2549274
 2.2661133  2.22744    2.1266532  1.9471546  1.7410531  1.4704101
 1.3017807  1.3092008  1.5752189  1.7757741  1.9877633  2.0280426
 2.0376627  2.043838   2.0296042  1.8893877  1.5557244  1.4460471
 1.4089063  1.3164643  1.2965328  1.2082468  1.2868652  1.4717861
 1.6526453  1.7635895  1.994548   2.2634633  2.5557318  2.3964183
 1.9372053  1.7052892  1.6665695  1.7564476  1.9328966  2.2019832
 2.4933848  2.761096   2.9229777  3.1036649  3.3332806  3.5900648
 3.7096229  3.5884776  3.3637664  2.977295   2.526679   2.0405507
 1.5555801  1.103457   0.68472695 0.83610725 1.542806   2.0560968
 2.2042537  2.4458327  2.7860498  2.464796   2.261744   2.137354
 2.0402806  1.9981961  2.0465274  2.0981932  1.9743347  1.7669996
 1.5213034  1.3220173  1.1715467  1.3701397  1.7097446  1.8872827
 2.2111523  2.236083   2.1212697  2.0487897  2.0308306  1.9253079
 1.6103563  1.4876548  1.4161459  1.2678202  1.1762722  1.0757619
 1.1164389  1.2861347  1.4032187  1.5194733  1.7611089  1.9877181
 2.2505393  2.0571387  1.6771536  1.4251362  1.2809054  1.3054377
 1.459245   1.6954409  1.986752   2.245697   2.4098096  2.610753
 2.948354   3.2854068  3.441547   3.3533487  3.1603408  2.84004
 2.447145   1.9886934  1.5869528  1.1243558  0.65008986 0.8059409
 1.4611942  1.8482023  2.0217295  2.2933836  2.6563275  2.3097606
 2.14951    1.993113   1.8225808  1.7974056  1.8619858  1.924896
 1.8639915  1.626621   1.3666695  1.115765   0.938668   1.2013389
 1.5550569  1.9086635  2.2525628  2.2865746 ]
