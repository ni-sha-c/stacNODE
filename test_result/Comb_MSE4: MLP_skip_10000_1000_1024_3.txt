time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 105.45%, model saved.
Epoch: 0 Train: 3869.74048 Test: 4327.13721
Epoch 100: New minimal relative error: 20.79%, model saved.
Epoch: 100 Train: 36.17123 Test: 32.14996
Epoch 200: New minimal relative error: 19.96%, model saved.
Epoch: 200 Train: 8.40480 Test: 6.42109
Epoch 300: New minimal relative error: 18.65%, model saved.
Epoch: 300 Train: 6.08666 Test: 5.65402
Epoch: 400 Train: 4.48371 Test: 3.70399
Epoch 500: New minimal relative error: 12.35%, model saved.
Epoch: 500 Train: 3.33030 Test: 2.58856
Epoch: 600 Train: 4.59962 Test: 2.38114
Epoch: 700 Train: 6.30510 Test: 4.19760
Epoch: 800 Train: 2.71410 Test: 2.13504
Epoch: 900 Train: 11.56843 Test: 10.44775
Epoch 1000: New minimal relative error: 10.11%, model saved.
Epoch: 1000 Train: 1.74123 Test: 1.38647
Epoch 1100: New minimal relative error: 8.36%, model saved.
Epoch: 1100 Train: 1.40221 Test: 1.09330
Epoch: 1200 Train: 2.88591 Test: 3.05461
Epoch: 1300 Train: 1.16029 Test: 0.81441
Epoch: 1400 Train: 1.88837 Test: 1.92815
Epoch: 1500 Train: 1.06032 Test: 0.82895
Epoch: 1600 Train: 2.33694 Test: 2.58204
Epoch: 1700 Train: 0.66860 Test: 0.57124
Epoch 1800: New minimal relative error: 7.97%, model saved.
Epoch: 1800 Train: 1.13548 Test: 0.97703
Epoch: 1900 Train: 1.89289 Test: 1.41885
Epoch: 2000 Train: 1.24020 Test: 1.07937
Epoch: 2100 Train: 0.60771 Test: 0.29168
Epoch: 2200 Train: 1.66824 Test: 1.89405
Epoch: 2300 Train: 2.41409 Test: 2.12920
Epoch: 2400 Train: 1.70704 Test: 1.46340
Epoch: 2500 Train: 0.36395 Test: 0.23213
Epoch: 2600 Train: 0.75651 Test: 0.76270
Epoch: 2700 Train: 0.84484 Test: 0.84480
Epoch: 2800 Train: 0.76714 Test: 0.79403
Epoch 2900: New minimal relative error: 7.34%, model saved.
Epoch: 2900 Train: 0.44439 Test: 0.48721
Epoch: 3000 Train: 1.63196 Test: 1.02993
Epoch: 3100 Train: 0.21892 Test: 0.16675
Epoch: 3200 Train: 0.43095 Test: 0.33669
Epoch: 3300 Train: 1.31137 Test: 1.10808
Epoch: 3400 Train: 1.09151 Test: 1.02458
Epoch: 3500 Train: 1.50276 Test: 1.40542
Epoch: 3600 Train: 0.18254 Test: 0.13997
Epoch: 3700 Train: 0.16933 Test: 0.11351
Epoch: 3800 Train: 0.58952 Test: 0.70162
Epoch: 3900 Train: 0.58459 Test: 0.69304
Epoch: 4000 Train: 0.71927 Test: 0.79404
Epoch: 4100 Train: 1.55016 Test: 0.91188
Epoch: 4200 Train: 2.03436 Test: 1.71189
Epoch 4300: New minimal relative error: 6.66%, model saved.
Epoch: 4300 Train: 0.21998 Test: 0.12875
Epoch: 4400 Train: 0.18968 Test: 0.10912
Epoch: 4500 Train: 0.20529 Test: 0.18287
Epoch: 4600 Train: 0.47059 Test: 0.50217
Epoch: 4700 Train: 0.28177 Test: 0.23431
Epoch: 4800 Train: 1.99335 Test: 1.79999
Epoch: 4900 Train: 1.03232 Test: 1.22953
Epoch: 5000 Train: 0.25148 Test: 0.29332
Epoch: 5100 Train: 0.14843 Test: 0.11406
Epoch: 5200 Train: 0.70771 Test: 0.78359
Epoch: 5300 Train: 0.77456 Test: 0.90752
Epoch: 5400 Train: 0.22152 Test: 0.22342
Epoch: 5500 Train: 0.71642 Test: 0.78067
Epoch 5600: New minimal relative error: 6.64%, model saved.
Epoch: 5600 Train: 0.16222 Test: 0.14279
Epoch: 5700 Train: 0.09286 Test: 0.07361
Epoch 5800: New minimal relative error: 5.11%, model saved.
Epoch: 5800 Train: 0.22236 Test: 0.22892
Epoch: 5900 Train: 0.20086 Test: 0.13925
Epoch: 6000 Train: 0.29793 Test: 0.31844
Epoch: 6100 Train: 0.09614 Test: 0.07389
Epoch: 6200 Train: 0.09734 Test: 0.07962
Epoch: 6300 Train: 0.08522 Test: 0.07289
Epoch: 6400 Train: 0.07568 Test: 0.06277
Epoch: 6500 Train: 0.10189 Test: 0.09730
Epoch: 6600 Train: 0.53671 Test: 0.70104
Epoch 6700: New minimal relative error: 4.37%, model saved.
Epoch: 6700 Train: 0.09342 Test: 0.14411
Epoch: 6800 Train: 0.15136 Test: 0.11549
Epoch: 6900 Train: 0.13662 Test: 0.15413
Epoch 7000: New minimal relative error: 4.08%, model saved.
Epoch: 7000 Train: 0.24031 Test: 0.27172
Epoch: 7100 Train: 0.06972 Test: 0.06231
Epoch: 7200 Train: 0.08448 Test: 0.08167
Epoch: 7300 Train: 0.18537 Test: 0.16554
Epoch: 7400 Train: 0.07529 Test: 0.06228
Epoch: 7500 Train: 0.06123 Test: 0.05317
Epoch: 7600 Train: 0.36144 Test: 0.32100
Epoch: 7700 Train: 0.06222 Test: 0.05053
Epoch: 7800 Train: 0.35757 Test: 0.38887
Epoch: 7900 Train: 0.09207 Test: 0.15928
Epoch: 8000 Train: 0.46135 Test: 0.54387
Epoch: 8100 Train: 0.19464 Test: 0.24270
Epoch: 8200 Train: 0.06034 Test: 0.05363
Epoch: 8300 Train: 0.05525 Test: 0.04720
Epoch: 8400 Train: 0.09234 Test: 0.10523
Epoch: 8500 Train: 0.21035 Test: 0.22111
Epoch: 8600 Train: 0.05012 Test: 0.04407
Epoch: 8700 Train: 0.09203 Test: 0.07774
Epoch: 8800 Train: 0.04871 Test: 0.04298
Epoch: 8900 Train: 0.04826 Test: 0.04292
Epoch: 9000 Train: 0.16293 Test: 0.05118
Epoch: 9100 Train: 0.11499 Test: 0.10253
Epoch: 9200 Train: 0.04606 Test: 0.04077
Epoch: 9300 Train: 0.04569 Test: 0.04098
Epoch: 9400 Train: 0.04551 Test: 0.04019
Epoch: 9500 Train: 0.33462 Test: 0.44624
Epoch: 9600 Train: 0.04372 Test: 0.03901
Epoch: 9700 Train: 0.04658 Test: 0.04735
Epoch: 9800 Train: 0.04255 Test: 0.03837
Epoch: 9900 Train: 0.04216 Test: 0.03775
Epoch: 9999 Train: 0.04345 Test: 0.03868
Training Loss: tensor(0.0435)
Test Loss: tensor(0.0387)
Learned LE: [ 0.87760556  0.05075046 -3.8956602 ]
True LE: [ 8.6860573e-01  4.2930921e-03 -1.4543645e+01]
Relative Error: [3.7011688  3.6209707  3.5256789  3.2893603  3.0194726  2.875017
 2.8926864  3.0714607  3.3954089  3.48524    3.3635085  3.2435105
 3.1846662  3.231657   3.2693102  3.3118718  3.4262764  3.645676
 3.7436743  3.5691638  3.1884148  2.7730484  2.7738311  2.7307374
 2.6616347  2.2039444  1.9944551  2.185793   2.301833   2.2850144
 2.325598   1.8642001  1.3182157  0.9652181  0.72429484 0.79569995
 0.94573534 1.2110192  1.5914804  1.4595764  1.2416154  1.0616964
 1.049007   1.2731293  1.3066536  1.4158834  1.4380747  1.3471806
 1.0675389  0.75585246 0.6174508  0.92091227 1.4049177  1.4903343
 1.3334152  1.6433024  2.1802542  2.618499   3.0784006  3.2311041
 3.2914882  3.5372488  3.6522615  3.5454364  3.3507843  3.0219157
 2.6805856  2.5259576  2.498404   2.6412213  2.9494786  3.3099399
 3.3311896  3.1303928  3.0103788  2.9821908  3.0751832  3.0996966
 3.1911712  3.4180493  3.5003998  3.360118   3.0796897  2.6269264
 2.5827003  2.6127264  2.5665915  2.1629806  2.0430038  2.1024284
 2.3507457  2.4552193  2.2719595  1.6557772  1.1165025  0.67074245
 0.5623678  0.6311079  0.790519   1.0804566  1.432849   1.9053946
 1.8065188  1.4957156  1.374759   1.576635   1.6583694  1.6632844
 1.7119129  1.6272926  1.3715389  0.9411507  0.94057804 1.1650486
 1.5373172  1.6994823  1.4128542  1.7621869  2.33943    2.915457
 3.248912   3.3645148  3.1978343  3.3581562  3.4742143  3.3511844
 3.1726592  2.808607   2.4604719  2.295687   2.220136   2.3096952
 2.521053   2.917432   3.2327857  3.0618815  2.8570569  2.721433
 2.6898396  2.8426201  2.996895   3.106915   3.11917    3.0528214
 2.8946373  2.5231397  2.4079695  2.510069   2.5430937  2.181578
 1.7393073  1.9189447  2.3924298  2.4392772  2.0302036  1.547185
 1.0188705  0.5586127  0.48280856 0.5064253  0.6333057  0.9745473
 1.263875   1.706222   2.2177804  2.151329   1.857656   1.7881062
 1.8913732  1.944541   2.0361834  2.0280054  1.7496367  1.276343
 1.192535   1.4542701  1.639245   1.8739302  1.6713988  1.8909767
 2.5873568  2.9771621  3.221853   3.3731508  3.067072   3.0658484
 3.199999   3.0921266  2.9698513  2.690182   2.3638153  2.1709142
 2.0603762  2.153315   2.363847   2.5941796  2.830236   2.9644797
 2.7419407  2.4688761  2.349007   2.5332217  2.737577   2.7588766
 2.7730591  2.843697   2.7621164  2.4423587  2.2657127  2.3632722
 2.452838   2.196256   1.6602769  1.6105368  2.192876   2.3074481
 1.8664962  1.5310417  1.03492    0.5978201  0.4320384  0.3593199
 0.5770507  0.95045304 1.1449237  1.4680554  1.8196818  2.3676417
 2.357238   2.1224687  2.0678015  2.127305   2.273003   2.3139405
 2.220789   1.772475   1.570345   1.7475374  1.931014   2.1136808
 2.0516157  2.1114697  2.6562045  2.946923   3.0729506  3.2530422
 3.0355346  2.7396164  2.8756335  2.8422565  2.7374918  2.6051104
 2.4500892  2.1955495  2.0022953  2.058472   2.2491548  2.3725078
 2.4870021  2.5790453  2.5436652  2.3277729  2.1567688  2.1758966
 2.422763   2.5389564  2.5873377  2.7392273  2.621021   2.3969748
 2.1903641  2.3321939  2.3790722  2.0709045  1.6464988  1.5374097
 1.7312328  1.9643154  1.7948362  1.5599502  1.1701843  0.81138813
 0.44988027 0.4146161  0.7014816  1.0558215  1.2302202  1.4025456
 1.5773085  1.9947497  2.4078808  2.4126687  2.2362256  2.3189185
 2.38008    2.520592   2.5699985  2.2996416  1.8716474  1.978716
 2.1039197  2.2188566  2.343839   2.3564231  2.6836162  2.8827753
 2.9657326  3.1100857  2.9764416  2.545389   2.4613633  2.5649307
 2.433407   2.5563152  2.5288644  2.285526   2.1249523  2.0972075
 2.195382   2.1921937  2.2162833  2.2112386  2.1597555  2.194061
 2.090261   2.003172   2.2340965  2.1919398  2.294847   2.4793558
 2.5649621  2.4002752  2.2058413  2.270273   2.3349028  2.1146398
 1.6540598  1.4212734  1.379336   1.6838543  1.6506355  1.606177
 1.4120632  1.1784049  0.8563292  0.78037477 0.97829634 1.2637306
 1.4140747  1.4059112  1.4846972  1.6706079  2.0068736  2.4045079
 2.35511    2.2284155  2.4765003  2.665024   2.5708287  2.5681133
 2.3056073  2.1000001  2.200838   2.3668008  2.4873936  2.5868948
 2.6753833  2.7858405  2.9160757  2.7448726  2.8034942  2.4954374
 2.1245663  2.2010186  2.1601067  2.3160036  2.5012844  2.3511014
 2.2076516  2.0883162  2.1052487  2.0203187  1.9684907  1.9731869
 1.8963095  1.9190978  2.019579   2.2090044  2.1625812  2.0602431
 1.9206381  2.0937686  2.2872658  2.335587   2.0972316  2.1848493
 2.3539279  2.140165   1.7744638  1.5628086 ]
