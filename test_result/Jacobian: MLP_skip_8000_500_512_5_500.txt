time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.23%, model saved.
Epoch: 0 Train: 32270.37500 Test: 4260.26807
Epoch 80: New minimal relative error: 93.05%, model saved.
Epoch: 80 Train: 8795.76953 Test: 1487.65381
Epoch: 160 Train: 7660.27490 Test: 1294.08850
Epoch: 240 Train: 7108.93457 Test: 1156.14270
Epoch: 320 Train: 7283.88330 Test: 1116.49377
Epoch 400: New minimal relative error: 73.44%, model saved.
Epoch: 400 Train: 5703.95752 Test: 847.42059
Epoch: 480 Train: 4769.04297 Test: 565.17938
Epoch: 560 Train: 2973.56714 Test: 235.32176
Epoch: 640 Train: 2213.76807 Test: 137.35352
Epoch: 720 Train: 1166.20654 Test: 257.33566
Epoch 800: New minimal relative error: 69.58%, model saved.
Epoch: 800 Train: 1016.65344 Test: 118.72709
Epoch: 880 Train: 543.55682 Test: 65.48363
Epoch: 960 Train: 414.06897 Test: 72.35187
Epoch 1040: New minimal relative error: 11.06%, model saved.
Epoch: 1040 Train: 276.82367 Test: 4.30307
Epoch 1120: New minimal relative error: 9.85%, model saved.
Epoch: 1120 Train: 242.71420 Test: 3.49006
Epoch 1200: New minimal relative error: 9.82%, model saved.
Epoch: 1200 Train: 217.11362 Test: 2.97680
Epoch: 1280 Train: 200.12898 Test: 2.95793
Epoch: 1360 Train: 218.60428 Test: 4.88251
Epoch 1440: New minimal relative error: 8.19%, model saved.
Epoch: 1440 Train: 185.85220 Test: 3.46762
Epoch: 1520 Train: 187.53105 Test: 3.64470
Epoch: 1600 Train: 149.39305 Test: 1.58739
Epoch: 1680 Train: 152.72696 Test: 1.72160
Epoch: 1760 Train: 139.80489 Test: 1.94484
Epoch: 1840 Train: 149.51622 Test: 24.53183
Epoch 1920: New minimal relative error: 4.08%, model saved.
Epoch: 1920 Train: 124.57586 Test: 1.22643
Epoch: 2000 Train: 121.56757 Test: 1.07953
Epoch: 2080 Train: 111.57224 Test: 1.11187
Epoch: 2160 Train: 106.75008 Test: 1.82400
Epoch: 2240 Train: 97.38349 Test: 0.92014
Epoch: 2320 Train: 92.23514 Test: 1.77073
Epoch: 2400 Train: 90.17712 Test: 1.50956
Epoch: 2480 Train: 90.60230 Test: 0.66476
Epoch: 2560 Train: 92.84043 Test: 1.17746
Epoch: 2640 Train: 85.64674 Test: 1.08674
Epoch: 2720 Train: 83.77476 Test: 2.61592
Epoch: 2800 Train: 81.06268 Test: 1.64606
Epoch: 2880 Train: 88.06631 Test: 4.88729
Epoch: 2960 Train: 75.01138 Test: 0.95818
Epoch: 3040 Train: 74.03909 Test: 1.08236
Epoch: 3120 Train: 70.88267 Test: 1.29328
Epoch: 3200 Train: 81.17834 Test: 3.02342
Epoch: 3280 Train: 75.82668 Test: 2.37807
Epoch: 3360 Train: 61.70272 Test: 0.31193
Epoch: 3440 Train: 63.39346 Test: 0.32894
Epoch: 3520 Train: 60.56774 Test: 0.56995
Epoch: 3600 Train: 60.24643 Test: 0.31660
Epoch: 3680 Train: 62.86506 Test: 2.77857
Epoch: 3760 Train: 58.88273 Test: 0.37944
Epoch: 3840 Train: 61.20490 Test: 0.34222
Epoch: 3920 Train: 61.16697 Test: 1.87926
Epoch: 4000 Train: 59.64219 Test: 2.39273
Epoch: 4080 Train: 57.26256 Test: 0.67199
Epoch: 4160 Train: 57.32987 Test: 0.35398
Epoch: 4240 Train: 56.91698 Test: 0.87646
Epoch: 4320 Train: 54.00312 Test: 0.53109
Epoch: 4400 Train: 56.77624 Test: 0.60147
Epoch: 4480 Train: 52.12143 Test: 0.23396
Epoch: 4560 Train: 51.48406 Test: 0.30211
Epoch: 4640 Train: 47.44468 Test: 0.18727
Epoch: 4720 Train: 45.86203 Test: 0.54826
Epoch: 4800 Train: 44.76181 Test: 1.41769
Epoch: 4880 Train: 48.15857 Test: 2.84108
Epoch: 4960 Train: 49.70478 Test: 1.16132
Epoch: 5040 Train: 46.95148 Test: 0.44118
Epoch: 5120 Train: 47.07904 Test: 0.72777
Epoch: 5200 Train: 46.93924 Test: 0.32753
Epoch: 5280 Train: 50.33425 Test: 0.92035
Epoch 5360: New minimal relative error: 1.84%, model saved.
Epoch: 5360 Train: 45.54581 Test: 0.20419
Epoch: 5440 Train: 46.02327 Test: 0.23236
Epoch: 5520 Train: 47.18360 Test: 0.26533
Epoch: 5600 Train: 45.84438 Test: 0.26930
Epoch: 5680 Train: 45.76773 Test: 0.78628
Epoch: 5760 Train: 46.50015 Test: 0.64617
Epoch: 5840 Train: 43.22556 Test: 0.31840
Epoch: 5920 Train: 43.25172 Test: 0.18608
Epoch: 6000 Train: 44.46727 Test: 0.26505
Epoch: 6080 Train: 43.08877 Test: 0.72038
Epoch: 6160 Train: 43.81108 Test: 0.32698
Epoch: 6240 Train: 45.29735 Test: 0.22187
Epoch: 6320 Train: 45.90196 Test: 0.91918
Epoch: 6400 Train: 42.72647 Test: 0.19164
Epoch: 6480 Train: 42.90863 Test: 0.67184
Epoch: 6560 Train: 40.74335 Test: 0.17282
Epoch 6640: New minimal relative error: 1.38%, model saved.
Epoch: 6640 Train: 40.05458 Test: 0.14887
Epoch: 6720 Train: 41.46235 Test: 0.29667
Epoch: 6800 Train: 39.83322 Test: 0.16497
Epoch: 6880 Train: 39.49550 Test: 0.22898
Epoch: 6960 Train: 38.92788 Test: 0.14947
Epoch: 7040 Train: 39.54510 Test: 0.24043
Epoch: 7120 Train: 38.90890 Test: 0.15808
Epoch: 7200 Train: 37.41455 Test: 0.23580
Epoch: 7280 Train: 38.62155 Test: 0.20160
Epoch: 7360 Train: 37.60868 Test: 0.18509
Epoch: 7440 Train: 37.88614 Test: 0.21187
Epoch: 7520 Train: 40.97781 Test: 0.23969
Epoch: 7600 Train: 49.42808 Test: 1.09744
Epoch: 7680 Train: 43.47059 Test: 0.23165
Epoch: 7760 Train: 41.90361 Test: 0.20233
Epoch: 7840 Train: 40.93750 Test: 0.20153
Epoch: 7920 Train: 40.21076 Test: 0.18451
Epoch: 7999 Train: 38.98238 Test: 0.16721
Training Loss: tensor(38.9824)
Test Loss: tensor(0.1672)
Learned LE: [  0.8385485    0.02558568 -14.531136  ]
True LE: [ 8.6601180e-01 -6.5042251e-03 -1.4541409e+01]
Relative Error: [5.533775   5.353513   5.3573823  5.281089   5.281715   5.3740873
 5.41322    5.2084713  4.969823   4.7690554  4.548748   4.383791
 4.1144686  3.689195   3.1424322  2.5202997  1.7763062  1.1047983
 0.69392455 1.1232432  2.0938654  3.2087474  3.301965   2.9924827
 2.583771   1.4820849  0.58302313 0.69679934 1.1289186  1.6933308
 2.22817    2.6227105  2.5201645  2.2809284  2.0947573  1.8077337
 1.6750721  1.4501889  1.1495056  1.0640446  1.6140666  2.6264324
 2.9017205  3.1365726  3.3780925  3.413023   3.3293295  3.4284413
 2.6904387  1.6818188  0.9706224  0.6882609  1.395222   2.171073
 2.8193624  3.3414478  3.7556562  4.0307255  4.2696586  4.761558
 5.242198   5.4593263  5.232588   4.998      4.9807     4.919857
 4.871728   4.954943   5.013322   4.822306   4.611492   4.449285
 4.314217   4.255154   3.9249454  3.5356789  3.0716815  2.5336134
 1.7406311  0.90014356 0.55124325 1.1095853  2.1030197  3.2218113
 3.0006506  2.6933892  2.2482638  1.2420492  0.49031302 0.80451506
 1.3700975  1.8588079  2.3530197  2.7772167  2.7508056  2.502604
 2.3004272  1.942213   1.7629132  1.5621852  1.2114297  0.9132484
 1.2681646  2.199044   2.8304658  3.0157893  3.1696043  3.1388905
 3.0072749  3.0865784  2.3461049  1.350018   0.67659456 0.52875865
 1.3470303  2.1166196  2.727329   3.1821322  3.5174322  3.7250988
 4.0603848  4.4599886  4.829922   5.0833507  4.9461226  4.6009917
 4.5262976  4.4752007  4.425641   4.471722   4.576001   4.3866625
 4.1918902  4.069212   4.0279427  3.9560344  3.6872838  3.4007747
 3.060944   2.5040045  1.7648784  0.9072979  0.43578047 1.0389304
 2.0174162  3.09954    2.7056146  2.4373934  2.1383135  1.1501148
 0.6378599  0.96176964 1.5288665  1.9608952  2.3638043  2.7927053
 2.9046118  2.6781816  2.4542284  2.0258484  1.847998   1.6135566
 1.2728075  0.8722972  0.9491121  1.7937841  2.818325   2.917751
 2.9306638  2.8818233  2.6953719  2.6429431  2.1130779  1.127841
 0.46030277 0.3889673  1.2329085  1.9866403  2.580524   3.008895
 3.2637649  3.4043565  3.7729998  4.116411   4.388518   4.587848
 4.5403814  4.2174797  4.0740924  3.9963357  3.9654207  3.9446924
 4.058225   3.9432633  3.726464   3.5974727  3.6116009  3.543132
 3.4109359  3.2249525  2.8842652  2.4185367  1.8076655  1.0322657
 0.33486372 0.9198803  1.8233048  2.8208756  2.4075906  2.17242
 2.0910993  1.1507901  0.81575114 1.09438    1.588292   2.0174775
 2.3187196  2.692921   2.9531088  2.773546   2.5916119  2.083674
 1.8910593  1.6175029  1.340107   0.8998017  0.72170293 1.3942152
 2.4097052  2.8379972  2.7601051  2.6726072  2.4102964  2.2566378
 2.0163336  0.9516693  0.30871516 0.5073985  1.202066   1.8707639
 2.3769255  2.727426   2.9581869  3.1066115  3.4423432  3.6967657
 3.9021683  4.0135694  4.021224   3.900405   3.57598    3.437762
 3.4290175  3.3428266  3.4056559  3.3917637  3.20809    3.0928876
 3.1044395  3.1059902  3.0705874  2.9595003  2.6800423  2.2937558
 1.8112282  1.1420995  0.43711329 0.7070156  1.4758965  2.392066
 2.1010118  1.8608164  2.006005   1.2229382  0.84850925 1.14475
 1.6130266  1.9585816  2.2003844  2.4976802  2.8170848  2.778201
 2.6063335  2.200408   1.9261228  1.718788   1.3856269  0.9281731
 0.6003361  1.0007205  1.8767294  2.816806   2.6372285  2.5146046
 2.2463856  1.9377997  1.7075626  1.0635527  0.48114216 0.67633206
 1.1997864  1.7608471  2.19378    2.4705684  2.6087575  2.7075498
 3.0704913  3.227793   3.3444495  3.4100287  3.4029744  3.3578854
 3.0725436  2.7926412  2.6770735  2.6533592  2.639849   2.704852
 2.658378   2.5211535  2.5345874  2.6039274  2.6134777  2.5360453
 2.4178965  2.1387155  1.7498441  1.2923226  0.76276547 0.28563166
 1.0734057  1.913437   1.8658711  1.520405   1.6650598  1.4073317
 0.9330027  1.0026484  1.52877    1.8581758  2.0697026  2.2546735
 2.4883337  2.6708102  2.5988717  2.4322727  1.9626744  1.8105464
 1.5229421  1.0760647  0.6346457  0.67652667 1.382632   2.2350807
 2.6386256  2.4573317  2.100573   1.7375062  1.4518093  1.3143051
 0.77955186 0.8304853  1.1060414  1.5800284  1.9623846  2.1950095
 2.2692664  2.3157814  2.5466547  2.6918354  2.74342    2.7498555
 2.7045543  2.6461327  2.676656   2.2830296  1.9661877  1.8913617
 1.7821054  1.8848625  1.9719774  1.973634   1.8907478  2.0279443
 2.141859   2.094753   2.0738544  1.9417608  1.6572094  1.4545007
 1.0800803  0.48124078 0.5858754  1.4446365  1.884296   1.3034992
 1.2574465  1.4938073  1.0838557  1.0017215 ]
