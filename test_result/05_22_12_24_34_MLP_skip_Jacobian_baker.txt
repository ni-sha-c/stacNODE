time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 20000
num_train: 2000
num_test: 2000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 3
reg_param: 200.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 390.825866699 Test: 5.288953781
Epoch 0: New minimal relative error: 5.29%, model saved.
Epoch: 200 Train: 20.549329758 Test: 13.857855797
Epoch: 400 Train: 13.163767815 Test: 11.664496422
Epoch: 600 Train: 13.213167191 Test: 11.757770538
Epoch: 800 Train: 13.122415543 Test: 11.556282043
Epoch: 1000 Train: 13.765341759 Test: 11.704453468
Epoch: 1200 Train: 13.911821365 Test: 11.340049744
Epoch: 1400 Train: 13.529347420 Test: 11.412805557
Epoch: 1600 Train: 14.165327072 Test: 11.705353737
Epoch: 1800 Train: 13.537206650 Test: 11.357011795
Epoch: 2000 Train: 13.196393013 Test: 11.781819344
Epoch: 2200 Train: 13.271530151 Test: 11.774271011
Epoch: 2400 Train: 13.186370850 Test: 11.545288086
Epoch: 2600 Train: 14.294192314 Test: 12.021465302
Epoch: 2800 Train: 14.106744766 Test: 11.740261078
Epoch: 3000 Train: 13.627427101 Test: 11.752193451
Epoch: 3200 Train: 14.817546844 Test: 11.840054512
Epoch: 3400 Train: 15.453584671 Test: 11.092728615
Epoch: 3600 Train: 15.166160583 Test: 11.754673958
Epoch: 3800 Train: 16.126907349 Test: 10.797818184
Epoch: 4000 Train: 15.073316574 Test: 10.979778290
Epoch: 4200 Train: 15.974866867 Test: 11.030427933
Epoch: 4400 Train: 15.578800201 Test: 11.126490593
Epoch: 4600 Train: 16.485549927 Test: 11.149723053
Epoch: 4800 Train: 15.679470062 Test: 11.298443794
Epoch: 5000 Train: 16.590494156 Test: 11.194490433
Epoch: 5200 Train: 16.085506439 Test: 11.202026367
Epoch: 5400 Train: 17.274820328 Test: 11.099021912
Epoch: 5600 Train: 16.596057892 Test: 11.177845001
Epoch: 5800 Train: 18.991634369 Test: 11.260380745
Epoch: 6000 Train: 24.246383667 Test: 10.714431763
Epoch: 6200 Train: 15.567441940 Test: 11.440016747
Epoch: 6400 Train: 15.081000328 Test: 11.193048477
Epoch: 6600 Train: 14.483703613 Test: 11.348062515
Epoch: 6800 Train: 15.163879395 Test: 11.389755249
Epoch: 7000 Train: 15.266789436 Test: 11.450830460
Epoch: 7200 Train: 16.602567673 Test: 11.469762802
Epoch: 7400 Train: 16.324022293 Test: 11.479744911
Epoch: 7600 Train: 16.351852417 Test: 11.302810669
Epoch: 7800 Train: 15.977331161 Test: 11.140604973
Epoch: 8000 Train: 16.951951981 Test: 11.302322388
Epoch: 8200 Train: 15.520540237 Test: 11.389789581
Epoch: 8400 Train: 17.075908661 Test: 11.429180145
Epoch: 8600 Train: 17.387287140 Test: 11.393601418
Epoch: 8800 Train: 16.408088684 Test: 11.391041756
Epoch: 9000 Train: 16.088380814 Test: 11.298115730
Epoch: 9200 Train: 15.433336258 Test: 11.517664909
Epoch: 9400 Train: 15.126781464 Test: 11.481219292
Epoch: 9600 Train: 14.874967575 Test: 11.493081093
Epoch: 9800 Train: 14.655096054 Test: 11.554697990
Epoch: 10000 Train: 14.645046234 Test: 11.575743675
Epoch: 10200 Train: 15.240952492 Test: 11.627473831
Epoch: 10400 Train: 16.771766663 Test: 11.681509018
Epoch: 10600 Train: 17.558975220 Test: 11.668845177
Epoch: 10800 Train: 18.521999359 Test: 11.653043747
Epoch: 11000 Train: 19.165004730 Test: 11.704701424
Epoch: 11200 Train: 19.881416321 Test: 11.680609703
Epoch: 11400 Train: 21.839326859 Test: 11.686321259
Epoch: 11600 Train: 22.963731766 Test: 11.694644928
Epoch: 11800 Train: 23.995378494 Test: 11.687968254
Epoch: 12000 Train: 25.084091187 Test: 11.674241066
Epoch: 12200 Train: 26.657989502 Test: 11.663607597
Epoch: 12400 Train: 29.292507172 Test: 11.663184166
Epoch: 12600 Train: 30.792770386 Test: 11.639241219
Epoch: 12800 Train: 32.154342651 Test: 11.566020012
Epoch: 13000 Train: 34.540985107 Test: 11.962492943
Epoch: 13200 Train: 29.657760620 Test: 11.817676544
Epoch: 13400 Train: 30.205766678 Test: 11.808197021
Epoch: 13600 Train: 29.052555084 Test: 11.677483559
Epoch: 13800 Train: 27.548995972 Test: 11.567558289
Epoch: 14000 Train: 26.721075058 Test: 11.448939323
Epoch: 14200 Train: 26.859493256 Test: 11.409276962
Epoch: 14400 Train: 26.582963943 Test: 11.410893440
Epoch: 14600 Train: 26.979885101 Test: 11.546730995
Epoch: 14800 Train: 26.612602234 Test: 11.593787193
Epoch: 15000 Train: 25.364725113 Test: 11.544926643
Epoch: 15200 Train: 23.607261658 Test: 11.501850128
Epoch: 15400 Train: 23.438911438 Test: 11.501226425
Epoch: 15600 Train: 23.724403381 Test: 11.512346268
Epoch: 15800 Train: 24.484420776 Test: 11.523826599
Epoch: 16000 Train: 24.314605713 Test: 11.516278267
Epoch: 16200 Train: 25.409908295 Test: 11.536604881
Epoch: 16400 Train: 26.168964386 Test: 11.540635109
Epoch: 16600 Train: 27.131134033 Test: 11.554655075
Epoch: 16800 Train: 27.388669968 Test: 11.571985245
Epoch: 17000 Train: 28.435245514 Test: 11.565684319
Epoch: 17200 Train: 29.827247620 Test: 11.566038132
Epoch: 17400 Train: 30.555093765 Test: 11.573938370
Epoch: 17600 Train: 31.301017761 Test: 11.579484940
Epoch: 17800 Train: 30.140319824 Test: 11.584768295
Epoch: 18000 Train: 29.753826141 Test: 11.604434013
Epoch: 18200 Train: 32.481975555 Test: 11.688399315
Epoch: 18400 Train: 35.130702972 Test: 11.846443176
Epoch: 18600 Train: 36.802776337 Test: 11.989150047
Epoch: 18800 Train: 37.960159302 Test: 12.043601990
Epoch: 19000 Train: 36.342994690 Test: 12.086627960
Epoch: 19200 Train: 34.951854706 Test: 12.158973694
Epoch: 19400 Train: 32.610637665 Test: 12.151246071
Epoch: 19600 Train: 30.159576416 Test: 12.138535500
Epoch: 19800 Train: 28.449134827 Test: 12.081581116
Epoch: 19999 Train: 27.289424896 Test: 12.065567017
Training Loss: tensor(27.2894)
Test Loss: tensor(12.0656)
True Mean x: tensor(3.1528, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(-0.0899, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.1648, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0066, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0751)
Jacobian term Test Loss: tensor(0.0818)
Learned LE: [ 0.4905595  -0.03097379]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
