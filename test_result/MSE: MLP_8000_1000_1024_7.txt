time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.04%, model saved.
Epoch: 0 Train: 3791.75513 Test: 4258.99707
Epoch 80: New minimal relative error: 49.30%, model saved.
Epoch: 80 Train: 39.19044 Test: 25.24824
Epoch 160: New minimal relative error: 31.90%, model saved.
Epoch: 160 Train: 22.67092 Test: 34.76545
Epoch: 240 Train: 10.88840 Test: 7.88342
Epoch: 320 Train: 0.95480 Test: 0.44521
Epoch: 400 Train: 0.80101 Test: 1.75102
Epoch: 480 Train: 2.46962 Test: 7.13980
Epoch: 560 Train: 8.87087 Test: 11.42791
Epoch: 640 Train: 5.79433 Test: 3.17587
Epoch 720: New minimal relative error: 31.73%, model saved.
Epoch: 720 Train: 0.99925 Test: 1.51236
Epoch 800: New minimal relative error: 31.38%, model saved.
Epoch: 800 Train: 2.20720 Test: 3.93375
Epoch 880: New minimal relative error: 25.77%, model saved.
Epoch: 880 Train: 4.93484 Test: 6.03562
Epoch: 960 Train: 1.44862 Test: 3.27274
Epoch 1040: New minimal relative error: 25.12%, model saved.
Epoch: 1040 Train: 0.88380 Test: 1.39199
Epoch 1120: New minimal relative error: 19.34%, model saved.
Epoch: 1120 Train: 2.93861 Test: 3.23290
Epoch: 1200 Train: 0.34982 Test: 0.38369
Epoch: 1280 Train: 1.84260 Test: 3.19589
Epoch: 1360 Train: 0.50301 Test: 0.66205
Epoch: 1440 Train: 2.17331 Test: 2.39976
Epoch: 1520 Train: 1.97889 Test: 1.85346
Epoch: 1600 Train: 1.68124 Test: 1.80310
Epoch: 1680 Train: 0.50222 Test: 0.50430
Epoch: 1760 Train: 1.13943 Test: 1.18982
Epoch: 1840 Train: 0.77982 Test: 0.96180
Epoch: 1920 Train: 1.33958 Test: 2.10976
Epoch: 2000 Train: 0.37209 Test: 0.54392
Epoch: 2080 Train: 4.74038 Test: 4.98027
Epoch: 2160 Train: 0.31329 Test: 0.40256
Epoch: 2240 Train: 0.24949 Test: 0.25231
Epoch: 2320 Train: 0.73211 Test: 0.74856
Epoch: 2400 Train: 0.57315 Test: 0.84530
Epoch: 2480 Train: 1.23590 Test: 0.88257
Epoch 2560: New minimal relative error: 16.60%, model saved.
Epoch: 2560 Train: 3.15006 Test: 3.08436
Epoch: 2640 Train: 0.52238 Test: 0.63114
Epoch: 2720 Train: 0.37775 Test: 0.36102
Epoch: 2800 Train: 0.68487 Test: 0.64338
Epoch 2880: New minimal relative error: 13.28%, model saved.
Epoch: 2880 Train: 0.30486 Test: 0.31008
Epoch: 2960 Train: 1.60693 Test: 1.75894
Epoch: 3040 Train: 1.26785 Test: 1.25620
Epoch: 3120 Train: 0.46874 Test: 0.62031
Epoch: 3200 Train: 0.31888 Test: 0.36349
Epoch: 3280 Train: 0.11431 Test: 0.05948
Epoch: 3360 Train: 0.27736 Test: 0.35528
Epoch: 3440 Train: 0.91831 Test: 1.05214
Epoch: 3520 Train: 0.64098 Test: 0.75984
Epoch: 3600 Train: 0.98561 Test: 1.09355
Epoch: 3680 Train: 0.27753 Test: 0.30777
Epoch: 3760 Train: 0.83606 Test: 1.06960
Epoch: 3840 Train: 1.89186 Test: 2.03741
Epoch: 3920 Train: 1.15496 Test: 1.03803
Epoch: 4000 Train: 0.39494 Test: 0.49677
Epoch: 4080 Train: 0.05652 Test: 0.06595
Epoch: 4160 Train: 0.04439 Test: 0.04229
Epoch: 4240 Train: 0.80568 Test: 0.88837
Epoch: 4320 Train: 0.85030 Test: 1.00672
Epoch: 4400 Train: 0.46799 Test: 0.46483
Epoch: 4480 Train: 0.27105 Test: 0.24294
Epoch: 4560 Train: 0.30968 Test: 0.34281
Epoch: 4640 Train: 0.03767 Test: 0.03888
Epoch: 4720 Train: 0.17779 Test: 0.17272
Epoch: 4800 Train: 0.13494 Test: 0.15174
Epoch: 4880 Train: 0.03423 Test: 0.05246
Epoch: 4960 Train: 0.04885 Test: 0.06100
Epoch: 5040 Train: 0.32123 Test: 0.37741
Epoch: 5120 Train: 0.13693 Test: 0.09367
Epoch: 5200 Train: 0.02102 Test: 0.02063
Epoch: 5280 Train: 0.27009 Test: 0.20906
Epoch: 5360 Train: 0.03160 Test: 0.04120
Epoch: 5440 Train: 0.09563 Test: 0.11769
Epoch: 5520 Train: 0.02355 Test: 0.02842
Epoch: 5600 Train: 0.04563 Test: 0.05492
Epoch: 5680 Train: 0.00985 Test: 0.01155
Epoch: 5760 Train: 0.03630 Test: 0.04281
Epoch: 5840 Train: 0.02216 Test: 0.02458
Epoch: 5920 Train: 0.01138 Test: 0.01391
Epoch: 6000 Train: 0.57057 Test: 0.55856
Epoch: 6080 Train: 0.02691 Test: 0.03804
Epoch: 6160 Train: 0.04466 Test: 0.04687
Epoch: 6240 Train: 0.13039 Test: 0.10148
Epoch: 6320 Train: 0.06079 Test: 0.07665
Epoch: 6400 Train: 0.23224 Test: 0.27737
Epoch: 6480 Train: 0.19376 Test: 0.18036
Epoch: 6560 Train: 0.05711 Test: 0.05767
Epoch: 6640 Train: 0.04531 Test: 0.05616
Epoch: 6720 Train: 0.24454 Test: 0.29656
Epoch: 6800 Train: 0.43127 Test: 0.47202
Epoch: 6880 Train: 0.01195 Test: 0.01877
Epoch: 6960 Train: 0.02258 Test: 0.02659
Epoch: 7040 Train: 0.02445 Test: 0.03050
Epoch: 7120 Train: 0.04552 Test: 0.04567
Epoch: 7200 Train: 0.64210 Test: 0.86420
Epoch: 7280 Train: 0.00564 Test: 0.00710
Epoch: 7360 Train: 0.00675 Test: 0.00803
Epoch: 7440 Train: 0.02220 Test: 0.02750
Epoch: 7520 Train: 0.05471 Test: 0.06806
Epoch: 7600 Train: 0.00716 Test: 0.00899
Epoch: 7680 Train: 0.01889 Test: 0.02001
Epoch: 7760 Train: 0.53033 Test: 0.61546
Epoch 7840: New minimal relative error: 11.57%, model saved.
Epoch: 7840 Train: 0.00710 Test: 0.01011
Epoch: 7920 Train: 0.00698 Test: 0.00956
Epoch: 7999 Train: 0.00451 Test: 0.00588
Training Loss: tensor(0.0045)
Test Loss: tensor(0.0059)
Learned LE: [ 0.868261   -0.01384713 -6.9313707 ]
True LE: [ 8.5358799e-01  9.2946589e-03 -1.4539361e+01]
Relative Error: [ 4.3507447   3.00789     1.6646825   1.0766898   2.0807204   3.3965285
  4.6393285   5.740092    6.6806026   7.454128    8.056347    8.484724
  8.741018    8.833926    8.782328    8.617147    8.380277    8.121506
  7.894989    7.7528477   7.733965    7.8561015   8.119246    8.512002
  9.009424    9.568626   10.136784   10.655961   11.06207    11.311009
 11.398637   11.3356     11.121473   10.750284   10.234955    9.620417
  8.970256    8.339387    7.75647     7.2275996   6.752376    6.3383555
  6.0041895   5.770463    5.6458445   5.6195693   5.66561     5.7541313
  5.861755    5.976221    6.096373    6.228929    6.383289    6.5646467
  6.7661076   6.9624944   7.109309    7.1476617   7.0155354   6.660475
  6.0520844   5.18956     4.1039157   2.857605    1.5675597   0.80051774
  1.6872959   2.90993     4.0567365   5.071559    5.94341     6.669361
  7.245779    7.669013    7.9381437   8.056802    8.036107    7.897911
  7.6743693   7.4066863   7.1450562   6.943871    6.848927    6.885027
  7.0566187   7.356539    7.764933    8.242739    8.742425    9.213332
  9.591896    9.829288    9.922343    9.885537    9.714215    9.391539
  8.92254     8.354107    7.757265    7.191281    6.68356     6.234005
  5.8357086   5.492751    5.223905    5.0510135   4.981729    5.0013433
  5.0792336   5.1828475   5.2889056   5.387031    5.4789805   5.575595
  5.6915803   5.839034    6.0184283   6.211211    6.37661     6.456437
  6.386318    6.110388    5.5947576   4.8363185   3.863848    2.734017
  1.5363041   0.6230617   1.3085947   2.4228356   3.4661884   4.3895974
  5.1883526   5.863261    6.412465    6.8323717   7.1204243   7.276091
  7.302765    7.2115684   7.022877    6.7663474   6.4842916   6.2319546
  6.0632806   6.012265    6.089853    6.293521    6.6091447   7.0025764
  7.4297667   7.849692    8.202425    8.4307375   8.530482    8.522029
  8.398106    8.129059    7.7088804   7.185096    6.6372433   6.130931
  5.693026    5.317818    4.990743    4.7112336   4.4981556   4.375803
  4.352446    4.410478    4.5152555   4.6317024   4.735969    4.818575
  4.8828735   4.941935    5.014538    5.1188674   5.264133    5.4405627
  5.6141677   5.72971     5.721611    5.529855    5.1158514   4.472221
  3.6246598   2.625867    1.5502387   0.5913204   0.9552692   1.9425836
  2.8755736   3.701769    4.42069     5.037507    5.5528398   5.9646235
  6.270738    6.4683623   6.554348    6.5289793   6.399115    6.17936
  5.8996425   5.6119695   5.3778424   5.242867    5.2268004   5.3327675
  5.5530353   5.8593946   6.209887    6.573289    6.899402    7.12087
  7.2262845   7.2444057   7.1705027   6.960977    6.5936713   6.1141524
  5.6104574   5.156107    4.7801967   4.472689    4.2105055   3.9862628
  3.8180022   3.7337854   3.745025    3.8333554   3.9608638   4.0899096
  4.195464    4.267799    4.310223    4.335949    4.365227    4.4208636
  4.520658    4.6659718   4.8334417   4.974228    5.023722    4.9181604
  4.612318    4.0921164   3.378237    2.5196152   1.5830872   0.68491894
  0.65322906  1.4816678   2.2980468   3.0219672   3.6535091   4.201993
  4.6718435   5.0636315   5.377956    5.6129484   5.7617216   5.815411
  5.767142    5.6145244   5.368982    5.071761    4.789033    4.57937
  4.474306    4.483934    4.607326    4.825043    5.096201    5.3945975
  5.6892624   5.905726    6.0145116   6.0526633   6.027173    5.883255
  5.576359    5.143339    4.6791983   4.26708     3.9420426   3.6934297
  3.4896982   3.3131306   3.1782084   3.1176543   3.1495578   3.2579262
  3.4032135   3.5451128   3.656793    3.7273114   3.7583644   3.76124
  3.7545073   3.7624717   3.8093355   3.908772    4.051797    4.2011757
  4.297407    4.2745275   4.0796566   3.6890779   3.114876    2.40054
  1.6088923   0.8242177   0.46398765  1.0590663   1.7515007   2.3704636
  2.9085095   3.3778741   3.7872787   4.1414      4.444937    4.7006893
  4.902725    5.0371294   5.0860243   5.030496    4.8591638   4.591039
  4.2880487   4.022103    3.8387704   3.7578132   3.7850528   3.9121377
  4.1038203   4.3290133   4.579771    4.7915287   4.9032483   4.949914
  4.963686    4.889435    4.654153    4.2745695   3.847893    3.4674792
  3.178614    2.9767075   2.8247957   2.690963    2.580035    2.527764
  2.5630052   2.6774442   2.8324258   2.9854348   3.1072795   3.1848617
  3.2175627   3.213069    3.1857777   3.1566553   3.1519015   3.195068
  3.2940097   3.4291375   3.5525966   3.6004357   3.5129867   3.254221
  2.8234072   2.2543325   1.6052376   0.9485344   0.45995834  0.7055941
  1.2572856   1.7720293   2.2151763   2.5975053   2.9320915   3.228163
  3.4955454   3.7442513   3.9744534   4.1731052   4.3179855   4.378424
  4.3216014   4.133858    3.8539827   3.5618079   3.3211389   3.1627088
  3.099539    3.1347322   3.2471385   3.397208  ]
