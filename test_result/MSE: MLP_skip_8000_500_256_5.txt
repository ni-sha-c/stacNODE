time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.59%, model saved.
Epoch: 0 Train: 4004.92944 Test: 4321.71777
Epoch 80: New minimal relative error: 85.30%, model saved.
Epoch: 80 Train: 192.83913 Test: 174.33473
Epoch 160: New minimal relative error: 36.63%, model saved.
Epoch: 160 Train: 34.51776 Test: 38.10375
Epoch: 240 Train: 39.35712 Test: 75.99551
Epoch 320: New minimal relative error: 15.62%, model saved.
Epoch: 320 Train: 8.13839 Test: 9.12876
Epoch: 400 Train: 6.19736 Test: 6.69750
Epoch: 480 Train: 6.48852 Test: 7.57996
Epoch: 560 Train: 15.82476 Test: 12.37607
Epoch: 640 Train: 4.03994 Test: 4.17170
Epoch: 720 Train: 3.72189 Test: 3.84836
Epoch: 800 Train: 5.18186 Test: 5.33449
Epoch: 880 Train: 5.91582 Test: 6.43184
Epoch 960: New minimal relative error: 14.56%, model saved.
Epoch: 960 Train: 3.90538 Test: 4.02112
Epoch: 1040 Train: 3.50092 Test: 3.64181
Epoch: 1120 Train: 3.39006 Test: 3.31583
Epoch: 1200 Train: 5.03036 Test: 5.50470
Epoch: 1280 Train: 3.09304 Test: 3.32730
Epoch: 1360 Train: 6.83634 Test: 6.78794
Epoch: 1440 Train: 2.63021 Test: 2.71751
Epoch: 1520 Train: 4.79414 Test: 5.17349
Epoch: 1600 Train: 5.39297 Test: 5.52544
Epoch: 1680 Train: 2.17515 Test: 2.21305
Epoch: 1760 Train: 2.32006 Test: 2.42060
Epoch: 1840 Train: 1.94937 Test: 1.95921
Epoch 1920: New minimal relative error: 9.37%, model saved.
Epoch: 1920 Train: 1.84619 Test: 1.83484
Epoch: 2000 Train: 1.81323 Test: 1.80721
Epoch: 2080 Train: 1.85695 Test: 1.71728
Epoch: 2160 Train: 1.56992 Test: 1.61371
Epoch: 2240 Train: 1.69725 Test: 1.46568
Epoch 2320: New minimal relative error: 7.92%, model saved.
Epoch: 2320 Train: 1.66969 Test: 1.46366
Epoch: 2400 Train: 1.69015 Test: 1.78569
Epoch: 2480 Train: 3.43029 Test: 4.45474
Epoch: 2560 Train: 1.02649 Test: 0.99020
Epoch: 2640 Train: 1.19103 Test: 1.13844
Epoch: 2720 Train: 0.88693 Test: 0.88443
Epoch: 2800 Train: 0.83859 Test: 0.84235
Epoch: 2880 Train: 0.78534 Test: 0.78553
Epoch: 2960 Train: 1.02948 Test: 0.77190
Epoch: 3040 Train: 3.00398 Test: 2.85610
Epoch: 3120 Train: 1.16077 Test: 1.08982
Epoch: 3200 Train: 0.72010 Test: 0.82237
Epoch: 3280 Train: 1.11150 Test: 1.43264
Epoch: 3360 Train: 0.56784 Test: 0.57858
Epoch: 3440 Train: 0.54225 Test: 0.54826
Epoch: 3520 Train: 0.53555 Test: 0.53581
Epoch: 3600 Train: 0.52805 Test: 0.59631
Epoch: 3680 Train: 1.03186 Test: 1.28428
Epoch: 3760 Train: 0.48797 Test: 0.49887
Epoch 3840: New minimal relative error: 7.69%, model saved.
Epoch: 3840 Train: 0.44601 Test: 0.45785
Epoch: 3920 Train: 1.77668 Test: 1.71784
Epoch: 4000 Train: 0.41519 Test: 0.42539
Epoch: 4080 Train: 0.54568 Test: 0.53525
Epoch: 4160 Train: 0.38995 Test: 0.40046
Epoch: 4240 Train: 0.41134 Test: 0.44169
Epoch: 4320 Train: 0.36713 Test: 0.37817
Epoch: 4400 Train: 0.36552 Test: 0.37879
Epoch: 4480 Train: 0.76573 Test: 0.96750
Epoch: 4560 Train: 0.33888 Test: 0.35106
Epoch: 4640 Train: 0.73186 Test: 1.03407
Epoch: 4720 Train: 0.31946 Test: 0.33084
Epoch: 4800 Train: 0.75555 Test: 0.92700
Epoch: 4880 Train: 0.30318 Test: 0.31334
Epoch 4960: New minimal relative error: 6.49%, model saved.
Epoch: 4960 Train: 0.32560 Test: 0.32580
Epoch: 5040 Train: 0.28897 Test: 0.29887
Epoch: 5120 Train: 0.28613 Test: 0.29405
Epoch: 5200 Train: 0.27631 Test: 0.28645
Epoch: 5280 Train: 0.29090 Test: 0.28951
Epoch: 5360 Train: 0.26823 Test: 0.28016
Epoch: 5440 Train: 0.27092 Test: 0.28629
Epoch: 5520 Train: 0.26034 Test: 0.27133
Epoch: 5600 Train: 1.48670 Test: 1.15117
Epoch 5680: New minimal relative error: 4.58%, model saved.
Epoch: 5680 Train: 0.24582 Test: 0.25599
Epoch: 5760 Train: 1.36346 Test: 1.67559
Epoch: 5840 Train: 0.23750 Test: 0.24649
Epoch: 5920 Train: 0.23381 Test: 0.24361
Epoch: 6000 Train: 0.24057 Test: 0.25886
Epoch: 6080 Train: 0.26746 Test: 0.26274
Epoch: 6160 Train: 0.31711 Test: 0.30826
Epoch: 6240 Train: 0.90100 Test: 0.92334
Epoch: 6320 Train: 0.21673 Test: 0.22460
Epoch: 6400 Train: 0.21087 Test: 0.21948
Epoch: 6480 Train: 0.22501 Test: 0.22151
Epoch: 6560 Train: 0.21667 Test: 0.25256
Epoch: 6640 Train: 0.20398 Test: 0.21485
Epoch: 6720 Train: 0.19842 Test: 0.20784
Epoch: 6800 Train: 0.24435 Test: 0.24273
Epoch: 6880 Train: 0.54217 Test: 0.61503
Epoch: 6960 Train: 0.19028 Test: 0.19910
Epoch: 7040 Train: 0.21709 Test: 0.22077
Epoch: 7120 Train: 0.18550 Test: 0.19461
Epoch: 7200 Train: 0.18672 Test: 0.19557
Epoch: 7280 Train: 0.18248 Test: 0.18982
Epoch: 7360 Train: 0.21361 Test: 0.22877
Epoch: 7440 Train: 0.17715 Test: 0.18608
Epoch: 7520 Train: 0.19607 Test: 0.21634
Epoch: 7600 Train: 0.17321 Test: 0.18249
Epoch: 7680 Train: 0.53797 Test: 0.51674
Epoch: 7760 Train: 0.16929 Test: 0.17820
Epoch: 7840 Train: 0.18320 Test: 0.18615
Epoch: 7920 Train: 0.16573 Test: 0.17438
Epoch: 7999 Train: 0.20194 Test: 0.18641
Training Loss: tensor(0.2019)
Test Loss: tensor(0.1864)
Learned LE: [ 0.65495086  0.04392041 -3.0530174 ]
True LE: [ 8.7053740e-01  3.0987842e-03 -1.4552080e+01]
Relative Error: [2.0386856  2.332032   2.7614398  3.1158495  3.3956149  3.52846
 3.512457   3.4101968  3.1901994  3.1775522  2.6350288  1.9823622
 1.448206   0.7251577  0.9550401  2.0980074  2.6211088  2.6387663
 2.5923653  2.621623   2.5567582  2.3186564  1.8045602  1.3448291
 1.061203   0.90428525 0.79923105 0.74918234 0.82723284 1.0965278
 1.4214501  1.4012911  1.3628762  0.93300194 0.86215025 1.214104
 1.7250782  1.539941   1.6147641  1.5184512  1.6803463  2.1100318
 2.5107868  2.6301975  2.8346283  2.900629   3.2051847  3.3700128
 3.3989997  3.2599866  2.6478286  2.1736739  1.8318299  1.6907407
 1.5594507  1.368496   1.1251336  0.98695076 1.0888338  1.2562779
 1.439827   1.4031749  1.4707538  1.8135157  2.3068812  2.8079002
 3.3023443  3.5801544  3.5336494  3.38212    3.0936127  3.0231428
 2.5402756  1.9313723  1.5648782  0.62069356 0.8639208  2.0685048
 2.0773807  2.266233   2.335329   2.4428594  2.4942675  2.1079297
 1.6185409  1.178529   0.8456555  0.62669015 0.4295812  0.24979112
 0.32337546 0.7551116  1.3401212  1.5756799  1.5506823  0.97454095
 0.95080066 1.2126708  1.6166393  1.5102117  1.4642076  1.4464422
 1.5441812  1.8833431  2.221894   2.3589811  2.4349763  2.4771056
 2.915006   2.9383729  2.8861384  2.7926774  2.2485366  1.7685732
 1.5861273  1.5688539  1.4846061  1.3252038  1.0686737  0.7535195
 0.7565482  0.7573896  0.67335236 0.69703746 0.84197855 1.224804
 1.780506   2.3091247  2.8102462  3.2773826  3.690289   3.4956415
 3.164414   2.9102182  2.6084785  1.933617   1.660352   0.68866044
 0.95464605 1.4133189  1.5538931  1.9257843  2.0881343  2.2930932
 2.3039026  1.9520285  1.4885645  1.1120081  0.7680916  0.5655534
 0.45132256 0.41633183 0.5439644  0.83013195 1.295867   1.6357379
 1.6970253  1.1602045  0.9662868  1.193415   1.5633624  1.6048458
 1.3980403  1.3734721  1.3915203  1.6559541  1.9783763  2.1656866
 1.9857401  2.227093   2.6142545  2.4674673  2.3823457  2.283055
 1.8333163  1.4272207  1.4485118  1.5561042  1.5077604  1.396262
 1.1116881  0.80987585 0.5680485  0.30090937 0.09493344 0.28867784
 0.6657318  0.897689   1.3257469  1.8633943  2.353547   2.7748542
 3.164459   3.5663981  3.395594   2.9512103  2.7111745  2.1210394
 1.6566256  0.8351868  0.9359499  0.9241627  1.041423   1.6074889
 1.8745847  2.1526299  2.0770774  1.7407705  1.440227   1.0679183
 0.8342782  0.7321634  0.7381591  0.7576215  0.71111345 0.9237631
 1.3470999  1.4964843  1.8077759  1.4618213  1.0239396  1.2442107
 1.4757265  1.7503085  1.446927   1.3211248  1.2314693  1.4230196
 1.7650095  1.9249918  1.785886   1.8652645  2.229999   1.9813061
 1.8225299  1.6921208  1.4574256  1.1552614  1.417874   1.5816882
 1.5748507  1.5194658  1.2430081  0.98543066 0.8850343  0.7931653
 0.72687346 0.806127   1.0644672  1.1381986  1.0756828  1.1339922
 1.4613345  1.8362468  2.2265165  2.6347     2.9427612  2.9264283
 2.945877   2.467856   1.772656   1.1216108  0.7761422  0.57821316
 0.64856315 1.2901343  1.6632087  1.9784322  1.8879583  1.5289367
 1.2875879  1.0796168  0.93335706 0.8868686  0.88461256 0.82793844
 0.76768214 0.9874979  1.3879244  1.3404887  1.64735    2.0304956
 1.2882731  1.0774857  1.4285109  1.7181947  1.5922807  1.3020735
 1.1039445  1.200343   1.4507871  1.643274   1.6973668  1.560575
 1.6997721  1.5789818  1.268067   1.1084576  1.0098783  0.91488147
 1.1531115  1.3605149  1.421284   1.3957063  1.2019435  1.1128987
 1.2533833  1.2251673  1.3418792  1.3561784  1.517562   1.447237
 0.8514274  0.45892373 0.7336461  1.1233666  1.44611    1.760179
 2.091014   2.3450983  2.3808599  2.528099   2.0753334  1.5220813
 0.6308172  0.30317515 0.4107015  1.0614073  1.4820784  1.7692943
 1.7306162  1.3050656  1.1053338  1.0672057  0.99030596 0.95834666
 0.86607945 0.75682133 0.7348385  0.9800082  1.4031876  1.24627
 1.2189237  1.7956551  1.8141999  1.1006749  1.0973369  1.5137795
 1.7993929  1.4047534  1.1340398  1.0546827  1.1573143  1.4059004
 1.5290266  1.3945045  1.215295   1.1973051  0.79466474 0.58273464
 0.7384942  0.77480936 0.8801093  1.1723703  1.3294744  1.3639325
 1.195154   0.9281163  0.89832777 1.2078776  1.7167115  1.963473
 2.0063922  1.8090317  1.1628488  0.55415535 0.12560342 0.6231615
 0.9473513  1.1479998  1.3578415  1.5869521  1.8071914  1.9514734
 1.7310792  1.9230956  1.0761896  0.63330233 0.5490819  0.8683844
 1.3858553  1.5566895  1.4840397  1.1747545  0.9787132  0.89683604
 0.8735461  0.90667146 0.7441977  0.61517584]
