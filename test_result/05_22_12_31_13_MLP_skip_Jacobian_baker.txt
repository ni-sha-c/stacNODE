time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 20000
num_train: 2000
num_test: 2000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 3
reg_param: 100.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 199.384231567 Test: 5.259116173
Epoch 0: New minimal relative error: 5.26%, model saved.
Epoch: 200 Train: 12.682561874 Test: 10.270914078
Epoch: 400 Train: 13.255254745 Test: 9.965360641
Epoch: 600 Train: 12.529704094 Test: 10.530305862
Epoch: 800 Train: 13.213086128 Test: 10.167435646
Epoch: 1000 Train: 12.542568207 Test: 10.285060883
Epoch: 1200 Train: 12.448781967 Test: 10.507912636
Epoch: 1400 Train: 12.368562698 Test: 10.589966774
Epoch: 1600 Train: 12.345846176 Test: 10.369418144
Epoch: 1800 Train: 12.115808487 Test: 10.462254524
Epoch: 2000 Train: 12.179366112 Test: 10.601848602
Epoch: 2200 Train: 12.313077927 Test: 10.614211082
Epoch: 2400 Train: 12.314136505 Test: 10.377932549
Epoch: 2600 Train: 12.137154579 Test: 10.679241180
Epoch: 2800 Train: 12.263363838 Test: 10.638208389
Epoch: 3000 Train: 12.944690704 Test: 10.562502861
Epoch: 3200 Train: 12.704799652 Test: 10.845680237
Epoch: 3400 Train: 13.832017899 Test: 10.893930435
Epoch: 3600 Train: 12.947534561 Test: 10.249101639
Epoch: 3800 Train: 13.161739349 Test: 10.723087311
Epoch: 4000 Train: 12.775164604 Test: 10.006967545
Epoch: 4200 Train: 13.027378082 Test: 9.798072815
Epoch: 4400 Train: 15.295972824 Test: 9.613586426
Epoch: 4600 Train: 13.844703674 Test: 10.365084648
Epoch: 4800 Train: 14.290349960 Test: 9.531369209
Epoch: 5000 Train: 13.712851524 Test: 9.723401070
Epoch: 5200 Train: 14.817336082 Test: 9.707207680
Epoch: 5400 Train: 14.419961929 Test: 9.874609947
Epoch: 5600 Train: 14.535009384 Test: 10.054170609
Epoch: 5800 Train: 15.580451965 Test: 10.276336670
Epoch: 6000 Train: 18.911432266 Test: 10.315482140
Epoch: 6200 Train: 23.185070038 Test: 10.334878922
Epoch: 6400 Train: 25.103315353 Test: 10.378801346
Epoch: 6600 Train: 19.161458969 Test: 9.945040703
Epoch: 6800 Train: 16.125890732 Test: 9.978785515
Epoch: 7000 Train: 13.940912247 Test: 10.221884727
Epoch: 7200 Train: 15.036508560 Test: 10.447582245
Epoch: 7400 Train: 15.454929352 Test: 10.368117332
Epoch: 7600 Train: 15.625915527 Test: 10.414008141
Epoch: 7800 Train: 16.147319794 Test: 10.449903488
Epoch: 8000 Train: 17.297405243 Test: 10.778504372
Epoch: 8200 Train: 16.244941711 Test: 10.744632721
Epoch: 8400 Train: 15.840486526 Test: 10.617192268
Epoch: 8600 Train: 16.160797119 Test: 10.424731255
Epoch: 8800 Train: 14.833555222 Test: 10.311092377
Epoch: 9000 Train: 14.340524673 Test: 10.388782501
Epoch: 9200 Train: 15.686939240 Test: 10.468947411
Epoch: 9400 Train: 19.933368683 Test: 10.567949295
Epoch: 9600 Train: 21.866779327 Test: 10.727517128
Epoch: 9800 Train: 18.743770599 Test: 10.755615234
Epoch: 10000 Train: 17.408235550 Test: 10.475475311
Epoch: 10200 Train: 17.319158554 Test: 10.456493378
Epoch: 10400 Train: 17.046146393 Test: 10.449986458
Epoch: 10600 Train: 16.819515228 Test: 10.403431892
Epoch: 10800 Train: 15.319274902 Test: 10.400282860
Epoch: 11000 Train: 13.970262527 Test: 10.441823006
Epoch: 11200 Train: 13.738204002 Test: 10.558500290
Epoch: 11400 Train: 14.369469643 Test: 10.599871635
Epoch: 11600 Train: 14.766746521 Test: 10.558895111
Epoch: 11800 Train: 15.878290176 Test: 10.482817650
Epoch: 12000 Train: 17.208187103 Test: 10.514327049
Epoch: 12200 Train: 18.380989075 Test: 10.531808853
Epoch: 12400 Train: 20.093673706 Test: 10.542627335
Epoch: 12600 Train: 21.954841614 Test: 10.586646080
Epoch: 12800 Train: 22.857154846 Test: 10.624723434
Epoch: 13000 Train: 22.537906647 Test: 10.664120674
Epoch: 13200 Train: 20.962173462 Test: 10.724286079
Epoch: 13400 Train: 21.941503525 Test: 10.893069267
Epoch: 13600 Train: 21.204174042 Test: 10.851706505
Epoch: 13800 Train: 21.187149048 Test: 10.818479538
Epoch: 14000 Train: 19.951705933 Test: 10.646424294
Epoch: 14200 Train: 19.257970810 Test: 10.488808632
Epoch: 14400 Train: 19.565000534 Test: 10.435510635
Epoch: 14600 Train: 19.204826355 Test: 10.523658752
Epoch: 14800 Train: 21.328315735 Test: 10.612117767
Epoch: 15000 Train: 21.600364685 Test: 10.650302887
Epoch: 15200 Train: 21.195163727 Test: 10.556544304
Epoch: 15400 Train: 20.311407089 Test: 10.461062431
Epoch: 15600 Train: 19.215240479 Test: 10.363781929
Epoch: 15800 Train: 18.561672211 Test: 10.226416588
Epoch: 16000 Train: 19.246974945 Test: 10.160305977
Epoch: 16200 Train: 19.661373138 Test: 10.080440521
Epoch: 16400 Train: 19.945589066 Test: 10.075286865
Epoch: 16600 Train: 20.614402771 Test: 10.118357658
Epoch: 16800 Train: 20.919769287 Test: 10.187442780
Epoch: 17000 Train: 21.046388626 Test: 10.239098549
Epoch: 17200 Train: 21.198734283 Test: 10.252080917
Epoch: 17400 Train: 21.670236588 Test: 10.260652542
Epoch: 17600 Train: 21.913375854 Test: 10.267109871
Epoch: 17800 Train: 22.674415588 Test: 10.278459549
Epoch: 18000 Train: 22.931915283 Test: 10.276655197
Epoch: 18200 Train: 23.764659882 Test: 10.292353630
Epoch: 18400 Train: 25.008670807 Test: 10.313239098
Epoch: 18600 Train: 25.678823471 Test: 10.324106216
Epoch: 18800 Train: 25.896917343 Test: 10.316129684
Epoch: 19000 Train: 26.333057404 Test: 10.309894562
Epoch: 19200 Train: 26.771183014 Test: 10.285049438
Epoch: 19400 Train: 26.860212326 Test: 10.262036324
Epoch: 19600 Train: 27.446098328 Test: 10.241260529
Epoch: 19800 Train: 27.212572098 Test: 10.227169991
Epoch: 19999 Train: 27.420745850 Test: 10.214664459
Training Loss: tensor(27.4207)
Test Loss: tensor(10.2147)
True Mean x: tensor(3.1528, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(-0.0869, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.1648, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0066, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.1705)
Jacobian term Test Loss: tensor(0.1807)
Learned LE: [ 0.5059514 -0.0302096]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
