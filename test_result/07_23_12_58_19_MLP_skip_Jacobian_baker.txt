time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 200.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 12.570914268 Test: 10.132476807
Epoch 0: New minimal relative error: 10.13%, model saved.
Epoch: 100 Train: 2.486301184 Test: 2.532820463
Epoch 100: New minimal relative error: 2.53%, model saved.
Epoch: 200 Train: 2.217324018 Test: 2.224267483
Epoch 200: New minimal relative error: 2.22%, model saved.
Epoch: 300 Train: 2.150266171 Test: 2.170806170
Epoch 300: New minimal relative error: 2.17%, model saved.
Epoch: 400 Train: 2.191551685 Test: 2.188994884
Epoch: 500 Train: 2.143283844 Test: 2.150303841
Epoch 500: New minimal relative error: 2.15%, model saved.
Epoch: 600 Train: 2.184388876 Test: 2.200247526
Epoch: 700 Train: 2.144919395 Test: 2.154000759
Epoch: 800 Train: 2.166273117 Test: 2.179562330
Epoch: 900 Train: 2.171603918 Test: 2.185885906
Epoch: 1000 Train: 2.179755211 Test: 2.200566292
Epoch: 1100 Train: 2.178500175 Test: 2.196169376
Epoch: 1200 Train: 2.206231594 Test: 2.223261118
Epoch: 1300 Train: 2.221347332 Test: 2.232711077
Epoch: 1400 Train: 2.224331856 Test: 2.239709854
Epoch: 1500 Train: 2.233823776 Test: 2.256361961
Epoch: 1600 Train: 2.226510763 Test: 2.239876509
Epoch: 1700 Train: 2.208661556 Test: 2.226585388
Epoch: 1800 Train: 2.211360693 Test: 2.216142178
Epoch: 1900 Train: 2.173209667 Test: 2.187794209
Epoch: 2000 Train: 2.138043404 Test: 2.145193100
Epoch 2000: New minimal relative error: 2.15%, model saved.
Epoch: 2100 Train: 2.140430450 Test: 2.150513172
Epoch: 2200 Train: 2.081384659 Test: 2.068437338
Epoch 2200: New minimal relative error: 2.07%, model saved.
Epoch: 2300 Train: 2.136654854 Test: 2.140400887
Epoch: 2400 Train: 2.133904934 Test: 2.145583391
Epoch: 2500 Train: 2.143832684 Test: 2.151848316
Epoch: 2600 Train: 2.145021677 Test: 2.147389412
Epoch: 2700 Train: 2.134125233 Test: 2.142463207
Epoch: 2800 Train: 2.159530163 Test: 2.160609961
Epoch: 2900 Train: 2.149528980 Test: 2.150432587
Epoch: 3000 Train: 2.159662962 Test: 2.158711195
Epoch: 3100 Train: 2.143219709 Test: 2.143355608
Epoch: 3200 Train: 2.157549381 Test: 2.159695625
Epoch: 3300 Train: 2.171494246 Test: 2.173315763
Epoch: 3400 Train: 2.179385424 Test: 2.183944702
Epoch: 3500 Train: 2.172019005 Test: 2.175929070
Epoch: 3600 Train: 2.176893950 Test: 2.192316532
Epoch: 3700 Train: 2.181178093 Test: 2.194877148
Epoch: 3800 Train: 2.184965372 Test: 2.204168797
Epoch: 3900 Train: 2.176906109 Test: 2.199422836
Epoch: 4000 Train: 2.191518068 Test: 2.218677282
Epoch: 4100 Train: 2.183219671 Test: 2.192317009
Epoch: 4200 Train: 2.179065228 Test: 2.189126492
Epoch: 4300 Train: 2.203332901 Test: 2.225531340
Epoch: 4400 Train: 2.237385988 Test: 2.252129078
Epoch: 4500 Train: 2.220041752 Test: 2.254078150
Epoch: 4600 Train: 2.228577852 Test: 2.256708145
Epoch: 4700 Train: 2.240494251 Test: 2.261989117
Epoch: 4800 Train: 2.244648695 Test: 2.262388229
Epoch: 4900 Train: 2.266355515 Test: 2.284668922
Epoch: 5000 Train: 2.255806684 Test: 2.275157928
Epoch: 5100 Train: 2.268596411 Test: 2.284775019
Epoch: 5200 Train: 2.270094872 Test: 2.288733959
Epoch: 5300 Train: 2.270819187 Test: 2.291738510
Epoch: 5400 Train: 2.273203850 Test: 2.290962696
Epoch: 5500 Train: 2.245514154 Test: 2.258758783
Epoch: 5600 Train: 2.258456469 Test: 2.270587444
Epoch: 5700 Train: 2.262986898 Test: 2.278580189
Epoch: 5800 Train: 2.278938532 Test: 2.291855335
Epoch: 5900 Train: 2.268471956 Test: 2.285231829
Epoch: 6000 Train: 2.267230034 Test: 2.284902573
Epoch: 6100 Train: 2.269983292 Test: 2.290996552
Epoch: 6200 Train: 2.280638218 Test: 2.300714016
Epoch: 6300 Train: 2.278870106 Test: 2.299657822
Epoch: 6400 Train: 2.279617548 Test: 2.299466610
Epoch: 6500 Train: 2.265545368 Test: 2.285235882
Epoch: 6600 Train: 2.261327028 Test: 2.280136347
Epoch: 6700 Train: 2.265538692 Test: 2.286021471
Epoch: 6800 Train: 2.263850212 Test: 2.282567263
Epoch: 6900 Train: 2.262156248 Test: 2.279449701
Epoch: 7000 Train: 2.265073299 Test: 2.283689976
Epoch: 7100 Train: 2.270400286 Test: 2.289123774
Epoch: 7200 Train: 2.270993948 Test: 2.288281202
Epoch: 7300 Train: 2.267522812 Test: 2.285459518
Epoch: 7400 Train: 2.262268066 Test: 2.280808449
Epoch: 7500 Train: 2.259422302 Test: 2.278505325
Epoch: 7600 Train: 2.260007381 Test: 2.280402422
Epoch: 7700 Train: 2.256026983 Test: 2.274606705
Epoch: 7800 Train: 2.257837296 Test: 2.278795242
Epoch: 7900 Train: 2.261814356 Test: 2.279117346
Epoch: 8000 Train: 2.270294189 Test: 2.290745258
Epoch: 8100 Train: 2.280035019 Test: 2.303940296
Epoch: 8200 Train: 2.265597820 Test: 2.285242081
Epoch: 8300 Train: 2.259160042 Test: 2.279366970
Epoch: 8400 Train: 2.256356239 Test: 2.276168823
Epoch: 8500 Train: 2.257431746 Test: 2.278941870
Epoch: 8600 Train: 2.261376858 Test: 2.284376144
Epoch: 8700 Train: 2.262457609 Test: 2.286756277
Epoch: 8800 Train: 2.271948814 Test: 2.297133684
Epoch: 8900 Train: 2.292769432 Test: 2.315104961
Epoch: 9000 Train: 2.282835484 Test: 2.304253578
Epoch: 9100 Train: 2.275584936 Test: 2.290490150
Epoch: 9200 Train: 2.276721478 Test: 2.296042919
Epoch: 9300 Train: 2.281711578 Test: 2.303037643
Epoch: 9400 Train: 2.288208485 Test: 2.311291218
Epoch: 9500 Train: 2.298972607 Test: 2.325438738
Epoch: 9600 Train: 2.305646420 Test: 2.331497431
Epoch: 9700 Train: 2.297631502 Test: 2.322449207
Epoch: 9800 Train: 2.293633223 Test: 2.320170879
Epoch: 9900 Train: 2.288411379 Test: 2.310504675
Epoch: 9999 Train: 2.282055855 Test: 2.301372766
Training Loss: tensor(2.2821)
Test Loss: tensor(2.3014)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0039)
Jacobian term Test Loss: tensor(0.0041)
Learned LE: [5.644994  0.0308017]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
