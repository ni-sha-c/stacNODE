time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 3659.41235 Test: 4114.01367
Epoch 80: New minimal relative error: 69.90%, model saved.
Epoch: 80 Train: 321.23233 Test: 393.80667
Epoch 160: New minimal relative error: 34.64%, model saved.
Epoch: 160 Train: 97.90354 Test: 69.49309
Epoch: 240 Train: 25.61277 Test: 24.05407
Epoch 320: New minimal relative error: 26.80%, model saved.
Epoch: 320 Train: 25.15324 Test: 16.89286
Epoch 400: New minimal relative error: 13.90%, model saved.
Epoch: 400 Train: 12.96167 Test: 7.44934
Epoch: 480 Train: 8.45347 Test: 9.17713
Epoch: 560 Train: 14.30156 Test: 13.07770
Epoch: 640 Train: 5.87950 Test: 3.58227
Epoch: 720 Train: 3.15618 Test: 2.77689
Epoch: 800 Train: 3.82228 Test: 4.48673
Epoch: 880 Train: 6.40707 Test: 4.98667
Epoch: 960 Train: 2.60377 Test: 3.29299
Epoch: 1040 Train: 7.75753 Test: 10.80559
Epoch: 1120 Train: 6.47331 Test: 5.66255
Epoch: 1200 Train: 4.84321 Test: 5.04229
Epoch: 1280 Train: 3.16814 Test: 3.03070
Epoch: 1360 Train: 0.72922 Test: 0.94738
Epoch: 1440 Train: 1.37653 Test: 1.50769
Epoch: 1520 Train: 3.13185 Test: 3.33546
Epoch: 1600 Train: 0.38533 Test: 0.53460
Epoch: 1680 Train: 0.89833 Test: 0.75050
Epoch 1760: New minimal relative error: 11.97%, model saved.
Epoch: 1760 Train: 0.35317 Test: 0.39549
Epoch: 1840 Train: 0.32755 Test: 0.34090
Epoch: 1920 Train: 0.30451 Test: 0.27130
Epoch 2000: New minimal relative error: 8.88%, model saved.
Epoch: 2000 Train: 0.69944 Test: 0.77403
Epoch: 2080 Train: 0.99351 Test: 0.90979
Epoch: 2160 Train: 0.73085 Test: 0.86745
Epoch: 2240 Train: 1.24178 Test: 1.29933
Epoch: 2320 Train: 0.68448 Test: 0.74569
Epoch: 2400 Train: 2.42439 Test: 2.68448
Epoch: 2480 Train: 1.62587 Test: 1.11690
Epoch: 2560 Train: 0.98587 Test: 1.26782
Epoch 2640: New minimal relative error: 8.45%, model saved.
Epoch: 2640 Train: 0.55606 Test: 0.52765
Epoch: 2720 Train: 1.18372 Test: 1.05502
Epoch: 2800 Train: 1.21307 Test: 1.40917
Epoch: 2880 Train: 1.05300 Test: 1.45976
Epoch: 2960 Train: 0.19472 Test: 0.20754
Epoch: 3040 Train: 1.30709 Test: 1.04952
Epoch: 3120 Train: 0.41524 Test: 0.62473
Epoch: 3200 Train: 2.10950 Test: 1.99018
Epoch: 3280 Train: 0.11487 Test: 0.30067
Epoch: 3360 Train: 2.09591 Test: 2.00025
Epoch: 3440 Train: 3.91852 Test: 2.75666
Epoch 3520: New minimal relative error: 3.48%, model saved.
Epoch: 3520 Train: 0.09430 Test: 0.10629
Epoch: 3600 Train: 0.92482 Test: 0.96438
Epoch: 3680 Train: 0.53912 Test: 0.61339
Epoch: 3760 Train: 0.43219 Test: 0.40731
Epoch: 3840 Train: 0.08646 Test: 0.07228
Epoch: 3920 Train: 0.25520 Test: 0.29965
Epoch: 4000 Train: 0.18192 Test: 0.13175
Epoch: 4080 Train: 0.13044 Test: 0.14593
Epoch: 4160 Train: 0.44176 Test: 0.51030
Epoch: 4240 Train: 0.77444 Test: 0.16725
Epoch: 4320 Train: 0.09888 Test: 0.12160
Epoch: 4400 Train: 1.16295 Test: 1.33319
Epoch: 4480 Train: 1.57072 Test: 1.88449
Epoch: 4560 Train: 0.12155 Test: 0.14498
Epoch: 4640 Train: 0.05216 Test: 0.05409
Epoch: 4720 Train: 0.04232 Test: 0.04836
Epoch: 4800 Train: 0.06923 Test: 0.07076
Epoch: 4880 Train: 0.07814 Test: 0.08321
Epoch: 4960 Train: 0.05223 Test: 0.06272
Epoch: 5040 Train: 0.65548 Test: 0.82343
Epoch: 5120 Train: 0.04954 Test: 0.05499
Epoch: 5200 Train: 0.04295 Test: 0.04709
Epoch: 5280 Train: 0.06016 Test: 0.09326
Epoch: 5360 Train: 0.03812 Test: 0.06173
Epoch: 5440 Train: 0.03734 Test: 0.05241
Epoch: 5520 Train: 1.71189 Test: 1.82409
Epoch 5600: New minimal relative error: 2.69%, model saved.
Epoch: 5600 Train: 0.03321 Test: 0.03983
Epoch: 5680 Train: 0.06002 Test: 0.06627
Epoch: 5760 Train: 0.04842 Test: 0.06398
Epoch: 5840 Train: 0.09978 Test: 0.09372
Epoch: 5920 Train: 0.05734 Test: 0.07354
Epoch: 6000 Train: 0.03223 Test: 0.03745
Epoch: 6080 Train: 0.03925 Test: 0.07242
Epoch: 6160 Train: 0.49272 Test: 0.66588
Epoch: 6240 Train: 0.02744 Test: 0.03396
Epoch: 6320 Train: 0.07039 Test: 0.16746
Epoch: 6400 Train: 0.19060 Test: 0.25357
Epoch: 6480 Train: 0.02615 Test: 0.03293
Epoch: 6560 Train: 0.03520 Test: 0.05460
Epoch: 6640 Train: 0.14227 Test: 0.19303
Epoch: 6720 Train: 0.09497 Test: 0.12080
Epoch: 6800 Train: 0.02378 Test: 0.03039
Epoch: 6880 Train: 0.02375 Test: 0.03045
Epoch: 6960 Train: 0.74887 Test: 0.88024
Epoch: 7040 Train: 0.90371 Test: 0.77822
Epoch: 7120 Train: 0.02220 Test: 0.02811
Epoch: 7200 Train: 0.41349 Test: 0.20980
Epoch: 7280 Train: 0.02128 Test: 0.02744
Epoch: 7360 Train: 0.02042 Test: 0.02656
Epoch: 7440 Train: 0.02195 Test: 0.03063
Epoch: 7520 Train: 0.02264 Test: 0.02681
Epoch: 7600 Train: 0.01959 Test: 0.02580
Epoch: 7680 Train: 0.05183 Test: 0.05622
Epoch: 7760 Train: 0.01911 Test: 0.02527
Epoch: 7840 Train: 0.02008 Test: 0.02611
Epoch: 7920 Train: 0.02008 Test: 0.02550
Epoch: 7999 Train: 0.01829 Test: 0.02444
Training Loss: tensor(0.0183)
Test Loss: tensor(0.0244)
Learned LE: [ 0.870269    0.01035571 -4.8004284 ]
True LE: [ 8.6655664e-01  1.0710658e-02 -1.4553177e+01]
Relative Error: [2.1867354  2.2788112  2.2325315  2.133731   2.0312097  1.9590944
 2.0401814  2.296845   2.588467   2.8058345  2.9096336  2.9309404
 2.9425898  2.9486747  2.9143722  2.856605   2.7737336  2.6398191
 2.4836786  2.3607135  2.2898185  2.2651556  2.3007193  2.4028015
 2.5415843  2.6744683  2.7417476  2.6890247  2.595853   2.403822
 2.05381    1.8867195  2.032581   2.4008756  2.5436897  2.1788013
 1.5990728  1.2798247  1.1865034  1.0087963  0.9634462  1.0969086
 1.3260584  1.5372491  1.6105837  1.5236217  1.3406368  1.1809798
 1.0861989  1.0313443  0.96511805 0.84583926 0.7097603  0.6872741
 0.8319116  1.0441056  1.2363678  1.3636943  1.4002298  1.3650458
 1.3668152  1.5101146  1.7636365  1.9820005  2.0546756  2.0241325
 1.9766291  1.9223928  1.9366194  2.1140513  2.3632674  2.5575893
 2.6501431  2.6454735  2.6203709  2.6292615  2.6430688  2.6582708
 2.6580176  2.578884   2.4344065  2.29607    2.1973712  2.1314926
 2.1143134  2.1667655  2.2729087  2.40154    2.5178614  2.5345976
 2.4284503  2.3384993  2.0887663  1.7403817  1.6594731  1.9212805
 2.279628   2.227983   1.7555773  1.2023622  0.93815804 0.8630244
 0.8065944  0.84827495 1.015029   1.2237549  1.3114952  1.2583662
 1.1129304  0.9776099  0.8943778  0.85093695 0.8114679  0.73780674
 0.63240635 0.5867183  0.690047   0.8886235  1.086553   1.2213942
 1.2760538  1.2582567  1.1903944  1.1660117  1.2931352  1.5498555
 1.7773548  1.8753957  1.8970373  1.8714124  1.8435065  1.9329443
 2.13066    2.3060815  2.3993764  2.398758   2.3387716  2.3098345
 2.3318646  2.394453   2.4878268  2.5043635  2.3963928  2.243096
 2.1129391  2.0078146  1.943586   1.9435272  1.9892397  2.0619843
 2.161263   2.2673726  2.2702138  2.171201   2.1152773  1.82554
 1.460671   1.421902   1.7556456  2.060603   1.8915263  1.4070479
 0.8726029  0.6123342  0.6771372  0.7145864  0.7302625  0.88372093
 1.0082426  0.9857214  0.887307   0.7858746  0.72208357 0.6892514
 0.66641057 0.6390344  0.6029035  0.5758885  0.6212317  0.7455954
 0.91159505 1.0642356  1.1315587  1.1018943  1.0259503  0.951653
 0.939292   1.0509306  1.297538   1.5482279  1.7049024  1.779083
 1.7362578  1.7223743  1.845013   2.005452   2.1074827  2.1396852
 2.1014497  2.0432065  2.0400202  2.0860941  2.1966624  2.32403
 2.3225052  2.1841617  2.0211482  1.8786778  1.7626373  1.7241054
 1.7377038  1.7497616  1.7727107  1.8414544  1.9427981  1.9584702
 1.91826    1.9408488  1.6779115  1.2441263  1.1419507  1.4844087
 1.7797474  1.5989046  1.1342902  0.61709255 0.34420368 0.62004685
 0.64806926 0.572973   0.69761986 0.7285347  0.6588759  0.5908154
 0.55202276 0.5346167  0.5242732  0.5206567  0.5694882  0.596843
 0.590507   0.6103517  0.6646985  0.7625226  0.87793386 0.9172219
 0.8349627  0.7090035  0.65413994 0.7057738  0.80517465 0.99222094
 1.2347704  1.4390237  1.565228   1.5045743  1.4809759  1.5921246
 1.7010043  1.7602189  1.7814722  1.7661492  1.76343    1.8212401
 1.8845989  1.9778888  2.089342   2.0757935  1.9251889  1.766517
 1.6182436  1.490924   1.4506462  1.430431   1.3969661  1.3937657
 1.4516122  1.5594703  1.591648   1.5809879  1.7288547  1.6669327
 1.1781199  0.8681236  1.0911212  1.4306371  1.358479   0.941885
 0.47007617 0.23966515 0.570862   0.6046927  0.39264554 0.5035607
 0.48866117 0.426105   0.39209434 0.37925342 0.3863632  0.37669975
 0.4490668  0.5639066  0.56433916 0.5138537  0.47370678 0.43598115
 0.43227348 0.4749021  0.48963228 0.39192188 0.29587027 0.40959156
 0.5805508  0.6858031  0.7501915  0.8847856  1.0548058  1.2184902
 1.1595583  1.0685815  1.1436111  1.2250088  1.2692158  1.3059089
 1.3530729  1.4332349  1.5827858  1.6753489  1.7102536  1.7702619
 1.765624   1.6494473  1.5510314  1.4434844  1.2858369  1.1914902
 1.097951   0.9850009  0.9523023  1.0286021  1.1500229  1.2396188
 1.168966   1.2676984  1.5783988  1.306779   0.81433386 0.66032517
 0.96615005 1.0859845  0.84130186 0.42823678 0.33598453 0.46953443
 0.583975   0.26014048 0.38234293 0.41453964 0.36450934 0.29733938
 0.25476885 0.26676917 0.26562652 0.41327798 0.52887934 0.5272818
 0.46951059 0.40910408 0.3139046  0.22660033 0.15505837 0.15107708
 0.3227744  0.58284694 0.79351515 0.8415658  0.803247   0.73752093
 0.6850576  0.71065587 0.84857804 0.9033961  0.66300297 0.54868066
 0.62218857 0.761233   0.7811363  0.8469234  1.0040388  1.2309561
 1.3952973  1.4185001  1.4164472  1.4141107  1.3594475  1.3531901
 1.374836   1.232593   1.0849464  0.97164565]
