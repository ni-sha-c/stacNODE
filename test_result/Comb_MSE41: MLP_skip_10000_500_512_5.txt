time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 103.86%, model saved.
Epoch: 0 Train: 3757.48584 Test: 3995.09497
Epoch 100: New minimal relative error: 28.65%, model saved.
Epoch: 100 Train: 68.62742 Test: 69.59032
Epoch: 200 Train: 34.69354 Test: 40.29154
Epoch 300: New minimal relative error: 27.87%, model saved.
Epoch: 300 Train: 9.58820 Test: 10.68164
Epoch 400: New minimal relative error: 14.17%, model saved.
Epoch: 400 Train: 12.52073 Test: 13.37932
Epoch: 500 Train: 4.88267 Test: 7.20486
Epoch: 600 Train: 2.97525 Test: 4.69216
Epoch: 700 Train: 7.10372 Test: 9.20026
Epoch 800: New minimal relative error: 10.81%, model saved.
Epoch: 800 Train: 2.62127 Test: 3.89083
Epoch: 900 Train: 1.73811 Test: 3.16378
Epoch: 1000 Train: 5.72120 Test: 6.15746
Epoch: 1100 Train: 1.83799 Test: 2.52389
Epoch: 1200 Train: 5.09540 Test: 6.95509
Epoch: 1300 Train: 1.33050 Test: 1.86767
Epoch: 1400 Train: 3.49698 Test: 3.74500
Epoch: 1500 Train: 4.78290 Test: 3.87756
Epoch 1600: New minimal relative error: 10.23%, model saved.
Epoch: 1600 Train: 3.30395 Test: 4.45208
Epoch 1700: New minimal relative error: 7.33%, model saved.
Epoch: 1700 Train: 0.60034 Test: 1.34181
Epoch: 1800 Train: 1.61127 Test: 1.70767
Epoch: 1900 Train: 0.90337 Test: 1.41719
Epoch: 2000 Train: 6.80770 Test: 9.13514
Epoch: 2100 Train: 0.92194 Test: 1.16820
Epoch: 2200 Train: 0.89218 Test: 1.55389
Epoch: 2300 Train: 0.47093 Test: 0.79372
Epoch: 2400 Train: 3.66754 Test: 3.59898
Epoch: 2500 Train: 1.37683 Test: 2.01032
Epoch: 2600 Train: 1.98359 Test: 2.23074
Epoch: 2700 Train: 0.33069 Test: 0.69583
Epoch: 2800 Train: 0.58398 Test: 0.99202
Epoch: 2900 Train: 0.51789 Test: 0.73910
Epoch: 3000 Train: 1.28316 Test: 1.87076
Epoch: 3100 Train: 0.21414 Test: 0.44806
Epoch: 3200 Train: 0.21016 Test: 0.44170
Epoch: 3300 Train: 0.52954 Test: 0.74670
Epoch: 3400 Train: 1.56366 Test: 1.74775
Epoch: 3500 Train: 2.46622 Test: 3.04359
Epoch: 3600 Train: 0.65827 Test: 0.93071
Epoch 3700: New minimal relative error: 5.81%, model saved.
Epoch: 3700 Train: 0.26942 Test: 0.47506
Epoch: 3800 Train: 0.17506 Test: 0.34491
Epoch: 3900 Train: 1.28564 Test: 1.56752
Epoch: 4000 Train: 2.74526 Test: 1.93361
Epoch: 4100 Train: 0.19195 Test: 0.40893
Epoch: 4200 Train: 0.28467 Test: 0.52461
Epoch: 4300 Train: 0.16492 Test: 0.43277
Epoch 4400: New minimal relative error: 5.02%, model saved.
Epoch: 4400 Train: 0.12968 Test: 0.31083
Epoch: 4500 Train: 0.11312 Test: 0.28666
Epoch: 4600 Train: 0.10686 Test: 0.26608
Epoch: 4700 Train: 0.16800 Test: 0.32865
Epoch: 4800 Train: 0.72061 Test: 1.01266
Epoch: 4900 Train: 0.44424 Test: 0.70983
Epoch: 5000 Train: 0.34109 Test: 0.48698
Epoch: 5100 Train: 1.00293 Test: 1.27011
Epoch: 5200 Train: 0.53512 Test: 0.64446
Epoch: 5300 Train: 0.11933 Test: 0.25596
Epoch: 5400 Train: 0.09007 Test: 0.23694
Epoch: 5500 Train: 0.19587 Test: 0.24554
Epoch: 5600 Train: 0.15364 Test: 0.32814
Epoch: 5700 Train: 0.89584 Test: 1.02901
Epoch: 5800 Train: 0.20744 Test: 0.37342
Epoch: 5900 Train: 0.86424 Test: 1.41499
Epoch: 6000 Train: 0.06771 Test: 0.19409
Epoch: 6100 Train: 0.34599 Test: 0.38897
Epoch: 6200 Train: 0.11157 Test: 0.24143
Epoch: 6300 Train: 0.76612 Test: 1.07835
Epoch: 6400 Train: 0.05956 Test: 0.17900
Epoch: 6500 Train: 0.10429 Test: 0.19514
Epoch: 6600 Train: 0.05845 Test: 0.17205
Epoch 6700: New minimal relative error: 4.58%, model saved.
Epoch: 6700 Train: 0.74315 Test: 0.84713
Epoch: 6800 Train: 0.19523 Test: 0.33722
Epoch: 6900 Train: 0.18307 Test: 0.33181
Epoch 7000: New minimal relative error: 4.52%, model saved.
Epoch: 7000 Train: 0.13085 Test: 0.19686
Epoch: 7100 Train: 0.44762 Test: 0.55758
Epoch: 7200 Train: 0.19928 Test: 0.29115
Epoch: 7300 Train: 0.04962 Test: 0.15984
Epoch: 7400 Train: 0.11120 Test: 0.25853
Epoch: 7500 Train: 0.08759 Test: 0.20251
Epoch: 7600 Train: 0.39283 Test: 0.43428
Epoch: 7700 Train: 0.36425 Test: 0.49479
Epoch: 7800 Train: 0.24827 Test: 0.43944
Epoch: 7900 Train: 0.05383 Test: 0.14333
Epoch: 8000 Train: 0.11326 Test: 0.20028
Epoch: 8100 Train: 0.11902 Test: 0.21734
Epoch: 8200 Train: 0.14243 Test: 0.30585
Epoch: 8300 Train: 0.11907 Test: 0.20462
Epoch: 8400 Train: 0.67867 Test: 0.83974
Epoch: 8500 Train: 0.24745 Test: 0.28809
Epoch: 8600 Train: 0.59469 Test: 0.61490
Epoch: 8700 Train: 0.06678 Test: 0.15110
Epoch: 8800 Train: 0.14516 Test: 0.20288
Epoch: 8900 Train: 0.04907 Test: 0.15324
Epoch: 9000 Train: 0.13062 Test: 0.21868
Epoch: 9100 Train: 0.15600 Test: 0.26966
Epoch: 9200 Train: 0.04659 Test: 0.12493
Epoch: 9300 Train: 0.04701 Test: 0.12748
Epoch: 9400 Train: 0.03775 Test: 0.11598
Epoch: 9500 Train: 0.16532 Test: 0.28688
Epoch: 9600 Train: 0.09214 Test: 0.16885
Epoch: 9700 Train: 0.11830 Test: 0.19250
Epoch: 9800 Train: 0.15768 Test: 0.26270
Epoch: 9900 Train: 0.04657 Test: 0.12017
Epoch: 9999 Train: 0.16696 Test: 0.22069
Training Loss: tensor(0.1670)
Test Loss: tensor(0.2207)
Learned LE: [ 0.8001937   0.09458227 -3.2957423 ]
True LE: [ 8.8216412e-01  1.0256812e-03 -1.4559378e+01]
Relative Error: [2.6061573  2.635034   2.611942   2.5991423  2.496706   2.3743088
 2.2134438  2.0728583  2.0281122  2.020741   2.0279515  2.1823606
 2.3944669  2.7085078  3.2057981  3.7970972  4.265747   4.2367654
 4.106325   3.7833865  3.1976256  2.8767977  2.681938   2.2262552
 2.055877   1.8837994  1.8999811  2.3952377  3.2642908  3.6388197
 3.999147   4.507122   4.9288745  5.073645   4.9467106  4.5710716
 3.9989119  3.926472   3.8101296  3.4988508  3.1019752  2.5849774
 2.2272902  1.9014409  1.7187152  1.9893825  2.2540221  2.4254262
 2.5865488  2.740814   3.0945263  3.3050597  3.1275132  2.9622233
 2.6813674  2.3461733  2.0166528  2.1284065  2.1650853  2.1637692
 2.2015193  2.3156693  2.4013696  2.4393868  2.4440138  2.3698106
 2.2703848  2.234422   2.0937984  1.9802346  1.9811254  2.0611098
 2.0141497  1.9962785  2.1303568  2.3600197  2.747182   3.2722273
 3.7713695  3.7657459  3.631254   3.398341   2.8765922  2.5376554
 2.3538878  1.9454409  1.831764   1.6580535  1.7421892  2.9355726
 3.193773   3.352622   3.9874713  4.752522   5.3496046  5.5196505
 5.3116126  4.8668966  4.4362607  4.107682   4.043107   3.8075783
 3.2509918  2.7061505  2.2398999  1.8161521  1.5119704  1.7013522
 2.0194612  2.1975887  2.5408447  2.6519992  2.9661207  3.1014931
 2.9029424  2.7433712  2.3919065  2.0573266  2.0794456  2.15451
 2.172297   1.9884888  1.9419872  2.0534787  2.165823   2.2106113
 2.2185247  2.1724415  2.0557296  2.045914   2.0116298  1.953815
 1.9953402  2.0577123  2.0166805  1.8760188  1.8734405  2.0339417
 2.3408375  2.7655797  3.265794   3.262198   3.197327   2.9728527
 2.5677886  2.2255745  2.045743   1.724373   1.6223146  1.4494718
 2.1395361  2.897523   2.9618096  3.27287    4.1540728  4.957985
 5.5432734  5.830154   5.745825   5.414932   4.9603477  4.55219
 4.1812687  4.0398784  3.5611684  2.8478673  2.2448373  1.7540547
 1.4605075  1.4462408  1.802709   1.9928453  2.441833   2.5385487
 2.837357   2.848253   2.7428162  2.528014   2.133485   1.8307347
 2.0237203  2.1622818  2.1461704  1.9305831  1.8335476  1.8634197
 1.9518986  2.0410924  2.0684307  2.0194943  1.952965   1.8913485
 1.8611603  1.806128   1.945287   2.0073514  1.9845164  1.8865119
 1.6943905  1.7405372  1.9255266  2.2806401  2.7456312  2.7676094
 2.7457523  2.5509176  2.2565596  1.931014   1.7482227  1.513714
 1.442692   1.350038   2.3416753  2.6650934  2.7952855  3.3525918
 4.245907   5.059735   5.696255   5.9915705  6.123753   6.046193
 5.601956   5.1237745  4.4376764  4.1295795  3.7908401  3.2550793
 2.3581572  1.7426338  1.4775118  1.2487303  1.5359333  1.7707728
 2.291278   2.4149482  2.6256251  2.6116478  2.5753152  2.3027182
 1.9275237  1.7603924  1.8951104  2.0573566  2.1781774  2.059517
 1.817886   1.7498804  1.8693479  1.9669645  2.0341353  1.9836273
 1.8173834  1.6333265  1.5814096  1.5352144  1.6452153  1.8275757
 1.9170463  1.83632    1.6351846  1.5577554  1.5813785  1.7973076
 2.1965787  2.3243456  2.3132617  2.130476   1.9681553  1.6372811
 1.518668   1.3313125  1.3361977  1.6038411  2.3117764  2.3825886
 2.78799    3.6063943  4.503438   5.2771764  5.83035    6.1992764
 6.4388685  6.399385   6.281188   5.7535257  4.9709425  4.114581
 3.839347   3.496058   2.6754968  1.8999119  1.4659706  1.225397
 1.235619   1.5347798  2.0665972  2.2497187  2.3813257  2.3911242
 2.3494503  2.1065452  1.7648481  1.6875539  1.7463768  1.9079373
 2.110792   2.23501    1.9090238  1.8204291  1.8914557  2.0382795
 1.9849805  1.846808   1.6422881  1.4609253  1.263942   1.1970342
 1.243037   1.3853272  1.5698571  1.7044343  1.5986148  1.4522772
 1.3631052  1.3733242  1.6206685  1.920875   1.8786696  1.7607182
 1.6248308  1.4384279  1.3433574  1.1658691  1.1536474  1.6002933
 2.2843235  1.9404557  2.7117972  3.5304592  4.1746573  4.8507023
 5.5245028  5.9531612  6.2986717  6.429365   6.595296   6.4013243
 5.601992   4.573604   3.6768603  3.477354   3.0609438  2.2058077
 1.4803725  1.2499517  1.160745   1.257596   1.7046213  2.0115733
 2.0730383  2.1507235  2.0967097  1.91322    1.625863   1.5480108
 1.5249906  1.7307138  1.953878   2.1068     1.988185   1.9416472
 2.025562   2.0187416  1.8424852  1.677634   1.4794469  1.2890812
 1.0435711  0.92048466 0.9270931  0.98888147 1.1330694  1.348051
 1.3622991  1.2888745  1.1822526  1.0517554  1.1462691  1.493447
 1.4396226  1.4176933  1.2302464  1.2310296  1.1686629  0.96542424
 0.9711795  1.3853004  1.924644   1.5307548 ]
