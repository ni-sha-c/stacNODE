time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.23%, model saved.
Epoch: 0 Train: 3819.56226 Test: 4064.18506
Epoch: 80 Train: 52.50776 Test: 38.85107
Epoch 160: New minimal relative error: 70.60%, model saved.
Epoch: 160 Train: 11.35352 Test: 11.77746
Epoch 240: New minimal relative error: 29.61%, model saved.
Epoch: 240 Train: 20.10784 Test: 9.46350
Epoch: 320 Train: 6.25523 Test: 4.99608
Epoch: 400 Train: 8.80972 Test: 13.90685
Epoch 480: New minimal relative error: 21.29%, model saved.
Epoch: 480 Train: 6.31102 Test: 11.11545
Epoch 560: New minimal relative error: 20.47%, model saved.
Epoch: 560 Train: 8.47889 Test: 8.34717
Epoch: 640 Train: 18.80711 Test: 11.64116
Epoch: 720 Train: 10.19984 Test: 11.54591
Epoch: 800 Train: 2.26767 Test: 1.39238
Epoch: 880 Train: 1.15083 Test: 1.19406
Epoch: 960 Train: 1.12089 Test: 1.46693
Epoch: 1040 Train: 14.13686 Test: 15.33150
Epoch 1120: New minimal relative error: 12.82%, model saved.
Epoch: 1120 Train: 1.26915 Test: 1.79588
Epoch: 1200 Train: 0.72271 Test: 0.63748
Epoch: 1280 Train: 1.29195 Test: 1.34398
Epoch: 1360 Train: 2.86265 Test: 3.69620
Epoch: 1440 Train: 1.04034 Test: 1.29954
Epoch: 1520 Train: 1.19151 Test: 0.94834
Epoch: 1600 Train: 0.66182 Test: 0.59672
Epoch: 1680 Train: 0.99816 Test: 0.81297
Epoch: 1760 Train: 0.61515 Test: 0.70526
Epoch: 1840 Train: 1.71960 Test: 2.35122
Epoch: 1920 Train: 0.95042 Test: 1.33797
Epoch: 2000 Train: 1.49470 Test: 1.68556
Epoch: 2080 Train: 0.98715 Test: 0.78180
Epoch 2160: New minimal relative error: 9.60%, model saved.
Epoch: 2160 Train: 0.30688 Test: 0.40477
Epoch 2240: New minimal relative error: 9.08%, model saved.
Epoch: 2240 Train: 0.55787 Test: 0.68188
Epoch: 2320 Train: 0.22324 Test: 0.24741
Epoch: 2400 Train: 1.41442 Test: 1.90803
Epoch 2480: New minimal relative error: 7.94%, model saved.
Epoch: 2480 Train: 0.60807 Test: 0.54583
Epoch: 2560 Train: 0.47001 Test: 0.51917
Epoch: 2640 Train: 1.00797 Test: 1.03854
Epoch: 2720 Train: 0.46555 Test: 0.63081
Epoch: 2800 Train: 0.55820 Test: 0.48227
Epoch: 2880 Train: 0.72973 Test: 0.67457
Epoch: 2960 Train: 0.99677 Test: 1.40930
Epoch: 3040 Train: 0.52422 Test: 0.21531
Epoch: 3120 Train: 2.18029 Test: 2.37089
Epoch: 3200 Train: 0.95631 Test: 1.12474
Epoch: 3280 Train: 5.30690 Test: 6.63292
Epoch: 3360 Train: 0.14399 Test: 0.16140
Epoch: 3440 Train: 0.37659 Test: 0.38715
Epoch: 3520 Train: 0.23715 Test: 0.29778
Epoch: 3600 Train: 0.26875 Test: 0.33498
Epoch: 3680 Train: 0.75517 Test: 0.84447
Epoch: 3760 Train: 0.23105 Test: 0.25959
Epoch 3840: New minimal relative error: 7.36%, model saved.
Epoch: 3840 Train: 0.37126 Test: 0.39131
Epoch: 3920 Train: 0.73840 Test: 0.58622
Epoch: 4000 Train: 0.23292 Test: 0.20133
Epoch: 4080 Train: 0.15868 Test: 0.18289
Epoch: 4160 Train: 0.16476 Test: 0.12502
Epoch 4240: New minimal relative error: 5.88%, model saved.
Epoch: 4240 Train: 0.19921 Test: 0.17804
Epoch: 4320 Train: 0.26885 Test: 0.31899
Epoch: 4400 Train: 0.50772 Test: 0.64012
Epoch: 4480 Train: 0.51570 Test: 0.63286
Epoch: 4560 Train: 0.39227 Test: 0.47923
Epoch: 4640 Train: 0.25546 Test: 0.29617
Epoch: 4720 Train: 0.41276 Test: 0.72107
Epoch: 4800 Train: 0.37517 Test: 0.46669
Epoch: 4880 Train: 0.45303 Test: 0.59652
Epoch: 4960 Train: 0.13480 Test: 0.11546
Epoch: 5040 Train: 0.14980 Test: 0.21116
Epoch: 5120 Train: 0.62982 Test: 0.85307
Epoch 5200: New minimal relative error: 5.61%, model saved.
Epoch: 5200 Train: 0.12420 Test: 0.09557
Epoch: 5280 Train: 0.14140 Test: 0.13387
Epoch: 5360 Train: 0.15034 Test: 0.10623
Epoch: 5440 Train: 0.06891 Test: 0.09396
Epoch: 5520 Train: 0.05219 Test: 0.13185
Epoch: 5600 Train: 0.06196 Test: 0.07883
Epoch: 5680 Train: 0.12456 Test: 0.14065
Epoch: 5760 Train: 0.41723 Test: 0.44234
Epoch: 5840 Train: 0.07718 Test: 0.08683
Epoch: 5920 Train: 0.17817 Test: 0.18938
Epoch: 6000 Train: 0.46294 Test: 0.45195
Epoch: 6080 Train: 0.82003 Test: 0.76825
Epoch: 6160 Train: 0.12485 Test: 0.10960
Epoch: 6240 Train: 0.03891 Test: 0.04698
Epoch: 6320 Train: 0.08254 Test: 0.08454
Epoch: 6400 Train: 0.27969 Test: 0.23009
Epoch: 6480 Train: 0.05939 Test: 0.06479
Epoch: 6560 Train: 0.25289 Test: 0.28430
Epoch: 6640 Train: 0.10572 Test: 0.10521
Epoch: 6720 Train: 0.13871 Test: 0.13515
Epoch: 6800 Train: 0.32358 Test: 0.39754
Epoch: 6880 Train: 0.12783 Test: 0.11279
Epoch: 6960 Train: 0.05742 Test: 0.06357
Epoch: 7040 Train: 0.04073 Test: 0.05447
Epoch: 7120 Train: 0.13186 Test: 0.17376
Epoch: 7200 Train: 0.06876 Test: 0.08013
Epoch: 7280 Train: 0.19485 Test: 0.06179
Epoch: 7360 Train: 0.05622 Test: 0.07339
Epoch 7440: New minimal relative error: 4.06%, model saved.
Epoch: 7440 Train: 0.05894 Test: 0.05830
Epoch: 7520 Train: 0.08393 Test: 0.10271
Epoch: 7600 Train: 0.16862 Test: 0.17136
Epoch: 7680 Train: 0.03552 Test: 0.03891
Epoch: 7760 Train: 0.08780 Test: 0.08625
Epoch: 7840 Train: 0.07867 Test: 0.08761
Epoch: 7920 Train: 0.03348 Test: 0.05075
Epoch: 7999 Train: 0.03081 Test: 0.04021
Training Loss: tensor(0.0308)
Test Loss: tensor(0.0402)
Learned LE: [ 0.8480978   0.01272374 -3.9178643 ]
True LE: [ 8.3723015e-01  8.1784856e-03 -1.4521864e+01]
Relative Error: [0.15842769 0.51380104 0.5084186  0.3566913  0.41398606 0.29195553
 0.32965308 0.4836347  0.8554562  0.8328584  0.7031477  0.6550482
 0.5825227  1.1567429  1.0329181  0.5777675  0.47605833 0.33211777
 0.29694775 0.483172   0.6355589  0.6623826  0.75277096 0.95702124
 0.93376404 1.1023434  1.2325996  1.1575013  0.9951845  0.82533693
 0.6109984  0.5589144  0.5329438  0.7153343  0.68573636 0.7706659
 0.9617834  1.4007698  1.9891515  2.1066372  1.5575967  1.183449
 0.98847383 0.80467266 0.63882387 0.49571404 0.5393336  0.3715467
 0.31906185 0.21167386 0.13032052 0.40414044 0.41101506 0.28354728
 0.33093578 0.36618662 0.315092   0.3548238  0.36827463 0.3845884
 0.32647353 0.330586   0.25885093 0.29150668 0.420203   0.42763716
 0.39450186 0.24994309 0.16737552 0.3855133  0.51471263 0.49919924
 0.31976354 0.2108602  0.46531788 1.0716058  1.0694923  0.72393805
 0.47286916 0.28627768 0.24378094 0.4129451  0.63583606 0.44416198
 0.44772947 0.52640504 0.6058872  0.6897641  0.855754   0.9223994
 0.9767989  0.84403783 0.7532357  0.58988804 0.5374765  0.7209542
 0.74747473 0.63570887 0.734269   0.8989549  1.1847008  1.8126816
 1.4186461  0.97991323 0.8941084  0.73641884 0.77757186 0.44730192
 0.36652923 0.2400738  0.12315855 0.05022092 0.13886486 0.27619505
 0.23715226 0.1096291  0.23165035 0.31227672 0.31396958 0.26773733
 0.37632823 0.3637545  0.42030025 0.45087528 0.42492542 0.20510119
 0.27545026 0.2529118  0.34500718 0.5189508  0.25422478 0.15528862
 0.37778723 0.3569177  0.25769374 0.09840889 0.43878913 0.7729712
 0.9185841  0.8673454  0.66203207 0.37167206 0.21303172 0.38059604
 0.49614167 0.42799595 0.3332081  0.2678412  0.3444312  0.29674456
 0.4805328  0.64695644 0.66334236 0.6158968  0.5670684  0.4488513
 0.46607512 0.45488566 0.68674207 0.6923772  0.4946006  0.5461015
 0.5465769  0.935505   1.1827003  0.93663716 0.4430978  0.50400615
 0.6151569  0.5269478  0.24492484 0.21516663 0.21661855 0.20512359
 0.18539372 0.32818344 0.350239   0.21105124 0.32377738 0.25984278
 0.22333395 0.19988877 0.30003163 0.39344114 0.46778023 0.45197928
 0.5331501  0.4866778  0.28198755 0.2806183  0.21851198 0.35973793
 0.6180747  0.4070539  0.37049747 0.46279672 0.3608998  0.43255413
 0.4602135  0.6749856  0.67707807 0.7271765  0.6569951  0.61549747
 0.35468593 0.31153554 0.40238568 0.40422386 0.5330855  0.5538635
 0.4868346  0.38164735 0.31384513 0.4246166  0.69523567 0.6150388
 0.52936846 0.24556062 0.37510625 0.4962707  0.33515704 0.5460448
 0.4586792  0.4199004  0.34483743 0.28536072 0.7840882  0.87196815
 0.575083   0.1065767  0.25578082 0.5074945  0.29957923 0.21547939
 0.17808022 0.35564315 0.3857049  0.34676093 0.4295431  0.3639787
 0.35509703 0.44175982 0.33500573 0.21959844 0.17890795 0.29441294
 0.53555983 0.5736022  0.5670876  0.5713626  0.41690683 0.2802345
 0.31060246 0.18179123 0.4057033  0.6598242  0.5353307  0.5876963
 0.44949538 0.58103997 0.6191162  0.7255964  0.5924117  0.5862199
 0.6378822  0.44113362 0.59873706 0.49089202 0.26828045 0.27469414
 0.45934337 0.7775478  0.7915093  0.6667667  0.6185052  0.57275283
 0.49382263 0.67565376 0.5620904  0.49127603 0.3576483  0.30598858
 0.38597748 0.3924407  0.4358381  0.28471613 0.41603395 0.41300213
 0.30105376 0.5756249  0.7072293  0.40320706 0.06658766 0.23097368
 0.4175311  0.33920708 0.36230168 0.24121813 0.4294147  0.4661814
 0.43061593 0.46858978 0.3537953  0.41432968 0.4236497  0.32377052
 0.21334298 0.20354809 0.2132746  0.43398896 0.5787718  0.54128695
 0.47487825 0.41549754 0.3506574  0.20122796 0.27036527 0.28263718
 0.58305436 0.5763482  0.6631546  0.49766427 0.7446557  0.7748764
 0.7592057  0.6896687  0.67179644 0.5302763  0.43205065 0.3695647
 0.4917091  0.17713442 0.24553019 0.45473793 0.7507796  0.89475495
 0.91639405 0.9120441  0.85724795 0.6539047  0.6730562  0.69553334
 0.637429   0.67507625 0.4245542  0.3632937  0.49554422 0.5599176
 0.23414414 0.32933712 0.65102494 0.33042613 0.3952731  0.6099985
 0.25890484 0.03202742 0.16776678 0.4597872  0.46367502 0.528627
 0.34282815 0.4735629  0.56514287 0.49959496 0.5826117  0.46911258
 0.48266757 0.49805441 0.34545323 0.22630763 0.25046897 0.19387805
 0.27767244 0.5092479  0.47878808 0.43950117 0.28059655 0.2546852
 0.1985495  0.20445988 0.27256334 0.3115996  0.52182156 0.44206074
 0.49518016 0.65324765 0.85927534 0.65908724 0.73033154 0.79300517
 0.58465433 0.4648896  0.11144286 0.36118358 0.41003516 0.38673863
 0.64530283 0.91034055 1.0485367  1.1946807 ]
