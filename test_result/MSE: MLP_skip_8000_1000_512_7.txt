time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.82%, model saved.
Epoch: 0 Train: 3911.92676 Test: 3980.64136
Epoch 80: New minimal relative error: 95.79%, model saved.
Epoch: 80 Train: 86.26350 Test: 122.63068
Epoch 160: New minimal relative error: 31.22%, model saved.
Epoch: 160 Train: 18.64835 Test: 31.62440
Epoch 240: New minimal relative error: 20.48%, model saved.
Epoch: 240 Train: 8.49928 Test: 10.58818
Epoch: 320 Train: 12.78053 Test: 30.76737
Epoch: 400 Train: 15.10305 Test: 11.80509
Epoch: 480 Train: 5.75957 Test: 4.13196
Epoch: 560 Train: 5.59472 Test: 6.54783
Epoch 640: New minimal relative error: 20.29%, model saved.
Epoch: 640 Train: 6.58339 Test: 8.37851
Epoch 720: New minimal relative error: 13.45%, model saved.
Epoch: 720 Train: 3.84610 Test: 3.37467
Epoch: 800 Train: 3.09044 Test: 3.56809
Epoch: 880 Train: 3.51556 Test: 2.12931
Epoch: 960 Train: 1.63042 Test: 1.31956
Epoch: 1040 Train: 3.44083 Test: 4.83725
Epoch: 1120 Train: 4.12012 Test: 5.63990
Epoch: 1200 Train: 4.59460 Test: 7.07594
Epoch: 1280 Train: 8.75356 Test: 6.91351
Epoch 1360: New minimal relative error: 11.31%, model saved.
Epoch: 1360 Train: 1.38684 Test: 1.45428
Epoch 1440: New minimal relative error: 10.14%, model saved.
Epoch: 1440 Train: 1.10275 Test: 1.58134
Epoch: 1520 Train: 11.31594 Test: 2.66270
Epoch: 1600 Train: 3.85133 Test: 3.08533
Epoch: 1680 Train: 1.21173 Test: 1.00685
Epoch 1760: New minimal relative error: 9.22%, model saved.
Epoch: 1760 Train: 0.86344 Test: 1.34154
Epoch: 1840 Train: 7.76536 Test: 3.26529
Epoch: 1920 Train: 6.07238 Test: 3.80865
Epoch: 2000 Train: 1.34187 Test: 1.04396
Epoch: 2080 Train: 0.45406 Test: 0.83891
Epoch: 2160 Train: 1.02744 Test: 1.36280
Epoch: 2240 Train: 1.40947 Test: 1.78136
Epoch 2320: New minimal relative error: 8.12%, model saved.
Epoch: 2320 Train: 0.29291 Test: 0.46006
Epoch: 2400 Train: 0.33989 Test: 0.55635
Epoch: 2480 Train: 2.10442 Test: 2.80491
Epoch: 2560 Train: 0.94616 Test: 0.74835
Epoch: 2640 Train: 1.90481 Test: 1.92155
Epoch: 2720 Train: 0.45574 Test: 0.65189
Epoch: 2800 Train: 0.40787 Test: 0.56328
Epoch: 2880 Train: 0.93720 Test: 0.64445
Epoch: 2960 Train: 2.73077 Test: 3.42319
Epoch: 3040 Train: 0.46322 Test: 0.73195
Epoch: 3120 Train: 3.21074 Test: 4.75733
Epoch: 3200 Train: 3.21122 Test: 4.11546
Epoch: 3280 Train: 0.17962 Test: 0.32463
Epoch: 3360 Train: 1.31727 Test: 1.43546
Epoch: 3440 Train: 2.23248 Test: 2.56113
Epoch: 3520 Train: 0.53768 Test: 0.70573
Epoch 3600: New minimal relative error: 5.70%, model saved.
Epoch: 3600 Train: 0.24518 Test: 0.45599
Epoch: 3680 Train: 0.43292 Test: 0.56553
Epoch: 3760 Train: 2.20079 Test: 3.08194
Epoch: 3840 Train: 0.26814 Test: 0.48179
Epoch: 3920 Train: 0.12616 Test: 0.27356
Epoch: 4000 Train: 0.53817 Test: 0.80478
Epoch: 4080 Train: 4.63364 Test: 3.40374
Epoch: 4160 Train: 2.49029 Test: 3.11527
Epoch: 4240 Train: 0.60855 Test: 0.66649
Epoch: 4320 Train: 1.43778 Test: 1.35585
Epoch: 4400 Train: 0.17085 Test: 0.35665
Epoch: 4480 Train: 0.25437 Test: 0.41930
Epoch: 4560 Train: 0.10759 Test: 0.26982
Epoch: 4640 Train: 0.17197 Test: 0.31455
Epoch: 4720 Train: 0.11844 Test: 0.24671
Epoch: 4800 Train: 0.60738 Test: 0.73660
Epoch: 4880 Train: 0.66271 Test: 0.78721
Epoch: 4960 Train: 0.72071 Test: 0.98256
Epoch 5040: New minimal relative error: 5.26%, model saved.
Epoch: 5040 Train: 0.21667 Test: 0.29243
Epoch: 5120 Train: 0.08360 Test: 0.21508
Epoch: 5200 Train: 0.10455 Test: 0.21663
Epoch: 5280 Train: 0.27108 Test: 0.42461
Epoch: 5360 Train: 0.25822 Test: 0.29094
Epoch: 5440 Train: 0.85560 Test: 0.87171
Epoch: 5520 Train: 0.54696 Test: 0.93143
Epoch 5600: New minimal relative error: 4.84%, model saved.
Epoch: 5600 Train: 0.06545 Test: 0.17727
Epoch: 5680 Train: 0.11705 Test: 0.22075
Epoch: 5760 Train: 0.09762 Test: 0.21398
Epoch: 5840 Train: 0.08400 Test: 0.20453
Epoch: 5920 Train: 0.06944 Test: 0.17723
Epoch: 6000 Train: 0.09391 Test: 0.19164
Epoch: 6080 Train: 0.06303 Test: 0.18560
Epoch: 6160 Train: 0.07324 Test: 0.20026
Epoch: 6240 Train: 0.08738 Test: 0.20672
Epoch: 6320 Train: 0.07529 Test: 0.18410
Epoch: 6400 Train: 0.05914 Test: 0.16614
Epoch: 6480 Train: 0.06585 Test: 0.17479
Epoch: 6560 Train: 0.37935 Test: 0.52430
Epoch: 6640 Train: 0.75671 Test: 1.09406
Epoch: 6720 Train: 0.05795 Test: 0.19181
Epoch: 6800 Train: 0.61638 Test: 0.55026
Epoch: 6880 Train: 0.12390 Test: 0.27004
Epoch: 6960 Train: 0.12134 Test: 0.17794
Epoch: 7040 Train: 0.44412 Test: 0.46396
Epoch: 7120 Train: 0.46555 Test: 0.51466
Epoch: 7200 Train: 0.44808 Test: 0.71156
Epoch: 7280 Train: 0.37122 Test: 0.33943
Epoch: 7360 Train: 0.33308 Test: 0.48594
Epoch: 7440 Train: 0.04584 Test: 0.14976
Epoch: 7520 Train: 0.04542 Test: 0.14919
Epoch: 7600 Train: 0.04602 Test: 0.14476
Epoch: 7680 Train: 0.08729 Test: 0.19989
Epoch: 7760 Train: 0.07833 Test: 0.21019
Epoch: 7840 Train: 0.05017 Test: 0.16567
Epoch: 7920 Train: 0.04432 Test: 0.14322
Epoch: 7999 Train: 0.11983 Test: 0.23800
Training Loss: tensor(0.1198)
Test Loss: tensor(0.2380)
Learned LE: [ 0.8170043  -0.05692225 -3.335911  ]
True LE: [ 8.79085898e-01  6.42589573e-03 -1.45602865e+01]
Relative Error: [0.9238192  0.8769055  0.9032862  0.8050437  0.58605844 0.41102666
 0.7479896  1.3911302  1.4772447  1.2282059  0.9490896  0.6784241
 0.08596197 0.47340524 0.7707162  0.91349745 1.0052722  0.86244464
 0.2859757  0.26842836 0.6998478  0.73036814 0.7577712  0.7210018
 0.6875202  0.54777944 0.30691448 0.11370944 0.14219067 0.36465287
 0.6847606  0.8190438  1.0850329  1.1617732  1.284045   1.5497229
 1.4499913  1.2770507  1.1627159  1.4821889  1.5762355  1.8035319
 1.9203948  1.7399251  1.3057573  0.9233948  0.8053098  0.89254826
 0.9955226  1.026107   0.88014466 0.66897875 0.5237746  0.422109
 0.46176085 0.58239007 0.5153359  0.48607564 0.6774109  0.9237511
 1.0793679  1.0918542  0.8919906  0.83933544 0.8826522  0.7573022
 0.6161408  0.49997377 0.38628805 0.9725476  0.9932518  0.8087597
 0.6533046  0.4024358  0.3619018  0.9470183  0.8242814  0.4546124
 0.8174586  0.6889002  0.19970585 0.40982628 0.8123882  1.0006614
 0.96366954 0.7897486  0.63973755 0.49451202 0.49062502 0.11729325
 0.15439779 0.14719737 0.36148453 0.5639072  0.94298106 1.0625641
 1.0327405  1.309128   1.2896012  1.0856999  0.9309944  1.2630728
 1.3274934  1.3651533  1.4060861  1.2819672  0.9038667  0.6812369
 0.6690158  0.79976153 0.79735565 0.8473064  0.7101471  0.4646917
 0.5382256  0.68832064 0.35623857 0.33225125 0.3755839  0.32100886
 0.5005359  0.7720783  0.8969632  1.0624546  0.9162399  0.80326444
 0.88798434 0.8592818  0.7633821  0.7570214  0.51993746 0.53079444
 0.57526463 0.44915244 0.3792096  0.19480862 0.4964152  0.8963077
 0.68550426 0.17030066 0.7365842  0.62281924 0.16101755 0.35196796
 0.64621043 0.8538784  0.8542566  0.66054296 0.5201923  0.31294572
 0.48360735 0.33699185 0.23729412 0.11430118 0.02024971 0.41002327
 0.6067617  0.9271146  0.9891795  0.92354554 1.0863034  1.0057571
 0.93054485 1.1129161  1.085844   0.904702   0.93254095 0.9212356
 0.63146615 0.41458005 0.47790524 0.5730975  0.6120421  0.6768036
 0.55695146 0.40309834 0.47842702 0.7786719  0.6049455  0.39340854
 0.35894975 0.26398253 0.44684777 0.61106956 0.56726116 0.75554013
 0.8867335  0.8089359  0.8864805  1.0061555  1.0398046  0.8952229
 0.7012197  0.5555254  0.44927773 0.45222506 0.3943421  0.2641774
 0.76358616 0.9153919  0.75519985 0.25087816 0.5574768  0.63305956
 0.3553253  0.26660177 0.36375552 0.49629226 0.4709257  0.2828192
 0.17262264 0.14600073 0.14051548 0.18925716 0.12669821 0.18844993
 0.1205013  0.26534712 0.44018114 0.5465977  1.0041063  0.80845237
 0.76281685 0.75124466 0.64344203 0.6979677  0.8391605  0.53457886
 0.70480806 0.822845   0.6270687  0.32720116 0.14754617 0.1647238
 0.28234014 0.42025208 0.48578075 0.40665105 0.38644123 0.63006616
 0.63685966 0.5017245  0.54161155 0.36771426 0.3733343  0.48161578
 0.43932703 0.4204236  0.6641813  0.727524   0.84157705 0.9868252
 1.2984738  1.0387543  0.53538793 0.503528   0.5416794  0.5284438
 0.5798722  0.58908176 0.77477384 1.0287095  1.029581   0.65927684
 0.4756166  0.54659975 0.46078283 0.17761181 0.21113026 0.6481579
 0.4603677  0.13986196 0.22900863 0.3186381  0.23438625 0.2921853
 0.3138271  0.32979628 0.28928912 0.4251333  0.45114276 0.289984
 0.43569332 0.8186458  0.5098342  0.47169313 0.274377   0.3314879
 0.45040837 0.2181949  0.30342466 0.6617565  0.83504266 0.4638609
 0.16246858 0.15040973 0.1403029  0.2721207  0.44934708 0.47088954
 0.34976503 0.2981601  0.3866174  0.36627376 0.5140371  0.61914647
 0.62583286 0.45272815 0.46448517 0.31729338 0.25407648 0.38621923
 0.6810901  0.85457164 1.103876   1.007157   0.41581726 0.36669013
 0.46150857 0.63172233 0.591018   0.49902117 0.62865597 1.0442663
 1.3824801  1.3187734  0.80309737 0.41145095 0.3317495  0.09102345
 0.14779358 0.54237497 0.76507103 0.3931379  0.24808387 0.517663
 0.64992636 0.46260184 0.42373216 0.6994819  0.6948439  0.7437622
 0.6606555  0.39384595 0.07978982 0.29655904 0.641777   0.3816601
 0.26665586 0.2068986  0.20409457 0.19402015 0.05696216 0.4298562
 0.6995307  0.51651114 0.48118266 0.34758344 0.2669611  0.41475546
 0.46528578 0.44737723 0.37401676 0.22910929 0.0959829  0.22651103
 0.47133616 0.60463494 0.7637732  0.8150823  0.5400793  0.44451553
 0.34456885 0.2867475  0.24049811 0.71121526 0.824618   0.8641761
 0.7000052  0.22386533 0.26295972 0.45538306 0.54998267 0.50098205
 0.6309909  0.5051916  1.0142307  1.6358025  1.6212702  0.89581245
 0.43013102 0.18771824 0.17359307 0.05530304 0.38780665 0.66637164
 0.85118043 0.40205023 0.38783535 0.48912135]
