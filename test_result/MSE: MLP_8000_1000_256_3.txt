time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.71%, model saved.
Epoch: 0 Train: 4109.77490 Test: 4007.15210
Epoch: 80 Train: 234.57516 Test: 240.28212
Epoch 160: New minimal relative error: 27.40%, model saved.
Epoch: 160 Train: 25.89440 Test: 26.42626
Epoch 240: New minimal relative error: 24.12%, model saved.
Epoch: 240 Train: 9.13581 Test: 10.35721
Epoch 320: New minimal relative error: 21.30%, model saved.
Epoch: 320 Train: 7.20799 Test: 9.83631
Epoch: 400 Train: 10.61883 Test: 13.80284
Epoch: 480 Train: 4.56227 Test: 6.13241
Epoch 560: New minimal relative error: 15.19%, model saved.
Epoch: 560 Train: 2.55775 Test: 3.09271
Epoch: 640 Train: 3.93974 Test: 4.62803
Epoch: 720 Train: 1.01701 Test: 1.22548
Epoch: 800 Train: 1.19799 Test: 1.97264
Epoch 880: New minimal relative error: 11.03%, model saved.
Epoch: 880 Train: 1.07135 Test: 1.49216
Epoch: 960 Train: 0.78818 Test: 0.87726
Epoch: 1040 Train: 0.61675 Test: 0.76113
Epoch: 1120 Train: 0.51241 Test: 0.62142
Epoch 1200: New minimal relative error: 10.52%, model saved.
Epoch: 1200 Train: 0.51333 Test: 0.58884
Epoch: 1280 Train: 0.54632 Test: 0.73002
Epoch: 1360 Train: 0.51448 Test: 0.59538
Epoch: 1440 Train: 0.65659 Test: 0.80399
Epoch: 1520 Train: 0.45502 Test: 0.53193
Epoch 1600: New minimal relative error: 9.78%, model saved.
Epoch: 1600 Train: 0.34571 Test: 0.44267
Epoch: 1680 Train: 0.67389 Test: 0.44180
Epoch: 1760 Train: 0.83248 Test: 0.80179
Epoch: 1840 Train: 0.61225 Test: 0.70237
Epoch 1920: New minimal relative error: 9.70%, model saved.
Epoch: 1920 Train: 0.30946 Test: 0.35487
Epoch: 2000 Train: 0.26489 Test: 0.30949
Epoch 2080: New minimal relative error: 8.71%, model saved.
Epoch: 2080 Train: 0.43580 Test: 0.57617
Epoch: 2160 Train: 0.31627 Test: 0.30531
Epoch: 2240 Train: 0.38994 Test: 0.41983
Epoch: 2320 Train: 0.20218 Test: 0.24932
Epoch: 2400 Train: 0.17615 Test: 0.23346
Epoch: 2480 Train: 0.19497 Test: 0.25643
Epoch 2560: New minimal relative error: 6.26%, model saved.
Epoch: 2560 Train: 0.16996 Test: 0.21895
Epoch: 2640 Train: 0.16125 Test: 0.21153
Epoch: 2720 Train: 0.15165 Test: 0.20114
Epoch: 2800 Train: 0.14780 Test: 0.19228
Epoch: 2880 Train: 0.18955 Test: 0.21243
Epoch: 2960 Train: 0.27840 Test: 0.38523
Epoch: 3040 Train: 0.98660 Test: 0.66196
Epoch: 3120 Train: 0.12691 Test: 0.17433
Epoch: 3200 Train: 0.18398 Test: 0.21226
Epoch: 3280 Train: 0.12624 Test: 0.17394
Epoch: 3360 Train: 0.14631 Test: 0.16761
Epoch: 3440 Train: 0.62395 Test: 0.80079
Epoch: 3520 Train: 0.31257 Test: 0.43030
Epoch: 3600 Train: 0.11826 Test: 0.17208
Epoch: 3680 Train: 0.10146 Test: 0.14716
Epoch: 3760 Train: 0.09424 Test: 0.13795
Epoch: 3840 Train: 0.10558 Test: 0.14969
Epoch: 3920 Train: 0.09637 Test: 0.13735
Epoch: 4000 Train: 0.09162 Test: 0.13116
Epoch: 4080 Train: 0.95555 Test: 1.23845
Epoch: 4160 Train: 0.08276 Test: 0.12556
Epoch: 4240 Train: 0.46123 Test: 0.45009
Epoch: 4320 Train: 0.65009 Test: 0.62465
Epoch: 4400 Train: 0.07777 Test: 0.11778
Epoch: 4480 Train: 0.07833 Test: 0.11759
Epoch: 4560 Train: 0.10932 Test: 0.16697
Epoch: 4640 Train: 0.09149 Test: 0.13654
Epoch: 4720 Train: 0.06936 Test: 0.10956
Epoch: 4800 Train: 0.16348 Test: 0.16283
Epoch: 4880 Train: 0.06618 Test: 0.10526
Epoch: 4960 Train: 0.12055 Test: 0.18625
Epoch: 5040 Train: 0.06347 Test: 0.10212
Epoch: 5120 Train: 0.07557 Test: 0.12220
Epoch: 5200 Train: 0.06113 Test: 0.09929
Epoch: 5280 Train: 0.05958 Test: 0.09696
Epoch: 5360 Train: 0.06416 Test: 0.09887
Epoch: 5440 Train: 0.06259 Test: 0.09913
Epoch: 5520 Train: 0.07316 Test: 0.11327
Epoch: 5600 Train: 0.05596 Test: 0.09264
Epoch: 5680 Train: 0.23346 Test: 0.34977
Epoch: 5760 Train: 0.05383 Test: 0.08961
Epoch: 5840 Train: 0.28226 Test: 0.39111
Epoch: 5920 Train: 0.05186 Test: 0.08773
Epoch: 6000 Train: 0.05084 Test: 0.08659
Epoch: 6080 Train: 0.05281 Test: 0.08581
Epoch: 6160 Train: 0.04932 Test: 0.08455
Epoch: 6240 Train: 0.07017 Test: 0.11652
Epoch: 6320 Train: 0.04809 Test: 0.08384
Epoch: 6400 Train: 0.04708 Test: 0.08180
Epoch: 6480 Train: 0.05349 Test: 0.09689
Epoch: 6560 Train: 0.04584 Test: 0.08024
Epoch: 6640 Train: 0.04511 Test: 0.07931
Epoch: 6720 Train: 0.33438 Test: 0.24159
Epoch: 6800 Train: 0.04411 Test: 0.07837
Epoch: 6880 Train: 0.04341 Test: 0.07723
Epoch: 6960 Train: 0.04277 Test: 0.07638
Epoch: 7040 Train: 0.04324 Test: 0.07594
Epoch: 7120 Train: 0.04174 Test: 0.07516
Epoch: 7200 Train: 0.23031 Test: 0.25047
Epoch: 7280 Train: 0.04071 Test: 0.07370
Epoch: 7360 Train: 0.04097 Test: 0.07343
Epoch: 7440 Train: 0.04017 Test: 0.07339
Epoch 7520: New minimal relative error: 5.96%, model saved.
Epoch: 7520 Train: 0.03935 Test: 0.07249
Epoch: 7600 Train: 0.03873 Test: 0.07111
Epoch: 7680 Train: 0.04126 Test: 0.07266
Epoch: 7760 Train: 0.03785 Test: 0.07015
Epoch: 7840 Train: 0.08872 Test: 0.14811
Epoch: 7920 Train: 0.03707 Test: 0.06901
Epoch: 7999 Train: 0.03668 Test: 0.06842
Training Loss: tensor(0.0367)
Test Loss: tensor(0.0684)
Learned LE: [ 0.9140245  -0.01413422 -5.7606807 ]
True LE: [ 8.6572701e-01 -8.4770648e-03 -1.4541748e+01]
Relative Error: [23.93206   23.597223  23.44334   23.411543  23.445112  23.501228
 23.553492  23.587757  23.597134  23.578041  23.528517  23.447248
 23.334393  23.193953  23.035034  22.871328  22.717445  22.577005
 22.432835  22.243265  21.958643  21.55539   21.069212  20.59163
 20.227833  20.043955  20.048553  20.210337  20.482372  20.821877
 21.197199  21.588846  21.98854   22.396622  22.819712  23.269203
 23.757832  24.297098  24.889915  25.527987  26.189915  26.84896
 27.481264  28.072771  28.618244  29.117584  29.567015  29.955948
 30.26443   30.464813  30.526129  30.422432  30.14032   29.681644
 29.060673  28.293137  27.397541  26.404226  25.368753  24.374495
 23.514036  22.863121  22.454088  22.266882  22.243414  22.313711
 22.419462  22.522818  22.603916  22.653997  22.669632  22.649687
 22.592598  22.497099  22.361225  22.187256  21.983517  21.766628
 21.555258  21.356972  21.153488  20.898201  20.53695   20.053999
 19.506489  19.008303  18.670168  18.544346  18.619432  18.849184
 19.180075  19.567554  19.980085  20.40067   20.823248  21.250761
 21.692297  22.162035  22.674639  23.240435  23.85831   24.511627
 25.171825  25.808823  26.402832  26.947832  27.446896  27.903969
 28.315973  28.669712  28.940693  29.096973  29.104906  28.938982
 28.589893  28.066885  27.388351  26.57269   25.639904  24.627594
 23.603966  22.663193  21.901508  21.385323  21.126049  21.079607
 21.169653  21.32102   21.478815  21.612585  21.70934   21.765038
 21.780188  21.755049  21.689472  21.580364  21.424042  21.219086
 20.971975  20.701351  20.431122  20.173883  19.911     19.589655
 19.152033  18.593914  17.997395  17.497656  17.203033  17.145674
 17.293665  17.588139  17.971447  18.398588  18.839882  19.280964
 19.718409  20.15845   20.613216  21.099428  21.633345  22.222809
 22.85976   23.517376  24.15998   24.758976  25.30354   25.798315
 26.254282  26.677391  27.062422  27.391266  27.633917  27.753565
 27.714294  27.491846  27.0823    26.501604  25.772789  24.914494
 23.950819  22.929504  21.932524  21.064703  20.42004   20.048515
 19.93563   20.014101  20.196276  20.406803  20.598541  20.749084
 20.851332  20.905132  20.913404  20.879202  20.803564  20.683142
 20.510675  20.279459  19.992628  19.669382  19.340054  19.024096
 18.702372  18.315619  17.803366  17.176542  16.545929  16.063927
 15.827732  15.844858  16.064014  16.41712   16.844603  17.302147
 17.763237  18.216057  18.6608    19.1072    19.570728  20.070908
 20.624016  21.233711  21.880508  22.527195  23.13426   23.680874
 24.169828  24.61762   25.039948  25.441608  25.81207   26.126852
 26.35035   26.440624  26.359888  26.08615   25.621944  24.990238
 24.216574  23.320557  22.331518  21.311445  20.358074  19.582407
 19.068974  18.842985  18.863218  19.04501   19.296675  19.548878
 19.762339  19.920696  20.020775  20.065107  20.059671  20.011215
 19.92287   19.792294  19.608868  19.358818  19.038382  18.665937
 18.278868  17.904995  17.526611  17.07619   16.4923    15.805314
 15.157619  14.711048  14.54376   14.636096  14.920731  15.323883
 15.785617  16.263836  16.735086  17.191372  17.63688   18.08452
 18.553623  19.066246  19.637161  20.260998  20.905016  21.521803
 22.07538   22.560146  22.994976  23.405891  23.808947  24.203651
 24.572775  24.884163  25.096363  25.163265  25.04582   24.725481
 24.212564  23.53595   22.722282  21.792116  20.78362   19.776306
 18.884026  18.217636  17.841553  17.751343  17.883633  18.146698
 18.452227  18.737091  18.966866  19.128384  19.219862  19.24648
 19.218136  19.146494  19.04026   18.898844  18.708914  18.448421
 18.102762  17.686268  17.243748  16.814342  16.382284  15.87171
 15.220902  14.485034  13.837462  13.440903  13.346958  13.510217
 13.851136  14.293663  14.77814   15.265806  15.7372875 16.189173
 16.629883  17.075964  17.549738  18.074835  18.661057  19.290266
 19.914202  20.480455  20.966513  21.387716  21.77864   22.168854
 22.56956   22.972374  23.351748  23.66864   23.875359  23.923119
 23.772541  23.410515  22.854782  22.139748  21.29074   20.329418
 19.307528  18.326128  17.51223   16.966707  16.723562  16.750036
 16.9726    17.303028  17.658077  17.975838  18.221777  18.382872
 18.458515  18.45749   18.394705  18.288233  18.154085  17.996635
 17.801952  17.539072  17.178627  16.72581   16.232162  15.751438
 15.270801  14.705154  13.993756  13.22183   12.59043   12.253215
 12.230357  12.455228  12.840464  13.308698 ]
