time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 97.63%, model saved.
Epoch: 0 Train: 59258.87500 Test: 3792.05469
Epoch 80: New minimal relative error: 81.58%, model saved.
Epoch: 80 Train: 4143.39014 Test: 366.75815
Epoch: 160 Train: 608.92755 Test: 246.93279
Epoch: 240 Train: 584.04761 Test: 212.31314
Epoch 320: New minimal relative error: 36.84%, model saved.
Epoch: 320 Train: 84.14017 Test: 13.89396
Epoch: 400 Train: 89.25895 Test: 72.09666
Epoch: 480 Train: 42.14644 Test: 16.66732
Epoch 560: New minimal relative error: 15.49%, model saved.
Epoch: 560 Train: 85.61093 Test: 26.87710
Epoch: 640 Train: 51.78144 Test: 25.66238
Epoch: 720 Train: 75.85537 Test: 47.70895
Epoch: 800 Train: 34.32067 Test: 22.95714
Epoch: 880 Train: 55.86934 Test: 37.83519
Epoch: 960 Train: 176.25803 Test: 24.34815
Epoch: 1040 Train: 79.36596 Test: 66.57693
Epoch 1120: New minimal relative error: 14.18%, model saved.
Epoch: 1120 Train: 3.85417 Test: 9.43874
Epoch: 1200 Train: 121.97489 Test: 52.29752
Epoch: 1280 Train: 22.22236 Test: 15.44718
Epoch 1360: New minimal relative error: 11.24%, model saved.
Epoch: 1360 Train: 9.03571 Test: 10.79149
Epoch: 1440 Train: 13.74240 Test: 14.14766
Epoch 1520: New minimal relative error: 9.13%, model saved.
Epoch: 1520 Train: 5.35105 Test: 9.64579
Epoch: 1600 Train: 8.94779 Test: 10.21522
Epoch: 1680 Train: 30.65795 Test: 13.20899
Epoch: 1760 Train: 36.96618 Test: 27.34912
Epoch: 1840 Train: 32.18640 Test: 13.17428
Epoch: 1920 Train: 6.51271 Test: 9.90312
Epoch: 2000 Train: 12.97875 Test: 11.81719
Epoch 2080: New minimal relative error: 2.28%, model saved.
Epoch: 2080 Train: 1.78222 Test: 7.87425
Epoch: 2160 Train: 2.43996 Test: 8.18787
Epoch: 2240 Train: 4.75240 Test: 8.65464
Epoch: 2320 Train: 6.24319 Test: 9.05099
Epoch: 2400 Train: 3.75679 Test: 7.90230
Epoch: 2480 Train: 8.25585 Test: 9.26767
Epoch: 2560 Train: 2.25488 Test: 7.59080
Epoch: 2640 Train: 14.59207 Test: 15.09312
Epoch: 2720 Train: 1.81202 Test: 8.27054
Epoch: 2800 Train: 1.26010 Test: 7.73681
Epoch: 2880 Train: 2.03648 Test: 7.91968
Epoch: 2960 Train: 2.08121 Test: 8.01246
Epoch: 3040 Train: 2.82302 Test: 8.30257
Epoch: 3120 Train: 5.17864 Test: 10.10439
Epoch: 3200 Train: 1.97614 Test: 7.48096
Epoch: 3280 Train: 2.76218 Test: 8.48660
Epoch 3360: New minimal relative error: 0.69%, model saved.
Epoch: 3360 Train: 0.55797 Test: 7.31809
Epoch: 3440 Train: 1.09845 Test: 7.25782
Epoch: 3520 Train: 3.75842 Test: 9.08305
Epoch: 3600 Train: 2.83614 Test: 8.49150
Epoch: 3680 Train: 3.54471 Test: 8.08271
Epoch: 3760 Train: 3.21881 Test: 8.23966
Epoch: 3840 Train: 9.22734 Test: 8.91907
Epoch: 3920 Train: 25.30687 Test: 9.46939
Epoch: 4000 Train: 0.46853 Test: 7.20158
Epoch: 4080 Train: 0.75716 Test: 7.35126
Epoch: 4160 Train: 1.02519 Test: 7.45822
Epoch: 4240 Train: 1.95125 Test: 7.46311
Epoch: 4320 Train: 0.78899 Test: 7.18229
Epoch: 4400 Train: 0.84976 Test: 7.20734
Epoch: 4480 Train: 0.70701 Test: 7.16996
Epoch: 4560 Train: 2.51298 Test: 7.97848
Epoch: 4640 Train: 4.75572 Test: 9.89159
Epoch: 4720 Train: 5.77240 Test: 8.40161
Epoch: 4800 Train: 1.03793 Test: 7.52343
Epoch: 4880 Train: 0.72968 Test: 7.18030
Epoch: 4960 Train: 7.80626 Test: 10.00453
Epoch: 5040 Train: 6.41766 Test: 8.29183
Epoch: 5120 Train: 0.81441 Test: 7.18214
Epoch: 5200 Train: 0.71238 Test: 7.09353
Epoch: 5280 Train: 0.63702 Test: 7.19651
Epoch: 5360 Train: 1.24228 Test: 7.52435
Epoch: 5440 Train: 1.09353 Test: 7.19950
Epoch: 5520 Train: 1.42483 Test: 7.29445
Epoch: 5600 Train: 0.56893 Test: 7.11392
Epoch: 5680 Train: 1.74965 Test: 7.87543
Epoch: 5760 Train: 0.23898 Test: 7.07134
Epoch: 5840 Train: 0.41952 Test: 7.09962
Epoch: 5920 Train: 0.80630 Test: 7.39451
Epoch: 6000 Train: 4.74739 Test: 9.21021
Epoch: 6080 Train: 2.18435 Test: 7.88147
Epoch: 6160 Train: 4.03985 Test: 8.17079
Epoch: 6240 Train: 0.19911 Test: 7.05469
Epoch: 6320 Train: 0.54338 Test: 7.14962
Epoch: 6400 Train: 1.12781 Test: 7.62500
Epoch: 6480 Train: 5.67873 Test: 7.84565
Epoch: 6560 Train: 0.25701 Test: 7.11937
Epoch: 6640 Train: 0.77022 Test: 7.33757
Epoch: 6720 Train: 2.88178 Test: 8.18020
Epoch: 6800 Train: 0.29633 Test: 7.15979
Epoch: 6880 Train: 0.18849 Test: 7.16197
Epoch: 6960 Train: 0.15807 Test: 7.13395
Epoch: 7040 Train: 0.60273 Test: 7.37412
Epoch: 7120 Train: 2.19328 Test: 7.46213
Epoch: 7200 Train: 3.80752 Test: 8.91337
Epoch: 7280 Train: 0.31752 Test: 7.27343
Epoch: 7360 Train: 0.27390 Test: 7.17974
Epoch: 7440 Train: 0.64242 Test: 7.31413
Epoch: 7520 Train: 2.21841 Test: 8.09351
Epoch: 7600 Train: 0.38287 Test: 7.28434
Epoch: 7680 Train: 1.32628 Test: 7.74412
Epoch: 7760 Train: 1.68747 Test: 7.57545
Epoch: 7840 Train: 0.66085 Test: 7.23960
Epoch: 7920 Train: 1.88744 Test: 7.52685
Epoch: 7999 Train: 2.60717 Test: 8.33482
Training Loss: tensor(2.6072)
Test Loss: tensor(8.3348)
Learned LE: [  0.79787457   0.02694644 -14.482671  ]
True LE: [ 8.2122070e-01 -1.3239830e-03 -1.4496347e+01]
Relative Error: [0.7133923  0.72583055 0.7479588  0.77406746 0.79871464 0.8138826
 0.8122622  0.79348034 0.76348835 0.7292026  0.6945847  0.6606974
 0.62762916 0.5950275  0.5631792  0.5332271  0.50648373 0.4836915
 0.46527353 0.45079806 0.44015735 0.43359292 0.4314864  0.43339902
 0.43683705 0.43812326 0.43603638 0.43590358 0.4485696  0.47990745
 0.51500845 0.531307   0.52003115 0.49827975 0.4886102  0.47935402
 0.45185396 0.422574   0.40671793 0.3983906  0.38996375 0.381356
 0.37457344 0.37052143 0.36938202 0.37172228 0.37797695 0.3884513
 0.4030788  0.42135403 0.44231126 0.46473342 0.48731756 0.5095911
 0.53203285 0.5547513  0.5773522  0.6010174  0.62924343 0.6615501
 0.6880939  0.69952065 0.7022993  0.71146995 0.73034656 0.7540928
 0.77640057 0.7879761  0.7809138  0.75689393 0.7237196  0.6883194
 0.6538756  0.62036353 0.58783144 0.55613446 0.52562666 0.49736416
 0.47263253 0.45198593 0.43525934 0.4221824  0.41260022 0.4067054
 0.4054442  0.40923423 0.41615543 0.4214075  0.42147636 0.41994974
 0.42738798 0.4527809  0.48456186 0.50007653 0.49179786 0.47728157
 0.47453177 0.46316442 0.43220943 0.40779182 0.39685097 0.38773054
 0.3761475  0.36524528 0.35691148 0.35094833 0.34786662 0.3490369
 0.35502183 0.3657973  0.3813648  0.40160295 0.42544812 0.45122546
 0.47686505 0.50119895 0.52439004 0.5477354  0.5710361  0.5937682
 0.6188575  0.6488011  0.6763008  0.6890971  0.6900683  0.6954558
 0.71080554 0.73159164 0.75149333 0.7594452  0.7474734  0.71925104
 0.6842992  0.6491696  0.6152679  0.5826938  0.55151695 0.52163726
 0.493057   0.46671933 0.443757   0.42438227 0.40853786 0.39619142
 0.38739246 0.38188285 0.38041303 0.38449007 0.39383644 0.40369356
 0.40764716 0.40620294 0.40941718 0.42853862 0.45629567 0.47088712
 0.46563214 0.45785898 0.46055418 0.44629472 0.41477555 0.39635584
 0.3876067  0.37517643 0.36050013 0.34815043 0.3378594  0.32915828
 0.32423443 0.32477495 0.33075395 0.3418164  0.3577907  0.37894052
 0.40495172 0.4342119  0.46383604 0.49108657 0.51535594 0.5384476
 0.56189924 0.5845544  0.6071054  0.63361114 0.66095835 0.6760154
 0.6762976  0.6776834  0.68885356 0.70635647 0.72357565 0.72816527
 0.7122347  0.6813014  0.646234   0.61220825 0.57956123 0.548566
 0.5197522  0.49238932 0.4660273  0.44155487 0.41978145 0.40060416
 0.38411888 0.3713204  0.36295688 0.35857418 0.35723194 0.3601303
 0.36949748 0.38311166 0.39262307 0.3932048  0.39341795 0.4062905
 0.42973143 0.4431709  0.44045678 0.43848014 0.44577223 0.42954487
 0.39990395 0.3869733  0.37745607 0.36085525 0.3442835  0.3306699
 0.31725144 0.3057045  0.3003255  0.30167443 0.30841926 0.31943402
 0.3349201  0.3555716  0.38174215 0.41287103 0.4462364  0.47788513
 0.504666   0.527295   0.54949456 0.5721012  0.59334034 0.61560047
 0.64134496 0.6592709  0.6605346  0.65803456 0.66457653 0.6781002
 0.69232416 0.69438434 0.6758921  0.6439616  0.6101785  0.5781961
 0.54717696 0.5184863  0.49278498 0.46820575 0.44392082 0.4214069
 0.40123165 0.38212475 0.36384928 0.34826484 0.3381962  0.33434808
 0.3347159  0.33729973 0.34434283 0.3585607  0.37395963 0.37980515
 0.37883514 0.3857013  0.4045307  0.4163664  0.41559967 0.41786283
 0.42971638 0.4135274  0.38696957 0.37814185 0.36596265 0.3456558
 0.32832754 0.31258866 0.29544583 0.28273124 0.278902   0.28222582
 0.28974354 0.3004754  0.31512278 0.33449116 0.35898465 0.38892648
 0.42332065 0.4588435  0.49033445 0.514553   0.5343989  0.55513287
 0.57612467 0.5953218  0.6171696  0.63716716 0.64185065 0.6367478
 0.6381344  0.6469966  0.6579551  0.6586255  0.63925564 0.60795003
 0.5767046  0.5470907  0.5181508  0.4923026  0.46966612 0.44732907
 0.42522326 0.40622413 0.39010692 0.37341982 0.353893   0.3336207
 0.31760845 0.3096087  0.30973715 0.3144604  0.32046926 0.3312734
 0.3492017  0.36310923 0.36487457 0.3670679  0.3804567  0.39061207
 0.39043802 0.39488232 0.41155127 0.39842188 0.3749699  0.36888885
 0.35371646 0.33083454 0.3128828  0.2942035  0.2743196  0.26262248
 0.2615281  0.26625913 0.27369544 0.28381056 0.29784933 0.31646746
 0.33956137 0.3666596  0.3980054  0.43315375 0.46855417 0.49781385
 0.5178576  0.53459144 0.55372643 0.5720165  0.58928293 0.60850847
 0.61841565 0.61363244 0.6099695  0.6136457  0.62079865 0.6210287
 0.6028385  0.5735644  0.5456938  0.5189845  0.49217576 0.46872422
 0.4482959  0.42768922 0.40855962 0.3950663  0.38587674 0.37429392
 0.35598597 0.33205295 0.30830774 0.2915471  0.2848862  0.28769723
 0.29605687 0.30499148 0.31872496 0.33848375]
