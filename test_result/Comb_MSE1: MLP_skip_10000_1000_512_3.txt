time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.31%, model saved.
Epoch: 0 Train: 4221.82129 Test: 3811.33594
Epoch 100: New minimal relative error: 66.50%, model saved.
Epoch: 100 Train: 77.99546 Test: 79.52141
Epoch 200: New minimal relative error: 29.57%, model saved.
Epoch: 200 Train: 17.24465 Test: 26.35729
Epoch 300: New minimal relative error: 12.47%, model saved.
Epoch: 300 Train: 7.82362 Test: 13.89463
Epoch: 400 Train: 12.59672 Test: 15.35182
Epoch: 500 Train: 4.40848 Test: 7.75634
Epoch: 600 Train: 3.49348 Test: 6.73113
Epoch: 700 Train: 2.87439 Test: 6.07448
Epoch: 800 Train: 2.67881 Test: 5.16422
Epoch: 900 Train: 2.27864 Test: 4.58574
Epoch: 1000 Train: 2.19188 Test: 4.18738
Epoch: 1100 Train: 3.58691 Test: 5.17261
Epoch: 1200 Train: 1.80791 Test: 3.63681
Epoch: 1300 Train: 3.41706 Test: 6.15494
Epoch: 1400 Train: 1.28207 Test: 2.93300
Epoch: 1500 Train: 1.19967 Test: 2.89428
Epoch: 1600 Train: 1.28818 Test: 2.94695
Epoch: 1700 Train: 1.89423 Test: 3.55730
Epoch: 1800 Train: 0.96472 Test: 2.16796
Epoch: 1900 Train: 0.84712 Test: 2.02130
Epoch 2000: New minimal relative error: 11.10%, model saved.
Epoch: 2000 Train: 1.77350 Test: 3.26167
Epoch: 2100 Train: 1.26745 Test: 2.23960
Epoch: 2200 Train: 0.72916 Test: 1.81124
Epoch: 2300 Train: 0.64465 Test: 1.75950
Epoch: 2400 Train: 1.43743 Test: 2.17699
Epoch: 2500 Train: 0.75673 Test: 1.68386
Epoch: 2600 Train: 1.87023 Test: 2.81106
Epoch: 2700 Train: 0.50179 Test: 1.46874
Epoch: 2800 Train: 0.68220 Test: 1.59694
Epoch: 2900 Train: 1.41510 Test: 2.22239
Epoch 3000: New minimal relative error: 10.23%, model saved.
Epoch: 3000 Train: 0.86795 Test: 1.53556
Epoch 3100: New minimal relative error: 9.31%, model saved.
Epoch: 3100 Train: 0.93950 Test: 1.83323
Epoch: 3200 Train: 0.41351 Test: 1.25526
Epoch: 3300 Train: 0.38286 Test: 1.22501
Epoch: 3400 Train: 0.38591 Test: 1.15179
Epoch: 3500 Train: 0.36037 Test: 1.20731
Epoch: 3600 Train: 0.37275 Test: 1.15779
Epoch: 3700 Train: 1.22033 Test: 2.23157
Epoch: 3800 Train: 0.48470 Test: 1.26550
Epoch: 3900 Train: 1.84484 Test: 2.72101
Epoch: 4000 Train: 0.43551 Test: 1.07376
Epoch: 4100 Train: 0.28725 Test: 0.98038
Epoch: 4200 Train: 0.26464 Test: 0.94042
Epoch: 4300 Train: 0.27032 Test: 0.96381
Epoch: 4400 Train: 0.61505 Test: 1.51265
Epoch 4500: New minimal relative error: 9.09%, model saved.
Epoch: 4500 Train: 0.43457 Test: 0.94956
Epoch: 4600 Train: 0.90720 Test: 1.30091
Epoch: 4700 Train: 0.60602 Test: 1.08626
Epoch: 4800 Train: 0.52890 Test: 1.15200
Epoch: 4900 Train: 0.40102 Test: 1.05587
Epoch: 5000 Train: 0.20943 Test: 0.83841
Epoch: 5100 Train: 0.26487 Test: 0.92088
Epoch: 5200 Train: 2.63443 Test: 3.13804
Epoch: 5300 Train: 0.67197 Test: 1.40171
Epoch: 5400 Train: 0.21039 Test: 0.74429
Epoch 5500: New minimal relative error: 7.69%, model saved.
Epoch: 5500 Train: 0.21741 Test: 0.72391
Epoch: 5600 Train: 0.17847 Test: 0.79504
Epoch: 5700 Train: 0.16726 Test: 0.72851
Epoch: 5800 Train: 0.16023 Test: 0.69618
Epoch: 5900 Train: 0.15981 Test: 0.67896
Epoch: 6000 Train: 0.18052 Test: 0.69761
Epoch: 6100 Train: 0.18256 Test: 0.73725
Epoch: 6200 Train: 0.28323 Test: 0.90446
Epoch: 6300 Train: 0.22724 Test: 0.80498
Epoch: 6400 Train: 0.88466 Test: 1.27023
Epoch: 6500 Train: 0.29813 Test: 0.67321
Epoch: 6600 Train: 0.13873 Test: 0.63856
Epoch: 6700 Train: 0.14045 Test: 0.64032
Epoch: 6800 Train: 0.12852 Test: 0.60572
Epoch: 6900 Train: 0.12826 Test: 0.58873
Epoch: 7000 Train: 0.26244 Test: 0.67520
Epoch: 7100 Train: 0.18739 Test: 0.64645
Epoch: 7200 Train: 0.12773 Test: 0.59877
Epoch: 7300 Train: 0.35604 Test: 0.70987
Epoch: 7400 Train: 0.11618 Test: 0.55855
Epoch: 7500 Train: 0.17042 Test: 0.61068
Epoch: 7600 Train: 0.12365 Test: 0.58126
Epoch: 7700 Train: 0.11484 Test: 0.55727
Epoch: 7800 Train: 0.11514 Test: 0.56434
Epoch: 7900 Train: 0.10418 Test: 0.53012
Epoch: 8000 Train: 0.16927 Test: 0.55068
Epoch: 8100 Train: 0.25975 Test: 0.76823
Epoch: 8200 Train: 0.29678 Test: 0.70101
Epoch: 8300 Train: 0.22823 Test: 0.65914
Epoch: 8400 Train: 0.10303 Test: 0.50215
Epoch: 8500 Train: 0.09525 Test: 0.49209
Epoch: 8600 Train: 0.09342 Test: 0.49562
Epoch: 8700 Train: 0.09219 Test: 0.48497
Epoch: 8800 Train: 0.09153 Test: 0.48319
Epoch: 8900 Train: 0.11953 Test: 0.52137
Epoch: 9000 Train: 0.27083 Test: 0.57898
Epoch: 9100 Train: 0.08696 Test: 0.46848
Epoch 9200: New minimal relative error: 7.37%, model saved.
Epoch: 9200 Train: 0.13077 Test: 0.54919
Epoch: 9300 Train: 0.08790 Test: 0.45580
Epoch: 9400 Train: 0.09103 Test: 0.47713
Epoch: 9500 Train: 0.09259 Test: 0.46555
Epoch: 9600 Train: 0.12852 Test: 0.46941
Epoch: 9700 Train: 0.08701 Test: 0.44648
Epoch 9800: New minimal relative error: 5.17%, model saved.
Epoch: 9800 Train: 0.16896 Test: 0.45867
Epoch: 9900 Train: 0.16254 Test: 0.60302
Epoch: 9999 Train: 0.07674 Test: 0.43522
Training Loss: tensor(0.0767)
Test Loss: tensor(0.4352)
Learned LE: [ 8.7219179e-01  1.8176574e-03 -4.1929679e+00]
True LE: [ 8.7385869e-01 -1.4750687e-03 -1.4548089e+01]
Relative Error: [ 6.93701    6.7670503  6.292676   6.078188   5.9347997  5.867179
  5.84289    6.0413218  6.1575575  6.0160713  6.020896   6.139559
  6.3859053  6.795262   7.558839   8.428161   9.307453  10.146247
 10.679165  10.260291   9.320724   8.571255   7.7819204  6.9092236
  6.816492   6.668444   6.278234   5.838454   5.4712048  4.7279143
  4.2625747  3.7709627  3.2653954  2.839108   2.4672034  2.3650718
  2.2995908  2.2755108  1.912868   1.5605099  1.5279896  1.7489129
  2.1278436  2.5879288  3.0853744  3.590948   4.072794   4.510158
  4.810886   5.0672297  5.2513947  5.124129   5.1769433  5.3729453
  5.2532606  5.087033   5.382764   5.6564393  5.9340997  6.0664997
  6.5082455  6.797279   6.5633097  6.6180854  6.177286   5.866196
  5.6139956  5.6439033  5.6182375  5.7928905  5.6774473  5.442833
  5.462835   5.6716194  6.001645   6.3664765  6.967473   7.856329
  8.666087   9.552437  10.261776   9.683219   8.707181   7.8808374
  7.1605644  6.2782636  6.2697835  6.0012174  5.496183   4.945499
  4.537105   4.229337   3.7900634  3.243041   2.686456   2.2281446
  2.0582533  2.0166419  2.043762   1.9839387  1.7018225  1.3095206
  1.2176284  1.4004141  1.738092   2.1819963  2.6907296  3.2297544
  3.776863   4.302765   4.756179   4.9928303  5.2052097  5.131989
  4.960327   5.042183   5.032005   4.8492913  5.131547   5.292803
  5.4636383  5.5129232  5.8299475  6.108922   6.2206955  6.3697844
  6.0387864  5.6664076  5.3505344  5.3583903  5.3002987  5.15498
  4.8900986  4.6089077  4.6517324  4.893845   5.22029    5.691107
  6.2866135  7.0226817  8.0281925  8.884415   9.199005   9.118429
  8.139751   7.0411325  6.641886   5.650213   5.478829   5.3619943
  4.8468776  4.1770315  3.736889   3.5880904  3.376305   2.7686255
  2.178604   1.7924281  1.6800126  1.7847373  1.8363856  1.8408437
  1.6463451  1.2711395  1.0673989  1.1010666  1.3618091  1.7929945
  2.326171   2.907309   3.493809   4.0475364  4.551169   4.8535795
  4.9215097  4.918924   4.6589193  4.659181   4.6120157  4.399588
  4.4831753  4.6202736  4.8451853  4.9734344  5.227521   5.58827
  5.9221754  6.0989184  5.786656   5.3281813  5.0317574  4.9548197
  4.9877915  4.499954   4.2736163  3.9493892  3.8347623  3.9636002
  4.3621755  4.9261193  5.334667   5.9303546  7.0115514  7.993292
  7.977454   7.954803   7.6890907  6.5200796  6.0448513  5.232681
  4.840153   4.6010036  4.1864557  3.4827802  3.1126018  2.8975754
  2.9165668  2.3398583  1.825576   1.6153349  1.558396   1.719911
  1.7469201  1.84563    1.6496964  1.4506367  1.3480346  1.1492124
  1.1181443  1.3991718  1.940757   2.6191552  3.2924871  3.903587
  4.247746   4.4611716  4.619845   4.673666   4.4442205  4.285131
  4.082507   3.9357953  3.8704128  3.8208385  3.8758848  4.051211
  4.438247   5.019321   5.6724186  5.853384   5.580683   4.9565663
  4.5468025  4.665909   4.7045817  4.1343985  3.9086838  3.6981874
  3.4833632  3.4516811  3.5927649  3.9124436  4.3365726  4.876422
  5.7407966  6.727102   6.9542885  6.762662   6.7130036  6.187542
  5.5609846  5.034923   4.359374   3.9591384  3.5194025  2.9078245
  2.5291257  2.2090778  2.1491675  2.1345356  1.80004    1.5045327
  1.3776151  1.4734956  1.560745   1.6439447  1.6986659  1.6806593
  1.5668406  1.5151567  1.1932124  1.1301308  1.5406017  2.2410405
  3.0371273  3.6554365  3.9984868  4.202396   4.242336   4.2668486
  4.17758    3.9529746  3.7124429  3.4563336  3.3700593  3.076396
  3.0098228  3.157368   3.3509254  4.20259    5.0392118  5.498291
  5.439618   4.674123   4.3789124  4.34521    4.2315025  3.791427
  3.4865324  3.3681388  3.1014574  3.114987   3.226024   3.2093751
  3.3660383  3.9817424  4.660001   5.1323338  5.8187356  5.8229637
  5.574359   5.4681754  4.863171   4.7034903  4.157893   3.5584848
  2.9781067  2.4654655  2.0917242  1.7645369  1.4598272  1.5595341
  1.8380145  1.4243941  1.1984113  1.3100868  1.4081767  1.4764994
  1.6317728  1.7779847  1.7044593  1.5907358  1.4447985  1.0979623
  1.196959   1.8545263  2.7188523  3.265575   3.7452023  4.0269294
  4.0837073  4.0070705  3.865844   3.648985   3.5603127  3.2357585
  3.0122318  2.7430768  2.2634876  2.1195912  2.3713293  3.1581686
  4.085191   4.887638   4.9190335  4.6835437  4.2379293  4.062261
  3.6750133  3.2464166  3.007298   2.9211175  2.645872   2.6524398
  2.7360108  2.7907665  2.944066   3.135231   3.8042836  3.8506076
  3.9505873  4.545845   4.5219736  4.2627664  4.0815344  3.758549
  3.636733   3.433232   2.7749574  2.2268598]
