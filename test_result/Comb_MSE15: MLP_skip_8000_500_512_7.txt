time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.51%, model saved.
Epoch: 0 Train: 3966.84863 Test: 3892.96436
Epoch: 80 Train: 322.88651 Test: 399.42493
Epoch 160: New minimal relative error: 55.39%, model saved.
Epoch: 160 Train: 61.52567 Test: 61.08636
Epoch 240: New minimal relative error: 23.69%, model saved.
Epoch: 240 Train: 27.14375 Test: 19.13101
Epoch: 320 Train: 24.03245 Test: 34.65210
Epoch 400: New minimal relative error: 20.57%, model saved.
Epoch: 400 Train: 6.24650 Test: 7.43082
Epoch: 480 Train: 14.75794 Test: 13.67546
Epoch: 560 Train: 4.76721 Test: 6.20320
Epoch: 640 Train: 7.64988 Test: 6.09422
Epoch 720: New minimal relative error: 17.55%, model saved.
Epoch: 720 Train: 4.60743 Test: 6.96728
Epoch 800: New minimal relative error: 16.30%, model saved.
Epoch: 800 Train: 5.60603 Test: 6.34499
Epoch: 880 Train: 7.95605 Test: 7.80035
Epoch: 960 Train: 10.05372 Test: 12.71966
Epoch: 1040 Train: 4.15119 Test: 5.55410
Epoch: 1120 Train: 20.52349 Test: 14.32580
Epoch: 1200 Train: 13.33260 Test: 12.66107
Epoch 1280: New minimal relative error: 10.65%, model saved.
Epoch: 1280 Train: 1.40638 Test: 1.75711
Epoch: 1360 Train: 2.72588 Test: 3.61037
Epoch: 1440 Train: 2.45788 Test: 2.40053
Epoch: 1520 Train: 1.55397 Test: 1.75834
Epoch: 1600 Train: 6.89251 Test: 6.78210
Epoch: 1680 Train: 1.77462 Test: 2.31927
Epoch: 1760 Train: 0.98948 Test: 1.35445
Epoch: 1840 Train: 1.82589 Test: 2.11976
Epoch: 1920 Train: 2.04796 Test: 2.68179
Epoch: 2000 Train: 2.81680 Test: 3.14146
Epoch 2080: New minimal relative error: 7.84%, model saved.
Epoch: 2080 Train: 1.79428 Test: 1.60809
Epoch: 2160 Train: 0.50283 Test: 0.71624
Epoch: 2240 Train: 1.27706 Test: 1.64476
Epoch: 2320 Train: 4.59121 Test: 4.27304
Epoch: 2400 Train: 0.83151 Test: 1.17788
Epoch: 2480 Train: 0.80470 Test: 1.00181
Epoch: 2560 Train: 0.77552 Test: 0.98732
Epoch: 2640 Train: 0.63206 Test: 0.88323
Epoch: 2720 Train: 0.62302 Test: 0.57776
Epoch: 2800 Train: 2.53687 Test: 2.57424
Epoch: 2880 Train: 0.42119 Test: 0.65064
Epoch: 2960 Train: 1.23352 Test: 1.23995
Epoch: 3040 Train: 0.39276 Test: 0.63652
Epoch: 3120 Train: 0.24345 Test: 0.38273
Epoch: 3200 Train: 1.97722 Test: 2.69741
Epoch: 3280 Train: 0.75298 Test: 1.05223
Epoch: 3360 Train: 0.77152 Test: 1.05397
Epoch: 3440 Train: 0.29003 Test: 0.43494
Epoch: 3520 Train: 0.35376 Test: 0.49850
Epoch: 3600 Train: 0.62756 Test: 0.75067
Epoch: 3680 Train: 0.23633 Test: 0.38325
Epoch: 3760 Train: 0.89173 Test: 1.14536
Epoch 3840: New minimal relative error: 6.69%, model saved.
Epoch: 3840 Train: 0.38869 Test: 0.53869
Epoch: 3920 Train: 0.68844 Test: 0.41455
Epoch: 4000 Train: 0.38743 Test: 0.56619
Epoch: 4080 Train: 0.50191 Test: 0.66119
Epoch: 4160 Train: 0.68963 Test: 0.64700
Epoch: 4240 Train: 0.24286 Test: 0.34753
Epoch: 4320 Train: 0.98281 Test: 1.21328
Epoch: 4400 Train: 0.15509 Test: 0.24262
Epoch: 4480 Train: 0.15463 Test: 0.22664
Epoch: 4560 Train: 0.22789 Test: 0.35773
Epoch: 4640 Train: 0.95655 Test: 1.38128
Epoch: 4720 Train: 0.10169 Test: 0.19520
Epoch: 4800 Train: 0.12258 Test: 0.29583
Epoch: 4880 Train: 1.43007 Test: 1.33051
Epoch: 4960 Train: 0.44758 Test: 0.58017
Epoch: 5040 Train: 0.12399 Test: 0.23761
Epoch: 5120 Train: 0.14721 Test: 0.27165
Epoch: 5200 Train: 0.63517 Test: 0.79016
Epoch: 5280 Train: 0.08737 Test: 0.17119
Epoch: 5360 Train: 0.13653 Test: 0.22969
Epoch: 5440 Train: 0.40549 Test: 0.38434
Epoch: 5520 Train: 0.54085 Test: 0.44366
Epoch: 5600 Train: 0.21445 Test: 0.28351
Epoch: 5680 Train: 0.63001 Test: 0.52853
Epoch: 5760 Train: 0.20417 Test: 0.29737
Epoch: 5840 Train: 0.08022 Test: 0.15862
Epoch: 5920 Train: 0.08237 Test: 0.15854
Epoch: 6000 Train: 0.12355 Test: 0.20281
Epoch: 6080 Train: 0.10799 Test: 0.17615
Epoch: 6160 Train: 0.45371 Test: 0.42410
Epoch: 6240 Train: 0.75809 Test: 0.51176
Epoch: 6320 Train: 0.70942 Test: 0.61435
Epoch: 6400 Train: 0.16565 Test: 0.25616
Epoch: 6480 Train: 0.07127 Test: 0.13901
Epoch: 6560 Train: 0.08395 Test: 0.15817
Epoch: 6640 Train: 0.08965 Test: 0.15306
Epoch: 6720 Train: 1.20989 Test: 1.10419
Epoch: 6800 Train: 0.06269 Test: 0.12753
Epoch: 6880 Train: 0.08004 Test: 0.14214
Epoch 6960: New minimal relative error: 4.89%, model saved.
Epoch: 6960 Train: 0.06680 Test: 0.12959
Epoch: 7040 Train: 0.07611 Test: 0.15120
Epoch: 7120 Train: 0.40706 Test: 0.46580
Epoch: 7200 Train: 0.38179 Test: 0.53616
Epoch: 7280 Train: 0.05786 Test: 0.12003
Epoch: 7360 Train: 0.06876 Test: 0.12658
Epoch: 7440 Train: 0.12261 Test: 0.12917
Epoch: 7520 Train: 0.91537 Test: 1.09335
Epoch: 7600 Train: 0.05337 Test: 0.11207
Epoch: 7680 Train: 0.05539 Test: 0.12255
Epoch: 7760 Train: 0.25674 Test: 0.36246
Epoch: 7840 Train: 0.23548 Test: 0.33897
Epoch: 7920 Train: 0.31230 Test: 0.32017
Epoch: 7999 Train: 0.05567 Test: 0.11367
Training Loss: tensor(0.0557)
Test Loss: tensor(0.1137)
Learned LE: [ 0.8425287   0.01505602 -3.4987478 ]
True LE: [ 8.7470067e-01  9.2279324e-03 -1.4564684e+01]
Relative Error: [21.060638  20.766808  20.401804  19.587551  18.17923   17.046986
 16.135092  15.216881  14.256046  13.258439  12.284124  11.445275
 10.8511915 10.296878   9.782118   9.186938   8.838336   8.62653
  8.316717   8.143313   8.287142   8.6781435  9.025636   9.377955
  9.516358   9.300698   9.100187   8.861895   9.187676   9.636855
 10.261076  10.989784  12.139628  12.878249  13.076965  13.351922
 13.853544  14.265253  14.561211  14.582666  14.583015  14.890485
 15.334135  15.775138  15.987503  16.150719  16.337774  16.44446
 16.504015  16.50244   16.66352   17.057405  17.5121    17.9555
 18.382942  18.78622   19.188292  19.537235  19.852745  19.969055
 19.86361   19.903296  19.819603  19.546154  19.129005  18.324594
 16.959621  15.88833   15.102175  14.016562  13.086097  12.129126
 11.206966  10.507211   9.966096   9.391307   8.840306   8.378718
  8.052397   7.642251   7.322551   7.138638   7.3207765  7.795126
  8.302405   8.732774   8.923162   8.773659   8.517032   8.184913
  8.319345   8.706717   9.346902  10.023867  11.099865  11.830081
 12.041415  12.306634  12.815096  13.194841  13.349783  13.268895
 13.375264  13.722295  14.277999  14.631106  14.846828  15.058938
 15.192938  15.282115  15.252317  15.270977  15.43383   15.816511
 16.187502  16.669086  17.105682  17.545261  17.93297   18.202534
 18.604006  18.754847  18.655989  18.53638   18.516262  18.340412
 17.911055  17.146223  15.825222  14.882384  14.098294  12.972939
 12.042497  11.192167  10.316674   9.664104   9.180886   8.575548
  8.074336   7.641461   7.222353   6.8589354  6.5398426  6.384595
  6.577835   6.9732256  7.53749    7.9803953  8.278879   8.194834
  7.992185   7.5678267  7.547201   7.8662505  8.392036   9.073302
 10.138548  10.827929  10.985933  11.221338  11.743994  12.137175
 12.184428  12.0324135 12.206427  12.579361  13.173124  13.514434
 13.764897  13.976882  14.094037  14.145376  14.125936  14.1802025
 14.20172   14.473568  14.812056  15.339549  15.841363  16.214155
 16.555254  16.877005  17.305788  17.54823   17.510588  17.28591
 17.172737  17.107336  16.70937   15.971918  14.810837  13.988566
 13.195427  12.038267  11.146859  10.349233   9.511273   8.888232
  8.467128   7.9580717  7.5550923  6.9998913  6.51178    6.1169343
  5.809557   5.5760155  5.7783422  6.105421   6.5389724  7.106071
  7.515955   7.6244264  7.453309   7.058785   6.9369397  7.0720897
  7.491733   8.112266   9.055864   9.829588   9.934366  10.146612
 10.594297  10.936582  11.002144  10.889364  11.003946  11.441169
 11.996057  12.401865  12.718921  12.960373  13.106794  13.10486
 13.049117  12.903702  12.842056  13.11437   13.44733   13.949092
 14.419728  14.834971  15.30265   15.583268  15.992125  16.333414
 16.397785  16.177063  15.924907  15.847352  15.55602   14.766283
 13.908869  13.148759  12.391491  11.246156  10.395376   9.572191
  8.742311   8.231895   7.839843   7.4447     7.0475802  6.546998
  6.026936   5.45625    5.031007   4.8368044  4.923644   5.2964983
  5.604734   6.03767    6.5817857  6.9421263  6.9182196  6.6222005
  6.3871036  6.407848   6.6312723  7.1255865  7.8745546  8.779037
  8.888356   9.109526   9.504537   9.718075   9.780814   9.704552
  9.788631  10.295771  10.7984085 11.334086  11.653899  11.961909
 12.033819  11.900032  11.607193  11.468637  11.368008  11.616661
 12.005547  12.523993  13.065367  13.54835   14.01743   14.41858
 14.82345   15.074897  15.157279  15.048641  14.666035  14.573224
 14.472787  13.68946   13.002319  12.28473   11.518058  10.527717
  9.684934   8.843453   8.063312   7.591359   7.2374234  6.8543262
  6.4745827  6.0278463  5.582145   5.0728354  4.5004463  4.2962694
  4.293286   4.4758935  4.776194   5.1832466  5.633148   6.116529
  6.3466563  6.1914253  5.8694634  5.7687554  5.7495594  6.117469
  6.7026787  7.4921103  7.731451   8.03282    8.3671465  8.56691
  8.567399   8.456077   8.561183   9.014537   9.633097  10.280315
 10.574121  10.882932  10.791078  10.437293  10.157989  10.0205965
 10.003855  10.20793   10.7294035 11.21388   11.731559  12.257307
 12.667627  13.101581  13.514823  13.855503  13.991774  13.924302
 13.501784  13.211531  13.100459  12.724868  12.139557  11.320589
 10.508497   9.632603   8.829684   8.120331   7.378602   6.859777
  6.4569354  6.1420293  5.779674   5.411456   5.031346   4.6796603
  4.274167   4.014419   3.9193366  3.8609498  4.092798   4.423018
  4.8858514  5.312642   5.6607356  5.628961 ]
