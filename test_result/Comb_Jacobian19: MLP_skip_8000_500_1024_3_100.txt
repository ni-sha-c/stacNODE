time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.74%, model saved.
Epoch: 0 Train: 9608.23828 Test: 4090.12134
Epoch 80: New minimal relative error: 96.86%, model saved.
Epoch: 80 Train: 2407.52734 Test: 995.47864
Epoch: 160 Train: 1643.62842 Test: 563.90326
Epoch: 240 Train: 937.62671 Test: 224.28969
Epoch: 320 Train: 464.24655 Test: 107.23397
Epoch 400: New minimal relative error: 80.16%, model saved.
Epoch: 400 Train: 269.80441 Test: 63.81440
Epoch 480: New minimal relative error: 17.66%, model saved.
Epoch: 480 Train: 219.20200 Test: 30.96514
Epoch 560: New minimal relative error: 14.30%, model saved.
Epoch: 560 Train: 117.63566 Test: 16.03764
Epoch: 640 Train: 152.41080 Test: 47.38668
Epoch: 720 Train: 81.65100 Test: 12.58564
Epoch 800: New minimal relative error: 12.97%, model saved.
Epoch: 800 Train: 76.45992 Test: 12.13265
Epoch 880: New minimal relative error: 7.86%, model saved.
Epoch: 880 Train: 50.98172 Test: 3.91524
Epoch: 960 Train: 97.70751 Test: 41.29964
Epoch 1040: New minimal relative error: 7.66%, model saved.
Epoch: 1040 Train: 42.78767 Test: 4.77960
Epoch 1120: New minimal relative error: 6.15%, model saved.
Epoch: 1120 Train: 33.52425 Test: 2.06094
Epoch: 1200 Train: 29.72050 Test: 3.10508
Epoch: 1280 Train: 30.64223 Test: 5.97197
Epoch: 1360 Train: 25.05056 Test: 2.48409
Epoch: 1440 Train: 29.42475 Test: 2.87535
Epoch: 1520 Train: 21.43693 Test: 0.99260
Epoch: 1600 Train: 34.67225 Test: 4.64175
Epoch: 1680 Train: 30.63725 Test: 2.92332
Epoch: 1760 Train: 23.66019 Test: 7.46208
Epoch: 1840 Train: 30.65680 Test: 7.91114
Epoch: 1920 Train: 16.20970 Test: 2.73325
Epoch: 2000 Train: 14.80690 Test: 0.99382
Epoch: 2080 Train: 15.39222 Test: 1.43781
Epoch: 2160 Train: 30.04491 Test: 17.08382
Epoch: 2240 Train: 30.20930 Test: 9.72729
Epoch: 2320 Train: 22.64782 Test: 7.76500
Epoch: 2400 Train: 21.38994 Test: 3.66372
Epoch: 2480 Train: 11.48111 Test: 1.06399
Epoch 2560: New minimal relative error: 4.33%, model saved.
Epoch: 2560 Train: 10.93549 Test: 0.18241
Epoch: 2640 Train: 10.48752 Test: 0.21828
Epoch: 2720 Train: 11.44675 Test: 0.86884
Epoch: 2800 Train: 21.39669 Test: 3.04283
Epoch: 2880 Train: 10.97266 Test: 1.66259
Epoch: 2960 Train: 9.26418 Test: 0.16161
Epoch: 3040 Train: 9.32267 Test: 0.13525
Epoch: 3120 Train: 9.63677 Test: 0.68969
Epoch: 3200 Train: 10.83590 Test: 0.59106
Epoch 3280: New minimal relative error: 4.10%, model saved.
Epoch: 3280 Train: 8.52561 Test: 0.91562
Epoch: 3360 Train: 16.80621 Test: 6.20466
Epoch: 3440 Train: 8.44415 Test: 0.81204
Epoch 3520: New minimal relative error: 2.59%, model saved.
Epoch: 3520 Train: 7.59509 Test: 0.22726
Epoch: 3600 Train: 7.60086 Test: 0.35237
Epoch: 3680 Train: 7.32659 Test: 0.08101
Epoch: 3760 Train: 7.64860 Test: 0.23015
Epoch: 3840 Train: 10.36689 Test: 1.42040
Epoch: 3920 Train: 6.62739 Test: 0.05906
Epoch: 4000 Train: 6.72359 Test: 0.11108
Epoch: 4080 Train: 6.80260 Test: 0.16995
Epoch: 4160 Train: 7.69909 Test: 0.43789
Epoch: 4240 Train: 10.87958 Test: 4.92565
Epoch: 4320 Train: 6.08579 Test: 0.05793
Epoch: 4400 Train: 6.10668 Test: 0.07872
Epoch: 4480 Train: 8.05418 Test: 0.56352
Epoch: 4560 Train: 5.78847 Test: 0.04575
Epoch: 4640 Train: 5.85845 Test: 0.06205
Epoch: 4720 Train: 6.41564 Test: 0.05915
Epoch: 4800 Train: 5.56294 Test: 0.03947
Epoch 4880: New minimal relative error: 2.34%, model saved.
Epoch: 4880 Train: 6.62994 Test: 0.08532
Epoch: 4960 Train: 5.67081 Test: 0.04207
Epoch: 5040 Train: 8.88041 Test: 1.88374
Epoch: 5120 Train: 5.52527 Test: 0.04013
Epoch: 5200 Train: 6.60595 Test: 1.87721
Epoch: 5280 Train: 5.52936 Test: 0.03829
Epoch: 5360 Train: 6.83403 Test: 0.39592
Epoch 5440: New minimal relative error: 1.78%, model saved.
Epoch: 5440 Train: 5.16809 Test: 0.03733
Epoch: 5520 Train: 7.49925 Test: 0.10033
Epoch: 5600 Train: 5.17824 Test: 0.04680
Epoch: 5680 Train: 5.06918 Test: 0.06912
Epoch: 5760 Train: 4.98037 Test: 0.03975
Epoch: 5840 Train: 5.09576 Test: 0.05532
Epoch: 5920 Train: 5.47754 Test: 0.29318
Epoch: 6000 Train: 4.93183 Test: 0.03315
Epoch: 6080 Train: 5.16647 Test: 0.06533
Epoch: 6160 Train: 4.82736 Test: 0.03302
Epoch: 6240 Train: 4.78259 Test: 0.05701
Epoch: 6320 Train: 4.60715 Test: 0.03759
Epoch: 6400 Train: 4.65496 Test: 0.17011
Epoch: 6480 Train: 4.69938 Test: 0.05837
Epoch: 6560 Train: 4.50508 Test: 0.04845
Epoch: 6640 Train: 4.48792 Test: 0.03624
Epoch: 6720 Train: 4.48326 Test: 0.05409
Epoch 6800: New minimal relative error: 1.56%, model saved.
Epoch: 6800 Train: 4.51807 Test: 0.04145
Epoch: 6880 Train: 5.03329 Test: 0.10711
Epoch: 6960 Train: 4.40040 Test: 0.03059
Epoch: 7040 Train: 4.33205 Test: 0.03888
Epoch: 7120 Train: 4.20656 Test: 0.03492
Epoch: 7200 Train: 4.34986 Test: 0.03295
Epoch: 7280 Train: 4.19903 Test: 0.02670
Epoch: 7360 Train: 4.58988 Test: 0.11736
Epoch: 7440 Train: 4.09525 Test: 0.02495
Epoch: 7520 Train: 4.09749 Test: 0.04530
Epoch: 7600 Train: 4.00267 Test: 0.02616
Epoch: 7680 Train: 4.02346 Test: 0.02877
Epoch: 7760 Train: 3.92926 Test: 0.02621
Epoch: 7840 Train: 4.75671 Test: 0.49629
Epoch: 7920 Train: 3.89604 Test: 0.02724
Epoch: 7999 Train: 5.74220 Test: 1.48237
Training Loss: tensor(5.7422)
Test Loss: tensor(1.4824)
Learned LE: [  0.8146121    0.04744692 -14.533525  ]
True LE: [ 8.5297197e-01  4.6953312e-03 -1.4535044e+01]
Relative Error: [2.004558   1.855053   1.2911955  0.53369844 0.27435067 0.7283473
 0.98551494 1.3057538  1.8112504  2.2470617  2.3488748  2.27202
 2.2566822  2.118036   1.9202822  2.1081707  2.156729   1.9725674
 1.6049825  1.3094295  1.1006138  0.94241744 0.84269756 0.79095227
 0.81128407 0.85799927 0.873427   0.66974294 0.41868544 0.40822938
 0.52297294 0.46189198 0.62157285 0.49686682 0.33630565 0.27368245
 0.38011155 0.6787923  0.95634043 0.94715756 1.1587828  1.3426521
 1.4435657  1.5920844  1.9603336  1.876777   1.4569019  1.0560397
 0.8625794  0.6903195  0.69686663 0.93009955 1.4682287  1.9208599
 2.212743   2.692727   2.0129967  1.2971973  0.5155722  0.58653545
 1.2068497  1.8602259  2.0766296  1.8700793  1.446585   0.7014397
 0.11150017 0.51050276 0.78899246 1.0978261  1.4294562  1.7902052
 2.0890768  2.0708227  1.9783622  1.868902   1.7409577  1.7618941
 2.0183113  1.8873311  1.5743589  1.2310697  1.0675368  0.92018133
 0.7288098  0.7183535  0.71387523 0.78001785 0.692469   0.53434
 0.3357219  0.35316747 0.49725288 0.38022014 0.41255328 0.58796406
 0.35809344 0.2565906  0.377662   0.68555015 0.9325916  0.86113197
 0.9585571  1.202056   1.3867089  1.4586827  1.635574   1.6207086
 1.3506907  0.95155287 0.7683285  0.5923244  0.55144143 0.8515012
 1.3370337  1.9152293  2.2449415  2.6913922  2.1402898  1.5421692
 0.7654576  0.26615575 0.8383925  1.3351083  1.83106    1.754875
 1.4260825  0.80797124 0.14714345 0.44576782 0.6465455  0.93668896
 1.1512461  1.3926784  1.599841   1.7626908  1.663178   1.5681218
 1.4830321  1.483136   1.6409574  1.7371287  1.5218654  1.2232215
 0.95346653 0.87937105 0.6726162  0.6347742  0.6290128  0.71238375
 0.6062395  0.4376727  0.25699604 0.2673655  0.3021036  0.34902337
 0.31916603 0.44929242 0.4169821  0.20831671 0.23175119 0.4490311
 0.87599856 0.85074323 0.812414   0.94233686 1.2007858  1.3620499
 1.3663907  1.3750687  1.2754366  0.8646705  0.6050731  0.5556143
 0.43548536 0.6789702  1.1067492  1.7055624  2.1036184  2.575048
 2.1699097  1.7547895  1.0715563  0.55987036 0.39547697 0.93328476
 1.5712034  1.8333873  1.603904   0.89042735 0.27516842 0.36865196
 0.568794   0.90563583 1.1438067  1.1626225  1.2256156  1.2483301
 1.3314198  1.2695074  1.214148   1.3015649  1.3188419  1.4147723
 1.41846    1.1571407  0.9458673  0.7736103  0.67236435 0.6413824
 0.6462544  0.62083143 0.67245734 0.4551102  0.3189631  0.21606621
 0.2135649  0.32858002 0.34963286 0.33239335 0.49334672 0.23098883
 0.19450702 0.219354   0.523528   0.7534358  0.697681   0.764838
 0.92055166 1.1310196  1.2290546  1.1092558  1.1201005  1.0291802
 0.54674506 0.37663263 0.31228104 0.3508993  0.7423555  1.2684271
 1.7492615  2.3362362  2.08144    1.7999923  1.4344373  1.093242
 0.49351278 0.5315443  1.1780205  1.5930252  1.7710791  1.0886079
 0.42532343 0.27354804 0.58396775 0.8691074  1.0880197  1.2196053
 1.0567609  0.93260676 0.8517986  0.94245344 1.0072842  1.0940347
 1.1560724  1.1123186  1.1280047  1.1736779  1.0140086  0.9058899
 0.75817317 0.6556974  0.7176877  0.679118   0.66155887 0.647933
 0.41777837 0.2619037  0.23021962 0.3078825  0.39136726 0.37524477
 0.35243055 0.43468863 0.14746463 0.2105189  0.23451594 0.5307443
 0.50688374 0.5049739  0.7401076  0.8893084  1.0569847  1.0175525
 0.8328724  1.019866   0.89957565 0.47473022 0.17619534 0.21535183
 0.3064092  0.63816625 1.1968402  1.8420233  1.8519977  1.6205134
 1.620688   1.4005573  0.9933713  0.4168711  0.7029963  1.315437
 1.5764253  1.4872588  0.6446639  0.21249917 0.40156823 0.77989066
 1.1020257  1.1578366  1.0467144  0.875123   0.7417743  0.7374174
 0.74583983 0.8883044  1.0073557  1.0221642  0.99758005 1.0085355
 1.0581805  0.9445958  0.9380728  0.8206017  0.83097976 0.7866664
 0.7553121  0.72655857 0.6227221  0.3983973  0.2604054  0.29010987
 0.3643219  0.42840654 0.44915518 0.293925   0.32717937 0.20116551
 0.29916003 0.18912753 0.3571404  0.3073497  0.37817407 0.77462006
 0.92419374 0.9107261  0.82818884 0.68861127 0.9610471  0.8301966
 0.35970524 0.1303505  0.23280266 0.28758746 0.5012133  0.94683754
 1.4489081  1.1824787  1.4532881  1.4888582  1.3192825  0.96867645
 0.47836363 0.8135316  1.2659024  1.4784931  1.0802358  0.28290585
 0.16659069 0.32382253 0.83495575 1.0893177  1.0725123  0.81789184
 0.63227165 0.7011785  0.6783061  0.645566   0.6941383  0.80205745
 0.88159096 0.96071243 0.85561854 0.78823596 0.7761908  0.8257454
 0.7663347  0.8101794  0.8249693  0.8882178 ]
