time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 500.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 17.903945923 Test: 14.916126251
Epoch 0: New minimal relative error: 14.92%, model saved.
Epoch: 100 Train: 3.510908127 Test: 3.549592495
Epoch 100: New minimal relative error: 3.55%, model saved.
Epoch: 200 Train: 3.465002537 Test: 3.487403870
Epoch 200: New minimal relative error: 3.49%, model saved.
Epoch: 300 Train: 3.468545675 Test: 3.487351894
Epoch 300: New minimal relative error: 3.49%, model saved.
Epoch: 400 Train: 3.434099197 Test: 3.432200432
Epoch 400: New minimal relative error: 3.43%, model saved.
Epoch: 500 Train: 3.454900026 Test: 3.455244064
Epoch: 600 Train: 3.421607971 Test: 3.447693825
Epoch: 700 Train: 3.440636396 Test: 3.469234705
Epoch: 800 Train: 3.437715054 Test: 3.450758457
Epoch: 900 Train: 3.394846678 Test: 3.413240433
Epoch 900: New minimal relative error: 3.41%, model saved.
Epoch: 1000 Train: 3.375336647 Test: 3.406311989
Epoch 1000: New minimal relative error: 3.41%, model saved.
Epoch: 1100 Train: 3.415072918 Test: 3.442179680
Epoch: 1200 Train: 3.409857750 Test: 3.424730778
Epoch: 1300 Train: 3.404556274 Test: 3.418286800
Epoch: 1400 Train: 3.425270319 Test: 3.443764210
Epoch: 1500 Train: 3.431645393 Test: 3.450499058
Epoch: 1600 Train: 3.448947430 Test: 3.472846985
Epoch: 1700 Train: 3.464965343 Test: 3.493381500
Epoch: 1800 Train: 3.472592831 Test: 3.499359846
Epoch: 1900 Train: 3.465155602 Test: 3.488164186
Epoch: 2000 Train: 3.471268892 Test: 3.500125885
Epoch: 2100 Train: 3.475936413 Test: 3.506368399
Epoch: 2200 Train: 3.493809700 Test: 3.529425144
Epoch: 2300 Train: 3.501621246 Test: 3.533525467
Epoch: 2400 Train: 3.525699854 Test: 3.552059174
Epoch: 2500 Train: 3.512455940 Test: 3.541359663
Epoch: 2600 Train: 3.483481169 Test: 3.512730598
Epoch: 2700 Train: 3.525768042 Test: 3.554478168
Epoch: 2800 Train: 3.499299526 Test: 3.539722443
Epoch: 2900 Train: 3.482717037 Test: 3.521495819
Epoch: 3000 Train: 3.499013424 Test: 3.532598495
Epoch: 3100 Train: 3.492809057 Test: 3.527359486
Epoch: 3200 Train: 3.477156639 Test: 3.523593903
Epoch: 3300 Train: 3.490712166 Test: 3.532733917
Epoch: 3400 Train: 3.482534647 Test: 3.522037983
Epoch: 3500 Train: 3.470763206 Test: 3.508331776
Epoch: 3600 Train: 3.514273882 Test: 3.531720638
Epoch: 3700 Train: 3.507580757 Test: 3.527144432
Epoch: 3800 Train: 3.495161772 Test: 3.519028664
Epoch: 3900 Train: 3.500285625 Test: 3.518978119
Epoch: 4000 Train: 3.496161222 Test: 3.515415430
Epoch: 4100 Train: 3.487023354 Test: 3.507562160
Epoch: 4200 Train: 3.469754457 Test: 3.500886917
Epoch: 4300 Train: 3.478856564 Test: 3.515419006
Epoch: 4400 Train: 3.493556976 Test: 3.536394835
Epoch: 4500 Train: 3.503720999 Test: 3.545433521
Epoch: 4600 Train: 3.521135807 Test: 3.555832148
Epoch: 4700 Train: 3.509652138 Test: 3.551975727
Epoch: 4800 Train: 3.518651485 Test: 3.553708076
Epoch: 4900 Train: 3.534189224 Test: 3.560462952
Epoch: 5000 Train: 3.514799833 Test: 3.552702427
Epoch: 5100 Train: 3.509317398 Test: 3.547151327
Epoch: 5200 Train: 3.513520241 Test: 3.550464630
Epoch: 5300 Train: 3.515279055 Test: 3.551452637
Epoch: 5400 Train: 3.516887665 Test: 3.551069736
Epoch: 5500 Train: 3.520463943 Test: 3.554858208
Epoch: 5600 Train: 3.514945984 Test: 3.548368454
Epoch: 5700 Train: 3.528086662 Test: 3.556351662
Epoch: 5800 Train: 3.512343884 Test: 3.546586990
Epoch: 5900 Train: 3.517148018 Test: 3.552560329
Epoch: 6000 Train: 3.519847393 Test: 3.552153826
Epoch: 6100 Train: 3.527290583 Test: 3.554802418
Epoch: 6200 Train: 3.521367550 Test: 3.554012299
Epoch: 6300 Train: 3.519892931 Test: 3.550647974
Epoch: 6400 Train: 3.525015831 Test: 3.555025578
Epoch: 6500 Train: 3.525770664 Test: 3.555201530
Epoch: 6600 Train: 3.520109653 Test: 3.550106525
Epoch: 6700 Train: 3.531505823 Test: 3.559382915
Epoch: 6800 Train: 3.540489674 Test: 3.581664085
Epoch: 6900 Train: 3.527077675 Test: 3.557330608
Epoch: 7000 Train: 3.530242443 Test: 3.551422596
Epoch: 7100 Train: 3.530515909 Test: 3.553684235
Epoch: 7200 Train: 3.529409885 Test: 3.547992706
Epoch: 7300 Train: 3.520134211 Test: 3.542546511
Epoch: 7400 Train: 3.524553061 Test: 3.543836117
Epoch: 7500 Train: 3.525020361 Test: 3.544985294
Epoch: 7600 Train: 3.522795916 Test: 3.546392441
Epoch: 7700 Train: 3.518650293 Test: 3.544689178
Epoch: 7800 Train: 3.517714977 Test: 3.542006731
Epoch: 7900 Train: 3.518468380 Test: 3.543167114
Epoch: 8000 Train: 3.515737295 Test: 3.540449142
Epoch: 8100 Train: 3.517093658 Test: 3.541487932
Epoch: 8200 Train: 3.515487671 Test: 3.543624401
Epoch: 8300 Train: 3.528606892 Test: 3.551390171
Epoch: 8400 Train: 3.521966219 Test: 3.550591946
Epoch: 8500 Train: 3.522008181 Test: 3.551194191
Epoch: 8600 Train: 3.518123388 Test: 3.548278809
Epoch: 8700 Train: 3.542482853 Test: 3.554042101
Epoch: 8800 Train: 3.523222208 Test: 3.543584108
Epoch: 8900 Train: 3.522599220 Test: 3.544533730
Epoch: 9000 Train: 3.520947933 Test: 3.543100834
Epoch: 9100 Train: 3.523304462 Test: 3.546356440
Epoch: 9200 Train: 3.526587009 Test: 3.548348665
Epoch: 9300 Train: 3.526981831 Test: 3.548994541
Epoch: 9400 Train: 3.528289557 Test: 3.550697565
Epoch: 9500 Train: 3.529069901 Test: 3.550722599
Epoch: 9600 Train: 3.532440901 Test: 3.552199364
Epoch: 9700 Train: 3.524631500 Test: 3.548908234
Epoch: 9800 Train: 3.522300005 Test: 3.547298908
Epoch: 9900 Train: 3.522344112 Test: 3.545982838
Epoch: 9999 Train: 3.520256519 Test: 3.545067787
Training Loss: tensor(3.5203)
Test Loss: tensor(3.5451)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0022)
Jacobian term Test Loss: tensor(0.0023)
Learned LE: [11.022889   1.3273364]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
