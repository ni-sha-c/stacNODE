time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.68%, model saved.
Epoch: 0 Train: 32513.29688 Test: 4068.17285
Epoch 80: New minimal relative error: 63.10%, model saved.
Epoch: 80 Train: 7119.43359 Test: 1206.35229
Epoch: 160 Train: 6677.72168 Test: 1377.70459
Epoch: 240 Train: 6961.22754 Test: 974.11029
Epoch: 320 Train: 5724.52832 Test: 758.32532
Epoch 400: New minimal relative error: 57.49%, model saved.
Epoch: 400 Train: 4190.06738 Test: 424.89224
Epoch: 480 Train: 3020.87720 Test: 253.04941
Epoch: 560 Train: 2369.00073 Test: 186.45575
Epoch: 640 Train: 1034.37317 Test: 43.74588
Epoch 720: New minimal relative error: 43.89%, model saved.
Epoch: 720 Train: 623.60699 Test: 24.10211
Epoch 800: New minimal relative error: 27.82%, model saved.
Epoch: 800 Train: 562.27722 Test: 17.46979
Epoch: 880 Train: 373.73734 Test: 17.49726
Epoch: 960 Train: 312.65414 Test: 17.48179
Epoch 1040: New minimal relative error: 8.85%, model saved.
Epoch: 1040 Train: 242.62259 Test: 3.64613
Epoch: 1120 Train: 220.83255 Test: 14.59840
Epoch: 1200 Train: 196.28792 Test: 9.80234
Epoch: 1280 Train: 182.19704 Test: 6.27352
Epoch: 1360 Train: 174.59526 Test: 4.45505
Epoch: 1440 Train: 191.75256 Test: 14.57599
Epoch: 1520 Train: 172.02660 Test: 5.93005
Epoch: 1600 Train: 133.75275 Test: 1.62275
Epoch: 1680 Train: 133.68611 Test: 1.48949
Epoch: 1760 Train: 140.58301 Test: 2.11511
Epoch: 1840 Train: 126.29701 Test: 1.79966
Epoch: 1920 Train: 124.29053 Test: 1.48207
Epoch 2000: New minimal relative error: 6.05%, model saved.
Epoch: 2000 Train: 102.11169 Test: 0.91797
Epoch: 2080 Train: 102.56395 Test: 5.24760
Epoch: 2160 Train: 95.55299 Test: 1.04531
Epoch 2240: New minimal relative error: 4.74%, model saved.
Epoch: 2240 Train: 89.46709 Test: 1.04208
Epoch: 2320 Train: 87.37850 Test: 0.82472
Epoch: 2400 Train: 106.73565 Test: 10.22804
Epoch: 2480 Train: 90.45974 Test: 3.71544
Epoch: 2560 Train: 94.07561 Test: 14.67649
Epoch: 2640 Train: 102.13460 Test: 3.04199
Epoch: 2720 Train: 72.16294 Test: 0.55599
Epoch: 2800 Train: 73.30534 Test: 1.19862
Epoch: 2880 Train: 64.81833 Test: 0.65250
Epoch: 2960 Train: 72.08514 Test: 4.11345
Epoch: 3040 Train: 63.52971 Test: 1.59815
Epoch 3120: New minimal relative error: 3.75%, model saved.
Epoch: 3120 Train: 62.11094 Test: 0.78309
Epoch: 3200 Train: 58.25181 Test: 0.42365
Epoch: 3280 Train: 73.65669 Test: 8.00079
Epoch 3360: New minimal relative error: 2.50%, model saved.
Epoch: 3360 Train: 55.36720 Test: 0.28843
Epoch: 3440 Train: 110.70732 Test: 38.65406
Epoch: 3520 Train: 50.88656 Test: 0.26158
Epoch: 3600 Train: 51.76204 Test: 1.55880
Epoch: 3680 Train: 44.97909 Test: 0.38064
Epoch: 3760 Train: 43.68475 Test: 0.23168
Epoch: 3840 Train: 44.40541 Test: 0.23134
Epoch: 3920 Train: 41.54235 Test: 0.14749
Epoch: 4000 Train: 42.62653 Test: 0.19445
Epoch: 4080 Train: 39.49283 Test: 0.15447
Epoch: 4160 Train: 39.49322 Test: 0.64482
Epoch: 4240 Train: 52.68082 Test: 2.75399
Epoch: 4320 Train: 38.32187 Test: 0.15469
Epoch: 4400 Train: 36.34300 Test: 0.13731
Epoch: 4480 Train: 35.24083 Test: 0.78918
Epoch: 4560 Train: 33.17432 Test: 0.23466
Epoch: 4640 Train: 32.40718 Test: 0.10570
Epoch: 4720 Train: 32.35091 Test: 0.20716
Epoch: 4800 Train: 31.71946 Test: 0.33520
Epoch: 4880 Train: 32.36278 Test: 0.51846
Epoch: 4960 Train: 48.41468 Test: 13.94897
Epoch: 5040 Train: 30.14412 Test: 0.10392
Epoch: 5120 Train: 29.25271 Test: 0.11783
Epoch 5200: New minimal relative error: 2.48%, model saved.
Epoch: 5200 Train: 28.78455 Test: 0.09528
Epoch: 5280 Train: 31.30283 Test: 0.75855
Epoch: 5360 Train: 27.79980 Test: 0.09936
Epoch: 5440 Train: 27.80075 Test: 1.01828
Epoch: 5520 Train: 26.21234 Test: 0.07646
Epoch: 5600 Train: 26.05544 Test: 0.12510
Epoch: 5680 Train: 25.99328 Test: 0.07680
Epoch: 5760 Train: 24.79071 Test: 0.06394
Epoch: 5840 Train: 26.14076 Test: 0.07313
Epoch: 5920 Train: 24.47381 Test: 0.06132
Epoch: 6000 Train: 24.86736 Test: 0.89133
Epoch: 6080 Train: 23.37754 Test: 0.08520
Epoch 6160: New minimal relative error: 2.27%, model saved.
Epoch: 6160 Train: 23.37207 Test: 0.05826
Epoch: 6240 Train: 23.65214 Test: 0.13695
Epoch: 6320 Train: 23.19069 Test: 0.06870
Epoch: 6400 Train: 23.49440 Test: 0.05982
Epoch 6480: New minimal relative error: 2.10%, model saved.
Epoch: 6480 Train: 23.17969 Test: 0.14290
Epoch: 6560 Train: 22.76244 Test: 0.06838
Epoch: 6640 Train: 23.29080 Test: 0.12500
Epoch: 6720 Train: 22.10941 Test: 0.14966
Epoch: 6800 Train: 23.37325 Test: 0.13040
Epoch: 6880 Train: 23.81547 Test: 0.07146
Epoch: 6960 Train: 22.80108 Test: 0.05782
Epoch: 7040 Train: 22.52748 Test: 0.05670
Epoch: 7120 Train: 22.68241 Test: 0.06451
Epoch: 7200 Train: 22.80118 Test: 0.06816
Epoch: 7280 Train: 21.98775 Test: 0.06009
Epoch: 7360 Train: 21.60981 Test: 0.08193
Epoch: 7440 Train: 21.49276 Test: 0.05766
Epoch: 7520 Train: 21.76108 Test: 0.81602
Epoch: 7600 Train: 20.61543 Test: 0.11319
Epoch: 7680 Train: 20.77257 Test: 0.06453
Epoch: 7760 Train: 22.92632 Test: 2.07244
Epoch: 7840 Train: 20.73570 Test: 0.05173
Epoch: 7920 Train: 20.74455 Test: 0.05615
Epoch: 7999 Train: 19.93906 Test: 0.05317
Training Loss: tensor(19.9391)
Test Loss: tensor(0.0532)
Learned LE: [ 8.7823933e-01 -1.1564011e-04 -1.4557190e+01]
True LE: [ 8.7465066e-01  4.0821671e-03 -1.4560904e+01]
Relative Error: [3.4009624  3.5602863  3.4666407  3.3370328  3.426777   3.0873017
 2.724503   2.3729155  2.1738424  1.8482519  1.8216382  2.319561
 3.0929136  3.5475903  4.0345087  4.3046293  4.3016253  4.1303773
 3.777114   3.286149   3.1795156  3.3114772  3.1554022  2.936537
 2.871823   2.6111543  2.2000284  1.999002   1.8053125  1.3989307
 1.0780678  1.1277044  2.1240656  3.4241543  3.9433155  3.9903674
 4.0280595  3.971044   3.740424   3.264237   2.5459366  2.4131727
 2.3075082  2.206417   2.2753594  2.4354846  2.9037795  3.6259227
 3.4703426  3.049352   2.7275925  2.5450718  2.4436538  2.4609756
 2.4660609  2.3850083  2.3221586  2.2651064  2.0624175  1.8635151
 2.00908    2.4545383  2.8820403  2.956009   2.8816116  2.6618102
 2.6912124  2.6265311  2.4543698  2.1181924  1.9507531  1.713052
 1.5962001  2.009012   2.436474   2.9205277  3.3228412  3.6003146
 3.69489    3.658667   3.3917527  2.9209156  2.5503962  2.6388266
 2.4833906  2.2846456  2.243729   2.318663   2.0266953  1.7452462
 1.6263797  1.3353357  1.0393778  1.0558473  1.8009768  3.1466954
 3.5994668  3.4971006  3.5030882  3.6068757  3.4774828  3.0684085
 2.5593667  2.1530974  2.249761   2.0314603  2.1101708  2.3192954
 2.8414829  3.481923   3.1957827  2.7201664  2.4588947  2.28847
 2.236426   2.2700264  2.2242806  2.083761   1.9645853  1.8574864
 1.8972569  1.7883973  1.6750679  1.9610641  2.4145725  2.7516105
 2.7571738  2.3481977  2.2988107  2.2376657  2.3929825  1.9942907
 1.8258008  1.4407313  1.2735771  1.5425084  1.6384561  2.2443795
 2.5506105  2.8951445  3.1722124  3.269285   3.1718445  2.84599
 2.2428164  2.2447124  2.0197713  1.669245   1.5975634  1.6512239
 1.7470907  1.6214364  1.4681482  1.2796901  0.9728531  1.0922437
 1.4570899  2.7413774  3.5383523  3.427019   3.1526256  3.114872
 3.0957882  2.8200269  2.5261781  2.084484   2.0614576  2.0729501
 1.9914309  2.2431254  2.717579   3.2694333  2.9631412  2.4546492
 2.2207942  2.0991528  2.1022522  2.1805246  2.120262   1.8676443
 1.7266519  1.6689891  1.7636919  1.8294499  1.5651306  1.5896752
 1.9653887  2.3511252  2.4700246  2.1804082  1.8399922  1.8869281
 1.9329939  2.058584   1.7267035  1.2973291  0.98222774 0.8908922
 1.1270258  1.3897231  1.8681133  2.1872506  2.612115   3.0825381
 3.2292252  3.0513117  2.5261319  2.1155577  1.9404448  1.463762
 1.1165991  1.0672393  1.307606   1.5981482  1.6293936  1.3647239
 1.0223747  0.98746556 1.2161925  1.967117   3.2508924  3.4290154
 3.0982637  2.864293   2.6101992  2.483854   2.410414   2.2318182
 1.8982645  2.119155   2.1274016  2.2309127  2.6488886  3.1286745
 2.7545233  2.2374034  2.0420947  1.9896362  2.0337248  2.144257
 2.1106849  1.8764513  1.599043   1.6551765  1.6914252  1.6516285
 1.7182986  1.4138986  1.446939   1.7322836  2.0239117  2.0329435
 1.5381739  1.4098606  1.5385724  1.6975224  1.7599115  1.3995632
 1.0269184  0.68123245 0.39912128 0.7504946  1.0454143  1.5478189
 2.0788403  2.8812232  3.3486047  3.4031074  3.0783393  2.363339
 2.0560455  1.5964683  1.0449781  1.0014068  1.0208888  1.2161646
 1.6217145  1.7715961  1.3632338  0.97483367 1.0693157  1.3231109
 2.4054267  3.164538   3.081559   2.7424672  2.4797385  2.1614265
 2.0977528  2.190792   2.0870035  1.8751327  2.223968   2.3280241
 2.5271695  2.9673357  2.7264132  2.310879   1.9365808  1.8965255
 1.9290596  2.0422387  2.0308669  1.7930133  1.5847169  1.6370343
 1.6270127  1.6228633  1.6322138  1.6034805  1.235301   1.2016052
 1.3983358  1.6282723  1.4521354  0.9500363  0.9866605  1.1931062
 1.4073523  1.4670875  1.0993469  0.89709294 0.47899112 0.2647925
 0.5290371  0.8871055  1.6394932  2.3510597  3.0224195  3.3169632
 3.187082   2.710265   2.0189302  1.8540397  1.3658165  1.1368387
 0.9836954  0.99964243 1.0657462  1.6325703  1.7906759  1.341143
 0.91292316 1.030415   1.359758   2.4998615  2.8905568  2.5469959
 2.4257383  2.0720594  1.8568108  1.8366113  2.0515037  1.9998386
 1.9636055  2.357458   2.4970148  2.7226567  2.7984378  2.484653
 2.117091   1.7679787  1.5939442  1.7287374  1.8279437  1.6992894
 1.5677392  1.6574897  1.6191608  1.6760671  1.6945057  1.6528394
 1.5757153  1.192343   1.1086438  1.4108588  1.63496    1.3908263
 0.7921686  0.6384942  0.9894364  1.2979677  1.2147233  0.9025012
 0.7239142  0.43939403 0.28709903 0.5180979  1.010471   1.8881251
 2.2305372  2.5501978  2.5977623  2.3639917  1.8690789  1.2641133
 1.2361296  1.0493574  1.0336871  1.0068481 ]
