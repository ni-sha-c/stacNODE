time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.69%, model saved.
Epoch: 0 Train: 3886.70483 Test: 3949.11401
Epoch: 80 Train: 229.47629 Test: 296.14639
Epoch 160: New minimal relative error: 19.22%, model saved.
Epoch: 160 Train: 23.58560 Test: 31.62270
Epoch: 240 Train: 11.23821 Test: 22.44604
Epoch: 320 Train: 7.81379 Test: 14.12687
Epoch 400: New minimal relative error: 9.38%, model saved.
Epoch: 400 Train: 4.61922 Test: 10.03037
Epoch: 480 Train: 6.85568 Test: 8.40371
Epoch: 560 Train: 2.68255 Test: 7.38971
Epoch: 640 Train: 2.94894 Test: 6.54987
Epoch: 720 Train: 1.78723 Test: 5.16401
Epoch: 800 Train: 1.42777 Test: 4.64932
Epoch: 880 Train: 1.25500 Test: 4.34928
Epoch: 960 Train: 1.80611 Test: 4.88310
Epoch: 1040 Train: 0.96205 Test: 3.92989
Epoch: 1120 Train: 0.97504 Test: 3.85201
Epoch: 1200 Train: 1.71637 Test: 5.54344
Epoch: 1280 Train: 1.09428 Test: 3.84049
Epoch: 1360 Train: 0.69830 Test: 3.34315
Epoch: 1440 Train: 0.55345 Test: 3.18392
Epoch 1520: New minimal relative error: 6.06%, model saved.
Epoch: 1520 Train: 0.65889 Test: 3.50345
Epoch: 1600 Train: 0.47866 Test: 3.16319
Epoch: 1680 Train: 0.52049 Test: 3.16210
Epoch 1760: New minimal relative error: 4.89%, model saved.
Epoch: 1760 Train: 0.42414 Test: 3.17596
Epoch: 1840 Train: 0.99749 Test: 3.65462
Epoch: 1920 Train: 0.47039 Test: 3.15328
Epoch: 2000 Train: 0.36783 Test: 3.01821
Epoch: 2080 Train: 0.40963 Test: 3.13479
Epoch: 2160 Train: 1.96905 Test: 4.82618
Epoch: 2240 Train: 1.30215 Test: 3.86608
Epoch: 2320 Train: 0.54861 Test: 3.42779
Epoch: 2400 Train: 0.77862 Test: 3.41890
Epoch: 2480 Train: 1.17747 Test: 3.75140
Epoch: 2560 Train: 0.83827 Test: 3.90060
Epoch: 2640 Train: 0.34566 Test: 3.31198
Epoch: 2720 Train: 0.31564 Test: 3.45753
Epoch: 2800 Train: 0.49300 Test: 3.90553
Epoch: 2880 Train: 0.37495 Test: 3.60430
Epoch: 2960 Train: 0.63593 Test: 3.88480
Epoch 3040: New minimal relative error: 4.22%, model saved.
Epoch: 3040 Train: 0.24572 Test: 3.63483
Epoch: 3120 Train: 0.21843 Test: 3.56738
Epoch: 3200 Train: 0.29866 Test: 3.60904
Epoch: 3280 Train: 0.20708 Test: 3.62506
Epoch: 3360 Train: 0.20442 Test: 3.58048
Epoch: 3440 Train: 0.18203 Test: 3.58732
Epoch: 3520 Train: 0.17725 Test: 3.59530
Epoch: 3600 Train: 0.63138 Test: 3.83269
Epoch: 3680 Train: 0.16719 Test: 3.63779
Epoch: 3760 Train: 0.18559 Test: 3.63192
Epoch: 3840 Train: 1.44349 Test: 4.70173
Epoch: 3920 Train: 0.15491 Test: 3.67087
Epoch: 4000 Train: 0.36516 Test: 3.94652
Epoch: 4080 Train: 0.15528 Test: 3.68908
Epoch: 4160 Train: 0.14482 Test: 3.67993
Epoch 4240: New minimal relative error: 3.94%, model saved.
Epoch: 4240 Train: 0.22259 Test: 3.77064
Epoch: 4320 Train: 0.13898 Test: 3.69322
Epoch: 4400 Train: 0.46696 Test: 4.14022
Epoch: 4480 Train: 0.44410 Test: 4.15329
Epoch: 4560 Train: 0.39771 Test: 4.15218
Epoch: 4640 Train: 0.19399 Test: 3.78569
Epoch: 4720 Train: 0.12904 Test: 3.75905
Epoch: 4800 Train: 0.22417 Test: 3.89353
Epoch: 4880 Train: 0.11989 Test: 3.82327
Epoch 4960: New minimal relative error: 3.86%, model saved.
Epoch: 4960 Train: 0.12023 Test: 3.84268
Epoch: 5040 Train: 0.15982 Test: 3.88796
Epoch: 5120 Train: 0.14460 Test: 3.91049
Epoch: 5200 Train: 0.80773 Test: 4.53385
Epoch 5280: New minimal relative error: 3.81%, model saved.
Epoch: 5280 Train: 0.10857 Test: 3.90785
Epoch: 5360 Train: 0.10698 Test: 3.95005
Epoch: 5440 Train: 0.10664 Test: 3.94776
Epoch: 5520 Train: 0.10292 Test: 3.98495
Epoch: 5600 Train: 0.10130 Test: 3.99359
Epoch: 5680 Train: 0.09987 Test: 4.03187
Epoch: 5760 Train: 0.09945 Test: 4.00234
Epoch: 5840 Train: 0.44669 Test: 4.14273
Epoch: 5920 Train: 0.09416 Test: 4.01301
Epoch: 6000 Train: 0.24069 Test: 4.16972
Epoch: 6080 Train: 0.09116 Test: 4.00203
Epoch: 6160 Train: 0.26309 Test: 4.21053
Epoch: 6240 Train: 0.08860 Test: 4.03225
Epoch 6320: New minimal relative error: 3.62%, model saved.
Epoch: 6320 Train: 0.08706 Test: 4.07725
Epoch: 6400 Train: 0.10639 Test: 4.16927
Epoch: 6480 Train: 0.08485 Test: 4.10363
Epoch: 6560 Train: 0.08330 Test: 4.14685
Epoch: 6640 Train: 0.09306 Test: 4.15287
Epoch: 6720 Train: 0.08105 Test: 4.16760
Epoch: 6800 Train: 0.07981 Test: 4.19915
Epoch: 6880 Train: 0.08223 Test: 4.16953
Epoch: 6960 Train: 0.07767 Test: 4.21908
Epoch: 7040 Train: 0.11245 Test: 4.32707
Epoch: 7120 Train: 0.07595 Test: 4.22440
Epoch: 7200 Train: 0.07461 Test: 4.26233
Epoch: 7280 Train: 0.07367 Test: 4.29029
Epoch: 7360 Train: 0.07307 Test: 4.28949
Epoch: 7440 Train: 0.07175 Test: 4.31869
Epoch: 7520 Train: 0.07079 Test: 4.34753
Epoch: 7600 Train: 0.07317 Test: 4.31556
Epoch: 7680 Train: 0.06926 Test: 4.37150
Epoch: 7760 Train: 0.06841 Test: 4.41274
Epoch: 7840 Train: 0.06756 Test: 4.44688
Epoch: 7920 Train: 0.06673 Test: 4.47456
Epoch: 7999 Train: 0.31612 Test: 4.71463
Training Loss: tensor(0.3161)
Test Loss: tensor(4.7146)
Learned LE: [ 0.83584523  0.03463928 -5.0402036 ]
True LE: [ 8.6415690e-01  1.3477914e-03 -1.4541009e+01]
Relative Error: [3.9770563  3.9025679  3.8737879  3.8407445  3.76893    3.6555648
 3.6705978  3.9188993  3.8717003  3.430957   2.8953843  2.3351338
 1.9108189  2.1237953  2.5620022  2.6645212  2.680422   2.7861044
 2.8374128  2.9977744  3.3313842  3.444237   3.5811453  3.7789493
 3.8529243  3.9570718  4.266571   4.61748    4.796292   4.613773
 3.9787133  3.177534   2.5829103  2.236972   2.0104923  1.865325
 1.797302   1.8244336  1.8613288  2.1722338  2.200295   1.8810097
 1.9575317  2.047461   2.3813655  2.928479   3.7928898  4.48067
 4.6298165  4.7282677  4.895987   5.0387626  5.070806   4.798437
 4.42975    4.1920605  4.0764413  4.047208   4.0713506  4.1260114
 4.0320296  3.6908925  3.418211   3.3257072  3.3475425  3.4195714
 3.4965146  3.5356805  3.5517473  3.73248    3.767273   3.385048
 2.8518567  2.3108928  1.7636633  1.3645438  1.7134683  2.0302896
 2.0303445  2.1605315  2.4599116  2.5650089  2.740797   2.8188593
 3.0222106  3.4259644  3.6301274  3.6603827  3.865717   4.279156
 4.648223   4.8683395  4.742243   4.071768   3.1259086  2.3157892
 1.7924912  1.5558207  1.399676   1.3552042  1.4884766  1.675589
 2.0461404  2.1702578  1.8143665  1.8124554  2.0614834  2.559
 3.1656876  3.6556091  3.8854547  4.113296   4.390737   4.521448
 4.581045   4.357014   3.8199008  3.4149323  3.2109592  3.1232693
 3.038022   2.9851236  3.030836   3.109822   2.9799993  2.7733288
 2.7144253  2.788516   2.9548798  3.1487937  3.302086   3.4786093
 3.653431   3.5008528  3.143423   2.6770933  2.1007524  1.5177407
 0.97099525 1.2760705  1.5936451  1.5504498  1.7522596  2.1794026
 2.4566083  2.526113   2.5963006  3.0912957  3.525142   3.6560037
 3.8263206  4.205341   4.6299767  4.8904896  5.094524   5.082206
 4.526611   3.558908   2.5576007  1.7135547  1.2510008  1.1302963
 1.0545933  1.2079105  1.564859   1.8804226  2.2195985  1.7493148
 1.7038437  2.0590184  2.7243693  3.103475   3.0540802  3.2731526
 3.6286025  3.9851804  4.090354   3.9717007  3.5180602  2.8862958
 2.4913754  2.320262   2.199369   2.0389366  2.0127146  2.0480254
 2.1540964  2.2827716  2.1563144  2.0956833  2.1754832  2.4025605
 2.6953614  2.9697018  3.3048027  3.4479778  3.403516   3.210246
 2.8285959  2.2331736  1.5904993  0.9254816  0.8034357  1.2685789
 1.2716789  1.4393213  1.9039568  2.4011455  2.5132797  2.7601132
 3.3642411  3.656759   3.9168873  4.33294    4.6761003  4.8488665
 4.7879815  4.7962375  4.850816   4.5981536  3.921688   3.106288
 2.25032    1.3244808  0.96151495 0.9503813  0.9899116  1.3005042
 1.6384026  2.085179   1.7445406  1.607883   1.9584047  2.7378662
 2.7432103  2.4980576  2.648157   3.0212338  3.4162066  3.489981
 3.3159592  2.8278015  2.1376266  1.696283   1.531092   1.3701314
 1.2390321  1.3282616  1.3902081  1.4147415  1.5972412  1.6750308
 1.5013766  1.534221   1.7267638  2.0648127  2.4911623  2.9693463
 3.1351526  3.3576992  3.4069235  3.1335506  2.6381674  1.9188052
 1.2315294  0.48657182 0.9630195  1.1441079  1.2390133  1.640694
 2.218034   2.6218262  3.0122468  3.6218598  3.8106434  4.23839
 4.8171678  4.907727   4.755924   4.369946   4.0824914  4.028257
 4.019071   3.802114   3.2791517  2.6879349  2.002816   1.2013968
 0.82097566 0.88939255 0.90760124 1.2662663  1.5989423  1.958945
 1.4791791  1.6728399  2.4803486  2.5304754  1.989707   2.040391
 2.303559   2.6696343  2.6795657  2.634838   2.3253787  1.6305165
 1.097079   0.937684   0.8435107  0.79219127 0.99003005 1.0886275
 1.0293067  1.0432014  1.2425817  1.2579178  1.0082781  1.063256
 1.3482554  1.904596   2.5728326  2.669455   3.0210161  3.4462066
 3.4589858  3.1215346  2.5863     1.8286827  1.0639423  0.45461398
 1.0178075  1.1339669  1.4331049  1.8682959  2.463212   3.025506
 3.7391465  3.9908566  4.3316255  4.8685365  4.8006043  4.3082604
 3.8508766  3.4563806  3.2734017  3.1954732  3.3408935  3.308291
 2.7993631  2.29607    1.7979985  1.3145916  0.7865575  0.8426441
 0.7311052  0.98883396 1.3588116  1.6783386  1.1795548  1.7290903
 2.5011258  1.8335285  1.4425838  1.6155972  1.9263048  1.8850299
 1.7683904  1.8241813  1.3755804  0.7967313  0.5813433  0.59845686
 0.5442316  0.75351256 0.9500371  0.8703134  0.7383256  0.7303837
 0.8955272  0.9971927  0.62894243 0.6460191  1.1275846  1.97985
 2.327497   2.1860576  2.7109468  3.2148528  3.3397942  3.060404
 2.6192946  1.9660825  1.25636    0.45385218 0.88229215 1.1792871
 1.5829489  2.0312357  2.5044274  3.2701488 ]
