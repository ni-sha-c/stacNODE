time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.67%, model saved.
Epoch: 0 Train: 3966.41284 Test: 4038.49805
Epoch: 100 Train: 127.12383 Test: 134.56525
Epoch 200: New minimal relative error: 28.16%, model saved.
Epoch: 200 Train: 15.17666 Test: 21.01466
Epoch 300: New minimal relative error: 25.27%, model saved.
Epoch: 300 Train: 7.04819 Test: 9.47614
Epoch 400: New minimal relative error: 16.10%, model saved.
Epoch: 400 Train: 3.81690 Test: 5.66030
Epoch: 500 Train: 3.73092 Test: 6.86205
Epoch: 600 Train: 2.03977 Test: 3.01854
Epoch 700: New minimal relative error: 13.96%, model saved.
Epoch: 700 Train: 2.36786 Test: 3.50850
Epoch: 800 Train: 8.19010 Test: 7.85682
Epoch 900: New minimal relative error: 9.33%, model saved.
Epoch: 900 Train: 1.07632 Test: 1.38497
Epoch: 1000 Train: 1.29814 Test: 1.78842
Epoch: 1100 Train: 0.68772 Test: 1.00865
Epoch: 1200 Train: 0.57584 Test: 0.85394
Epoch: 1300 Train: 0.66540 Test: 0.99617
Epoch: 1400 Train: 5.65676 Test: 4.79507
Epoch: 1500 Train: 0.74469 Test: 0.69856
Epoch 1600: New minimal relative error: 8.80%, model saved.
Epoch: 1600 Train: 0.43703 Test: 0.55852
Epoch: 1700 Train: 2.25458 Test: 2.96221
Epoch 1800: New minimal relative error: 6.97%, model saved.
Epoch: 1800 Train: 0.36511 Test: 0.51250
Epoch: 1900 Train: 0.40461 Test: 0.54334
Epoch: 2000 Train: 0.99138 Test: 1.46414
Epoch: 2100 Train: 0.36594 Test: 0.49370
Epoch: 2200 Train: 0.26266 Test: 0.35444
Epoch: 2300 Train: 0.29989 Test: 0.38308
Epoch: 2400 Train: 0.24892 Test: 0.33474
Epoch: 2500 Train: 0.24337 Test: 0.31959
Epoch: 2600 Train: 0.34800 Test: 0.38044
Epoch: 2700 Train: 0.80854 Test: 0.88666
Epoch: 2800 Train: 3.34074 Test: 3.45726
Epoch: 2900 Train: 3.13392 Test: 2.95661
Epoch: 3000 Train: 1.16732 Test: 0.92414
Epoch 3100: New minimal relative error: 6.17%, model saved.
Epoch: 3100 Train: 0.19985 Test: 0.29121
Epoch: 3200 Train: 0.34954 Test: 0.34603
Epoch: 3300 Train: 0.15936 Test: 0.22128
Epoch: 3400 Train: 0.14481 Test: 0.20461
Epoch: 3500 Train: 0.12736 Test: 0.19377
Epoch: 3600 Train: 0.12416 Test: 0.18919
Epoch: 3700 Train: 0.12285 Test: 0.18985
Epoch: 3800 Train: 0.12582 Test: 0.18400
Epoch: 3900 Train: 0.17476 Test: 0.25385
Epoch: 4000 Train: 1.69513 Test: 1.32373
Epoch: 4100 Train: 0.10280 Test: 0.16019
Epoch: 4200 Train: 0.10115 Test: 0.15881
Epoch: 4300 Train: 1.41170 Test: 1.26377
Epoch: 4400 Train: 0.09423 Test: 0.15090
Epoch: 4500 Train: 0.61778 Test: 0.49579
Epoch: 4600 Train: 0.08779 Test: 0.14328
Epoch: 4700 Train: 0.09411 Test: 0.14448
Epoch: 4800 Train: 0.10182 Test: 0.16395
Epoch: 4900 Train: 0.40093 Test: 0.31587
Epoch: 5000 Train: 0.08238 Test: 0.14132
Epoch: 5100 Train: 0.07600 Test: 0.12877
Epoch: 5200 Train: 0.08394 Test: 0.14147
Epoch: 5300 Train: 0.07481 Test: 0.12404
Epoch: 5400 Train: 0.07036 Test: 0.12130
Epoch: 5500 Train: 0.29332 Test: 0.34860
Epoch: 5600 Train: 0.06608 Test: 0.11624
Epoch: 5700 Train: 0.06864 Test: 0.11633
Epoch: 5800 Train: 0.06261 Test: 0.11179
Epoch: 5900 Train: 0.06346 Test: 0.11293
Epoch: 6000 Train: 0.05989 Test: 0.10810
Epoch: 6100 Train: 0.06541 Test: 0.10943
Epoch: 6200 Train: 0.05731 Test: 0.10506
Epoch: 6300 Train: 0.38560 Test: 0.34757
Epoch 6400: New minimal relative error: 4.92%, model saved.
Epoch: 6400 Train: 0.05502 Test: 0.10211
Epoch: 6500 Train: 0.06136 Test: 0.11708
Epoch: 6600 Train: 0.05318 Test: 0.09938
Epoch: 6700 Train: 0.05148 Test: 0.09765
Epoch: 6800 Train: 0.07221 Test: 0.11498
Epoch: 6900 Train: 0.10944 Test: 0.13846
Epoch: 7000 Train: 0.04874 Test: 0.09345
Epoch: 7100 Train: 0.04822 Test: 0.09273
Epoch: 7200 Train: 0.04871 Test: 0.09254
Epoch: 7300 Train: 0.04612 Test: 0.09081
Epoch: 7400 Train: 0.04674 Test: 0.09024
Epoch: 7500 Train: 0.04445 Test: 0.08861
Epoch: 7600 Train: 0.04337 Test: 0.08753
Epoch: 7700 Train: 0.04336 Test: 0.08649
Epoch: 7800 Train: 0.04193 Test: 0.08574
Epoch: 7900 Train: 0.04163 Test: 0.08478
Epoch: 8000 Train: 0.04055 Test: 0.08374
Epoch: 8100 Train: 0.05681 Test: 0.08791
Epoch: 8200 Train: 0.03928 Test: 0.08200
Epoch: 8300 Train: 0.27701 Test: 0.22272
Epoch: 8400 Train: 0.03808 Test: 0.08046
Epoch: 8500 Train: 0.03737 Test: 0.07948
Epoch: 8600 Train: 0.04242 Test: 0.07966
Epoch: 8700 Train: 0.03638 Test: 0.07813
Epoch: 8800 Train: 0.03574 Test: 0.07723
Epoch: 8900 Train: 0.04930 Test: 0.08362
Epoch: 9000 Train: 0.03468 Test: 0.07579
Epoch: 9100 Train: 0.03440 Test: 0.07774
Epoch: 9200 Train: 0.03370 Test: 0.07451
Epoch: 9300 Train: 0.03317 Test: 0.07399
Epoch: 9400 Train: 0.03286 Test: 0.07391
Epoch: 9500 Train: 0.03228 Test: 0.07264
Epoch: 9600 Train: 0.04130 Test: 0.08582
Epoch: 9700 Train: 0.03178 Test: 0.07128
Epoch: 9800 Train: 0.03094 Test: 0.07105
Epoch: 9900 Train: 0.14788 Test: 0.10431
Epoch: 9999 Train: 0.03014 Test: 0.06995
Training Loss: tensor(0.0301)
Test Loss: tensor(0.0700)
Learned LE: [ 0.8962767  -0.00994013 -4.9951487 ]
True LE: [ 8.6570466e-01 -2.5457232e-03 -1.4544127e+01]
Relative Error: [2.7830431  3.1026456  3.4491816  3.6658075  3.7457187  3.7221577
 3.619729   3.464367   3.2815444  3.087581   2.894465   2.7232447
 2.603545   2.552539   2.555332   2.563393   2.5085497  2.336618
 2.0726588  1.819808   1.5388682  1.2470107  1.8317806  2.9405913
 4.0420856  4.737447   5.026684   5.085622   4.807987   4.288342
 3.715182   3.2437994  2.8993204  2.5834     2.368472   2.421124
 2.6402645  2.8618264  3.027419   3.2199566  3.5957086  4.157513
 4.7452374  5.099868   4.9291887  4.365577   4.0087028  4.171182
 4.7240553  5.2147927  5.882317   6.246673   5.1231256  3.7899992
 2.961301   2.37448    1.9800985  1.796588   1.8027656  1.9574652
 2.1908517  2.3846166  2.566499   2.880797   3.225562   3.4373794
 3.5189066  3.507787   3.4265046  3.3004065  3.154699   2.9957273
 2.8103611  2.5983348  2.4026566  2.289236   2.2837536  2.3435962
 2.3823197  2.3040307  2.0621111  1.7302898  1.3537781  0.7625034
 0.9224582  2.1481297  3.3541222  4.2585144  4.670577   4.728909
 4.3990197  3.8174603  3.1876557  2.6568587  2.2702556  1.9348993
 1.6847625  1.7077591  1.9230176  2.1775396  2.4137585  2.6111653
 2.853725   3.301802   3.930319   4.5525184  4.8315625  4.4654713
 3.8359258  3.6250916  3.9975085  4.50073    5.0370502  5.519713
 5.025451   3.5567894  2.597814   1.906144   1.4652874  1.293794
 1.3368767  1.5491816  1.8703824  2.1380353  2.2987127  2.5474825
 2.8721185  3.0906162  3.1860986  3.1923852  3.1275856  3.0179021
 2.899803   2.7927291  2.6772351  2.5107226  2.285501   2.0828626
 2.020176   2.110317   2.2526972  2.322256   2.2144578  1.9313492
 1.5661458  1.0504248  0.43353456 1.4155062  2.670684   3.6580958
 4.207399   4.3242764  4.0200176  3.4381044  2.781563   2.1911654
 1.7452646  1.436344   1.2167592  1.1672226  1.3091822  1.510927
 1.7378796  2.0145638  2.2830184  2.532601   2.948993   3.5723276
 4.2258406  4.4623246  3.9502356  3.320535   3.2760415  3.7523396
 4.1783733  4.723122   4.733977   3.5280306  2.3915012  1.5623709
 0.9971728  0.79023093 0.8378893  1.08512    1.486349   1.8279953
 1.9540393  2.1016054  2.41019    2.6746905  2.8126466  2.8507292
 2.803246   2.6930954  2.564589   2.4598155  2.3861032  2.3078063
 2.1685457  1.9562969  1.7906196  1.8308105  2.0331314  2.23743
 2.3122013  2.1793513  1.9063824  1.5505016  1.0731754  1.0578135
 2.0788982  2.9783509  3.6040008  3.8720272  3.6977243  3.1897411
 2.5377517  1.8873272  1.3399768  1.0368495  0.94899005 0.8462456
 0.87303805 1.0249026  1.179513   1.3809973  1.6932051  2.000388
 2.199818   2.5283494  3.1018946  3.7698395  4.0217085  3.438031
 2.8238301  2.914574   3.3707492  3.802661   4.0829325  3.7153456
 2.3822134  1.4337136  0.66325426 0.3540829  0.37707466 0.56513476
 0.9963723  1.4010751  1.5297546  1.5939093  1.8618832  2.1977053
 2.408597   2.501048   2.4893117  2.3873014  2.2378159  2.1010242
 2.010513   1.9527681  1.8879222  1.7789648  1.6245035  1.5377388
 1.6719265  1.9375246  2.16032    2.2253845  2.0935004  1.8678805
 1.566528   1.304105   1.6598358  2.404923   2.8494039  3.2928019
 3.3769534  3.0484884  2.463492   1.7766887  1.1060308  0.66772515
 0.69384223 0.6910553  0.5968057  0.7470834  0.90447056 1.0127977
 1.1919653  1.4590718  1.7225742  1.8607159  2.0767481  2.5520597
 3.18508    3.504987   2.962129   2.3249273  2.4747033  2.8709822
 3.3124175  3.3271153  2.7148457  1.5560681  0.65316886 0.06476057
 0.3786774  0.28692847 0.41125193 0.83820486 1.0037727  1.0658065
 1.2394779  1.6012315  1.9134008  2.0875292  2.1428044  2.083118
 1.9324577  1.7571121  1.6321223  1.5714175  1.524608   1.4505306
 1.3660072  1.297615   1.2755814  1.413873   1.6847954  1.9166179
 1.9917026  1.8908477  1.7255986  1.5124366  1.5028423  1.9155343
 2.241718   2.4615285  2.8827877  2.8889933  2.5104237  1.8780268
 1.135177   0.4839111  0.31960806 0.5064172  0.47223485 0.59338087
 0.8374007  0.92647403 0.97508764 1.1048932  1.2595973  1.4356291
 1.5528189  1.6436818  1.9492494  2.4843776  2.8551488  2.531885
 1.8089954  1.9136223  2.2803783  2.705025   2.6610806  2.0107396
 0.98065436 0.3075776  0.5173373  0.6081032  0.29767102 0.275881
 0.43937713 0.49199894 0.615032   0.84952533 1.2516536  1.5547317
 1.7109953  1.734671   1.6359028  1.4516473  1.2701789  1.1811584
 1.163731   1.1266904  1.0454088  0.9796174  0.97005296 0.98271483
 1.0335077  1.250575   1.5112994  1.6361445  1.585068   1.4767445
 1.3486391  1.497366   1.7810253  1.8125567 ]
