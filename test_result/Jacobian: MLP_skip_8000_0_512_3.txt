time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.46%, model saved.
Epoch: 0 Train: 171832.42188 Test: 4357.02344
Epoch: 80 Train: 44205.22656 Test: 2108.93042
Epoch: 160 Train: 37699.53125 Test: 1516.52319
Epoch: 240 Train: 39473.37500 Test: 1441.10132
Epoch 320: New minimal relative error: 64.64%, model saved.
Epoch: 320 Train: 33204.50391 Test: 1335.75928
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.46%, model saved.
Epoch: 0 Train: 171832.42188 Test: 4357.02344
Epoch: 80 Train: 44205.22656 Test: 2108.93042
Epoch: 160 Train: 37699.53125 Test: 1516.52319
Epoch: 240 Train: 39473.37500 Test: 1441.10132
Epoch 320: New minimal relative error: 64.64%, model saved.
Epoch: 320 Train: 33204.50391 Test: 1335.75928
Epoch: 400 Train: 36935.32812 Test: 1288.84119
Epoch: 480 Train: 34693.65234 Test: 1258.92004
Epoch: 560 Train: 39595.44531 Test: 1549.37317
Epoch 640: New minimal relative error: 60.89%, model saved.
Epoch: 640 Train: 40979.22656 Test: 1483.02222
Epoch: 720 Train: 35996.62500 Test: 1443.80701
Epoch: 800 Train: 36344.96484 Test: 1486.53638
Epoch: 880 Train: 40419.19141 Test: 1591.37720
Epoch: 960 Train: 33187.76562 Test: 1347.00464
Epoch: 1040 Train: 35768.36328 Test: 1310.56665
Epoch: 1120 Train: 32792.94922 Test: 1275.32422
Epoch: 1200 Train: 32053.72656 Test: 1008.89789
Epoch: 1280 Train: 30108.79688 Test: 898.98889
Epoch: 1360 Train: 23333.92188 Test: 644.99713
Epoch: 1440 Train: 21389.76172 Test: 520.22394
Epoch: 1520 Train: 19148.27148 Test: 408.37729
Epoch: 1600 Train: 16885.36523 Test: 302.24103
Epoch: 1680 Train: 13947.55762 Test: 194.79024
Epoch: 1760 Train: 12813.38281 Test: 169.57234
Epoch: 1840 Train: 11690.73535 Test: 138.91483
Epoch 1920: New minimal relative error: 33.34%, model saved.
Epoch: 1920 Train: 10313.41797 Test: 106.41296
Epoch: 2000 Train: 8895.88574 Test: 86.27139
Epoch: 2080 Train: 6932.36279 Test: 94.44916
Epoch: 2160 Train: 5503.16650 Test: 76.26269
Epoch: 2240 Train: 5423.60791 Test: 91.64351
Epoch: 2320 Train: 6376.27393 Test: 67.33297
Epoch 2400: New minimal relative error: 15.64%, model saved.
Epoch: 2400 Train: 5070.81543 Test: 47.52012
Epoch: 2480 Train: 4224.79736 Test: 42.80545
Epoch: 2560 Train: 3977.96777 Test: 33.28048
Epoch: 2640 Train: 3586.55200 Test: 26.84461
Epoch: 2720 Train: 3802.16406 Test: 31.89399
Epoch: 2800 Train: 3867.38477 Test: 35.36293
Epoch: 2880 Train: 4017.59473 Test: 66.59655
Epoch: 2960 Train: 4618.62354 Test: 72.91156
Epoch: 3040 Train: 3724.66724 Test: 60.84431
Epoch: 3120 Train: 3455.94482 Test: 22.34914
Epoch: 3200 Train: 3460.35278 Test: 32.66615
Epoch: 3280 Train: 3998.39111 Test: 30.83901
Epoch: 3360 Train: 4028.72583 Test: 47.86804
Epoch: 3440 Train: 3392.32275 Test: 52.66550
Epoch: 3520 Train: 3532.79150 Test: 38.26565
Epoch: 3600 Train: 2715.12036 Test: 19.77667
Epoch: 3680 Train: 2628.94507 Test: 29.49033
Epoch: 3760 Train: 2694.45459 Test: 28.94569
Epoch: 3840 Train: 2904.64917 Test: 23.38000
Epoch: 3920 Train: 2959.57275 Test: 23.26024
Epoch: 4000 Train: 2267.34448 Test: 13.33500
Epoch 4080: New minimal relative error: 13.90%, model saved.
Epoch: 4080 Train: 2408.22974 Test: 11.42969
Epoch: 4160 Train: 2277.15991 Test: 9.56065
Epoch: 4240 Train: 2058.03979 Test: 15.75403
Epoch 4320: New minimal relative error: 11.96%, model saved.
Epoch: 4320 Train: 2194.46094 Test: 7.98159
Epoch: 4400 Train: 2312.06982 Test: 11.55421
Epoch: 4480 Train: 2109.22778 Test: 14.70401
Epoch: 4560 Train: 2228.60986 Test: 10.34166
Epoch: 4640 Train: 2236.39160 Test: 29.85818
Epoch: 4720 Train: 2102.35718 Test: 12.69067
Epoch: 4800 Train: 2080.12256 Test: 9.95370
Epoch: 4880 Train: 1967.90894 Test: 11.20293
Epoch: 4960 Train: 1980.89221 Test: 9.58452
Epoch: 5040 Train: 2009.82190 Test: 59.14153
Epoch: 5120 Train: 2045.14111 Test: 60.08229
Epoch: 5200 Train: 2113.03711 Test: 29.79635
Epoch 5280: New minimal relative error: 8.55%, model saved.
Epoch: 5280 Train: 2041.95032 Test: 10.77900
Epoch: 5360 Train: 1927.36340 Test: 12.25429
Epoch: 5440 Train: 1635.14600 Test: 7.73844
Epoch: 5520 Train: 1604.13660 Test: 5.90294
Epoch: 5600 Train: 1590.34216 Test: 5.59234
Epoch: 5680 Train: 1948.82166 Test: 8.84349
Epoch: 5760 Train: 1716.89172 Test: 15.17392
Epoch: 5840 Train: 1555.06714 Test: 5.52284
Epoch: 5920 Train: 1683.54138 Test: 8.15191
Epoch: 6000 Train: 1735.61401 Test: 12.16128
Epoch: 6080 Train: 1496.70728 Test: 5.27296
Epoch: 6160 Train: 1465.69409 Test: 6.72980
Epoch: 6240 Train: 1601.04688 Test: 8.95168
Epoch: 6320 Train: 1379.63330 Test: 4.33188
Epoch: 6400 Train: 1417.63831 Test: 9.22241
Epoch 6480: New minimal relative error: 5.77%, model saved.
Epoch: 6480 Train: 1286.15625 Test: 7.64232
Epoch: 6560 Train: 1235.92444 Test: 5.84026
Epoch: 6640 Train: 1134.26953 Test: 3.94527
Epoch: 6720 Train: 1193.83508 Test: 6.93135
Epoch: 6800 Train: 1101.08276 Test: 3.70653
Epoch: 6880 Train: 1266.16992 Test: 5.94019
Epoch: 6960 Train: 1060.54163 Test: 4.39065
Epoch: 7040 Train: 1014.67163 Test: 5.37964
Epoch: 7120 Train: 951.92511 Test: 2.85479
Epoch: 7200 Train: 874.82556 Test: 2.24194
Epoch: 7280 Train: 866.06866 Test: 1.96646
Epoch: 7360 Train: 869.01477 Test: 2.99207
Epoch: 7440 Train: 891.22974 Test: 2.95177
Epoch: 7520 Train: 835.58234 Test: 2.92743
Epoch: 7600 Train: 872.97797 Test: 2.60475
Epoch: 7680 Train: 852.88226 Test: 4.16648
Epoch: 7760 Train: 793.81641 Test: 2.94590
Epoch: 7840 Train: 791.97815 Test: 2.21678
Epoch: 7920 Train: 720.60736 Test: 1.67410
Epoch: 7999 Train: 718.82635 Test: 1.27927
Training Loss: tensor(718.8264)
Test Loss: tensor(1.2793)
Learned LE: [  0.99854565  -0.14852412 -14.491844  ]
True LE: [ 8.7351203e-01  2.9408818e-03 -1.4553443e+01]
Relative Error: [28.870848  26.571823  24.134787  22.081966  20.181784  18.060442
 15.737991  13.255043  10.584177   8.254636   6.9780045  7.6784215
  9.440528  10.975184  12.283302  13.465004  14.369919  15.144108
 15.88801   16.468294  17.042635  17.468792  17.903776  18.107304
 17.97429   17.473421  16.817596  16.275593  15.777944  15.213723
 14.418554  13.308414  12.091772  10.808841   9.580419   8.642845
 10.217229  11.682776  14.277135  17.75836   21.0903    24.205112
 27.078646  29.724117  32.133667  34.32885   36.186893  37.647568
 39.1641    40.41851   41.32895   41.792023  41.916187  41.38522
 40.447926  39.152782  37.439816  35.34229   34.00924   32.99075
 31.714476  29.995779  28.06592   25.939285  23.64312   21.497238
 19.739305  17.739725  15.505417  13.054162  10.410201   7.817651
  6.529025   7.251214   8.980868  10.526366  11.792682  12.904474
 13.835634  14.562571  15.184067  15.711367  16.175745  16.589792
 16.979332  17.208681  17.292671  16.917067  16.349802  15.672018
 15.290769  14.872077  14.2632065 13.444541  12.366378  11.241506
 10.14604    8.886026   7.987025   9.640034  11.098322  14.322497
 17.729069  21.000668  24.078922  26.88009   29.34291   31.576582
 33.575718  35.274567  36.50129   37.806435  38.730255  39.376472
 39.489048  39.16261   38.35585   37.250626  35.75338   33.874138
 32.201813  31.436317  30.449217  29.063622  27.365421  25.446457
 23.330019  21.068924  19.457233  17.621046  15.525373  13.191721
 10.617631   7.7947965  6.302359   6.635511   8.360878   9.910065
 11.194611  12.264103  13.160461  13.859662  14.425113  14.896056
 15.251203  15.597582  15.883355  16.131737  16.300888  16.20315
 15.756074  15.168728  14.618322  14.312871  13.923374  13.358267
 12.537169  11.589515  10.7182665  9.61809    8.377329   7.139793
  8.72272   10.295037  13.73584   17.1383    20.356794  23.348276
 26.09318   28.60637   30.659126  32.474663  34.038383  35.071926
 36.09368   36.768784  37.11184   36.95116   36.29794   35.346344
 34.112858  32.493256  30.610048  29.767864  29.116886  28.060488
 26.707798  25.059341  23.187819  21.139805  19.314316  17.713854
 15.896855  13.798586  11.399388   8.700376   6.5581927  5.8709226
  7.6114936  9.142061  10.480222  11.5364685 12.417223  13.076485
 13.583398  13.948443  14.279946  14.51108   14.74674   14.958164
 15.047768  15.170021  14.974589  14.514118  13.8884325 13.489925
 13.32199   12.973907  12.446989  11.697367  11.12112   10.301524
  9.223311   7.8604207  6.28473    7.5804853  9.2655525 12.741261
 16.136118  19.30379   22.214104  24.862299  27.256605  29.397058
 31.042524  32.45843   33.400364  34.114178  34.595985  34.714924
 34.225723  33.463974  32.442345  31.112951  29.484962  27.856186
 27.559338  26.928299  25.968588  24.674574  23.107737  21.33016
 19.362663  18.02659   16.516783  14.750363  12.725073  10.297103
  7.5164294  5.837467   6.738662   8.33375    9.678556  10.742383
 11.5996275 12.274154  12.743345  13.070194  13.260918  13.424685
 13.560631  13.69999   13.826969  13.908831  14.020181  13.822534
 13.379034  12.752109  12.411351  12.259188  11.960505  11.531606
 11.096824  10.745533   9.970137   8.894449   7.5623856  6.0754027
  6.248967   7.9410567 11.130907  14.547734  17.738945  20.648611
 23.22188   25.51538   27.565536  29.409492  30.803976  31.829653
 32.299034  32.563927  32.38923   31.554111  30.72507   29.685343
 28.350561  26.78231   25.603373  25.43823   24.905836  24.110844
 22.945223  21.43887   19.76957   18.089474  17.041796  15.7945175
 14.130063  12.007412   9.469635   6.6184087  5.365966   7.1303463
  8.635507   9.913521  10.811583  11.51363   12.024492  12.353462
 12.559756  12.663333  12.771511  12.968284  12.977017  12.897687
 12.729834  12.70766   12.519455  12.116095  11.563701  11.220075
 11.176565  10.96749   10.565628  10.5159445 10.314436   9.667084
  8.70947    7.571937   6.2015085  4.973325   6.230376   8.6018
 12.017612  15.347714  18.42762   21.068146  23.369032  25.466228
 27.378046  28.92634   29.971521  30.450079  30.505222  30.29773
 29.271152  28.126196  27.096012  25.822136  24.367512  23.33394
 23.339777  23.019474  22.354338  21.288078  19.976053  18.619802
 17.173622  16.404837  15.202755  13.6254425 11.630394   9.16836
  6.2448716  5.4259086  7.19359    8.641184   9.764979  10.666194
 11.325629  11.661525  11.87406   11.980661  12.0714    12.08617
 12.04924   12.128144  12.006586  11.725056 ]
