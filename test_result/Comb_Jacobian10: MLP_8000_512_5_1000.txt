time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.80%, model saved.
Epoch: 0 Train: 59745.44922 Test: 3955.37109
Epoch: 80 Train: 6377.91895 Test: 821.87891
Epoch: 160 Train: 2070.21753 Test: 535.59021
Epoch 240: New minimal relative error: 44.60%, model saved.
Epoch: 240 Train: 167.09402 Test: 68.93527
Epoch 320: New minimal relative error: 34.06%, model saved.
Epoch: 320 Train: 127.29766 Test: 20.62530
Epoch 400: New minimal relative error: 29.64%, model saved.
Epoch: 400 Train: 76.85432 Test: 14.29582
Epoch 480: New minimal relative error: 6.75%, model saved.
Epoch: 480 Train: 17.44312 Test: 5.08697
Epoch: 560 Train: 81.89960 Test: 11.61699
Epoch: 640 Train: 17.88601 Test: 7.88729
Epoch: 720 Train: 30.09179 Test: 5.33540
Epoch: 800 Train: 50.59625 Test: 20.04488
Epoch: 880 Train: 61.41076 Test: 21.71285
Epoch: 960 Train: 17.48376 Test: 7.06479
Epoch: 1040 Train: 44.77045 Test: 11.94757
Epoch: 1120 Train: 16.76846 Test: 8.22572
Epoch: 1200 Train: 29.98507 Test: 6.45394
Epoch: 1280 Train: 8.62637 Test: 5.05495
Epoch: 1360 Train: 19.13937 Test: 1.43474
Epoch: 1440 Train: 10.85640 Test: 4.23598
Epoch: 1520 Train: 8.48046 Test: 2.63675
Epoch: 1600 Train: 30.01636 Test: 7.81076
Epoch 1680: New minimal relative error: 3.75%, model saved.
Epoch: 1680 Train: 13.97833 Test: 5.00883
Epoch: 1760 Train: 13.80536 Test: 4.52418
Epoch: 1840 Train: 10.21615 Test: 4.26125
Epoch: 1920 Train: 29.70577 Test: 6.31598
Epoch: 2000 Train: 11.60236 Test: 3.90092
Epoch: 2080 Train: 22.69671 Test: 2.46743
Epoch: 2160 Train: 13.81636 Test: 3.99753
Epoch: 2240 Train: 6.32489 Test: 1.39304
Epoch: 2320 Train: 8.47428 Test: 1.85737
Epoch 2400: New minimal relative error: 2.77%, model saved.
Epoch: 2400 Train: 2.09530 Test: 0.16443
Epoch: 2480 Train: 6.70911 Test: 2.38323
Epoch: 2560 Train: 6.07787 Test: 1.78803
Epoch: 2640 Train: 2.57459 Test: 0.24598
Epoch: 2720 Train: 4.12680 Test: 1.22293
Epoch: 2800 Train: 5.64134 Test: 2.62786
Epoch: 2880 Train: 6.24352 Test: 3.52638
Epoch: 2960 Train: 18.44414 Test: 4.29139
Epoch: 3040 Train: 6.51340 Test: 2.11992
Epoch: 3120 Train: 11.69507 Test: 0.84332
Epoch: 3200 Train: 8.12832 Test: 1.20067
Epoch: 3280 Train: 7.10549 Test: 3.36663
Epoch: 3360 Train: 3.22551 Test: 0.30772
Epoch: 3440 Train: 1.17425 Test: 0.25460
Epoch: 3520 Train: 19.25998 Test: 6.32402
Epoch: 3600 Train: 4.84294 Test: 1.48535
Epoch: 3680 Train: 31.02419 Test: 12.13940
Epoch: 3760 Train: 1.39494 Test: 0.43029
Epoch: 3840 Train: 4.43461 Test: 1.54856
Epoch: 3920 Train: 8.22880 Test: 3.04747
Epoch: 4000 Train: 9.76985 Test: 2.66388
Epoch: 4080 Train: 6.10126 Test: 1.22610
Epoch: 4160 Train: 4.04071 Test: 0.88435
Epoch: 4240 Train: 1.82058 Test: 0.37716
Epoch: 4320 Train: 5.35513 Test: 1.51606
Epoch: 4400 Train: 7.45203 Test: 1.86353
Epoch: 4480 Train: 5.00278 Test: 1.60238
Epoch: 4560 Train: 11.21184 Test: 4.89172
Epoch: 4640 Train: 6.37123 Test: 1.86131
Epoch: 4720 Train: 2.48830 Test: 0.70306
Epoch: 4800 Train: 1.83851 Test: 0.47612
Epoch: 4880 Train: 1.64318 Test: 0.63520
Epoch: 4960 Train: 5.99374 Test: 1.65190
Epoch: 5040 Train: 4.20174 Test: 1.15743
Epoch: 5120 Train: 3.81065 Test: 0.83240
Epoch: 5200 Train: 5.51518 Test: 2.06354
Epoch 5280: New minimal relative error: 2.75%, model saved.
Epoch: 5280 Train: 0.80156 Test: 0.21429
Epoch: 5360 Train: 1.71623 Test: 0.36254
Epoch: 5440 Train: 3.47175 Test: 1.00278
Epoch: 5520 Train: 1.46567 Test: 0.49898
Epoch: 5600 Train: 1.02272 Test: 0.28325
Epoch: 5680 Train: 2.61463 Test: 0.77684
Epoch 5760: New minimal relative error: 1.84%, model saved.
Epoch: 5760 Train: 0.85073 Test: 0.28156
Epoch: 5840 Train: 0.36396 Test: 0.08776
Epoch: 5920 Train: 1.09672 Test: 0.41932
Epoch: 6000 Train: 3.23733 Test: 0.70724
Epoch: 6080 Train: 0.33102 Test: 0.11106
Epoch: 6160 Train: 0.63175 Test: 0.28277
Epoch: 6240 Train: 3.78604 Test: 1.05291
Epoch: 6320 Train: 6.75393 Test: 1.76320
Epoch: 6400 Train: 5.94392 Test: 3.19706
Epoch: 6480 Train: 0.15442 Test: 0.04885
Epoch: 6560 Train: 0.23959 Test: 0.05990
Epoch: 6640 Train: 0.53833 Test: 0.15288
Epoch: 6720 Train: 0.78230 Test: 0.20694
Epoch: 6800 Train: 0.53072 Test: 0.07808
Epoch: 6880 Train: 0.72172 Test: 0.21530
Epoch: 6960 Train: 0.97792 Test: 0.24525
Epoch: 7040 Train: 1.62367 Test: 0.67593
Epoch: 7120 Train: 0.34597 Test: 0.10897
Epoch: 7200 Train: 0.27746 Test: 0.08622
Epoch: 7280 Train: 1.47300 Test: 0.52451
Epoch: 7360 Train: 0.63051 Test: 0.11776
Epoch 7440: New minimal relative error: 0.97%, model saved.
Epoch: 7440 Train: 0.12889 Test: 0.04923
Epoch: 7520 Train: 0.52552 Test: 0.21322
Epoch: 7600 Train: 0.63665 Test: 0.24034
Epoch 7680: New minimal relative error: 0.59%, model saved.
Epoch: 7680 Train: 0.15124 Test: 0.05286
Epoch: 7760 Train: 0.22391 Test: 0.08644
Epoch: 7840 Train: 0.26735 Test: 0.10544
Epoch: 7920 Train: 1.18420 Test: 0.35836
Epoch: 7999 Train: 0.35461 Test: 0.06119
Training Loss: tensor(0.3546)
Test Loss: tensor(0.0612)
Learned LE: [ 8.6732304e-01  2.1494608e-03 -1.4531644e+01]
True LE: [ 8.6947155e-01 -6.6091372e-03 -1.4534564e+01]
Relative Error: [0.4242139  0.4293248  0.43485916 0.44048348 0.44578952 0.4507022
 0.4551688  0.4596265  0.4645687  0.47028023 0.4770831  0.48459944
 0.49237967 0.4994476  0.5049583  0.5076813  0.5066658  0.5011443
 0.4909552  0.47673383 0.46076718 0.44448444 0.42843428 0.41210046
 0.39522633 0.3791608  0.36590773 0.35696194 0.35251772 0.35143426
 0.35257125 0.3546741  0.35741004 0.36062485 0.36468488 0.3699973
 0.37674588 0.38506037 0.39511704 0.40697074 0.42035308 0.4338223
 0.4452973  0.4525845  0.45612338 0.45929185 0.46634236 0.47717088
 0.4851203  0.48334134 0.47259033 0.46079051 0.45212406 0.4439512
 0.43318006 0.42005527 0.40830207 0.39989096 0.39535382 0.39397636
 0.39539644 0.39881027 0.40368167 0.4094046  0.41553876 0.42148033
 0.4267421  0.4313547  0.4351226  0.4385641  0.44212985 0.44616213
 0.45143896 0.4577775  0.46468726 0.4711458  0.47618413 0.47898963
 0.47848448 0.47393596 0.46488115 0.4517311  0.43609455 0.42018393
 0.40465626 0.38891557 0.3726848  0.3571364  0.3442265  0.3358792
 0.33197802 0.33123168 0.33213696 0.33371687 0.33539397 0.3373743
 0.34022975 0.3443247  0.3499178  0.35722613 0.36618555 0.37686324
 0.38910195 0.40207598 0.413716   0.42145437 0.4246234  0.4261016
 0.43177503 0.44356743 0.45463857 0.4556526  0.44574887 0.43363672
 0.42516944 0.41741785 0.40657496 0.39379323 0.38293794 0.3757045
 0.3722532  0.37184563 0.37396193 0.37812296 0.38376835 0.39033824
 0.39706635 0.4035251  0.40897435 0.41345862 0.41665345 0.41927248
 0.4213403  0.42385763 0.42739502 0.4323253  0.43813866 0.44408217
 0.44878134 0.45137292 0.45130154 0.44788265 0.4403832  0.42866623
 0.4138645  0.39824596 0.38329303 0.3684095  0.3529044  0.3377766
 0.32511094 0.3168791  0.3129413  0.31214696 0.31277534 0.3136712
 0.3142778  0.31482136 0.31598493 0.31852365 0.32302073 0.329346
 0.3373429  0.3467736  0.3577057  0.36971244 0.3815273  0.390433
 0.3940588  0.39424473 0.397646   0.40913224 0.42345294 0.4286197
 0.42031956 0.40768123 0.399272   0.39231455 0.38203725 0.36973768
 0.35966232 0.3533631  0.3506433  0.3506214  0.35312033 0.35780296
 0.36413512 0.37143597 0.37886068 0.38592193 0.39183703 0.3964757
 0.39955434 0.40140212 0.402407   0.40336373 0.405148   0.40831432
 0.41305715 0.41826606 0.42277524 0.42527044 0.42545876 0.42288715
 0.41709816 0.4073042  0.39382434 0.3788284  0.3643536  0.35043165
 0.33586958 0.32117483 0.30834413 0.29965496 0.29527283 0.2941297
 0.29444784 0.29473588 0.294452   0.29348502 0.29284024 0.29348344
 0.2963251  0.3016257  0.30873147 0.31716102 0.32668144 0.33714983
 0.34845334 0.35850933 0.36392498 0.3641585  0.36478812 0.37396428
 0.390639   0.40147033 0.39653635 0.3834744  0.37464553 0.36873892
 0.3595106  0.34806284 0.3386765  0.33281723 0.3298893  0.32969975
 0.33214617 0.3371101  0.34398428 0.35204485 0.36025143 0.3680431
 0.3745818  0.37963435 0.3829936  0.38465524 0.38517216 0.384937
 0.3848964  0.386311   0.38946387 0.39380994 0.39823204 0.4009361
 0.40113312 0.39906892 0.3945887  0.38705122 0.37573448 0.36168617
 0.3476682  0.33449697 0.32125214 0.30721727 0.29409277 0.2843159
 0.2788638  0.2769364  0.2768646  0.27684352 0.27619788 0.27434662
 0.27183628 0.27022168 0.2707828  0.2744625  0.28060064 0.28830242
 0.29657638 0.30541345 0.31519532 0.32541046 0.3334062  0.33552334
 0.33441025 0.3390848  0.355124   0.3722597  0.37378228 0.36132598
 0.3511615  0.34665665 0.3394349  0.32895884 0.3198257  0.3137222
 0.31012568 0.30890554 0.31083646 0.31562454 0.32279184 0.33141464
 0.34043884 0.3489444  0.3563338  0.3621232  0.36601728 0.36818165
 0.36871314 0.36804566 0.36693954 0.36647967 0.36766854 0.37079272
 0.3747782  0.37795097 0.37870792 0.37674835 0.3728383  0.36721212
 0.3586538  0.34659767 0.33298874 0.32035527 0.30844843 0.2956768
 0.28238812 0.27118307 0.26401332 0.26053134 0.2596238  0.25958169
 0.25927827 0.2575189  0.25423813 0.25048295 0.24820115 0.24905929
 0.2534446  0.26025853 0.26799855 0.27564174 0.28328866 0.2920955
 0.30147147 0.30725092 0.30702594 0.3070521  0.31775784 0.33840463
 0.35013828 0.34171987 0.32918304 0.3253811  0.3213501  0.312359
 0.3033685  0.29641238 0.29117295 0.2883537  0.2889292  0.29304639
 0.30018535 0.3091852  0.31889245 0.32813329 0.33626223 0.34273726
 0.34746587 0.35040376 0.35168412 0.35149023 0.35030925 0.3489382
 0.34835365 0.34946433 0.35245308 0.3559494  0.35781977 0.35661477
 0.35274932 0.34761244 0.34137732 0.3323577  0.32030898 0.30765387
 0.29655492 0.2856809  0.27322957 0.26083353]
