time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.58%, model saved.
Epoch: 0 Train: 31831.66406 Test: 4016.88062
Epoch: 100 Train: 8775.67773 Test: 1525.02478
Epoch: 200 Train: 7332.13477 Test: 1302.59192
Epoch: 300 Train: 7365.02686 Test: 1327.24548
Epoch: 400 Train: 6754.42676 Test: 978.25446
Epoch 500: New minimal relative error: 98.71%, model saved.
Epoch: 500 Train: 4974.87500 Test: 514.45038
Epoch: 600 Train: 3096.86597 Test: 297.72305
Epoch 700: New minimal relative error: 69.14%, model saved.
Epoch: 700 Train: 1251.36243 Test: 79.70744
Epoch 800: New minimal relative error: 30.14%, model saved.
Epoch: 800 Train: 603.34949 Test: 18.07969
Epoch: 900 Train: 384.92102 Test: 6.91896
Epoch: 1000 Train: 289.63638 Test: 4.25319
Epoch: 1100 Train: 451.98853 Test: 66.38733
Epoch 1200: New minimal relative error: 11.35%, model saved.
Epoch: 1200 Train: 226.02867 Test: 3.57299
Epoch 1300: New minimal relative error: 4.97%, model saved.
Epoch: 1300 Train: 187.39091 Test: 2.84154
Epoch: 1400 Train: 175.92236 Test: 12.47131
Epoch: 1500 Train: 145.64156 Test: 1.19217
Epoch: 1600 Train: 136.63525 Test: 1.10437
Epoch: 1700 Train: 126.34877 Test: 4.00297
Epoch: 1800 Train: 140.28624 Test: 10.72853
Epoch: 1900 Train: 110.36620 Test: 1.06640
Epoch: 2000 Train: 113.47794 Test: 12.78111
Epoch: 2100 Train: 115.34634 Test: 9.62003
Epoch: 2200 Train: 91.56593 Test: 2.02944
Epoch: 2300 Train: 109.73241 Test: 1.28036
Epoch: 2400 Train: 81.45489 Test: 0.40234
Epoch: 2500 Train: 78.09058 Test: 0.40704
Epoch: 2600 Train: 86.64970 Test: 1.51072
Epoch 2700: New minimal relative error: 3.13%, model saved.
Epoch: 2700 Train: 88.43080 Test: 0.61934
Epoch: 2800 Train: 122.23045 Test: 16.66200
Epoch: 2900 Train: 72.17561 Test: 0.32145
Epoch: 3000 Train: 64.71610 Test: 0.41226
Epoch: 3100 Train: 65.30280 Test: 0.27357
Epoch: 3200 Train: 58.63691 Test: 0.36293
Epoch: 3300 Train: 56.35828 Test: 0.25686
Epoch: 3400 Train: 57.80034 Test: 1.25485
Epoch: 3500 Train: 55.14535 Test: 0.54490
Epoch: 3600 Train: 52.12403 Test: 0.56031
Epoch: 3700 Train: 52.21186 Test: 0.20644
Epoch: 3800 Train: 51.14754 Test: 0.24376
Epoch 3900: New minimal relative error: 1.18%, model saved.
Epoch: 3900 Train: 51.70444 Test: 0.19259
Epoch: 4000 Train: 51.39614 Test: 0.27020
Epoch: 4100 Train: 51.36432 Test: 0.19772
Epoch: 4200 Train: 53.71399 Test: 0.79925
Epoch: 4300 Train: 45.87285 Test: 0.32474
Epoch: 4400 Train: 50.47918 Test: 0.33291
Epoch: 4500 Train: 45.63552 Test: 0.25208
Epoch: 4600 Train: 41.64170 Test: 0.25778
Epoch: 4700 Train: 45.99179 Test: 2.35995
Epoch: 4800 Train: 37.94757 Test: 0.49444
Epoch: 4900 Train: 36.25853 Test: 0.33364
Epoch: 5000 Train: 38.22458 Test: 1.86569
Epoch: 5100 Train: 42.90792 Test: 5.45492
Epoch: 5200 Train: 34.13742 Test: 0.67950
Epoch: 5300 Train: 35.28567 Test: 0.82010
Epoch: 5400 Train: 33.61073 Test: 0.36524
Epoch: 5500 Train: 35.20775 Test: 0.23543
Epoch: 5600 Train: 31.73060 Test: 0.25616
Epoch: 5700 Train: 31.68689 Test: 0.71736
Epoch: 5800 Train: 30.80976 Test: 0.35576
Epoch: 5900 Train: 34.65359 Test: 2.12757
Epoch: 6000 Train: 31.05219 Test: 0.36318
Epoch: 6100 Train: 31.67208 Test: 2.02077
Epoch: 6200 Train: 30.28587 Test: 0.09032
Epoch: 6300 Train: 33.96246 Test: 0.11126
Epoch: 6400 Train: 33.88438 Test: 0.41624
Epoch: 6500 Train: 32.76651 Test: 0.08779
Epoch: 6600 Train: 32.65634 Test: 0.09104
Epoch: 6700 Train: 33.16878 Test: 0.51924
Epoch: 6800 Train: 31.75849 Test: 0.41644
Epoch: 6900 Train: 30.19034 Test: 0.12019
Epoch: 7000 Train: 30.36898 Test: 0.20302
Epoch: 7100 Train: 31.57184 Test: 1.25022
Epoch: 7200 Train: 29.38326 Test: 0.70123
Epoch: 7300 Train: 27.55701 Test: 0.28987
Epoch: 7400 Train: 29.00336 Test: 1.61309
Epoch: 7500 Train: 28.10325 Test: 0.31938
Epoch 7600: New minimal relative error: 0.91%, model saved.
Epoch: 7600 Train: 28.00776 Test: 0.06264
Epoch: 7700 Train: 28.53134 Test: 0.25052
Epoch: 7800 Train: 27.32132 Test: 0.09330
Epoch: 7900 Train: 28.21792 Test: 0.67151
Epoch: 8000 Train: 27.04647 Test: 0.10347
Epoch: 8100 Train: 27.58827 Test: 0.07092
Epoch: 8200 Train: 30.48937 Test: 0.34894
Epoch: 8300 Train: 27.63072 Test: 0.06452
Epoch: 8400 Train: 26.74713 Test: 0.06001
Epoch: 8500 Train: 27.74812 Test: 0.49692
Epoch: 8600 Train: 28.09239 Test: 0.07563
Epoch: 8700 Train: 28.34006 Test: 0.18891
Epoch: 8800 Train: 29.22207 Test: 0.09923
Epoch: 8900 Train: 27.53065 Test: 0.09032
Epoch: 9000 Train: 28.12976 Test: 0.06847
Epoch: 9100 Train: 27.55504 Test: 0.16430
Epoch: 9200 Train: 27.26913 Test: 0.06306
Epoch: 9300 Train: 27.66966 Test: 0.31298
Epoch: 9400 Train: 26.41325 Test: 0.16504
Epoch: 9500 Train: 24.85080 Test: 0.14146
Epoch: 9600 Train: 25.18112 Test: 0.06514
Epoch: 9700 Train: 25.74801 Test: 0.24066
Epoch: 9800 Train: 25.58167 Test: 0.39548
Epoch: 9900 Train: 24.34602 Test: 0.06175
Epoch: 9999 Train: 24.47804 Test: 0.05393
Training Loss: tensor(24.4780)
Test Loss: tensor(0.0539)
Learned LE: [ 8.3472890e-01  9.9179940e-03 -1.4508067e+01]
True LE: [ 8.5023159e-01 -2.0245370e-03 -1.4525001e+01]
Relative Error: [0.38490114 0.58632886 0.82336915 1.0424889  1.0740147  1.1012454
 1.175205   1.4566547  1.5850487  1.5550548  1.3894141  1.1376774
 0.90481406 0.96207035 0.99463177 0.9668287  0.91395164 0.85193866
 0.83744663 0.7483601  0.61382127 0.6481753  0.6582063  0.8367833
 0.8229008  0.6593092  0.64650935 0.73776793 0.8128506  0.86039275
 0.8839478  0.8742148  0.84824795 0.8010581  0.85956204 1.1556159
 1.6790963  2.3666387  2.5610583  2.3046336  1.9733105  1.9126345
 1.8656486  1.8022412  1.6810585  1.2580068  0.958436   0.779854
 0.6860751  0.6645163  0.74901336 0.96704674 1.1653775  1.2337984
 1.3140941  1.4105139  1.4627606  1.5198401  1.496452   1.2642848
 1.0183706  0.6087824  0.42035896 0.51137453 0.7253549  0.87756675
 0.9628798  1.045727   1.165127   1.4285886  1.5182753  1.4635444
 1.2770025  1.0254787  0.9248498  0.95940137 0.9773791  0.90548545
 0.8249486  0.82009286 0.9056644  0.73602    0.5611277  0.49744165
 0.6484163  0.9528725  0.96139383 0.7260586  0.6826015  0.68939114
 0.74437034 0.7904796  0.8046859  0.809401   0.73533857 0.65379214
 0.72819686 1.0602025  1.6234515  2.3050632  2.309935   1.8618574
 1.5884018  1.6363163  1.6410705  1.615418   1.3949108  1.0037178
 0.7315099  0.5530603  0.4640652  0.5060445  0.65926343 0.8402367
 1.0990318  1.1979057  1.297268   1.4231565  1.5307987  1.5904579
 1.586515   1.4209757  1.1693044  0.7597006  0.49227193 0.5063795
 0.70885414 0.7739201  0.89195734 1.008469   1.1130021  1.3564692
 1.4491057  1.3789968  1.2011236  0.987366   0.95203173 0.9763587
 1.0178853  0.90227836 0.8289975  0.86379087 0.9082361  0.7169169
 0.529763   0.50588155 0.7145834  0.99120736 1.0600599  0.85337216
 0.794744   0.7598454  0.77965546 0.82078165 0.8299328  0.73631734
 0.62924033 0.5535441  0.6497222  1.0222386  1.6119322  2.218516
 1.9989595  1.4586072  1.222833   1.3922476  1.5485919  1.57341
 1.276827   0.8509984  0.4977397  0.31427798 0.25745153 0.41324753
 0.61250144 0.7899117  1.0074153  1.1675018  1.2617607  1.3480229
 1.4427145  1.5135899  1.5079219  1.3217099  1.167349   0.8918517
 0.63930804 0.5359045  0.60862786 0.6750954  0.8242302  0.9324625
 1.0399169  1.2881957  1.3777549  1.323451   1.1639143  0.96588665
 0.978824   1.0017599  1.0512061  0.9107711  0.9157927  0.9607179
 0.91422665 0.7303539  0.49571148 0.5067544  0.7242289  0.9784344
 1.0913817  1.0211227  0.9196778  0.84638613 0.8482459  0.90849215
 0.89145815 0.782557   0.5735599  0.5014214  0.64317846 1.0366253
 1.5981834  2.0544796  1.6660832  1.1297346  1.0725155  1.3471069
 1.5729748  1.6020831  1.2253284  0.83088267 0.48521987 0.20617235
 0.18655159 0.39978054 0.5742189  0.75557053 0.90308285 0.8921676
 0.9354611  1.0246453  1.176371   1.3032982  1.3562586  1.1988157
 1.0313022  0.9073095  0.73169184 0.5909016  0.51809496 0.5844447
 0.7083648  0.82504386 0.94235957 1.2103112  1.3027712  1.2707453
 1.1457603  0.97512853 0.99362004 1.006501   1.0608566  0.9793387
 0.99220526 1.0618557  0.98259264 0.65763885 0.4356933  0.45404252
 0.6768422  0.9062879  1.1893427  1.1540825  1.0021285  0.88410366
 0.917767   0.98045653 0.97445416 0.90230024 0.6191597  0.5116329
 0.67597944 1.0739877  1.5916672  1.8416905  1.3707733  0.9247189
 0.9507525  1.2636327  1.5461538  1.524559   1.1788951  0.8305651
 0.5084557  0.24298173 0.29152998 0.45695505 0.5915945  0.5989011
 0.5077849  0.5313551  0.6307313  0.7654797  0.95575994 1.1043055
 1.120731   0.9109454  0.84991866 0.9513037  0.7603991  0.62908804
 0.5144952  0.48447365 0.5550518  0.7098103  0.84033376 1.1328729
 1.2399712  1.2114588  1.0984     1.0017246  0.9813438  0.9899467
 1.0803862  1.0241678  1.0593157  1.1704949  1.0091356  0.5713537
 0.35395777 0.3549005  0.58368444 0.7904347  1.252496   1.2458847
 1.0092247  0.90541244 0.95025355 1.0237272  1.0560129  0.9968902
 0.74297434 0.5405253  0.7259357  1.1279454  1.5548018  1.6331656
 1.1275527  0.72742397 0.81350124 1.1527745  1.4951028  1.4520553
 1.1103914  0.7600439  0.4825105  0.31156918 0.44341987 0.56285286
 0.4367915  0.3239662  0.26145655 0.27740124 0.3995087  0.5746721
 0.8053266  0.9512241  0.87815124 0.6773414  0.6575253  0.8521351
 0.79638624 0.6311727  0.5776183  0.40448952 0.401946   0.5909317
 0.7303603  1.056845   1.1917372  1.1560345  1.0241866  0.97076267
 0.937724   0.94008607 1.100438   1.0497202  1.1208394  1.2439194
 0.946104   0.5502361  0.2890111  0.2864502  0.5003626  0.7536816
 1.2651002  1.2266488  0.9788445  0.88608384]
