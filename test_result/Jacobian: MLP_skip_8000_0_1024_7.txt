time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.36%, model saved.
Epoch: 0 Train: 172780.59375 Test: 4092.22632
Epoch 80: New minimal relative error: 95.60%, model saved.
Epoch: 80 Train: 40405.75391 Test: 1523.09937
Epoch: 160 Train: 45786.51562 Test: 3810.45459
Epoch: 240 Train: 33322.76562 Test: 2670.65894
Epoch: 320 Train: 35741.20312 Test: 8260.79883
Epoch: 400 Train: 40143.40625 Test: 1404.71204
Epoch: 480 Train: 35358.51172 Test: 1927.80298
Epoch: 560 Train: 45860.26953 Test: 2364.26318
Epoch: 640 Train: 33734.06641 Test: 1401.27991
Epoch 720: New minimal relative error: 46.97%, model saved.
Epoch: 720 Train: 35155.17188 Test: 1236.49878
Epoch: 800 Train: 36334.60156 Test: 1284.28992
Epoch: 880 Train: 36202.85938 Test: 1260.78235
Epoch: 960 Train: 32124.34570 Test: 1191.12415
Epoch: 1040 Train: 36582.53906 Test: 1278.61719
Epoch: 1120 Train: 38990.82031 Test: 1866.35669
Epoch: 1200 Train: 35436.16797 Test: 1372.15796
Epoch: 1280 Train: 35790.07422 Test: 2664.47095
Epoch: 1360 Train: 33756.79688 Test: 1295.28503
Epoch: 1440 Train: 36167.15234 Test: 1193.89539
Epoch: 1520 Train: 32623.44141 Test: 1230.22205
Epoch: 1600 Train: 34215.76172 Test: 1427.67834
Epoch: 1680 Train: 32798.60547 Test: 1194.12122
Epoch: 1760 Train: 33986.12500 Test: 1160.50427
Epoch: 1840 Train: 32297.35742 Test: 1249.24902
Epoch: 1920 Train: 34990.62109 Test: 1204.81775
Epoch: 2000 Train: 31643.34570 Test: 989.31531
Epoch: 2080 Train: 32184.52148 Test: 1081.29187
Epoch: 2160 Train: 31485.14844 Test: 1199.41028
Epoch: 2240 Train: 32031.49805 Test: 1076.22229
Epoch: 2320 Train: 33395.89062 Test: 1194.26526
Epoch: 2400 Train: 33739.95703 Test: 1272.85376
Epoch: 2480 Train: 33198.68359 Test: 1185.96252
Epoch: 2560 Train: 30613.90234 Test: 1030.06470
Epoch: 2640 Train: 31590.56445 Test: 1102.78491
Epoch: 2720 Train: 30159.55664 Test: 1053.94434
Epoch: 2800 Train: 31892.14453 Test: 1143.44617
Epoch: 2880 Train: 32487.20312 Test: 1245.14648
Epoch: 2960 Train: 30064.82031 Test: 1092.19629
Epoch: 3040 Train: 28946.98828 Test: 962.22192
Epoch: 3120 Train: 29823.52930 Test: 873.62274
Epoch: 3200 Train: 30712.60742 Test: 1096.84924
Epoch: 3280 Train: 29085.30469 Test: 1054.45679
Epoch: 3360 Train: 30241.97656 Test: 961.38818
Epoch: 3440 Train: 30633.01367 Test: 1052.01599
Epoch: 3520 Train: 26862.94531 Test: 845.12317
Epoch: 3600 Train: 26958.57227 Test: 786.96643
Epoch: 3680 Train: 25167.57812 Test: 707.99597
Epoch: 3760 Train: 21543.86328 Test: 574.06091
Epoch: 3840 Train: 18889.80469 Test: 443.13562
Epoch: 3920 Train: 20740.75391 Test: 514.95551
Epoch: 4000 Train: 15674.00195 Test: 272.87823
Epoch: 4080 Train: 12774.49316 Test: 178.05545
Epoch: 4160 Train: 6412.99902 Test: 80.80258
Epoch 4240: New minimal relative error: 34.92%, model saved.
Epoch: 4240 Train: 4453.29590 Test: 34.95480
Epoch: 4320 Train: 4767.90283 Test: 158.27435
Epoch 4400: New minimal relative error: 9.00%, model saved.
Epoch: 4400 Train: 2073.52612 Test: 13.60669
Epoch 4480: New minimal relative error: 6.62%, model saved.
Epoch: 4480 Train: 1151.13672 Test: 5.85123
Epoch 4560: New minimal relative error: 6.09%, model saved.
Epoch: 4560 Train: 968.62646 Test: 5.19549
Epoch: 4640 Train: 837.84619 Test: 3.63108
Epoch 4720: New minimal relative error: 2.82%, model saved.
Epoch: 4720 Train: 851.11493 Test: 3.03655
Epoch: 4800 Train: 816.64923 Test: 6.66145
Epoch: 4880 Train: 661.44885 Test: 2.47719
Epoch: 4960 Train: 738.52063 Test: 6.53389
Epoch: 5040 Train: 637.15662 Test: 2.13291
Epoch: 5120 Train: 690.20996 Test: 5.22387
Epoch: 5200 Train: 543.16925 Test: 4.91978
Epoch: 5280 Train: 603.51624 Test: 3.36366
Epoch: 5360 Train: 601.34460 Test: 7.97300
Epoch: 5440 Train: 503.50461 Test: 1.35448
Epoch: 5520 Train: 468.23080 Test: 1.09184
Epoch: 5600 Train: 470.35370 Test: 1.70329
Epoch: 5680 Train: 436.53479 Test: 5.98545
Epoch: 5760 Train: 593.31970 Test: 5.27152
Epoch: 5840 Train: 456.02551 Test: 2.05692
Epoch: 5920 Train: 555.23517 Test: 8.59510
Epoch 6000: New minimal relative error: 2.11%, model saved.
Epoch: 6000 Train: 418.88428 Test: 1.01809
Epoch: 6080 Train: 412.03409 Test: 1.29369
Epoch: 6160 Train: 424.07730 Test: 1.08108
Epoch: 6240 Train: 421.09442 Test: 0.96172
Epoch: 6320 Train: 401.63135 Test: 1.21724
Epoch: 6400 Train: 382.24670 Test: 1.23671
Epoch: 6480 Train: 387.98776 Test: 1.37748
Epoch: 6560 Train: 553.40295 Test: 8.60908
Epoch: 6640 Train: 359.93661 Test: 1.00983
Epoch 6720: New minimal relative error: 2.02%, model saved.
Epoch: 6720 Train: 353.77271 Test: 0.65330
Epoch: 6800 Train: 350.58530 Test: 0.69569
Epoch: 6880 Train: 353.61844 Test: 1.91787
Epoch: 6960 Train: 376.48883 Test: 0.88394
Epoch: 7040 Train: 321.31009 Test: 0.97419
Epoch: 7120 Train: 311.86584 Test: 0.71739
Epoch: 7200 Train: 313.02155 Test: 1.36254
Epoch: 7280 Train: 298.63995 Test: 0.82428
Epoch: 7360 Train: 296.64520 Test: 1.63260
Epoch: 7440 Train: 317.16870 Test: 0.74712
Epoch: 7520 Train: 337.99118 Test: 0.79007
Epoch: 7600 Train: 337.12085 Test: 0.66691
Epoch: 7680 Train: 336.37979 Test: 0.60575
Epoch 7760: New minimal relative error: 1.66%, model saved.
Epoch: 7760 Train: 333.16104 Test: 0.78275
Epoch: 7840 Train: 335.34787 Test: 0.86671
Epoch: 7920 Train: 336.99234 Test: 1.52633
Epoch: 7999 Train: 318.01996 Test: 0.86453
Training Loss: tensor(318.0200)
Test Loss: tensor(0.8645)
Learned LE: [  0.858319     0.04946267 -14.579098  ]
True LE: [ 8.7082696e-01  6.7094765e-03 -1.4549653e+01]
Relative Error: [4.7866006 4.967691  4.9802895 4.6396885 4.2753386 4.2037497 4.1710234
 4.2057967 4.130912  4.230417  4.274003  4.1518936 3.9507833 3.85537
 3.792438  4.0404778 4.3264394 4.5896134 4.809804  5.068555  5.017455
 4.909443  4.4673095 3.898451  3.4383118 3.1594918 2.975161  2.836093
 2.5467765 2.134658  2.0042577 2.6237185 3.0137706 3.1736863 3.48017
 4.1652126 4.8260007 4.934894  4.929138  4.988019  5.098097  5.13337
 4.72876   4.558528  4.3093476 3.969962  3.4682822 2.8363044 2.314843
 2.2935514 2.509255  2.5213704 2.4152317 1.957537  1.7942274 1.8583708
 2.1483269 2.3903866 2.7818017 3.184402  3.5391264 4.008735  4.355147
 4.564375  4.6980357 4.6719394 4.457115  4.3779793 4.317567  4.178686
 4.085866  4.1796517 4.2910466 4.169511  3.8051717 3.5037405 3.380578
 3.3796296 3.6933165 3.965658  4.288397  4.449105  4.7086005 4.6426272
 4.596477  4.005221  3.4495955 3.113112  2.899979  2.6726332 2.5226126
 2.2030952 1.8030753 1.9783158 2.4931128 2.7204618 2.8756073 3.382906
 4.0987253 4.415862  4.562519  4.609542  4.618352  4.5251746 4.4596353
 4.314853  4.018572  3.876454  3.6217623 3.1470053 2.499518  2.2947497
 2.5075452 2.5369227 2.5026715 2.010424  1.7896544 1.8107877 2.0384243
 2.0888007 2.404224  2.9930186 3.2279418 3.4729018 3.8431273 4.199143
 4.4333267 4.677267  4.679254  4.5251465 4.5382833 4.4288034 4.2534146
 4.1245008 4.2204356 4.2251253 3.9521296 3.4787219 3.1489458 3.0888357
 2.9793005 3.266408  3.5774932 3.8745456 4.045245  4.2811837 4.3022
 4.12835   3.5454602 3.025027  2.8529367 2.592795  2.3777068 2.336028
 1.9037346 1.6098689 2.0323563 2.3908806 2.456541  2.7002695 3.150053
 3.8099494 3.9947343 4.3024964 4.2651405 3.991899  3.8663619 4.039106
 3.8004026 3.646573  3.603836  3.2407017 2.8755386 2.4075572 2.3629715
 2.550226  2.5935643 2.2053297 1.7508324 1.7526802 1.9404881 1.973407
 2.1292129 2.5214808 2.8051953 2.9989827 3.2106829 3.55729   3.9843717
 4.3619285 4.615012  4.6020856 4.513316  4.4992967 4.3722486 4.2858615
 4.1485558 4.288168  4.2222505 3.7678103 3.2134876 2.9724483 2.8222187
 2.6670568 2.8874469 3.245943  3.368855  3.6073713 3.7991505 3.9270618
 3.7231755 3.126173  2.6409726 2.5510786 2.3306189 2.2137697 2.1352746
 1.7857356 1.5984243 2.0364661 2.3160203 2.2416224 2.4745295 2.7853901
 3.2940674 3.682027  3.963453  3.7425272 3.309923  3.3174229 3.6017566
 3.44965   3.349265  3.1542995 2.8959286 2.6320603 2.3986516 2.4440277
 2.513624  2.3120651 1.8314414 1.5447321 1.6386207 1.6308805 1.7884018
 2.1007571 2.282912  2.4844906 2.662367  2.8898258 3.1640892 3.5813558
 4.032577  4.274476  4.3850684 4.3316984 4.2820077 4.0811477 4.100709
 4.228856  4.3723593 4.1302733 3.6466665 3.0895238 2.766664  2.582836
 2.5195355 2.5093782 2.733818  2.9753144 3.193297  3.4459457 3.6629517
 3.5308177 2.9198654 2.3956363 2.2141442 2.146807  2.1363292 2.0131464
 1.8248554 1.5723752 1.874226  2.0609622 1.9830449 2.2036774 2.4136713
 2.8462925 3.2495267 3.5508819 3.2376454 2.734325  2.848905  3.1406825
 3.1814873 3.0137196 2.7052822 2.5627432 2.4512112 2.3064528 2.3770542
 2.456606  2.0964143 1.6031291 1.387828  1.5337011 1.4803863 1.6792742
 1.750933  1.9229938 2.1243372 2.272626  2.4701202 2.7162156 3.0336807
 3.505146  3.8005254 3.979681  4.0458517 3.953906  3.8207135 3.9059858
 4.038389  4.2583475 4.0928044 3.557182  3.0298266 2.6421483 2.4823806
 2.5142617 2.2465658 2.2490435 2.6446245 2.9597907 3.3609378 3.556858
 3.7198386 3.141062  2.3376162 1.952369  1.6908342 1.7835351 1.7235417
 1.5685744 1.4025122 1.314805  1.4662025 1.5119225 1.6128137 1.8318586
 2.4666731 2.7220414 3.062686  2.8796632 2.4060946 2.4474301 2.6179047
 2.7699893 2.6988153 2.4240932 2.2184162 2.1644478 2.042569  2.2087455
 2.367496  1.9905356 1.472353  1.3096565 1.3985457 1.3688554 1.4627316
 1.5492631 1.6776122 1.7240067 1.8722951 2.104217  2.2705781 2.4859633
 2.8239372 3.2394302 3.407318  3.5917983 3.61838   3.7357311 3.7151682
 3.939435  4.176928  3.9567585 3.5090756 3.078182  2.5734422 2.4632237
 2.4294004 2.2536647 2.0041838 2.2284749 2.5863514 2.9647617 3.3870752
 3.5933304]
