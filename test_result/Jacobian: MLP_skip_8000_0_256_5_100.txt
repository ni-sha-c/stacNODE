time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.74%, model saved.
Epoch: 0 Train: 9786.87695 Test: 4110.49805
Epoch: 80 Train: 2793.47363 Test: 1129.85449
Epoch: 160 Train: 2737.46631 Test: 1103.53577
Epoch: 240 Train: 2454.92578 Test: 919.53973
Epoch 320: New minimal relative error: 95.94%, model saved.
Epoch: 320 Train: 1183.08569 Test: 284.48251
Epoch: 400 Train: 518.61621 Test: 94.17626
Epoch: 480 Train: 305.96021 Test: 66.37904
Epoch 560: New minimal relative error: 29.93%, model saved.
Epoch: 560 Train: 172.62219 Test: 24.05252
Epoch 640: New minimal relative error: 25.42%, model saved.
Epoch: 640 Train: 121.24309 Test: 9.86616
Epoch 720: New minimal relative error: 18.97%, model saved.
Epoch: 720 Train: 101.13322 Test: 19.41120
Epoch: 800 Train: 79.41517 Test: 7.00042
Epoch 880: New minimal relative error: 18.93%, model saved.
Epoch: 880 Train: 64.21755 Test: 4.38930
Epoch 960: New minimal relative error: 18.64%, model saved.
Epoch: 960 Train: 57.05092 Test: 3.53649
Epoch 1040: New minimal relative error: 4.59%, model saved.
Epoch: 1040 Train: 51.67459 Test: 2.66612
Epoch: 1120 Train: 53.50575 Test: 4.41443
Epoch: 1200 Train: 42.95901 Test: 1.89753
Epoch: 1280 Train: 41.18144 Test: 1.83721
Epoch: 1360 Train: 41.15597 Test: 6.72460
Epoch: 1440 Train: 42.93438 Test: 1.93870
Epoch: 1520 Train: 32.34188 Test: 1.14292
Epoch 1600: New minimal relative error: 4.55%, model saved.
Epoch: 1600 Train: 31.24384 Test: 1.23600
Epoch: 1680 Train: 33.09343 Test: 1.36143
Epoch 1760: New minimal relative error: 4.13%, model saved.
Epoch: 1760 Train: 29.20551 Test: 1.87960
Epoch: 1840 Train: 29.30839 Test: 0.96978
Epoch: 1920 Train: 33.66167 Test: 4.52250
Epoch: 2000 Train: 28.85157 Test: 1.47970
Epoch: 2080 Train: 27.86305 Test: 1.14186
Epoch: 2160 Train: 26.81168 Test: 1.67395
Epoch: 2240 Train: 24.34134 Test: 1.17684
Epoch: 2320 Train: 22.92552 Test: 0.86117
Epoch: 2400 Train: 21.77204 Test: 0.75232
Epoch: 2480 Train: 21.60283 Test: 1.16795
Epoch: 2560 Train: 21.05262 Test: 0.89555
Epoch: 2640 Train: 22.14422 Test: 1.84201
Epoch: 2720 Train: 22.64961 Test: 1.10702
Epoch: 2800 Train: 19.71641 Test: 0.73724
Epoch: 2880 Train: 20.57884 Test: 1.29259
Epoch: 2960 Train: 20.69835 Test: 1.54763
Epoch 3040: New minimal relative error: 2.56%, model saved.
Epoch: 3040 Train: 19.03391 Test: 0.68711
Epoch: 3120 Train: 17.92670 Test: 0.52239
Epoch: 3200 Train: 19.34034 Test: 0.96007
Epoch: 3280 Train: 23.18387 Test: 6.65784
Epoch: 3360 Train: 16.87000 Test: 0.46131
Epoch: 3440 Train: 19.32297 Test: 2.94884
Epoch: 3520 Train: 22.90051 Test: 4.96253
Epoch: 3600 Train: 18.92278 Test: 2.96837
Epoch: 3680 Train: 20.56890 Test: 5.59259
Epoch: 3760 Train: 15.81966 Test: 0.37207
Epoch: 3840 Train: 16.09259 Test: 0.84920
Epoch: 3920 Train: 16.04279 Test: 1.25864
Epoch: 4000 Train: 17.24884 Test: 2.74434
Epoch: 4080 Train: 16.34718 Test: 1.25112
Epoch: 4160 Train: 15.50727 Test: 0.48358
Epoch: 4240 Train: 17.18275 Test: 2.19114
Epoch: 4320 Train: 15.86907 Test: 1.63215
Epoch: 4400 Train: 15.53758 Test: 1.30014
Epoch: 4480 Train: 20.09759 Test: 4.43917
Epoch: 4560 Train: 14.41641 Test: 0.58886
Epoch 4640: New minimal relative error: 1.36%, model saved.
Epoch: 4640 Train: 13.71091 Test: 0.32250
Epoch: 4720 Train: 13.67979 Test: 0.34487
Epoch: 4800 Train: 13.67579 Test: 0.54322
Epoch: 4880 Train: 13.23562 Test: 0.34459
Epoch: 4960 Train: 13.35390 Test: 0.32819
Epoch: 5040 Train: 13.57149 Test: 0.42617
Epoch: 5120 Train: 13.15928 Test: 0.43619
Epoch: 5200 Train: 13.43661 Test: 0.59497
Epoch: 5280 Train: 13.69918 Test: 1.29767
Epoch: 5360 Train: 12.87117 Test: 0.51593
Epoch: 5440 Train: 13.08993 Test: 0.54773
Epoch: 5520 Train: 13.70392 Test: 0.66161
Epoch: 5600 Train: 13.19396 Test: 0.67708
Epoch: 5680 Train: 13.26384 Test: 0.51060
Epoch: 5760 Train: 13.00886 Test: 0.38160
Epoch: 5840 Train: 12.95806 Test: 0.33635
Epoch: 5920 Train: 14.05414 Test: 0.97497
Epoch: 6000 Train: 13.24928 Test: 0.41825
Epoch: 6080 Train: 12.87134 Test: 0.31454
Epoch: 6160 Train: 14.68227 Test: 1.74472
Epoch: 6240 Train: 12.78581 Test: 0.30047
Epoch: 6320 Train: 12.97963 Test: 0.60041
Epoch: 6400 Train: 13.05205 Test: 0.52808
Epoch: 6480 Train: 12.45119 Test: 0.36973
Epoch: 6560 Train: 12.26715 Test: 0.28443
Epoch: 6640 Train: 12.15209 Test: 0.31582
Epoch: 6720 Train: 12.08119 Test: 0.29241
Epoch: 6800 Train: 11.91022 Test: 0.27520
Epoch: 6880 Train: 12.05614 Test: 0.43615
Epoch: 6960 Train: 11.58486 Test: 0.26291
Epoch: 7040 Train: 11.63975 Test: 0.25004
Epoch: 7120 Train: 11.51257 Test: 0.29460
Epoch: 7200 Train: 11.29992 Test: 0.28256
Epoch: 7280 Train: 11.32236 Test: 0.22837
Epoch: 7360 Train: 11.16267 Test: 0.24820
Epoch: 7440 Train: 12.00899 Test: 0.77413
Epoch: 7520 Train: 10.98721 Test: 0.21259
Epoch: 7600 Train: 11.83043 Test: 0.74399
Epoch: 7680 Train: 10.86920 Test: 0.21497
Epoch: 7760 Train: 10.87616 Test: 0.24341
Epoch: 7840 Train: 10.79371 Test: 0.22528
Epoch: 7920 Train: 10.66768 Test: 0.20459
Epoch: 7999 Train: 10.55391 Test: 0.21498
Training Loss: tensor(10.5539)
Test Loss: tensor(0.2150)
Learned LE: [  0.80522084   0.03236161 -14.494438  ]
True LE: [ 8.4429783e-01  3.6374833e-03 -1.4521419e+01]
Relative Error: [1.4299959  2.1750982  2.268846   1.4927468  0.9107303  0.96501064
 1.2685957  1.6205626  1.8521401  1.95157    1.8237865  1.499574
 1.0894151  0.94662064 1.2251258  1.9478188  2.899181   2.7173862
 2.1279824  1.6401591  1.2819431  1.0681173  1.0579172  1.2468107
 1.5140102  1.6881274  1.9477239  2.0594854  2.0605004  2.198079
 2.063064   2.0332148  1.808326   1.595222   1.3449287  1.3065423
 1.4886508  1.8885676  2.0511117  1.9361976  1.6773213  1.4214412
 1.6092622  1.8509521  1.9544452  1.8529454  1.5742099  1.1509441
 0.73779213 1.0091354  1.9755363  2.674821   2.7030568  2.2585838
 1.8029498  1.4258914  1.4212594  1.369174   1.3518159  1.3116221
 1.3601689  1.2702647  1.2005095  1.80101    2.5034099  1.9718853
 1.1297472  0.89832    1.1386929  1.5338428  1.8239945  1.926698
 1.7849188  1.4130307  0.8786081  0.6422663  0.9456855  1.6794728
 2.4994345  2.2715552  1.7306037  1.3523259  1.0653577  0.93327
 0.89622897 1.0643373  1.3418149  1.5533673  1.7809652  1.9805452
 2.0948658  2.065746   1.87513    1.7592596  1.7570119  1.4979079
 1.2702914  1.2440195  1.4472946  1.817637   1.943439   1.8027816
 1.8023587  1.3429682  1.485324   1.7375058  1.861918   1.8143383
 1.6012646  1.1634092  0.5969958  0.9095311  1.937836   2.269014
 2.4529316  2.0009806  1.6360303  1.3482906  1.27606    1.3121341
 1.3664329  1.3609234  1.5067987  1.3716084  1.0676953  1.4853201
 2.09475    2.5391264  1.5066369  0.9388172  1.0463525  1.431651
 1.7846038  1.9135906  1.7870781  1.4071513  0.7586777  0.3865066
 0.7892125  1.5421313  2.0892317  1.9673944  1.537454   1.1758058
 0.9053874  0.68056405 0.7122407  0.94443357 1.2387455  1.4917221
 1.6385303  1.9009795  1.9448578  2.048783   1.8704786  1.6198483
 1.772398   1.3957621  1.1660379  1.1667202  1.4130849  1.7918278
 1.899969   1.7800103  1.6993976  1.2791505  1.2310034  1.5159938
 1.7193532  1.782708   1.5851164  1.0856831  0.44868538 0.7037424
 1.7026265  1.8115786  2.2112021  1.9215385  1.4703404  1.2422303
 1.0914717  1.230458   1.3281562  1.3468482  1.615926   1.4515904
 1.0540687  1.2214042  1.7395275  2.3638747  2.036259   1.1406007
 0.99927497 1.324285   1.7251741  1.9000136  1.7422423  1.306203
 0.6358233  0.34387133 0.76686156 1.5719492  1.9376706  1.7577834
 1.4381357  1.0843351  0.78636754 0.65441525 0.7168556  0.9470981
 1.245934   1.5247504  1.6551151  1.8151091  1.9526522  1.9007987
 1.9316018  1.7046843  1.7142061  1.462993   1.0512716  1.0278338
 1.3133079  1.7265955  1.917734   1.7471861  1.5769107  1.2815646
 0.9971584  1.2611212  1.578413   1.7521385  1.5544786  1.1107329
 0.43431726 0.52097243 1.3222613  1.3405286  1.7183284  1.8687162
 1.4715568  1.1175082  1.0024456  1.0723674  1.262576   1.3447948
 1.6273707  1.4406914  0.95703536 0.79042214 1.2560383  1.8992758
 2.4794888  1.61255    1.0531164  1.2114569  1.6226085  1.7817441
 1.6450975  1.226111   0.5572849  0.30951646 0.69170564 1.5084805
 1.8301865  1.6478579  1.5698053  1.0557466  0.7011685  0.6620352
 0.79027766 1.0083121  1.3463567  1.6323696  1.7579665  1.7596798
 1.8247766  1.770542   1.7023693  1.6373922  1.6637396  1.7162168
 1.179347   0.8742611  1.0850967  1.5514688  1.7756231  1.6349865
 1.4467868  1.2193754  0.9788098  0.9467769  1.3275502  1.5933199
 1.4974177  1.1508934  0.5611138  0.35029474 1.0980961  0.9340713
 1.1613021  1.7207537  1.4469663  1.1266974  0.8790316  0.8091949
 1.1758982  1.3749874  1.4607979  1.3766738  0.9420326  0.5449532
 0.6760101  1.1405818  1.8261852  2.2731638  1.252783   1.1142871
 1.5031025  1.6071807  1.5097548  1.1271406  0.5508907  0.26148513
 0.5245587  1.321571   1.7242157  1.5501769  1.5188041  1.0931895
 0.77070284 0.7177982  0.8619567  1.1467997  1.4120288  1.6633309
 1.8544797  1.8322799  1.7073628  1.6327537  1.5051609  1.4262414
 1.5491364  1.6676599  1.4655757  1.0827291  0.7563608  1.077174
 1.5082365  1.5291259  1.2424833  1.1814327  0.89932    0.70053625
 0.93426156 1.3470457  1.4253758  1.2305812  0.7694705  0.31215662
 1.0810441  0.7003276  0.69911546 1.0625262  1.3933676  1.1220838
 0.9013858  0.7635426  0.8661503  1.3407347  1.2522023  1.2793641
 0.99749064 0.6748331  0.3832712  0.5719199  1.0479544  1.6238958
 1.7792857  1.065119   1.2777768  1.4786801  1.4500995  1.0843395
 0.614226   0.31399274 0.3430708  1.0345031  1.6588566  1.4029914
 1.2562368  1.124103   0.81656677 0.74553174 0.85463    1.007745
 1.2746545  1.6401125  1.7123498  1.7598705 ]
