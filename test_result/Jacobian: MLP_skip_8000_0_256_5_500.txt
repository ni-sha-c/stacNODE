time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.18%, model saved.
Epoch: 0 Train: 31359.39258 Test: 3888.17432
Epoch 80: New minimal relative error: 95.90%, model saved.
Epoch: 80 Train: 8600.63086 Test: 1479.77515
Epoch: 160 Train: 8137.80957 Test: 1285.19873
Epoch 240: New minimal relative error: 64.79%, model saved.
Epoch: 240 Train: 7845.85352 Test: 1161.63171
Epoch: 320 Train: 7273.32715 Test: 1522.67969
Epoch: 400 Train: 6813.40283 Test: 965.92029
Epoch: 480 Train: 5102.99316 Test: 610.27625
Epoch 560: New minimal relative error: 56.55%, model saved.
Epoch: 560 Train: 3939.45435 Test: 446.28543
Epoch: 640 Train: 2295.52856 Test: 183.99858
Epoch: 720 Train: 1422.98608 Test: 75.60514
Epoch 800: New minimal relative error: 27.12%, model saved.
Epoch: 800 Train: 1113.61047 Test: 47.13269
Epoch 880: New minimal relative error: 15.80%, model saved.
Epoch: 880 Train: 874.94391 Test: 37.08101
Epoch 960: New minimal relative error: 14.85%, model saved.
Epoch: 960 Train: 600.38013 Test: 17.48396
Epoch: 1040 Train: 512.08374 Test: 13.36221
Epoch: 1120 Train: 452.50809 Test: 20.43027
Epoch: 1200 Train: 428.29828 Test: 18.85954
Epoch: 1280 Train: 363.48386 Test: 14.94254
Epoch: 1360 Train: 324.25159 Test: 6.73499
Epoch 1440: New minimal relative error: 9.90%, model saved.
Epoch: 1440 Train: 287.60318 Test: 5.62597
Epoch: 1520 Train: 300.40070 Test: 18.49385
Epoch 1600: New minimal relative error: 8.18%, model saved.
Epoch: 1600 Train: 258.60324 Test: 5.23247
Epoch: 1680 Train: 258.05026 Test: 8.99199
Epoch: 1760 Train: 283.91852 Test: 7.44889
Epoch: 1840 Train: 258.25909 Test: 6.38154
Epoch: 1920 Train: 261.43240 Test: 15.27390
Epoch: 2000 Train: 208.48558 Test: 3.53372
Epoch 2080: New minimal relative error: 7.07%, model saved.
Epoch: 2080 Train: 199.78526 Test: 3.89976
Epoch: 2160 Train: 196.95416 Test: 8.07886
Epoch: 2240 Train: 194.01775 Test: 6.52214
Epoch: 2320 Train: 202.99045 Test: 4.92007
Epoch: 2400 Train: 183.95239 Test: 4.28937
Epoch: 2480 Train: 168.26987 Test: 3.36635
Epoch: 2560 Train: 171.14468 Test: 2.60620
Epoch 2640: New minimal relative error: 6.41%, model saved.
Epoch: 2640 Train: 175.79861 Test: 2.87410
Epoch 2720: New minimal relative error: 2.52%, model saved.
Epoch: 2720 Train: 161.46738 Test: 2.10941
Epoch: 2800 Train: 154.87372 Test: 1.98096
Epoch: 2880 Train: 142.78925 Test: 1.65809
Epoch: 2960 Train: 134.56744 Test: 2.14455
Epoch: 3040 Train: 141.21532 Test: 1.91358
Epoch: 3120 Train: 135.61705 Test: 2.20634
Epoch: 3200 Train: 137.51872 Test: 2.28277
Epoch: 3280 Train: 136.19101 Test: 2.83937
Epoch: 3360 Train: 152.01645 Test: 3.21822
Epoch: 3440 Train: 149.07935 Test: 7.79663
Epoch: 3520 Train: 128.71404 Test: 1.44676
Epoch: 3600 Train: 122.06760 Test: 1.15382
Epoch: 3680 Train: 120.08245 Test: 1.82981
Epoch: 3760 Train: 125.09703 Test: 2.06995
Epoch: 3840 Train: 116.51883 Test: 1.25964
Epoch: 3920 Train: 119.74677 Test: 2.63555
Epoch: 4000 Train: 120.02134 Test: 1.52687
Epoch: 4080 Train: 134.84242 Test: 2.19126
Epoch: 4160 Train: 124.89869 Test: 2.52220
Epoch: 4240 Train: 132.77466 Test: 1.75647
Epoch: 4320 Train: 121.63751 Test: 1.53355
Epoch: 4400 Train: 109.78680 Test: 1.18840
Epoch: 4480 Train: 109.58054 Test: 1.93469
Epoch: 4560 Train: 104.98402 Test: 1.06218
Epoch: 4640 Train: 105.11984 Test: 2.99451
Epoch: 4720 Train: 97.63975 Test: 2.09089
Epoch: 4800 Train: 95.08092 Test: 1.23979
Epoch: 4880 Train: 88.66091 Test: 0.84307
Epoch: 4960 Train: 83.88068 Test: 0.96515
Epoch: 5040 Train: 85.14085 Test: 0.73868
Epoch: 5120 Train: 78.93058 Test: 0.83264
Epoch: 5200 Train: 87.59377 Test: 2.61164
Epoch: 5280 Train: 78.81913 Test: 0.66888
Epoch: 5360 Train: 80.95499 Test: 0.72200
Epoch 5440: New minimal relative error: 2.48%, model saved.
Epoch: 5440 Train: 77.84252 Test: 0.74096
Epoch 5520: New minimal relative error: 1.40%, model saved.
Epoch: 5520 Train: 75.18900 Test: 0.56293
Epoch: 5600 Train: 72.35172 Test: 0.57692
Epoch: 5680 Train: 72.11288 Test: 0.49734
Epoch: 5760 Train: 72.58389 Test: 1.76715
Epoch: 5840 Train: 70.10587 Test: 0.53747
Epoch: 5920 Train: 68.80924 Test: 0.52141
Epoch: 6000 Train: 80.84763 Test: 5.33295
Epoch: 6080 Train: 69.20707 Test: 0.57109
Epoch: 6160 Train: 66.33884 Test: 0.50466
Epoch: 6240 Train: 64.93882 Test: 0.49314
Epoch: 6320 Train: 64.97449 Test: 0.45813
Epoch: 6400 Train: 63.21904 Test: 0.46787
Epoch: 6480 Train: 64.74090 Test: 0.58398
Epoch: 6560 Train: 63.90158 Test: 0.51790
Epoch: 6640 Train: 64.24320 Test: 0.48955
Epoch: 6720 Train: 62.44439 Test: 0.49340
Epoch: 6800 Train: 62.04123 Test: 0.58166
Epoch: 6880 Train: 61.35649 Test: 0.47423
Epoch: 6960 Train: 60.05077 Test: 0.43905
Epoch: 7040 Train: 59.15696 Test: 0.52871
Epoch 7120: New minimal relative error: 1.07%, model saved.
Epoch: 7120 Train: 57.85346 Test: 0.37756
Epoch: 7200 Train: 58.10455 Test: 0.35322
Epoch: 7280 Train: 57.19736 Test: 0.51425
Epoch: 7360 Train: 55.89464 Test: 0.31390
Epoch: 7440 Train: 54.65300 Test: 0.31581
Epoch: 7520 Train: 53.33994 Test: 0.29263
Epoch: 7600 Train: 52.78954 Test: 0.29961
Epoch: 7680 Train: 51.41163 Test: 0.33676
Epoch: 7760 Train: 50.59874 Test: 0.28901
Epoch: 7840 Train: 50.36423 Test: 0.26398
Epoch: 7920 Train: 50.82286 Test: 0.27623
Epoch: 7999 Train: 48.90368 Test: 0.36259
Training Loss: tensor(48.9037)
Test Loss: tensor(0.3626)
Learned LE: [  0.9020544   -0.04267702 -14.541592  ]
True LE: [ 8.8015002e-01 -4.0863231e-03 -1.4551449e+01]
Relative Error: [0.75539404 0.38443938 0.23743449 0.35240018 0.47948098 0.47419545
 0.38263845 0.5261608  0.5781503  0.5848085  0.69210386 0.9131686
 1.0244218  0.99830747 1.0022563  0.880233   0.6594465  0.71296453
 0.7654841  0.67937714 0.54237837 0.57322305 0.6050425  0.6799413
 0.6136573  0.43897298 0.2971101  0.2365112  0.49930996 0.86346316
 1.2748682  1.5923281  1.2690544  0.9091775  0.5386087  0.38190967
 0.84898865 0.91610885 0.985754   0.98321855 0.57845175 0.2521429
 0.8661516  1.3648108  1.4032537  1.129401   0.9300964  0.9163836
 0.8869782  0.96436214 0.99302053 0.96379447 0.83596843 0.793759
 1.03324    1.0047365  0.86559904 0.63819385 0.5966744  0.6461508
 0.6899057  0.8868238  1.1490391  0.70898587 0.2874974  0.18667315
 0.419658   0.5348575  0.48715317 0.59815824 0.6154003  0.58332765
 0.6710362  0.8458962  0.91621923 0.8353503  0.85420674 0.8061573
 0.6635055  0.70313203 0.82574415 0.8067227  0.75718594 0.7875881
 0.7804604  0.75882435 0.63031006 0.53442216 0.32042584 0.18667947
 0.32897037 0.6836829  1.0405066  1.4238626  1.721313   1.478307
 1.0298014  0.5399926  0.6490496  0.7657182  0.93274796 0.963239
 0.62641037 0.22486788 0.7694402  1.3094219  1.3683032  1.0731556
 0.8609373  0.87012714 0.87689626 1.0186797  1.0490575  0.96158075
 0.88208586 0.9014235  1.0080072  1.0345726  0.81751245 0.61041594
 0.437643   0.5035359  0.5292455  0.73458135 1.0252985  1.2776613
 0.77754986 0.35171697 0.25382438 0.50638777 0.59925395 0.6139049
 0.6721362  0.5647222  0.5981432  0.68696064 0.730815   0.7500769
 0.6844466  0.7496778  0.66614956 0.65613514 0.84114385 0.9448891
 0.8379984  0.7723711  0.72491133 0.5171489  0.4669493  0.54055905
 0.42657307 0.24007562 0.31328025 0.63817674 1.0117989  1.2953361
 1.6244441  1.800417   1.6479824  1.0351893  0.59374744 0.55845535
 0.7143665  0.7968205  0.61558557 0.35290238 0.48040077 1.1964961
 1.2380943  1.0315849  0.8344803  0.8647561  0.9115558  1.061496
 1.1152102  1.0515957  0.94801503 1.0583291  1.0828564  1.0215508
 0.87904817 0.6617621  0.45422208 0.26866287 0.27880365 0.39171347
 0.7784847  1.3036255  1.4562263  0.87762934 0.37840122 0.29627833
 0.52779263 0.525962   0.63701886 0.6502752  0.5262097  0.58128923
 0.63014555 0.6634268  0.6544795  0.7396352  0.75849766 0.6192701
 0.810522   0.82576585 0.7429878  0.7536906  0.7734192  0.37938192
 0.35177    0.37933108 0.37529257 0.25904483 0.4305654  0.69411385
 0.98241204 1.1341121  1.3607806  1.679281   1.8903013  1.7670212
 1.0879548  0.59619814 0.3461498  0.5701919  0.5775006  0.5195306
 0.28772396 0.9121723  1.1645598  0.93183637 0.84653693 0.8626077
 0.97932833 1.1794865  1.2953753  1.3114389  1.2800463  1.2504269
 1.289055   1.1790665  0.938828   0.75608796 0.5373786  0.29326436
 0.24554631 0.35489458 0.3381887  0.63123536 1.2032852  1.4044188
 1.0263969  0.44643202 0.36930498 0.48175728 0.5245623  0.647504
 0.5780231  0.5912343  0.61851114 0.518639   0.6321629  0.7286045
 0.82002795 0.8189077  0.7734722  0.83237815 0.7228846  0.5601552
 0.7769792  0.44312066 0.3200636  0.35635695 0.4008377  0.50968945
 0.6780133  0.57965803 0.70982784 0.95545584 0.95440817 1.0485028
 1.3332155  1.5895082  1.8577294  1.1555449  0.31884044 0.285619
 0.54259014 0.64996445 0.4280736  0.4997934  1.1964731  0.9365279
 0.9006137  0.85259414 0.96979785 1.1874324  1.4302894  1.6024435
 1.7313881  1.7148901  1.5106926  1.3542933  1.1682427  0.83958143
 0.5244782  0.28045756 0.29431435 0.30955684 0.25472406 0.2767807
 0.6160452  1.1016638  1.215665   1.1436089  0.5441334  0.38674077
 0.53768224 0.69100595 0.61684114 0.5201029  0.7160029  0.50998396
 0.40195867 0.7569837  0.80001724 0.865096   0.83698726 0.8149812
 0.9253856  0.68454164 0.51109564 0.47627655 0.4367051  0.3182626
 0.4153413  0.67262787 0.79222554 0.8006869  0.43478823 0.5807539
 0.57293904 0.4486891  0.6538591  1.0085496  1.2086508  1.521336
 1.2026348  0.5240551  0.45684716 0.6863293  0.64241534 0.30654758
 0.73524517 0.9779127  0.9165052  1.0602081  1.007573   1.1758015
 1.4629133  1.7776871  2.0870779  2.1403167  1.7611116  1.4874836
 1.2627386  1.010301   0.73919696 0.5060477  0.3954731  0.3227346
 0.28587463 0.22891966 0.27638435 0.51989913 0.9349781  1.0395594
 1.0944973  0.8532607  0.4647239  0.61425775 0.7130763  0.65791184
 0.48314807 0.508088   0.33239055 0.4070705  0.7466606  0.77084386
 0.886512   0.685431   0.8055631  0.90380704 0.6837635  0.48826462
 0.53964293 0.40847898 0.3152244  0.51577467]
