time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.70%, model saved.
Epoch: 0 Train: 10070.51758 Test: 4007.69238
Epoch: 100 Train: 2840.40649 Test: 1108.60754
Epoch: 200 Train: 2726.48242 Test: 1011.11560
Epoch 300: New minimal relative error: 79.98%, model saved.
Epoch: 300 Train: 1391.86768 Test: 375.09979
Epoch 400: New minimal relative error: 37.15%, model saved.
Epoch: 400 Train: 542.81348 Test: 122.42065
Epoch: 500 Train: 204.52914 Test: 24.85121
Epoch 600: New minimal relative error: 35.54%, model saved.
Epoch: 600 Train: 110.66601 Test: 11.13009
Epoch 700: New minimal relative error: 29.02%, model saved.
Epoch: 700 Train: 74.84210 Test: 5.22820
Epoch: 800 Train: 56.25895 Test: 4.02373
Epoch 900: New minimal relative error: 7.17%, model saved.
Epoch: 900 Train: 49.21742 Test: 3.18436
Epoch: 1000 Train: 85.60851 Test: 26.72555
Epoch: 1100 Train: 39.18776 Test: 2.05285
Epoch: 1200 Train: 38.08315 Test: 2.25612
Epoch: 1300 Train: 35.30967 Test: 3.46095
Epoch 1400: New minimal relative error: 6.99%, model saved.
Epoch: 1400 Train: 31.57776 Test: 1.86881
Epoch: 1500 Train: 56.21867 Test: 22.87160
Epoch: 1600 Train: 25.29236 Test: 3.54523
Epoch: 1700 Train: 27.78047 Test: 1.46603
Epoch: 1800 Train: 24.05565 Test: 2.72875
Epoch: 1900 Train: 21.48470 Test: 1.29383
Epoch: 2000 Train: 19.94425 Test: 0.72476
Epoch: 2100 Train: 20.28499 Test: 2.65824
Epoch 2200: New minimal relative error: 5.26%, model saved.
Epoch: 2200 Train: 17.73366 Test: 0.63601
Epoch: 2300 Train: 23.92824 Test: 4.91573
Epoch: 2400 Train: 17.00395 Test: 1.10707
Epoch: 2500 Train: 17.42125 Test: 0.93558
Epoch: 2600 Train: 20.22599 Test: 3.74547
Epoch: 2700 Train: 15.27172 Test: 0.89948
Epoch: 2800 Train: 15.34712 Test: 1.76004
Epoch: 2900 Train: 15.09338 Test: 1.58563
Epoch: 3000 Train: 14.81610 Test: 1.06846
Epoch: 3100 Train: 14.62911 Test: 2.13086
Epoch: 3200 Train: 21.12816 Test: 6.76020
Epoch: 3300 Train: 12.39076 Test: 0.50860
Epoch: 3400 Train: 16.29223 Test: 5.67323
Epoch: 3500 Train: 13.19464 Test: 1.64055
Epoch: 3600 Train: 12.00256 Test: 0.95626
Epoch: 3700 Train: 11.47939 Test: 0.80420
Epoch 3800: New minimal relative error: 3.99%, model saved.
Epoch: 3800 Train: 11.44397 Test: 0.53673
Epoch: 3900 Train: 11.47029 Test: 0.84783
Epoch: 4000 Train: 19.93875 Test: 7.78881
Epoch: 4100 Train: 14.18004 Test: 3.59036
Epoch: 4200 Train: 10.92449 Test: 0.64778
Epoch: 4300 Train: 12.12139 Test: 0.85182
Epoch: 4400 Train: 11.67184 Test: 1.76079
Epoch: 4500 Train: 10.19463 Test: 0.71920
Epoch 4600: New minimal relative error: 3.38%, model saved.
Epoch: 4600 Train: 9.61799 Test: 0.26682
Epoch: 4700 Train: 10.66747 Test: 0.89259
Epoch: 4800 Train: 11.05566 Test: 1.12021
Epoch: 4900 Train: 10.81671 Test: 1.14587
Epoch: 5000 Train: 11.68279 Test: 1.40233
Epoch: 5100 Train: 10.20419 Test: 0.72141
Epoch: 5200 Train: 10.03323 Test: 0.78403
Epoch: 5300 Train: 9.83691 Test: 0.62457
Epoch: 5400 Train: 10.18123 Test: 0.60873
Epoch: 5500 Train: 10.61162 Test: 1.18873
Epoch: 5600 Train: 10.52152 Test: 1.01639
Epoch: 5700 Train: 9.52542 Test: 0.50653
Epoch: 5800 Train: 10.81269 Test: 1.76706
Epoch: 5900 Train: 9.01245 Test: 0.29940
Epoch 6000: New minimal relative error: 1.73%, model saved.
Epoch: 6000 Train: 8.95352 Test: 0.28137
Epoch: 6100 Train: 9.97257 Test: 1.12200
Epoch: 6200 Train: 9.24786 Test: 0.52871
Epoch: 6300 Train: 10.03959 Test: 0.95454
Epoch: 6400 Train: 9.14594 Test: 0.70839
Epoch: 6500 Train: 9.02949 Test: 0.76249
Epoch: 6600 Train: 9.20905 Test: 0.80508
Epoch: 6700 Train: 8.43218 Test: 0.32840
Epoch: 6800 Train: 8.16100 Test: 0.23170
Epoch: 6900 Train: 7.96879 Test: 0.26436
Epoch: 7000 Train: 8.08349 Test: 0.44030
Epoch: 7100 Train: 8.02114 Test: 0.62448
Epoch: 7200 Train: 7.60449 Test: 0.26859
Epoch: 7300 Train: 7.63790 Test: 0.25949
Epoch: 7400 Train: 7.68527 Test: 0.18452
Epoch: 7500 Train: 8.14291 Test: 0.62314
Epoch: 7600 Train: 7.63028 Test: 0.42639
Epoch: 7700 Train: 7.56612 Test: 0.25929
Epoch: 7800 Train: 7.44208 Test: 0.32796
Epoch: 7900 Train: 7.04163 Test: 0.15824
Epoch: 8000 Train: 7.19997 Test: 0.20509
Epoch: 8100 Train: 7.15879 Test: 0.18427
Epoch: 8200 Train: 6.98063 Test: 0.22371
Epoch: 8300 Train: 6.76937 Test: 0.15339
Epoch: 8400 Train: 6.87897 Test: 0.15340
Epoch: 8500 Train: 7.77231 Test: 0.60313
Epoch: 8600 Train: 6.72266 Test: 0.13695
Epoch: 8700 Train: 6.75034 Test: 0.13958
Epoch: 8800 Train: 7.74303 Test: 0.61453
Epoch: 8900 Train: 6.76764 Test: 0.18949
Epoch: 9000 Train: 7.04850 Test: 0.39349
Epoch: 9100 Train: 6.70963 Test: 0.16992
Epoch: 9200 Train: 6.73129 Test: 0.18398
Epoch: 9300 Train: 7.46967 Test: 0.72503
Epoch: 9400 Train: 6.58945 Test: 0.11307
Epoch: 9500 Train: 6.62451 Test: 0.13212
Epoch: 9600 Train: 6.48101 Test: 0.10801
Epoch: 9700 Train: 6.48779 Test: 0.10584
Epoch: 9800 Train: 6.32324 Test: 0.10455
Epoch: 9900 Train: 6.26477 Test: 0.11773
Epoch: 9999 Train: 6.15992 Test: 0.11758
Training Loss: tensor(6.1599)
Test Loss: tensor(0.1176)
Learned LE: [ 8.3452827e-01  1.1018823e-02 -1.4500335e+01]
True LE: [ 8.4429783e-01  3.6374833e-03 -1.4521419e+01]
Relative Error: [3.9476645  3.398388   2.9283242  2.5457509  2.458948   2.5681372
 2.4549549  2.0995584  1.9320959  2.1332018  2.3749447  2.3969011
 2.2700746  2.0691752  2.2347035  2.5394974  2.8080986  3.0017374
 3.1229753  3.013608   2.8028126  2.8340888  2.808494   2.7485251
 2.5511568  2.4046884  2.2150264  2.0255818  2.0726411  2.1968317
 2.278841   2.3243499  2.086629   1.7444366  1.4783461  1.8224564
 2.365479   2.9661894  3.5664706  4.0522976  4.5865884  4.4821725
 4.3056583  3.9531286  3.512311   3.3315573  3.2711902  3.2625542
 3.1631951  2.9804413  2.792312   2.714705   2.6242225  2.443664
 2.2229378  2.1692097  2.1517313  2.4131916  3.0756047  3.567038
 3.9578145  3.8940985  3.5415177  3.0519958  2.557535   2.1583626
 2.0359237  2.1145086  2.2157562  1.8691884  1.9668194  2.0946972
 2.2667081  2.2339628  2.0825737  2.084109   2.244589   2.523807
 2.7693515  2.9008775  2.952817   2.8933043  2.586383   2.3054686
 2.2829957  2.1751566  1.9953411  1.8592355  1.7988229  1.7740889
 1.8255051  2.0681481  2.2623372  2.3786473  2.1688325  1.7110301
 1.2667683  1.5391948  2.2764137  2.9596486  3.530864   3.9834049
 4.3737144  4.1432424  3.9493482  3.7111804  3.2709699  3.0297616
 2.9841309  3.0004497  2.8889797  2.7287998  2.505417   2.3683658
 2.3530827  2.1374605  1.8754979  1.740448   1.7756803  1.9761467
 2.7024784  3.1933043  3.46164    3.5937178  3.1975303  2.8373392
 2.290978   1.8282825  1.7093921  1.7362863  1.7693284  1.9494236
 1.9964324  2.0483737  2.119566   2.0616372  1.8126624  1.9015366
 2.1923265  2.443895   2.6445334  2.757633   2.7838933  2.7531054
 2.4884696  2.0941067  1.6481194  1.5464483  1.44233    1.3886182
 1.4175268  1.4822862  1.5626779  1.976086   2.2920911  2.464594
 2.3039472  1.8600345  1.2445103  1.2306314  2.1581821  2.8937573
 3.3864205  3.8757782  4.1810055  3.8148336  3.578229   3.5211067
 3.0733144  2.8511305  2.7301424  2.736783   2.6469696  2.5474179
 2.3337848  2.122139   2.054425   1.7103255  1.6038232  1.3569707
 1.344973   1.4595723  2.122161   2.683463   2.829581   2.9425497
 2.7907882  2.5162215  2.1118896  1.5868489  1.399028   1.3675964
 1.2918649  1.7543135  1.9465168  1.9580649  1.9802611  1.9188087
 1.601902   1.6978284  2.0872931  2.3622975  2.6221993  2.7919962
 2.8571193  2.7857442  2.6074643  2.1830924  1.6987787  1.1997092
 1.0966591  1.0103551  1.0547721  1.1488464  1.3307132  1.7661756
 2.262461   2.5889626  2.4917243  2.0305834  1.4191065  1.0547129
 1.8229604  2.8014226  3.3275745  3.8490016  4.1031218  3.734629
 3.3727837  3.390269   3.0656626  2.7557905  2.6311183  2.5222497
 2.3643985  2.2779033  2.1740816  1.9734925  1.7800007  1.3429962
 1.2497917  1.1135926  0.9322157  0.9283154  1.448262   2.1115274
 2.305017   2.4112597  2.4358246  2.0744154  1.7796252  1.3862349
 1.0892977  1.0199287  0.92372215 1.3547981  1.8894718  1.8046901
 1.7579492  1.7932911  1.4095441  1.4979906  1.8954258  2.4314282
 2.7515917  2.8877175  2.8608174  2.7343738  2.5522707  2.2402189
 1.7700411  1.3995129  1.0318214  0.821024   0.6528333  0.7640283
 1.0022073  1.4160112  2.0473518  2.435049   2.4546046  2.1831627
 1.5372976  0.9669438  1.5432024  2.61153    3.3156662  3.7147229
 3.9356253  3.6944947  3.4136791  3.2733421  3.140564   2.7252219
 2.5109134  2.3207643  2.0568013  2.0154223  1.9821519  1.8264266
 1.6514009  1.2072926  0.958249   0.8664944  0.8004805  0.58225167
 0.7702458  1.5673574  1.8058101  1.9431393  1.9808972  1.7974052
 1.3548478  0.98244864 0.7425824  0.6611932  0.6193367  1.0214409
 1.4859902  1.7741117  1.6234761  1.5538886  1.1881992  1.2596093
 1.7862195  2.4510252  2.8304396  2.931838   2.8669727  2.6684415
 2.4248223  2.1505075  1.8881016  1.5467582  1.2513521  0.78888667
 0.63170034 0.6025578  0.7159435  1.0722871  1.6761774  2.1864266
 2.3193388  2.1950574  1.85543    1.1185119  0.99531955 1.9684993
 3.209506   3.623745   3.7091386  3.6555429  3.4455836  3.1967952
 3.1536374  2.8347135  2.5466564  2.3566558  1.9706136  1.75821
 1.7435302  1.7204248  1.6277946  1.2196496  0.7824818  0.76046
 0.82885474 0.60655296 0.3204223  0.99885714 1.4062591  1.5483335
 1.5912153  1.4522822  1.1411399  0.6268687  0.4187507  0.28113958
 0.22841184 0.6124921  1.0658572  1.4297409  1.5521607  1.3560491
 1.088898   0.8213832  1.5770379  2.2881668  2.7416625  2.9653678
 2.857133   2.5971706  2.2618454  2.044693   1.8231446  1.568131
 1.3584164  1.1076714  0.7430774  0.7202518 ]
