time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.76%, model saved.
Epoch: 0 Train: 4257.02930 Test: 4279.58252
Epoch: 100 Train: 182.01556 Test: 199.77617
Epoch 200: New minimal relative error: 51.40%, model saved.
Epoch: 200 Train: 41.75901 Test: 46.84062
Epoch 300: New minimal relative error: 47.74%, model saved.
Epoch: 300 Train: 18.93967 Test: 24.51578
Epoch 400: New minimal relative error: 39.46%, model saved.
Epoch: 400 Train: 20.61777 Test: 20.95824
Epoch 500: New minimal relative error: 36.59%, model saved.
Epoch: 500 Train: 5.95458 Test: 11.35574
Epoch: 600 Train: 20.26061 Test: 15.61957
Epoch: 700 Train: 6.21231 Test: 9.13941
Epoch 800: New minimal relative error: 30.87%, model saved.
Epoch: 800 Train: 4.08594 Test: 8.13979
Epoch 900: New minimal relative error: 23.07%, model saved.
Epoch: 900 Train: 13.70343 Test: 13.62583
Epoch: 1000 Train: 2.70914 Test: 5.84213
Epoch: 1100 Train: 7.59207 Test: 11.44162
Epoch: 1200 Train: 3.72159 Test: 5.98122
Epoch: 1300 Train: 2.27323 Test: 4.96753
Epoch: 1400 Train: 2.19458 Test: 4.98914
Epoch: 1500 Train: 1.72152 Test: 3.93415
Epoch 1600: New minimal relative error: 20.75%, model saved.
Epoch: 1600 Train: 1.46914 Test: 4.17442
Epoch: 1700 Train: 6.65173 Test: 10.96827
Epoch: 1800 Train: 0.87886 Test: 3.03148
Epoch 1900: New minimal relative error: 19.45%, model saved.
Epoch: 1900 Train: 4.29920 Test: 8.65420
Epoch: 2000 Train: 0.89074 Test: 2.99915
Epoch 2100: New minimal relative error: 16.72%, model saved.
Epoch: 2100 Train: 0.78480 Test: 3.00380
Epoch: 2200 Train: 0.73354 Test: 2.67729
Epoch: 2300 Train: 3.94132 Test: 7.15189
Epoch 2400: New minimal relative error: 13.46%, model saved.
Epoch: 2400 Train: 0.74489 Test: 2.42973
Epoch: 2500 Train: 1.11303 Test: 2.64820
Epoch: 2600 Train: 0.97783 Test: 2.79811
Epoch: 2700 Train: 2.54654 Test: 3.99503
Epoch: 2800 Train: 0.91101 Test: 2.51897
Epoch: 2900 Train: 2.52425 Test: 4.21751
Epoch: 3000 Train: 0.80116 Test: 2.97458
Epoch: 3100 Train: 0.51198 Test: 2.06819
Epoch: 3200 Train: 0.71155 Test: 2.35386
Epoch: 3300 Train: 0.66462 Test: 2.15398
Epoch: 3400 Train: 0.58711 Test: 1.93556
Epoch: 3500 Train: 0.46754 Test: 1.91930
Epoch: 3600 Train: 7.51227 Test: 8.92848
Epoch: 3700 Train: 0.46998 Test: 1.96375
Epoch: 3800 Train: 0.35623 Test: 1.76746
Epoch: 3900 Train: 0.36403 Test: 1.70252
Epoch: 4000 Train: 0.39013 Test: 1.76111
Epoch: 4100 Train: 9.60441 Test: 10.28152
Epoch 4200: New minimal relative error: 13.45%, model saved.
Epoch: 4200 Train: 0.95712 Test: 2.49102
Epoch: 4300 Train: 1.55102 Test: 2.95915
Epoch 4400: New minimal relative error: 12.57%, model saved.
Epoch: 4400 Train: 0.41333 Test: 1.81092
Epoch: 4500 Train: 0.42248 Test: 1.76681
Epoch 4600: New minimal relative error: 12.07%, model saved.
Epoch: 4600 Train: 0.30298 Test: 1.48792
Epoch: 4700 Train: 0.28120 Test: 1.45775
Epoch: 4800 Train: 2.45303 Test: 3.05856
Epoch: 4900 Train: 1.47136 Test: 2.22152
Epoch: 5000 Train: 0.80588 Test: 2.28941
Epoch: 5100 Train: 0.28461 Test: 1.39416
Epoch: 5200 Train: 0.34392 Test: 1.48558
Epoch: 5300 Train: 4.75606 Test: 5.34656
Epoch 5400: New minimal relative error: 10.40%, model saved.
Epoch: 5400 Train: 0.95619 Test: 1.65338
Epoch: 5500 Train: 1.37716 Test: 2.86886
Epoch: 5600 Train: 4.08799 Test: 3.87530
Epoch 5700: New minimal relative error: 8.72%, model saved.
Epoch: 5700 Train: 0.28734 Test: 1.38254
Epoch: 5800 Train: 1.10870 Test: 2.26128
Epoch: 5900 Train: 0.21333 Test: 1.17671
Epoch: 6000 Train: 1.55670 Test: 2.44109
Epoch: 6100 Train: 0.36173 Test: 1.36706
Epoch: 6200 Train: 0.37856 Test: 1.24156
Epoch: 6300 Train: 0.38397 Test: 1.25301
Epoch: 6400 Train: 0.54712 Test: 1.46898
Epoch: 6500 Train: 0.18659 Test: 1.12513
Epoch: 6600 Train: 0.18595 Test: 1.13672
Epoch: 6700 Train: 0.17920 Test: 1.06538
Epoch: 6800 Train: 0.18183 Test: 1.14256
Epoch: 6900 Train: 0.18737 Test: 1.07661
Epoch: 7000 Train: 0.17238 Test: 1.05380
Epoch: 7100 Train: 0.17102 Test: 1.05872
Epoch: 7200 Train: 0.18981 Test: 1.17677
Epoch: 7300 Train: 0.20839 Test: 1.05471
Epoch: 7400 Train: 1.29074 Test: 1.89649
Epoch: 7500 Train: 0.15905 Test: 1.00200
Epoch: 7600 Train: 0.29143 Test: 1.20089
Epoch: 7700 Train: 0.15824 Test: 0.99595
Epoch: 7800 Train: 0.15287 Test: 0.97800
Epoch: 7900 Train: 0.41116 Test: 1.24589
Epoch: 8000 Train: 0.14905 Test: 0.95269
Epoch: 8100 Train: 0.92690 Test: 1.51436
Epoch: 8200 Train: 1.27947 Test: 2.08694
Epoch: 8300 Train: 0.14439 Test: 0.93311
Epoch: 8400 Train: 0.87490 Test: 1.37543
Epoch: 8500 Train: 0.16425 Test: 0.95514
Epoch: 8600 Train: 0.13809 Test: 0.90644
Epoch: 8700 Train: 0.13810 Test: 0.90804
Epoch: 8800 Train: 0.13481 Test: 0.89782
Epoch: 8900 Train: 0.13399 Test: 0.87970
Epoch: 9000 Train: 0.13221 Test: 0.88224
Epoch: 9100 Train: 0.17655 Test: 0.89860
Epoch: 9200 Train: 0.54785 Test: 1.30483
Epoch: 9300 Train: 0.12691 Test: 0.85379
Epoch: 9400 Train: 0.57220 Test: 1.61485
Epoch: 9500 Train: 0.12418 Test: 0.83756
Epoch: 9600 Train: 0.12589 Test: 0.85375
Epoch: 9700 Train: 0.12168 Test: 0.82617
Epoch: 9800 Train: 0.99102 Test: 1.82394
Epoch: 9900 Train: 0.11931 Test: 0.81581
Epoch: 9999 Train: 0.11949 Test: 0.82573
Training Loss: tensor(0.1195)
Test Loss: tensor(0.8257)
Learned LE: [ 0.84200555 -0.08312654 -2.6025677 ]
True LE: [ 8.4666973e-01 -3.5190729e-03 -1.4530328e+01]
Relative Error: [12.680314  12.7372875 12.857459  13.032873  13.253345  13.461462
 13.711041  13.860978  14.442905  14.910802  15.256765  15.495232
 15.518091  15.245475  14.639713  13.9679165 12.805994  11.445063
  9.8420725  8.507214   7.9054585  8.199371   9.125245  10.296926
 11.90655   13.388883  14.604809  15.529609  16.178637  16.431696
 16.680841  16.77363   17.181929  17.412872  17.60955   17.877125
 17.681879  17.33907   16.984705  16.633139  16.27123   15.966476
 15.823214  16.571217  17.85305   19.115894  19.986162  20.571548
 19.72802   18.469318  17.094978  15.875008  14.849859  14.425637
 14.172283  13.623259  12.9151125 12.421958  11.976589  11.751942
 11.656098  11.656099  11.716857  11.851708  12.106503  12.425111
 12.752284  13.110921  13.400139  13.529881  13.892346  14.428374
 14.8315    15.061094  15.125847  14.804794  14.2024355 13.379884
 12.346879  10.937159   9.203554   7.7488346  7.1381993  7.6566086
  8.864649  10.171273  11.589207  13.168383  14.428883  15.123309
 15.498742  15.55423   15.711793  15.794368  16.28737   16.703579
 17.086967  17.22897   17.297354  16.889206  16.371433  15.947543
 15.534967  15.143706  14.805494  15.209248  16.33865   17.654547
 18.552122  19.238495  18.275988  17.058441  15.700349  14.490466
 13.507231  13.175848  12.974912  12.326563  11.682106  11.108826
 10.814634  10.677752  10.632091  10.681831  10.829871  11.083679
 11.45696   11.90947   12.412068  12.847456  13.136628  13.308698
 13.4636965 14.020742  14.474972  14.756125  14.791809  14.574617
 13.949235  13.037059  12.033634  10.630567   8.785201   7.146725
  6.4073405  7.0190163  8.491252   9.984352  11.47048   12.857646
 13.772397  14.56643   15.028876  15.135103  15.136252  15.166511
 15.423325  15.776051  16.27017   16.42405   16.188475  15.961506
 15.521617  14.99666   14.663095  14.471595  14.072765  13.945985
 14.796221  16.044777  17.053347  17.826181  16.907228  15.778518
 14.480516  13.199346  12.245776  11.978024  11.7431965 11.12885
 10.430076   9.936239   9.746183   9.66989    9.6211815  9.726935
  9.959439  10.285847  10.71206   11.293277  11.90621   12.431129
 12.9178505 13.251533  13.341589  13.815403  14.225915  14.529252
 14.655243  14.474349  13.910307  12.989789  11.78269   10.474998
  8.6178665  6.7312016  5.653251   6.21574    7.9014378  9.638629
 11.287286  12.320544  13.253766  14.081481  14.678786  14.912168
 14.838687  14.82746   15.057926  15.205935  15.108406  15.087168
 15.113672  14.974959  14.748338  14.255171  13.64111   13.280582
 13.110905  12.935593  13.423316  14.499958  15.387519  16.264225
 15.582758  14.580846  13.375659  12.0736065 11.09852   10.810969
 10.564192  10.001196   9.281909   8.873961   8.737651   8.62509
  8.593053   8.797278   9.008635   9.361567   9.898568  10.569533
 11.115458  11.673643  12.158539  12.619201  13.067344  13.505711
 14.038701  14.290012  14.462405  14.518246  14.084968  13.227652
 11.97422   10.548631   8.793101   6.5831094  5.026755   5.238307
  6.941767   9.011724  10.921656  11.993151  12.857467  13.623931
 14.306733  14.630723  14.574825  14.452442  14.640421  14.682434
 14.402021  14.041726  13.859306  13.92565   13.888269  13.515427
 12.903041  12.240719  11.862681  11.729252  11.99829   12.839748
 13.831115  14.59952   14.326503  13.450709  12.387228  11.112919
 10.020323   9.633546   9.4630165  8.929888   8.227972   7.8596616
  7.7609406  7.5623693  7.614199   7.7762103  8.025609   8.385603
  8.914146   9.625364  10.261876  10.841002  11.412448  11.911116
 12.36643   12.784754  13.413863  13.904863  14.046457  14.097431
 14.080785  13.525423  12.494907  10.900356   9.299693   6.881003
  4.850851   4.190756   5.6073527  7.974581  10.116773  11.527957
 12.603932  13.163582  13.913575  14.2428875 14.159382  14.153725
 14.561239  14.3751545 14.058499  13.556838  13.07851   12.810996
 12.708057  12.687357  12.265835  11.570679  10.828289  10.432314
 10.331597  10.998537  12.107827  13.003927  13.110894  12.300905
 11.42076   10.272243   9.151673   8.436016   8.338593   7.925102
  7.2609     6.9196963  6.7521787  6.5374894  6.5760403  6.729929
  6.931836   7.3764143  8.055909   8.8231     9.435135  10.041709
 10.488704  10.84844   11.255051  11.639167  11.96462   12.6590805
 13.185806  13.4372015 13.412248  13.43659   12.839373  11.5235615
  9.852186   7.8568897  5.2641263  3.6501532  4.180175   6.471566
  8.965474  10.824962  12.27247   12.938641 ]
