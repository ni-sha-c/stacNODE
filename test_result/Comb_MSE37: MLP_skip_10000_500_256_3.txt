time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.33%, model saved.
Epoch: 0 Train: 3914.09985 Test: 3731.85400
Epoch 100: New minimal relative error: 69.84%, model saved.
Epoch: 100 Train: 171.61925 Test: 179.99744
Epoch 200: New minimal relative error: 20.53%, model saved.
Epoch: 200 Train: 23.67014 Test: 29.03657
Epoch: 300 Train: 11.90006 Test: 15.40796
Epoch 400: New minimal relative error: 15.89%, model saved.
Epoch: 400 Train: 7.75681 Test: 10.39312
Epoch: 500 Train: 5.83732 Test: 7.98232
Epoch: 600 Train: 4.55645 Test: 6.41079
Epoch: 700 Train: 4.35317 Test: 6.34202
Epoch: 800 Train: 3.50811 Test: 4.92006
Epoch 900: New minimal relative error: 14.12%, model saved.
Epoch: 900 Train: 3.16054 Test: 4.50354
Epoch: 1000 Train: 2.73156 Test: 3.98172
Epoch 1100: New minimal relative error: 9.15%, model saved.
Epoch: 1100 Train: 2.46991 Test: 3.62791
Epoch: 1200 Train: 2.33892 Test: 3.42199
Epoch: 1300 Train: 2.83102 Test: 4.38595
Epoch: 1400 Train: 2.00417 Test: 2.82963
Epoch: 1500 Train: 1.91888 Test: 2.74873
Epoch: 1600 Train: 1.54089 Test: 2.32927
Epoch: 1700 Train: 2.01868 Test: 2.43725
Epoch: 1800 Train: 1.29419 Test: 1.96996
Epoch: 1900 Train: 1.60334 Test: 2.07152
Epoch: 2000 Train: 0.95531 Test: 1.54731
Epoch: 2100 Train: 1.01988 Test: 1.53907
Epoch: 2200 Train: 1.08384 Test: 1.63019
Epoch: 2300 Train: 0.86770 Test: 1.31877
Epoch 2400: New minimal relative error: 7.77%, model saved.
Epoch: 2400 Train: 0.70385 Test: 1.15894
Epoch: 2500 Train: 0.69509 Test: 1.11110
Epoch: 2600 Train: 0.59646 Test: 1.01336
Epoch: 2700 Train: 0.70643 Test: 1.11202
Epoch: 2800 Train: 0.68668 Test: 1.03560
Epoch: 2900 Train: 0.49516 Test: 0.84664
Epoch: 3000 Train: 0.50559 Test: 0.85379
Epoch: 3100 Train: 1.02917 Test: 1.26154
Epoch: 3200 Train: 0.68842 Test: 0.75185
Epoch: 3300 Train: 0.39630 Test: 0.69265
Epoch: 3400 Train: 0.54984 Test: 0.86189
Epoch: 3500 Train: 0.53688 Test: 0.74962
Epoch: 3600 Train: 0.42437 Test: 0.68631
Epoch: 3700 Train: 0.39007 Test: 0.64537
Epoch: 3800 Train: 1.13556 Test: 1.54016
Epoch: 3900 Train: 0.31075 Test: 0.55015
Epoch: 4000 Train: 0.30148 Test: 0.53538
Epoch: 4100 Train: 0.36934 Test: 0.55058
Epoch: 4200 Train: 0.36883 Test: 0.59535
Epoch: 4300 Train: 0.30850 Test: 0.52581
Epoch: 4400 Train: 0.30178 Test: 0.50423
Epoch: 4500 Train: 0.26184 Test: 0.46588
Epoch: 4600 Train: 0.30712 Test: 0.50785
Epoch: 4700 Train: 0.26286 Test: 0.47850
Epoch: 4800 Train: 0.45155 Test: 0.63010
Epoch 4900: New minimal relative error: 6.60%, model saved.
Epoch: 4900 Train: 0.40119 Test: 0.57659
Epoch: 5000 Train: 0.23058 Test: 0.41165
Epoch: 5100 Train: 0.23777 Test: 0.40759
Epoch: 5200 Train: 0.25572 Test: 0.42696
Epoch: 5300 Train: 0.25716 Test: 0.43072
Epoch: 5400 Train: 0.21731 Test: 0.38342
Epoch 5500: New minimal relative error: 5.09%, model saved.
Epoch: 5500 Train: 0.21419 Test: 0.37614
Epoch: 5600 Train: 0.21231 Test: 0.37117
Epoch: 5700 Train: 0.20069 Test: 0.35734
Epoch: 5800 Train: 0.19132 Test: 0.34322
Epoch: 5900 Train: 0.18712 Test: 0.33726
Epoch: 6000 Train: 0.18193 Test: 0.32914
Epoch: 6100 Train: 0.20691 Test: 0.35858
Epoch: 6200 Train: 0.41544 Test: 0.57813
Epoch: 6300 Train: 0.36051 Test: 0.51238
Epoch: 6400 Train: 0.18029 Test: 0.32140
Epoch: 6500 Train: 0.17635 Test: 0.31155
Epoch: 6600 Train: 0.17266 Test: 0.30563
Epoch: 6700 Train: 0.18734 Test: 0.32386
Epoch: 6800 Train: 0.16056 Test: 0.29625
Epoch: 6900 Train: 0.18962 Test: 0.31282
Epoch: 7000 Train: 0.16865 Test: 0.29447
Epoch: 7100 Train: 0.15248 Test: 0.27526
Epoch: 7200 Train: 0.14986 Test: 0.27052
Epoch: 7300 Train: 0.15752 Test: 0.28000
Epoch: 7400 Train: 0.24501 Test: 0.37121
Epoch: 7500 Train: 0.14370 Test: 0.25924
Epoch: 7600 Train: 0.14227 Test: 0.25641
Epoch 7700: New minimal relative error: 5.08%, model saved.
Epoch: 7700 Train: 0.15122 Test: 0.25683
Epoch: 7800 Train: 0.13968 Test: 0.24948
Epoch: 7900 Train: 0.18472 Test: 0.29335
Epoch: 8000 Train: 0.25261 Test: 0.33754
Epoch 8100: New minimal relative error: 4.61%, model saved.
Epoch: 8100 Train: 0.32561 Test: 0.41340
Epoch: 8200 Train: 0.20062 Test: 0.29171
Epoch: 8300 Train: 0.14034 Test: 0.24039
Epoch: 8400 Train: 0.14540 Test: 0.24364
Epoch: 8500 Train: 0.14634 Test: 0.25537
Epoch: 8600 Train: 0.12479 Test: 0.22495
Epoch: 8700 Train: 0.12392 Test: 0.22272
Epoch: 8800 Train: 0.15796 Test: 0.27132
Epoch: 8900 Train: 0.12060 Test: 0.21751
Epoch: 9000 Train: 0.11945 Test: 0.21522
Epoch: 9100 Train: 0.11795 Test: 0.21292
Epoch: 9200 Train: 0.11751 Test: 0.21120
Epoch: 9300 Train: 0.21396 Test: 0.30647
Epoch: 9400 Train: 0.11428 Test: 0.20607
Epoch: 9500 Train: 0.11301 Test: 0.20385
Epoch: 9600 Train: 0.11987 Test: 0.21229
Epoch: 9700 Train: 0.11394 Test: 0.20411
Epoch: 9800 Train: 0.11172 Test: 0.20031
Epoch: 9900 Train: 0.10861 Test: 0.19579
Epoch: 9999 Train: 0.10781 Test: 0.19418
Training Loss: tensor(0.1078)
Test Loss: tensor(0.1942)
Learned LE: [ 0.81427264  0.04680312 -4.74088   ]
True LE: [ 8.7861407e-01  6.9035240e-03 -1.4562479e+01]
Relative Error: [4.716334   5.0599613  5.1207247  4.0730376  3.3658154  2.705708
 2.0550127  1.7690763  1.3897104  1.2485037  1.4338799  1.543412
 1.52627    1.383082   1.2910521  1.3487886  1.8787357  2.6541142
 3.6298885  4.58279    4.8834014  3.952298   3.611519   3.9102335
 4.550665   5.342556   5.605471   5.020137   4.4560304  4.031976
 3.6652565  3.2592502  2.903474   2.539418   2.4043357  2.2043931
 1.9662347  2.0335138  1.8544915  1.3912897  1.1737695  1.3340294
 1.7318374  1.8634684  1.8681122  1.9052055  1.9558308  2.0054812
 2.0461035  1.908859   1.7160792  1.6261908  1.5755637  1.4998891
 1.397137   1.2778952  1.2965447  1.6247295  2.326458   2.9390492
 2.9895716  3.2846422  4.0076284  4.5183716  4.393127   3.5066218
 3.0293717  2.5654082  2.0910556  1.8111033  1.4156785  1.2389565
 1.3336661  1.4237797  1.4001166  1.2451932  0.96873015 0.72201055
 0.8361511  1.3609334  2.2633011  3.1305466  4.2770004  3.4946268
 2.7726254  2.9717696  3.6146483  4.428615   5.0210414  4.58333
 4.1146646  3.8021672  3.5402708  3.2070808  2.959515   2.697854
 2.2346375  2.1688373  2.0420637  2.0700107  1.9306061  1.5885296
 1.17691    1.2268469  1.5318489  1.6173238  1.5532149  1.6488248
 1.8476436  1.9858075  2.0353627  2.0479681  1.9528309  1.8807119
 1.8121636  1.6962799  1.540168   1.3233589  1.1918325  1.4725415
 1.9833164  2.4949126  2.485018   2.554744   3.2913115  3.8991876
 3.7221735  3.1432595  2.7704422  2.4982667  2.2021093  1.7878536
 1.479452   1.2986197  1.3392875  1.4500246  1.482405   1.3979326
 1.1618725  0.8220989  0.6734069  0.83277535 1.2761133  1.796282
 2.6263366  3.084062   2.2309408  2.0109205  2.5704722  3.419248
 4.2959514  4.1084914  3.718486   3.5066657  3.3664725  3.1196265
 2.9924712  2.88282    2.2008228  2.1230404  2.1314294  2.1229885
 1.9909358  1.5846686  1.1626886  1.1221012  1.3629738  1.4200245
 1.270704   1.3746749  1.6415905  1.9315345  2.1781087  2.2727518
 2.298136   2.115159   1.9698318  1.7987462  1.606803   1.3614072
 1.1183034  1.3108569  1.840874   2.1948211  1.9552579  2.0491195
 2.4687767  3.1854253  3.1043031  2.795115   2.527235   2.442953
 2.2216916  1.7984986  1.5581628  1.3582468  1.4004675  1.5465324
 1.618291   1.5462494  1.3423507  1.0546997  0.7842867  0.67169803
 0.8536597  1.1108446  1.4625794  2.2974963  1.7400099  1.2628416
 1.625524   2.5059288  3.4686418  3.5957549  3.2533147  3.1199992
 3.1169255  2.997443   2.9621673  2.8535788  2.2659805  2.0531158
 2.1688967  2.0834596  1.8633971  1.6062893  1.250389   1.0516958
 1.2254386  1.2152268  1.0829521  1.1785318  1.4690112  1.7450203
 2.020307   2.3162432  2.4628084  2.388432   2.1334465  1.9330882
 1.6389364  1.3476859  1.1301196  1.1688613  1.7610701  2.108305
 1.8085227  1.6940413  1.7968446  2.357002   2.4688787  2.3913107
 2.2376325  2.340834   2.1834128  1.8103063  1.6240003  1.4306244
 1.4635863  1.609279   1.6991416  1.7146071  1.6043864  1.3397481
 1.1051608  0.70835406 0.5086098  0.62874097 0.7455908  1.2009871
 1.7327979  1.1268438  1.268806   1.8011487  2.6490898  3.3339791
 2.9774613  2.621847   2.7203138  2.7661362  2.7835379  2.73377
 2.3548293  2.1427193  2.169206   1.9968003  1.7760165  1.5897632
 1.2726198  0.8491279  1.1557943  1.2073929  1.172798   1.0640815
 1.2235674  1.4444265  1.8229223  2.1437764  2.3525312  2.5744562
 2.5035653  2.1104867  1.7784946  1.5168931  1.2199928  1.1712848
 1.4797937  2.2347999  1.7699016  1.7589176  1.6616642  1.6111997
 1.8228438  1.8422414  1.9203117  2.1167045  2.1096942  1.8374403
 1.6638908  1.5209209  1.4931489  1.6406987  1.8117822  1.9813643
 1.981586   1.741096   1.5477422  1.1616623  0.59015846 0.19290753
 0.19506325 0.52264243 1.3899784  1.759735   1.1205317  1.3877347
 1.9621851  2.807086   2.8901799  2.4966714  2.3106449  2.3637333
 2.4857414  2.5753112  2.3696597  2.1834404  2.2525418  2.032091
 1.7634143  1.6578615  1.3535042  0.7693709  1.0037838  1.3682097
 1.3795947  1.2309844  1.1071453  1.1943394  1.5309478  1.8495817
 2.18827    2.438845   2.587586   2.5145826  2.0509636  1.6652482
 1.3625187  1.1739823  1.3634429  1.9469522  1.9253429  1.5728669
 1.6809393  1.6899718  1.4431926  1.2378367  1.422781   1.7129304
 1.9317806  1.7998427  1.6595553  1.6442318  1.675956   1.7019829
 1.7642345  1.8912101  2.0076127  1.9295273  1.8171566  1.702605
 1.2193186  0.5067807  0.09800508 0.2367542  0.68392986 1.5709909
 1.7272912  1.1835707  1.4385238  1.9617376 ]
