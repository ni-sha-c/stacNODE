time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.19%, model saved.
Epoch: 0 Train: 9649.93066 Test: 4237.86328
Epoch: 100 Train: 2751.21191 Test: 1213.51318
Epoch: 200 Train: 2712.00928 Test: 1173.70374
Epoch: 300 Train: 2056.63794 Test: 972.90863
Epoch: 400 Train: 1279.18860 Test: 223.80693
Epoch 500: New minimal relative error: 100.13%, model saved.
Epoch: 500 Train: 333.27158 Test: 68.54773
Epoch 600: New minimal relative error: 41.77%, model saved.
Epoch: 600 Train: 233.66237 Test: 120.52902
Epoch: 700 Train: 194.29024 Test: 59.80281
Epoch 800: New minimal relative error: 39.53%, model saved.
Epoch: 800 Train: 117.68021 Test: 9.18851
Epoch 900: New minimal relative error: 25.46%, model saved.
Epoch: 900 Train: 100.40658 Test: 17.55415
Epoch 1000: New minimal relative error: 16.74%, model saved.
Epoch: 1000 Train: 86.54286 Test: 8.68960
Epoch: 1100 Train: 68.28465 Test: 10.65275
Epoch 1200: New minimal relative error: 10.85%, model saved.
Epoch: 1200 Train: 77.22973 Test: 13.36558
Epoch: 1300 Train: 56.78715 Test: 12.85965
Epoch 1400: New minimal relative error: 8.16%, model saved.
Epoch: 1400 Train: 52.00697 Test: 5.03342
Epoch: 1500 Train: 41.54063 Test: 5.34435
Epoch 1600: New minimal relative error: 6.55%, model saved.
Epoch: 1600 Train: 37.71153 Test: 2.90651
Epoch: 1700 Train: 36.28888 Test: 1.79308
Epoch: 1800 Train: 34.08223 Test: 3.77749
Epoch: 1900 Train: 28.72224 Test: 1.30727
Epoch: 2000 Train: 35.61039 Test: 5.78586
Epoch 2100: New minimal relative error: 6.35%, model saved.
Epoch: 2100 Train: 26.59263 Test: 1.06225
Epoch: 2200 Train: 33.08459 Test: 7.04252
Epoch: 2300 Train: 30.94981 Test: 6.02194
Epoch 2400: New minimal relative error: 3.69%, model saved.
Epoch: 2400 Train: 25.22121 Test: 1.12708
Epoch: 2500 Train: 36.97396 Test: 8.83638
Epoch: 2600 Train: 28.57899 Test: 5.00836
Epoch: 2700 Train: 24.43043 Test: 1.25169
Epoch: 2800 Train: 22.95893 Test: 0.87808
Epoch: 2900 Train: 23.63247 Test: 2.73068
Epoch: 3000 Train: 27.13071 Test: 7.60597
Epoch: 3100 Train: 19.57089 Test: 0.83758
Epoch: 3200 Train: 21.43019 Test: 0.81591
Epoch: 3300 Train: 20.76217 Test: 3.01222
Epoch: 3400 Train: 32.74167 Test: 11.29867
Epoch: 3500 Train: 22.90596 Test: 2.59280
Epoch: 3600 Train: 19.46401 Test: 1.58240
Epoch: 3700 Train: 25.49343 Test: 5.37996
Epoch: 3800 Train: 25.85156 Test: 5.23108
Epoch: 3900 Train: 19.87337 Test: 2.82897
Epoch: 4000 Train: 17.26380 Test: 0.90057
Epoch: 4100 Train: 18.17150 Test: 1.08094
Epoch: 4200 Train: 21.80459 Test: 3.17104
Epoch: 4300 Train: 16.84562 Test: 2.31858
Epoch: 4400 Train: 15.77281 Test: 0.49793
Epoch: 4500 Train: 19.56235 Test: 2.91541
Epoch: 4600 Train: 16.51866 Test: 2.15519
Epoch: 4700 Train: 16.97083 Test: 2.39343
Epoch: 4800 Train: 23.81648 Test: 6.90494
Epoch: 4900 Train: 14.39647 Test: 0.90723
Epoch: 5000 Train: 14.91049 Test: 0.59574
Epoch: 5100 Train: 14.31874 Test: 0.40067
Epoch: 5200 Train: 15.17731 Test: 2.11150
Epoch: 5300 Train: 17.49794 Test: 1.63240
Epoch: 5400 Train: 17.48618 Test: 1.41900
Epoch: 5500 Train: 13.93621 Test: 0.46683
Epoch: 5600 Train: 14.20084 Test: 0.62265
Epoch: 5700 Train: 15.25803 Test: 2.89998
Epoch: 5800 Train: 12.89925 Test: 1.00784
Epoch: 5900 Train: 12.70687 Test: 0.52759
Epoch: 6000 Train: 13.77452 Test: 1.03751
Epoch: 6100 Train: 13.41654 Test: 2.55249
Epoch: 6200 Train: 11.86876 Test: 0.64441
Epoch: 6300 Train: 11.42090 Test: 0.62675
Epoch: 6400 Train: 12.01821 Test: 0.72820
Epoch: 6500 Train: 11.64998 Test: 1.29054
Epoch: 6600 Train: 10.63021 Test: 0.45036
Epoch: 6700 Train: 10.79047 Test: 0.64903
Epoch: 6800 Train: 12.07522 Test: 1.92456
Epoch: 6900 Train: 10.72964 Test: 0.44286
Epoch 7000: New minimal relative error: 1.93%, model saved.
Epoch: 7000 Train: 10.65939 Test: 0.29038
Epoch: 7100 Train: 10.92777 Test: 0.37409
Epoch: 7200 Train: 11.90444 Test: 1.96375
Epoch: 7300 Train: 10.58201 Test: 0.32869
Epoch: 7400 Train: 14.37046 Test: 3.94285
Epoch: 7500 Train: 11.02518 Test: 0.57411
Epoch: 7600 Train: 11.05993 Test: 0.32846
Epoch: 7700 Train: 10.56231 Test: 0.38513
Epoch: 7800 Train: 10.49532 Test: 0.44932
Epoch: 7900 Train: 11.19820 Test: 0.57898
Epoch: 8000 Train: 10.08789 Test: 0.37364
Epoch: 8100 Train: 11.11605 Test: 0.59734
Epoch: 8200 Train: 9.94217 Test: 0.60124
Epoch: 8300 Train: 9.52041 Test: 0.24040
Epoch: 8400 Train: 9.45219 Test: 0.24152
Epoch: 8500 Train: 9.50981 Test: 0.91166
Epoch: 8600 Train: 9.06249 Test: 0.54902
Epoch: 8700 Train: 9.50882 Test: 0.40292
Epoch: 8800 Train: 8.61024 Test: 0.20552
Epoch: 8900 Train: 8.56475 Test: 0.15657
Epoch: 9000 Train: 9.27443 Test: 0.47332
Epoch: 9100 Train: 8.07593 Test: 0.26178
Epoch: 9200 Train: 8.06139 Test: 0.15334
Epoch: 9300 Train: 8.23225 Test: 0.19836
Epoch: 9400 Train: 7.70809 Test: 0.14531
Epoch: 9500 Train: 7.67381 Test: 0.14689
Epoch: 9600 Train: 7.71241 Test: 0.25069
Epoch: 9700 Train: 7.32830 Test: 0.13208
Epoch: 9800 Train: 7.31758 Test: 0.17972
Epoch: 9900 Train: 7.14419 Test: 0.12700
Epoch: 9999 Train: 7.36900 Test: 0.67156
Training Loss: tensor(7.3690)
Test Loss: tensor(0.6716)
Learned LE: [  0.81020755   0.06236617 -14.475868  ]
True LE: [ 8.6566323e-01 -2.7890876e-03 -1.4537115e+01]
Relative Error: [3.8828378  3.860094   3.566728   3.3697634  3.256342   2.956877
 2.7916098  2.5894604  2.3789887  2.2637866  2.2581387  2.3857543
 2.6962242  2.9120903  3.1846304  3.3963192  3.5168521  3.4593952
 3.4662752  3.2668345  2.9314601  2.2578344  1.2089748  0.63633925
 0.4785079  0.8121024  1.0616279  1.4355917  1.799659   2.081149
 2.191009   2.0984244  1.9004384  1.6775393  1.5110557  1.6908873
 2.0984313  2.3525987  2.6770544  3.0836196  3.5084584  3.8607635
 4.1871653  4.8722277  5.7933517  6.3304415  6.0059147  5.6243496
 5.027899   3.5743036  2.1991813  1.2925596  0.740409   0.680253
 0.65821934 0.70255446 0.8799113  1.1734726  1.4302135  1.7124513
 2.2496266  3.0373375  3.601789   3.518941   3.2325618  3.0785234
 2.843139   2.6116195  2.4703522  2.186722   2.0543308  1.9653527
 1.9300494  2.011249   2.3221304  2.4246588  2.568867   2.8677442
 2.9808934  2.857811   2.883066   2.816659   2.5396981  2.2183607
 1.4937466  0.7968491  0.24633217 0.5114099  0.8305868  1.1169997
 1.4633815  1.7741674  1.9070086  1.8116987  1.6630696  1.5484278
 1.4600981  1.6686374  1.8291845  2.0691304  2.4761093  2.802742
 3.1150146  3.5928535  3.8276222  4.376755   5.2597833  5.9857645
 5.91874    5.527768   4.9037833  3.7225928  2.2973754  1.3238283
 0.77260256 0.71937096 0.6936124  0.6969517  0.8032455  1.0294762
 1.3046105  1.6067631  2.164818   2.9642367  3.2773077  3.1593337
 3.001307   2.7448547  2.5205464  2.361002   2.1555982  1.9259098
 1.7806289  1.7936299  1.7846174  1.7686116  1.873667   1.9921671
 2.0414908  2.3287933  2.4742358  2.3351896  2.2827349  2.2872202
 2.2340221  1.929612   1.6316417  1.157403   0.35373905 0.29245982
 0.6658668  0.8872863  1.1094847  1.2572811  1.3920847  1.3224661
 1.1654419  1.0862939  1.198499   1.4789639  1.58215    1.8474134
 2.124705   2.342989   2.600314   3.0717251  3.4007165  3.8684175
 4.670783   5.5029535  5.509583   5.3898196  4.8929806  3.9373927
 2.5482013  1.4774542  0.9102296  0.80226314 0.7888893  0.76773554
 0.70007306 0.86533976 1.2023476  1.5048155  2.1653614  2.9607766
 2.9667723  2.8728871  2.9254408  2.644226   2.3805757  2.207153
 1.9511005  1.8088719  1.7015675  1.6647922  1.6981413  1.6343822
 1.6307957  1.7091364  1.7210689  1.9063998  2.0576303  1.9745685
 1.8457452  1.7836504  1.8151995  1.7345438  1.5717002  1.3451138
 0.74087805 0.10506459 0.44429234 0.8045763  0.8914095  0.8183844
 1.0627     1.1210558  1.0143498  0.86901176 0.81998956 1.0339861
 1.318915   1.5944855  1.7212348  1.9350553  2.1499383  2.5179188
 2.7300873  2.9456043  3.6918654  4.5319114  4.8213887  4.8508997
 4.7614417  4.1271815  3.0046687  1.8330563  1.1592445  0.9186635
 0.90950525 0.7738693  0.57537353 0.7783397  1.2458973  1.6739057
 2.2834296  3.0015004  2.7936223  2.6707287  2.687522   2.547477
 2.407476   2.216989   1.8973995  1.815974   1.745031   1.6406095
 1.5998068  1.589584   1.6186107  1.5867146  1.5753902  1.6081486
 1.7711593  1.7895417  1.5884926  1.4742329  1.3608713  1.4495527
 1.4487715  1.3349224  1.0450236  0.4789562  0.16195817 0.58310235
 0.7455414  0.69357336 0.6162672  0.81378603 0.83153296 0.7598814
 0.75046    0.79981655 1.0090318  1.2364675  1.3585014  1.5408623
 1.7845322  1.950813   1.9622892  2.082286   2.4990337  3.4681087
 4.130185   4.0495915  4.075929   3.9518495  3.3783865  2.4010725
 1.51669    1.0256028  0.83495396 0.6330219  0.41376793 0.68176913
 1.2843336  1.8303609  2.3481817  3.0024345  2.7044342  2.5050175
 2.411853   2.3401892  2.3507395  2.2718632  1.9843796  1.8604252
 1.8302428  1.7498517  1.6371263  1.6800582  1.7323062  1.7153363
 1.6232809  1.5642692  1.6408926  1.7379516  1.6077118  1.323378
 1.143249   1.0012764  1.2999449  1.2899584  1.1271753  0.81334996
 0.27712828 0.25215027 0.44748917 0.56823367 0.4991043  0.4856058
 0.58918375 0.623      0.56144226 0.5949969  0.78274    0.9279927
 1.0237545  1.2211002  1.3859673  1.4420369  1.3690448  1.4049214
 1.6108774  2.189886   3.2077572  3.3755546  3.203522   3.1493838
 3.2570608  2.7748125  2.0243506  1.2814568  0.8688584  0.5480071
 0.29301763 0.48111477 1.0776871  1.7168771  2.2698646  2.7985415
 2.6321084  2.4162056  2.217948   2.1931388  2.2707527  2.3238816
 2.173461   1.9852221  1.8079175  1.7175591  1.6699381  1.6632084
 1.7232271  1.6037475  1.5567815  1.4078821  1.3450425  1.5049226
 1.5742096  1.5637434  1.2421141  0.9422084  0.8733282  1.18238
 1.2006015  1.0278496  0.6730926  0.17328471]
