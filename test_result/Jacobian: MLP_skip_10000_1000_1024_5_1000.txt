time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.26%, model saved.
Epoch: 0 Train: 60425.41797 Test: 4001.56885
Epoch: 100 Train: 13335.63281 Test: 1466.57629
Epoch: 200 Train: 11796.66797 Test: 1109.77966
Epoch: 300 Train: 11321.19531 Test: 1457.72729
Epoch 400: New minimal relative error: 77.62%, model saved.
Epoch: 400 Train: 15471.82227 Test: 1441.83984
Epoch: 500 Train: 12519.23145 Test: 1139.07446
Epoch: 600 Train: 16181.59863 Test: 1789.14746
Epoch: 700 Train: 12592.52148 Test: 1249.04895
Epoch: 800 Train: 12371.51953 Test: 1178.74231
Epoch: 900 Train: 11374.91016 Test: 1106.21106
Epoch: 1000 Train: 11034.24023 Test: 844.16687
Epoch 1100: New minimal relative error: 60.76%, model saved.
Epoch: 1100 Train: 8898.24219 Test: 693.59222
Epoch: 1200 Train: 7974.27539 Test: 486.90143
Epoch: 1300 Train: 7190.20508 Test: 471.29099
Epoch: 1400 Train: 7374.94287 Test: 327.47964
Epoch: 1500 Train: 6092.91260 Test: 270.99960
Epoch: 1600 Train: 5473.56934 Test: 225.03737
Epoch: 1700 Train: 4574.51709 Test: 172.67503
Epoch: 1800 Train: 1924.47925 Test: 61.09817
Epoch 1900: New minimal relative error: 18.31%, model saved.
Epoch: 1900 Train: 839.61688 Test: 16.32596
Epoch: 2000 Train: 489.91599 Test: 9.76295
Epoch: 2100 Train: 386.78656 Test: 3.84017
Epoch 2200: New minimal relative error: 6.52%, model saved.
Epoch: 2200 Train: 310.83234 Test: 2.82794
Epoch: 2300 Train: 268.35522 Test: 20.35068
Epoch: 2400 Train: 337.44440 Test: 21.29300
Epoch: 2500 Train: 171.55550 Test: 1.81097
Epoch: 2600 Train: 146.80574 Test: 1.03808
Epoch: 2700 Train: 181.93694 Test: 2.92893
Epoch: 2800 Train: 150.06480 Test: 1.51712
Epoch: 2900 Train: 147.31805 Test: 1.32265
Epoch: 3000 Train: 126.01478 Test: 0.97365
Epoch 3100: New minimal relative error: 3.89%, model saved.
Epoch: 3100 Train: 126.97777 Test: 1.97525
Epoch: 3200 Train: 112.99562 Test: 1.29353
Epoch: 3300 Train: 103.43509 Test: 1.08973
Epoch: 3400 Train: 114.14358 Test: 2.55208
Epoch: 3500 Train: 107.03175 Test: 1.73018
Epoch: 3600 Train: 111.66926 Test: 2.86996
Epoch: 3700 Train: 91.29287 Test: 1.14428
Epoch: 3800 Train: 91.64294 Test: 1.11124
Epoch: 3900 Train: 80.59037 Test: 0.92349
Epoch: 4000 Train: 84.00199 Test: 2.28603
Epoch: 4100 Train: 80.50088 Test: 2.04365
Epoch: 4200 Train: 76.91899 Test: 0.67715
Epoch: 4300 Train: 93.12060 Test: 4.36483
Epoch: 4400 Train: 82.42989 Test: 2.20519
Epoch 4500: New minimal relative error: 3.22%, model saved.
Epoch: 4500 Train: 67.70287 Test: 0.58767
Epoch: 4600 Train: 63.51939 Test: 0.69896
Epoch: 4700 Train: 64.32365 Test: 0.84861
Epoch: 4800 Train: 72.15205 Test: 1.31349
Epoch: 4900 Train: 73.25157 Test: 1.11092
Epoch: 5000 Train: 65.68058 Test: 0.86747
Epoch: 5100 Train: 68.11018 Test: 1.97932
Epoch: 5200 Train: 64.24113 Test: 0.60639
Epoch: 5300 Train: 82.72727 Test: 6.39098
Epoch: 5400 Train: 67.60537 Test: 1.30171
Epoch 5500: New minimal relative error: 2.99%, model saved.
Epoch: 5500 Train: 61.55624 Test: 0.56160
Epoch: 5600 Train: 74.49427 Test: 1.00641
Epoch: 5700 Train: 68.85706 Test: 0.75299
Epoch: 5800 Train: 59.60800 Test: 0.35912
Epoch: 5900 Train: 53.89886 Test: 0.37985
Epoch: 6000 Train: 54.75627 Test: 0.76267
Epoch: 6100 Train: 66.20480 Test: 1.74791
Epoch: 6200 Train: 74.85276 Test: 5.35537
Epoch: 6300 Train: 58.87759 Test: 0.66807
Epoch: 6400 Train: 52.86276 Test: 0.36381
Epoch: 6500 Train: 55.86437 Test: 1.14842
Epoch: 6600 Train: 53.98233 Test: 1.64599
Epoch 6700: New minimal relative error: 2.88%, model saved.
Epoch: 6700 Train: 48.67333 Test: 0.26598
Epoch: 6800 Train: 46.50285 Test: 0.32851
Epoch: 6900 Train: 50.28537 Test: 0.28277
Epoch 7000: New minimal relative error: 2.81%, model saved.
Epoch: 7000 Train: 52.47452 Test: 0.50271
Epoch: 7100 Train: 45.84187 Test: 0.47730
Epoch: 7200 Train: 41.22152 Test: 0.40110
Epoch 7300: New minimal relative error: 2.66%, model saved.
Epoch: 7300 Train: 42.19867 Test: 0.26379
Epoch: 7400 Train: 42.53830 Test: 0.23039
Epoch: 7500 Train: 43.81110 Test: 0.34434
Epoch: 7600 Train: 44.70538 Test: 0.67423
Epoch: 7700 Train: 40.02104 Test: 0.23948
Epoch: 7800 Train: 41.12174 Test: 0.75902
Epoch: 7900 Train: 43.80572 Test: 0.81012
Epoch: 8000 Train: 43.06469 Test: 1.17448
Epoch: 8100 Train: 51.27164 Test: 3.86940
Epoch: 8200 Train: 38.07918 Test: 0.27542
Epoch: 8300 Train: 38.56087 Test: 0.97416
Epoch: 8400 Train: 48.87994 Test: 1.57598
Epoch: 8500 Train: 38.08941 Test: 0.25503
Epoch: 8600 Train: 49.50629 Test: 0.49705
Epoch: 8700 Train: 43.53930 Test: 0.33020
Epoch: 8800 Train: 43.14630 Test: 0.35302
Epoch: 8900 Train: 43.94574 Test: 1.05603
Epoch: 9000 Train: 52.51063 Test: 0.62082
Epoch: 9100 Train: 49.34123 Test: 0.43333
Epoch: 9200 Train: 48.28201 Test: 0.77773
Epoch: 9300 Train: 47.02909 Test: 0.44593
Epoch: 9400 Train: 45.87601 Test: 0.48893
Epoch: 9500 Train: 47.20450 Test: 0.61728
Epoch: 9600 Train: 45.46301 Test: 0.43426
Epoch: 9700 Train: 51.16000 Test: 1.84906
Epoch 9800: New minimal relative error: 2.09%, model saved.
Epoch: 9800 Train: 41.92230 Test: 0.36380
Epoch: 9900 Train: 38.88298 Test: 0.42213
Epoch: 9999 Train: 34.77462 Test: 0.13275
Training Loss: tensor(34.7746)
Test Loss: tensor(0.1327)
Learned LE: [  0.76792854   0.05546455 -14.499958  ]
True LE: [ 8.1707698e-01 -3.3141158e-03 -1.4485752e+01]
Relative Error: [1.2413814  0.9883995  0.82922184 0.89933825 1.1338706  1.5626609
 1.6104164  1.7609516  2.0322595  2.312857   2.4178367  2.2743373
 1.9489622  1.423859   1.1722522  1.0845051  1.0804039  0.96019626
 0.81196475 0.9939778  1.306764   1.6428305  1.9342076  2.3136365
 2.769567   3.070305   3.1059787  3.1364706  3.0933032  2.9412465
 2.6594396  2.5605502  2.4949017  2.4266393  2.2883947  2.251554
 2.1228774  1.9686291  1.836936   1.591551   1.1925495  0.60351354
 0.25112626 0.2101505  0.28788036 0.57064754 1.0682161  1.3770328
 1.7696731  1.8363844  1.7989168  1.8590225  2.0346246  2.3773453
 2.7980084  2.997867   3.0758955  2.884132   2.6629057  2.5767534
 2.232725   1.7903702  1.541604   1.3119862  0.9721316  0.8442965
 1.0021939  1.3499619  1.6211563  1.6254514  1.7762825  2.1227975
 2.2119093  2.2144144  1.8946059  1.4384294  0.9980809  0.87100416
 0.9412323  0.8915626  0.7553851  0.8208101  1.0735974  1.2713119
 1.4562397  1.6649591  1.9948814  2.3064706  2.4182005  2.3845994
 2.4011724  2.426755   2.3453758  2.2078097  2.147727   2.1118839
 2.1184154  2.1646059  2.2577758  2.1237833  1.8696064  1.6225059
 1.3882678  0.98674333 0.4717793  0.37155038 0.25842926 0.34433103
 0.59870946 1.0600675  1.4336929  1.7188023  1.6600761  1.7710155
 1.9878464  2.2720816  2.6547089  2.810161   2.895921   3.003841
 2.8492308  2.8075128  2.5906336  2.1882553  1.7595936  1.5493369
 1.3401828  1.0492846  0.9796412  1.1324937  1.4810333  1.6406583
 1.6082138  1.7920146  2.024072   2.0827167  1.9206194  1.509392
 1.0682359  0.78155303 0.7477519  0.78174007 0.6940181  0.6508298
 0.79895395 0.94487846 1.0375004  1.084878   1.2091721  1.4495788
 1.6099883  1.6417772  1.6971906  1.8044755  1.9004011  1.8683853
 1.7670344  1.7303036  1.7708997  1.8552896  1.9585056  2.110223
 2.0137057  1.7469909  1.4008148  1.1933717  0.859792   0.43926948
 0.25869602 0.32552436 0.42730463 0.6415539  1.0257521  1.3798333
 1.5306103  1.572154   1.7966313  2.190011   2.5505145  2.6724858
 2.7153244  2.721247   2.7409422  2.6843007  2.8977258  2.7256465
 2.309722   1.8944899  1.6665592  1.3201028  1.2935275  1.1596557
 1.206823   1.5612196  1.6301591  1.568891   1.8198858  2.0157804
 1.9064158  1.5848609  1.201279   0.8361724  0.65116173 0.6782144
 0.5691788  0.44370642 0.5337142  0.6975777  0.72839993 0.6883521
 0.71359724 0.67312753 0.6947597  0.79687524 0.857215   0.9382664
 1.0272101  1.1716443  1.3138664  1.3822963  1.2896326  1.3647168
 1.4921036  1.6774846  1.8355483  1.8285916  1.645886   1.2563806
 1.0329943  0.8796462  0.45200083 0.16318393 0.44180816 0.50731695
 0.6705122  0.96324915 1.3256145  1.4268734  1.525395   1.7488006
 2.3072553  2.6454356  2.4227855  2.2363348  2.268292   2.3258216
 2.3403573  2.565336   2.5625997  2.417168   2.162892   1.8319557
 1.5139135  1.3946804  1.3535846  1.2894309  1.5619614  1.6887459
 1.6460038  1.8254688  1.9315431  1.7316964  1.3553972  1.0515445
 0.76001805 0.65063655 0.688398   0.407168   0.35940805 0.43000063
 0.49553818 0.5497133  0.47888824 0.4118259  0.30472475 0.21887657
 0.22789477 0.28401202 0.3708206  0.4916354  0.6311625  0.87446237
 1.0681566  1.0353384  0.9272242  1.0877802  1.3282094  1.5538064
 1.5462404  1.4408573  1.2436299  0.9880556  0.9258885  0.63099074
 0.3653081  0.4462334  0.6064552  0.62162524 0.78046757 1.1370143
 1.317391   1.4454825  1.6648926  2.212307   2.523491   2.0636044
 1.8798643  1.9571658  2.0850945  2.2284665  2.2745688  2.3938012
 2.2586398  2.1675508  2.1632156  1.8237005  1.5872025  1.4830807
 1.4836606  1.5187616  1.6359346  1.6845088  1.7651466  1.8447835
 1.694165   1.252851   0.9648433  0.7675889  0.7233234  0.5279626
 0.20196836 0.18139938 0.26916692 0.33369347 0.2661976  0.3427298
 0.57196534 0.7079029  0.87021494 1.0641422  1.1915785  1.082695
 0.94420993 0.89817274 0.9395394  1.2258874  1.2553236  0.9175014
 0.91703963 1.0535468  1.3644203  1.4101838  1.2729068  1.1698009
 1.1556108  0.8935457  0.71930873 0.47641465 0.30640933 0.4967869
 0.54973334 0.6154799  0.8449564  1.134632   1.2519399  1.547396
 2.0141606  2.105289   1.961755   1.7336938  1.7171012  1.9646065
 1.9992613  1.9173483  2.0493734  1.9268242  1.8576674  1.973176
 2.129624   1.9596106  1.6940687  1.67336    1.6160402  1.6164867
 1.6278256  1.6270597  1.7106658  1.6956773  1.2603855  0.8316816
 0.74404424 0.78938335 0.6811558  0.24369483 0.05630826 0.21563904
 0.19321449 0.04997407 0.2627437  0.53884417]
