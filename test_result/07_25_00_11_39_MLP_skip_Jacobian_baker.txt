time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 3000
num_train: 2000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 6
reg_param: 25.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 37.004142761 Test: 10.205945969
Epoch 0: New minimal relative error: 10.21%, model saved.
Epoch: 30 Train: 6.856980801 Test: 2.996831656
Epoch 30: New minimal relative error: 3.00%, model saved.
Epoch: 60 Train: 5.859628677 Test: 3.231525421
Epoch: 90 Train: 5.762686729 Test: 3.285698891
Epoch: 120 Train: 5.617210865 Test: 3.407855272
Epoch: 150 Train: 5.889967442 Test: 3.182978868
Epoch: 180 Train: 5.417611122 Test: 3.224928141
Epoch: 210 Train: 5.434669971 Test: 3.225315809
Epoch: 240 Train: 5.363705158 Test: 3.247444391
Epoch: 270 Train: 5.537206173 Test: 3.171555996
Epoch: 300 Train: 5.176104546 Test: 3.308452606
Epoch: 330 Train: 5.164196968 Test: 3.350966930
Epoch: 360 Train: 5.298868179 Test: 3.075805664
Epoch: 390 Train: 5.269809246 Test: 3.334340334
Epoch: 420 Train: 5.247974396 Test: 3.356557846
Epoch: 450 Train: 5.202560902 Test: 3.388046265
Epoch: 480 Train: 5.149155140 Test: 3.377408981
Epoch: 510 Train: 5.049704552 Test: 3.400566578
Epoch: 540 Train: 5.036512375 Test: 3.389608622
Epoch: 570 Train: 4.990397930 Test: 3.503157139
Epoch: 600 Train: 4.939871788 Test: 3.650112867
Epoch: 630 Train: 4.950981617 Test: 3.713417768
Epoch: 660 Train: 4.991308689 Test: 3.619672298
Epoch: 690 Train: 5.055472374 Test: 3.519068718
Epoch: 720 Train: 4.962524891 Test: 3.551974297
Epoch: 750 Train: 5.091953278 Test: 3.646292210
Epoch: 780 Train: 4.987349033 Test: 3.559334040
Epoch: 810 Train: 4.908890247 Test: 3.692050457
Epoch: 840 Train: 4.905870438 Test: 3.717406034
Epoch: 870 Train: 4.999980450 Test: 3.696601152
Epoch: 900 Train: 4.985146046 Test: 3.715180874
Epoch: 930 Train: 5.003224373 Test: 3.779483080
Epoch: 960 Train: 4.953880310 Test: 3.690718889
Epoch: 990 Train: 4.971827030 Test: 3.796587229
Epoch: 1020 Train: 4.947665691 Test: 3.760732651
Epoch: 1050 Train: 4.942999363 Test: 3.678664446
Epoch: 1080 Train: 5.002339363 Test: 3.768282413
Epoch: 1110 Train: 4.966990948 Test: 3.761162281
Epoch: 1140 Train: 4.940625191 Test: 3.743050575
Epoch: 1170 Train: 4.933219910 Test: 3.753267288
Epoch: 1200 Train: 4.917223930 Test: 3.790716171
Epoch: 1230 Train: 4.915539742 Test: 3.705738783
Epoch: 1260 Train: 4.917185783 Test: 3.741780281
Epoch: 1290 Train: 4.928046227 Test: 3.733903408
Epoch: 1320 Train: 4.921730042 Test: 3.689005375
Epoch: 1350 Train: 4.897370338 Test: 3.767075539
Epoch: 1380 Train: 4.906588078 Test: 3.855075836
Epoch: 1410 Train: 4.912399292 Test: 3.762646198
Epoch: 1440 Train: 4.888363361 Test: 3.727857828
Epoch: 1470 Train: 4.862884998 Test: 3.691443443
Epoch: 1500 Train: 4.900029182 Test: 3.784762621
Epoch: 1530 Train: 4.873448849 Test: 3.739270926
Epoch: 1560 Train: 4.887047291 Test: 3.735811234
Epoch: 1590 Train: 4.887532711 Test: 3.729425192
Epoch: 1620 Train: 4.863651276 Test: 3.726620674
Epoch: 1650 Train: 4.847511768 Test: 3.696628809
Epoch: 1680 Train: 4.864451885 Test: 3.714983225
Epoch: 1710 Train: 4.929168224 Test: 3.469560623
Epoch: 1740 Train: 5.054246902 Test: 3.378224134
Epoch: 1770 Train: 4.971561432 Test: 3.403294086
Epoch: 1800 Train: 4.920222282 Test: 3.468882799
Epoch: 1830 Train: 4.896727085 Test: 3.529029608
Epoch: 1860 Train: 4.913836002 Test: 3.556565762
Epoch: 1890 Train: 4.933181286 Test: 3.570086479
Epoch: 1920 Train: 4.920512199 Test: 3.589035749
Epoch: 1950 Train: 4.905224800 Test: 3.610134363
Epoch: 1980 Train: 4.898291111 Test: 3.614795208
Epoch: 2010 Train: 4.898903847 Test: 3.644612074
Epoch: 2040 Train: 4.900337219 Test: 3.641897678
Epoch: 2070 Train: 4.898126602 Test: 3.664225578
Epoch: 2100 Train: 4.901448727 Test: 3.683405161
Epoch: 2130 Train: 4.904837608 Test: 3.689263344
Epoch: 2160 Train: 4.904726982 Test: 3.714183807
Epoch: 2190 Train: 4.901235580 Test: 3.728641748
Epoch: 2220 Train: 4.902315617 Test: 3.736160755
Epoch: 2250 Train: 4.900504112 Test: 3.739367962
Epoch: 2280 Train: 4.894685745 Test: 3.742746353
Epoch: 2310 Train: 4.899737835 Test: 3.747799635
Epoch: 2340 Train: 4.906256676 Test: 3.745643377
Epoch: 2370 Train: 4.907124043 Test: 3.753757000
Epoch: 2400 Train: 4.901685715 Test: 3.757159948
Epoch: 2430 Train: 4.897201538 Test: 3.759579182
Epoch: 2460 Train: 4.892723083 Test: 3.759878635
Epoch: 2490 Train: 4.896201134 Test: 3.754029751
Epoch: 2520 Train: 4.887607574 Test: 3.751402378
Epoch: 2550 Train: 4.882187843 Test: 3.759479284
Epoch: 2580 Train: 4.877493382 Test: 3.747178793
Epoch: 2610 Train: 4.884592056 Test: 3.754591942
Epoch: 2640 Train: 4.883447170 Test: 3.760692120
Epoch: 2670 Train: 4.881823063 Test: 3.764609337
Epoch: 2700 Train: 4.882019997 Test: 3.765783787
Epoch: 2730 Train: 4.880402088 Test: 3.765913248
Epoch: 2760 Train: 4.880362988 Test: 3.766229391
Epoch: 2790 Train: 4.877718449 Test: 3.766970634
Epoch: 2820 Train: 4.877747536 Test: 3.762960672
Epoch: 2850 Train: 4.878116131 Test: 3.745918512
Epoch: 2880 Train: 4.877878189 Test: 3.738586187
Epoch: 2910 Train: 4.873059273 Test: 3.742044210
Epoch: 2940 Train: 4.871117592 Test: 3.745202780
Epoch: 2970 Train: 4.864498138 Test: 3.743856192
Epoch: 2999 Train: 4.863916397 Test: 3.748204947
Training Loss: tensor(4.8639)
Test Loss: tensor(3.7482)
True Mean x: tensor(2.9991, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(6.9296e+09, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3920, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(5.5434e+20, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0447)
Jacobian term Test Loss: tensor(0.0007)
Learned LE: [1.6126236 0.4448369]
True LE: tensor([ 0.6932, -0.7437], dtype=torch.float64)
