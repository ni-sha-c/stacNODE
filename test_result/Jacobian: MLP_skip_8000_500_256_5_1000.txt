time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.77%, model saved.
Epoch: 0 Train: 59160.58984 Test: 3910.84912
Epoch: 80 Train: 15743.49707 Test: 1532.20654
Epoch: 160 Train: 14247.86133 Test: 1537.08679
Epoch 240: New minimal relative error: 84.60%, model saved.
Epoch: 240 Train: 13440.02246 Test: 1206.21790
Epoch 320: New minimal relative error: 74.82%, model saved.
Epoch: 320 Train: 14190.86523 Test: 1106.95178
Epoch: 400 Train: 13820.51172 Test: 1281.67078
Epoch 480: New minimal relative error: 63.18%, model saved.
Epoch: 480 Train: 13620.41699 Test: 1070.96692
Epoch: 560 Train: 13334.02441 Test: 1264.12268
Epoch: 640 Train: 13224.79688 Test: 1252.76184
Epoch: 720 Train: 12273.68652 Test: 1044.52649
Epoch: 800 Train: 12272.47461 Test: 1083.51990
Epoch 880: New minimal relative error: 61.07%, model saved.
Epoch: 880 Train: 10730.35742 Test: 890.69501
Epoch: 960 Train: 10216.97852 Test: 827.61078
Epoch: 1040 Train: 8743.76562 Test: 587.09100
Epoch: 1120 Train: 7626.42871 Test: 424.35599
Epoch: 1200 Train: 6011.66895 Test: 339.21973
Epoch: 1280 Train: 3285.63647 Test: 106.47356
Epoch: 1360 Train: 2860.66431 Test: 82.86783
Epoch 1440: New minimal relative error: 47.14%, model saved.
Epoch: 1440 Train: 2406.67114 Test: 64.86132
Epoch 1520: New minimal relative error: 26.14%, model saved.
Epoch: 1520 Train: 2101.13916 Test: 41.16329
Epoch: 1600 Train: 1882.29407 Test: 36.18114
Epoch: 1680 Train: 1785.63696 Test: 33.83776
Epoch 1760: New minimal relative error: 21.68%, model saved.
Epoch: 1760 Train: 1696.92908 Test: 32.24039
Epoch: 1840 Train: 1542.40967 Test: 28.67019
Epoch: 1920 Train: 1514.48792 Test: 34.74880
Epoch: 2000 Train: 1343.14673 Test: 21.56899
Epoch: 2080 Train: 1198.80371 Test: 18.83935
Epoch: 2160 Train: 967.96881 Test: 13.89245
Epoch 2240: New minimal relative error: 20.00%, model saved.
Epoch: 2240 Train: 914.83160 Test: 13.44154
Epoch: 2320 Train: 1056.16064 Test: 17.32303
Epoch 2400: New minimal relative error: 8.43%, model saved.
Epoch: 2400 Train: 821.06348 Test: 12.05317
Epoch: 2480 Train: 722.05469 Test: 13.17669
Epoch: 2560 Train: 603.42560 Test: 9.90864
Epoch: 2640 Train: 492.11996 Test: 11.24439
Epoch: 2720 Train: 440.20370 Test: 4.75761
Epoch: 2800 Train: 405.63858 Test: 4.31503
Epoch 2880: New minimal relative error: 6.12%, model saved.
Epoch: 2880 Train: 365.57202 Test: 3.84198
Epoch: 2960 Train: 343.94836 Test: 4.02410
Epoch: 3040 Train: 345.01022 Test: 3.19844
Epoch: 3120 Train: 361.70236 Test: 3.77488
Epoch: 3200 Train: 341.54987 Test: 3.15317
Epoch: 3280 Train: 326.49216 Test: 3.28319
Epoch 3360: New minimal relative error: 5.76%, model saved.
Epoch: 3360 Train: 302.98849 Test: 2.75331
Epoch: 3440 Train: 341.70364 Test: 4.94097
Epoch: 3520 Train: 301.17529 Test: 2.63244
Epoch: 3600 Train: 294.08389 Test: 2.63853
Epoch 3680: New minimal relative error: 3.77%, model saved.
Epoch: 3680 Train: 285.04355 Test: 2.50170
Epoch: 3760 Train: 278.33514 Test: 2.73978
Epoch: 3840 Train: 276.33157 Test: 2.55146
Epoch: 3920 Train: 268.75525 Test: 2.58068
Epoch: 4000 Train: 278.15076 Test: 2.60146
Epoch: 4080 Train: 264.78412 Test: 2.45363
Epoch: 4160 Train: 255.45251 Test: 2.24725
Epoch: 4240 Train: 257.25882 Test: 2.41313
Epoch: 4320 Train: 246.13707 Test: 2.14789
Epoch: 4400 Train: 257.49423 Test: 2.25692
Epoch: 4480 Train: 265.15024 Test: 2.43180
Epoch: 4560 Train: 257.76770 Test: 2.11629
Epoch: 4640 Train: 276.95883 Test: 2.91199
Epoch: 4720 Train: 272.03851 Test: 3.06379
Epoch: 4800 Train: 260.18179 Test: 2.50374
Epoch: 4880 Train: 253.29071 Test: 2.66157
Epoch: 4960 Train: 232.68187 Test: 2.46249
Epoch: 5040 Train: 218.16956 Test: 1.77457
Epoch 5120: New minimal relative error: 2.97%, model saved.
Epoch: 5120 Train: 219.95070 Test: 1.79397
Epoch: 5200 Train: 230.44373 Test: 3.25678
Epoch: 5280 Train: 228.96719 Test: 1.96310
Epoch: 5360 Train: 238.47278 Test: 2.28386
Epoch: 5440 Train: 258.77383 Test: 2.66123
Epoch: 5520 Train: 242.10416 Test: 2.50192
Epoch: 5600 Train: 228.07501 Test: 2.18425
Epoch: 5680 Train: 223.79199 Test: 1.75846
Epoch: 5760 Train: 226.86952 Test: 1.87318
Epoch: 5840 Train: 228.38635 Test: 2.03485
Epoch: 5920 Train: 240.03001 Test: 7.48808
Epoch: 6000 Train: 275.39230 Test: 5.06307
Epoch: 6080 Train: 312.62344 Test: 3.51128
Epoch: 6160 Train: 252.80177 Test: 3.07558
Epoch: 6240 Train: 232.77762 Test: 2.05010
Epoch: 6320 Train: 219.89139 Test: 1.93393
Epoch: 6400 Train: 213.56000 Test: 1.74498
Epoch: 6480 Train: 205.75230 Test: 1.92753
Epoch: 6560 Train: 190.76488 Test: 1.58947
Epoch: 6640 Train: 185.13835 Test: 1.35172
Epoch: 6720 Train: 187.69606 Test: 1.53408
Epoch: 6800 Train: 188.69814 Test: 1.41538
Epoch: 6880 Train: 189.65564 Test: 1.59738
Epoch: 6960 Train: 194.60995 Test: 1.71957
Epoch: 7040 Train: 174.72766 Test: 1.57048
Epoch: 7120 Train: 180.54945 Test: 1.56123
Epoch: 7200 Train: 171.75153 Test: 1.38153
Epoch: 7280 Train: 178.73074 Test: 1.48949
Epoch: 7360 Train: 175.13055 Test: 1.67953
Epoch: 7440 Train: 191.00778 Test: 2.03475
Epoch: 7520 Train: 175.63742 Test: 1.48599
Epoch: 7600 Train: 179.59323 Test: 1.91484
Epoch: 7680 Train: 178.78946 Test: 2.43875
Epoch: 7760 Train: 160.45439 Test: 1.54967
Epoch: 7840 Train: 167.88954 Test: 1.32253
Epoch: 7920 Train: 182.57266 Test: 2.18698
Epoch: 7999 Train: 249.67317 Test: 3.71245
Training Loss: tensor(249.6732)
Test Loss: tensor(3.7125)
Learned LE: [  0.8265562    0.02669726 -14.564376  ]
True LE: [ 8.7128180e-01 -6.1653648e-03 -1.4541613e+01]
Relative Error: [2.3723235  1.9390231  1.3735565  0.7346951  0.88967776 1.9197342
 2.584211   2.7318287  3.0166438  2.547464   2.0777836  1.5860515
 1.2672775  1.0862038  0.9774304  0.683033   0.58099496 0.454676
 0.44237688 0.6511436  0.9345281  1.2674667  1.4559215  1.7360823
 1.7994509  1.6358731  1.5462916  1.4833431  1.4423697  1.5264144
 1.5940152  1.1776463  0.8748537  0.7841522  1.0289352  1.2880996
 1.611227   1.6880016  1.7400444  1.7914249  1.7061291  1.5544091
 1.9253614  2.1356568  2.1488144  2.0125709  1.8304533  1.783037
 1.7356994  1.8573929  2.03757    2.2399948  1.9387238  1.6877655
 1.5344145  1.6019049  1.6026874  1.4726369  1.8019788  2.1761858
 2.436735   2.4275768  2.2558491  2.0661309  1.5058846  0.8074467
 0.74171054 1.7097017  2.3406703  2.451453   2.6108942  2.2081418
 1.8940102  1.4221483  1.1623676  1.0522884  0.8290958  0.6276025
 0.62421286 0.5083828  0.50401634 0.60261816 0.8728652  1.1643062
 1.4151852  1.6505435  1.9947628  1.9664302  1.857131   1.7594641
 1.6531013  1.6805857  1.6768988  1.4115531  1.00571    0.62099636
 0.7001224  1.0152732  1.3189892  1.4681482  1.6037385  1.7582765
 1.791401   1.7300255  2.0152287  2.199256   2.2033925  2.0404105
 1.8258067  1.7270552  1.4524984  1.4998134  1.761286   1.7727442
 1.5372835  1.3273845  1.1838932  1.1487406  1.1158292  1.0967919
 1.3541644  1.7710669  2.058907   2.181029   2.1701462  2.059737
 1.715667   0.98103654 0.5718042  1.4020079  2.0737653  2.1032746
 2.2108998  1.9225885  1.65839    1.3443507  1.102185   1.0484638
 0.7368569  0.59366494 0.6407178  0.6271627  0.69039273 0.82709557
 0.8339746  0.9942462  1.2406611  1.6095281  2.1374943  2.4245615
 2.340118   2.2168362  2.03442    1.9659797  1.86434    1.6093116
 1.4026185  0.7398673  0.49650544 0.7643464  0.9878367  1.2945341
 1.4355695  1.6683131  1.8192198  1.8655006  1.9847032  2.236916
 2.2516093  2.1142273  1.9111406  1.7710326  1.4128608  1.1262555
 1.4623547  1.4441051  1.1767334  1.0283426  0.8036259  0.60677737
 0.6961439  0.8351427  1.0730904  1.4524496  1.7706603  2.0057786
 2.0391204  2.0093155  1.8637531  1.2734777  0.5803777  1.0253677
 1.7963883  1.7184563  1.819102   1.6858916  1.4279585  1.3236552
 1.1101146  0.9526084  0.6927154  0.5746679  0.62400305 0.78902656
 0.6615636  0.55780643 0.4947343  0.5992488  0.82144105 1.134592
 1.6709605  2.230223   2.9646165  2.8353996  2.5959275  2.423989
 2.2481513  1.9174224  1.6870853  1.2029811  0.50131726 0.4704128
 0.6213368  0.94469935 1.1585124  1.4636217  1.7143486  1.876566
 1.8453386  2.2054913  2.288376   2.2110567  2.1119008  1.9640181
 1.6589694  1.2982255  1.253639   1.1876075  0.9512014  0.9463545
 0.6031233  0.32951897 0.41203365 0.64770293 1.028712   1.2388666
 1.5252103  1.8918679  2.029576   1.9882498  1.835618   1.6590579
 0.9333198  0.669025   1.4941882  1.350762   1.4255888  1.5095038
 1.2456675  1.2654909  1.1640393  0.97299993 0.7141061  0.6124558
 0.63723713 0.6334406  0.49669462 0.36048818 0.36606824 0.41525838
 0.63419443 0.8971097  1.3394538  1.8780093  2.385939   3.1481423
 3.3876884  3.0903604  2.8531232  2.4686115  2.0683584  1.561457
 1.0347122  0.41160548 0.36367807 0.6547473  0.9604085  1.1591493
 1.5013555  1.8086102  1.8681936  2.0086522  2.3019753  2.4233992
 2.4109335  2.2934365  2.140281   1.7971864  1.6191     1.1934104
 1.1359624  1.143427   0.99380577 0.81419516 0.71040905 0.714921
 0.9662631  1.2098286  1.3000883  1.7254018  2.026362   2.079941
 1.9246624  1.6965892  1.4876198  0.69341254 1.0540881  1.0171909
 1.0678166  1.4317583  1.2022265  1.1945896  1.3267052  1.1135154
 0.8052225  0.67113006 0.582251   0.5003557  0.5940824  0.39707828
 0.29024878 0.43277735 0.62185156 0.92697406 1.2547872  1.7271053
 2.2265265  2.6753016  3.4224756  3.6919622  3.7173245  3.300349
 2.7652855  2.0870957  1.6148608  1.1552451  0.7527182  0.5809986
 0.8763903  0.88733333 1.144801   1.6217719  1.8533896  1.7900867
 2.1977785  2.4879482  2.6091022  2.590482   2.4720194  2.2411652
 2.1074724  1.7284803  1.4854574  1.6050613  1.6467797  1.5361955
 1.3996464  1.1530446  1.1173227  1.2326232  1.2009704  1.4287702
 1.8959827  2.1281161  2.1184585  1.9435627  1.6161565  1.2704302
 0.66123086 0.8188     0.843604   1.1592615  1.2377827  1.135417
 1.3076264  1.400216   1.0682894  0.92494714 0.51027805 0.40569353
 0.6245827  0.58002645 0.41520637 0.5025906  0.76931363 1.0736171
 1.4252977  1.7576654  1.8515788  2.0052834 ]
