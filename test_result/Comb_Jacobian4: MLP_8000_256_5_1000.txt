time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.88%, model saved.
Epoch: 0 Train: 59153.72656 Test: 3710.39746
Epoch: 80 Train: 10684.89062 Test: 1144.43469
Epoch 160: New minimal relative error: 38.67%, model saved.
Epoch: 160 Train: 1453.45764 Test: 80.52234
Epoch: 240 Train: 187.82230 Test: 28.59012
Epoch 320: New minimal relative error: 29.34%, model saved.
Epoch: 320 Train: 172.32980 Test: 49.25112
Epoch: 400 Train: 68.09066 Test: 16.33176
Epoch 480: New minimal relative error: 15.70%, model saved.
Epoch: 480 Train: 73.14687 Test: 6.03206
Epoch: 560 Train: 37.22144 Test: 14.09498
Epoch 640: New minimal relative error: 13.14%, model saved.
Epoch: 640 Train: 26.46226 Test: 4.82593
Epoch 720: New minimal relative error: 11.95%, model saved.
Epoch: 720 Train: 16.68229 Test: 3.51820
Epoch: 800 Train: 109.38994 Test: 40.46931
Epoch: 880 Train: 15.98535 Test: 3.58546
Epoch: 960 Train: 31.49023 Test: 6.55032
Epoch 1040: New minimal relative error: 2.89%, model saved.
Epoch: 1040 Train: 7.28482 Test: 1.04243
Epoch: 1120 Train: 11.32746 Test: 2.78065
Epoch: 1200 Train: 10.29186 Test: 2.58149
Epoch: 1280 Train: 7.76513 Test: 1.48294
Epoch: 1360 Train: 8.11665 Test: 1.68936
Epoch: 1440 Train: 45.40000 Test: 19.38181
Epoch: 1520 Train: 9.12259 Test: 3.24078
Epoch: 1600 Train: 12.11830 Test: 3.17932
Epoch: 1680 Train: 18.38589 Test: 4.25002
Epoch: 1760 Train: 9.63077 Test: 5.10171
Epoch: 1840 Train: 7.69357 Test: 1.84410
Epoch: 1920 Train: 9.74671 Test: 1.87867
Epoch: 2000 Train: 8.57409 Test: 3.53905
Epoch: 2080 Train: 19.29587 Test: 4.54675
Epoch: 2160 Train: 26.88497 Test: 9.28074
Epoch 2240: New minimal relative error: 0.41%, model saved.
Epoch: 2240 Train: 2.30369 Test: 1.04643
Epoch: 2320 Train: 2.25714 Test: 1.13404
Epoch: 2400 Train: 2.68201 Test: 1.48237
Epoch: 2480 Train: 1.95727 Test: 1.24286
Epoch: 2560 Train: 2.92142 Test: 1.45233
Epoch: 2640 Train: 5.61148 Test: 2.34500
Epoch: 2720 Train: 11.85157 Test: 6.21895
Epoch: 2800 Train: 4.72725 Test: 2.82485
Epoch: 2880 Train: 8.46919 Test: 3.28742
Epoch: 2960 Train: 25.82004 Test: 8.98704
Epoch: 3040 Train: 19.43327 Test: 11.20312
Epoch: 3120 Train: 4.93835 Test: 2.47363
Epoch: 3200 Train: 11.14276 Test: 3.91954
Epoch: 3280 Train: 33.41850 Test: 18.58401
Epoch: 3360 Train: 26.90507 Test: 8.18921
Epoch: 3440 Train: 1.46913 Test: 1.48376
Epoch: 3520 Train: 1.81335 Test: 1.58731
Epoch: 3600 Train: 18.25854 Test: 7.42074
Epoch: 3680 Train: 1.11660 Test: 1.48635
Epoch: 3760 Train: 1.49397 Test: 1.55317
Epoch: 3840 Train: 13.79533 Test: 4.94938
Epoch: 3920 Train: 1.03156 Test: 1.53270
Epoch: 4000 Train: 2.04420 Test: 1.69239
Epoch: 4080 Train: 0.91328 Test: 1.52708
Epoch: 4160 Train: 0.98167 Test: 1.55369
Epoch: 4240 Train: 2.12167 Test: 2.05590
Epoch: 4320 Train: 0.86311 Test: 1.58505
Epoch: 4400 Train: 0.85030 Test: 1.61850
Epoch: 4480 Train: 2.73277 Test: 2.13180
Epoch: 4560 Train: 2.54578 Test: 2.33980
Epoch: 4640 Train: 0.80395 Test: 1.66161
Epoch: 4720 Train: 0.86145 Test: 1.66370
Epoch: 4800 Train: 0.93256 Test: 1.70734
Epoch: 4880 Train: 3.80075 Test: 2.61036
Epoch: 4960 Train: 0.69113 Test: 1.68682
Epoch: 5040 Train: 1.34831 Test: 2.03635
Epoch: 5120 Train: 1.11316 Test: 1.79804
Epoch: 5200 Train: 1.27912 Test: 1.93756
Epoch: 5280 Train: 0.65663 Test: 1.72406
Epoch: 5360 Train: 0.86640 Test: 1.83690
Epoch: 5440 Train: 11.16721 Test: 5.67371
Epoch: 5520 Train: 0.58176 Test: 1.74270
Epoch: 5600 Train: 0.93370 Test: 1.77460
Epoch: 5680 Train: 0.57937 Test: 1.77854
Epoch: 5760 Train: 6.09015 Test: 3.41303
Epoch: 5840 Train: 0.54738 Test: 1.77191
Epoch: 5920 Train: 0.71116 Test: 1.82948
Epoch: 6000 Train: 0.53910 Test: 1.78883
Epoch: 6080 Train: 3.03326 Test: 2.36691
Epoch: 6160 Train: 0.51936 Test: 1.79763
Epoch: 6240 Train: 8.13294 Test: 4.41029
Epoch: 6320 Train: 0.47817 Test: 1.79465
Epoch: 6400 Train: 0.52901 Test: 1.83748
Epoch: 6480 Train: 0.50648 Test: 1.81833
Epoch: 6560 Train: 3.08149 Test: 2.39601
Epoch: 6640 Train: 0.46574 Test: 1.82929
Epoch: 6720 Train: 0.44001 Test: 1.82682
Epoch: 6800 Train: 0.58971 Test: 1.89522
Epoch: 6880 Train: 0.44517 Test: 1.84221
Epoch: 6960 Train: 3.49999 Test: 3.10268
Epoch: 7040 Train: 0.40799 Test: 1.83925
Epoch: 7120 Train: 0.82417 Test: 1.91039
Epoch 7200: New minimal relative error: 0.28%, model saved.
Epoch: 7200 Train: 0.39520 Test: 1.84635
Epoch: 7280 Train: 0.42252 Test: 1.87809
Epoch: 7360 Train: 6.85672 Test: 3.89673
Epoch: 7440 Train: 0.37642 Test: 1.86494
Epoch: 7520 Train: 1.98407 Test: 2.69043
Epoch: 7600 Train: 0.36642 Test: 1.86591
Epoch: 7680 Train: 0.48576 Test: 1.91660
Epoch: 7760 Train: 0.35379 Test: 1.87769
Epoch: 7840 Train: 0.52021 Test: 1.94639
Epoch: 7920 Train: 0.34356 Test: 1.88713
Epoch: 7999 Train: 0.36391 Test: 1.90226
Training Loss: tensor(0.3639)
Test Loss: tensor(1.9023)
Learned LE: [ 8.8192451e-01  1.7423290e-04 -1.4579426e+01]
True LE: [ 8.6743355e-01  1.1959025e-02 -1.4549875e+01]
Relative Error: [0.5356432  0.51355135 0.45382854 0.35740942 0.23676676 0.10807363
 0.04210132 0.16262859 0.29617426 0.42772168 0.52950734 0.5733014
 0.57215154 0.5627416  0.5476064  0.5128357  0.4691375  0.43420318
 0.41220045 0.40256548 0.4058918  0.41627553 0.41801244 0.40151078
 0.3828692  0.3915477  0.43025663 0.47449198 0.51797056 0.5570705
 0.54280525 0.47036782 0.4303275  0.4146177  0.38572472 0.38686144
 0.41929275 0.44663227 0.4487381  0.4261025  0.39068028 0.35733998
 0.33469144 0.32254994 0.3171187  0.31647894 0.32114545 0.3328166
 0.35195914 0.37829244 0.4092964  0.44151464 0.46994722 0.49007392
 0.49799016 0.49173477 0.47275802 0.44648463 0.4215143  0.40714723
 0.40891102 0.42502204 0.44579372 0.4551126  0.43600374 0.37930593
 0.2890226  0.1808375  0.07380731 0.05009639 0.14509399 0.25080407
 0.35498506 0.4264412  0.44293398 0.43217418 0.42323753 0.40265438
 0.36825413 0.33899838 0.31744206 0.29864982 0.28884065 0.29965347
 0.329247   0.35035115 0.338262   0.31092748 0.3139802  0.3540307
 0.39006963 0.4104332  0.43682373 0.4324458  0.36392447 0.32960677
 0.32235965 0.29687145 0.30538264 0.34191456 0.3672454  0.36586818
 0.34141016 0.30856436 0.28045624 0.26147163 0.2481224  0.23773135
 0.23105371 0.23000787 0.23599946 0.2500298  0.27192497 0.29979566
 0.33006343 0.35865995 0.3806296  0.392071   0.39033142 0.3758724
 0.352891   0.32969287 0.31568637 0.3171459  0.33319002 0.3566159
 0.37231702 0.36269894 0.31754446 0.24111374 0.150548   0.0666086
 0.03742554 0.09841673 0.17829493 0.26433575 0.32208353 0.3274977
 0.3128549  0.3010381  0.27703553 0.25266495 0.23955198 0.22293814
 0.19952871 0.18415153 0.19340461 0.2360865  0.28425568 0.2891352
 0.2513297  0.23098113 0.26633704 0.3101459  0.32026637 0.3248459
 0.3347684  0.28265825 0.24153239 0.24495006 0.21956295 0.2251215
 0.26207155 0.28867304 0.28925413 0.26872623 0.24143097 0.21911061
 0.20226745 0.1872981  0.17349388 0.16310968 0.15782139 0.15826303
 0.1653972  0.18007393 0.20175207 0.22812329 0.2551055  0.2786095
 0.2942037  0.29863575 0.2900536  0.27096286 0.24832615 0.23192978
 0.22886337 0.24001159 0.26159757 0.28308958 0.28805164 0.2627521
 0.20753741 0.13737208 0.07332568 0.0326633  0.04460017 0.09786381
 0.1720501  0.2317979  0.23911875 0.21823105 0.19960396 0.17424478
 0.16079494 0.16084528 0.14565192 0.11992706 0.10610995 0.10984465
 0.14311959 0.20730706 0.25078192 0.22679378 0.17286682 0.16763265
 0.21590702 0.2466895  0.23880178 0.23846477 0.22621283 0.17024486
 0.17783707 0.15977457 0.14675565 0.17910948 0.21024513 0.21962295
 0.20874192 0.19019084 0.17450924 0.16153838 0.14794181 0.13463244
 0.12386379 0.11677173 0.11285058 0.11262025 0.11798693 0.13061465
 0.14977913 0.17255422 0.19530864 0.21428873 0.22557843 0.22547646
 0.21246903 0.19000702 0.16710728 0.15456036 0.1559719  0.1684213
 0.18760823 0.20351312 0.20183423 0.17431003 0.12841909 0.0817917
 0.05056123 0.0367261  0.04173185 0.08807658 0.15165782 0.17955062
 0.16158848 0.13684599 0.10982796 0.09839626 0.11204804 0.10584034
 0.08131416 0.07481535 0.07346686 0.07312463 0.11249386 0.18070926
 0.21617335 0.17718346 0.11539341 0.11037993 0.1527103  0.17858206
 0.16609323 0.16139375 0.14028434 0.11060761 0.12482189 0.08806487
 0.09419076 0.12815039 0.15115887 0.15665992 0.1502706  0.14348452
 0.13775212 0.1295407  0.1187934  0.10853045 0.10048815 0.0948476
 0.09125055 0.0907757  0.09567934 0.10682909 0.12274188 0.1410476
 0.15953185 0.17591079 0.18660511 0.18693243 0.17367843 0.14879669
 0.12156645 0.10460385 0.10219603 0.10769135 0.1169843  0.12532683
 0.12311793 0.10523376 0.07874282 0.05846144 0.05428584 0.0567194
 0.05607299 0.07792942 0.12136073 0.14044853 0.12742981 0.1041677
 0.0757494  0.08150266 0.10030738 0.08671521 0.06885772 0.0712207
 0.07148148 0.05119557 0.06961822 0.13830677 0.17791669 0.15024258
 0.08473823 0.06260511 0.08750118 0.11402296 0.11162394 0.10211498
 0.09315774 0.07862498 0.08597785 0.04010462 0.04211975 0.0725572
 0.09761116 0.11031429 0.11543215 0.12030939 0.12096217 0.11451371
 0.10365365 0.0929633  0.08532661 0.08081811 0.07945488 0.08263467
 0.09087016 0.10231508 0.11460032 0.12688996 0.13951235 0.15283388
 0.16516061 0.1720035  0.16773681 0.14972647 0.12204372 0.09640206
 0.0830768  0.07881588 0.07448365 0.06749631 0.05890776 0.04848772
 0.03980803 0.04113207 0.0537003  0.06499535 0.06709419 0.06849727
 0.08553109 0.11535315 0.12335448 0.10931761 0.07514694 0.06858759
 0.08903991 0.08778113 0.07367459 0.06242167]
