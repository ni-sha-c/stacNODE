time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.96%, model saved.
Epoch: 0 Train: 31591.69922 Test: 3889.91357
Epoch 80: New minimal relative error: 57.24%, model saved.
Epoch: 80 Train: 6036.28271 Test: 1184.83728
Epoch: 160 Train: 3773.86548 Test: 349.10339
Epoch 240: New minimal relative error: 14.72%, model saved.
Epoch: 240 Train: 37.38091 Test: 22.97436
Epoch: 320 Train: 89.27642 Test: 29.78713
Epoch: 400 Train: 64.36608 Test: 47.88059
Epoch: 480 Train: 53.67375 Test: 8.88842
Epoch 560: New minimal relative error: 13.98%, model saved.
Epoch: 560 Train: 4.19070 Test: 2.69319
Epoch: 640 Train: 43.31440 Test: 19.20585
Epoch 720: New minimal relative error: 10.81%, model saved.
Epoch: 720 Train: 7.20336 Test: 5.48005
Epoch: 800 Train: 43.57523 Test: 10.61850
Epoch 880: New minimal relative error: 5.56%, model saved.
Epoch: 880 Train: 2.51149 Test: 0.31817
Epoch: 960 Train: 3.07815 Test: 0.16883
Epoch: 1040 Train: 3.04590 Test: 2.11203
Epoch: 1120 Train: 12.96115 Test: 5.53064
Epoch: 1200 Train: 13.97824 Test: 7.15664
Epoch: 1280 Train: 10.04439 Test: 4.31370
Epoch 1360: New minimal relative error: 4.48%, model saved.
Epoch: 1360 Train: 1.26123 Test: 0.84605
Epoch: 1440 Train: 13.49201 Test: 3.37635
Epoch: 1520 Train: 5.87955 Test: 2.62463
Epoch: 1600 Train: 5.50237 Test: 1.04958
Epoch 1680: New minimal relative error: 2.91%, model saved.
Epoch: 1680 Train: 1.51385 Test: 0.08367
Epoch: 1760 Train: 1.15196 Test: 0.60271
Epoch: 1840 Train: 4.02977 Test: 0.34018
Epoch: 1920 Train: 4.04824 Test: 0.62740
Epoch: 2000 Train: 7.80426 Test: 1.27899
Epoch: 2080 Train: 27.71985 Test: 10.63417
Epoch 2160: New minimal relative error: 1.40%, model saved.
Epoch: 2160 Train: 0.66669 Test: 0.10605
Epoch: 2240 Train: 4.43022 Test: 1.10980
Epoch: 2320 Train: 8.59933 Test: 3.57996
Epoch: 2400 Train: 1.06271 Test: 0.06968
Epoch: 2480 Train: 22.44452 Test: 6.96906
Epoch: 2560 Train: 0.32150 Test: 0.03920
Epoch: 2640 Train: 3.08638 Test: 0.78972
Epoch: 2720 Train: 9.90353 Test: 2.51633
Epoch: 2800 Train: 0.31054 Test: 0.04665
Epoch: 2880 Train: 3.17776 Test: 0.67832
Epoch: 2960 Train: 8.37813 Test: 3.54759
Epoch: 3040 Train: 1.92721 Test: 0.76521
Epoch: 3120 Train: 0.23943 Test: 0.03325
Epoch: 3200 Train: 0.41696 Test: 0.03544
Epoch: 3280 Train: 4.37386 Test: 1.15826
Epoch: 3360 Train: 0.50242 Test: 0.05262
Epoch: 3440 Train: 1.41167 Test: 0.25070
Epoch: 3520 Train: 0.56182 Test: 0.19240
Epoch: 3600 Train: 0.95493 Test: 0.26802
Epoch: 3680 Train: 0.43014 Test: 0.13699
Epoch: 3760 Train: 1.18088 Test: 0.37973
Epoch: 3840 Train: 1.73732 Test: 0.62555
Epoch: 3920 Train: 1.14304 Test: 0.30027
Epoch: 4000 Train: 2.20695 Test: 0.80344
Epoch: 4080 Train: 2.90400 Test: 1.23364
Epoch: 4160 Train: 2.63793 Test: 1.23004
Epoch: 4240 Train: 2.65552 Test: 1.01016
Epoch: 4320 Train: 0.65052 Test: 0.25191
Epoch: 4400 Train: 9.26357 Test: 3.25983
Epoch: 4480 Train: 7.40172 Test: 2.89855
Epoch: 4560 Train: 6.91902 Test: 2.86140
Epoch: 4640 Train: 6.18100 Test: 2.00549
Epoch: 4720 Train: 0.63677 Test: 0.15866
Epoch: 4800 Train: 0.77171 Test: 0.19456
Epoch: 4880 Train: 0.48937 Test: 0.11225
Epoch: 4960 Train: 3.09650 Test: 1.09428
Epoch: 5040 Train: 1.11686 Test: 0.22595
Epoch: 5120 Train: 3.32100 Test: 1.29734
Epoch: 5200 Train: 0.10065 Test: 0.01911
Epoch 5280: New minimal relative error: 0.29%, model saved.
Epoch: 5280 Train: 0.09765 Test: 0.01817
Epoch: 5360 Train: 3.18904 Test: 1.00622
Epoch: 5440 Train: 3.09008 Test: 1.30501
Epoch: 5520 Train: 2.00365 Test: 0.70738
Epoch: 5600 Train: 1.94389 Test: 0.55520
Epoch: 5680 Train: 0.09311 Test: 0.02015
Epoch: 5760 Train: 0.17246 Test: 0.02731
Epoch: 5840 Train: 0.67842 Test: 0.18250
Epoch: 5920 Train: 0.32548 Test: 0.09730
Epoch: 6000 Train: 1.21970 Test: 0.51180
Epoch: 6080 Train: 0.29582 Test: 0.12043
Epoch: 6160 Train: 0.19088 Test: 0.06107
Epoch: 6240 Train: 0.68724 Test: 0.27781
Epoch: 6320 Train: 0.99164 Test: 0.28015
Epoch: 6400 Train: 0.57761 Test: 0.24639
Epoch: 6480 Train: 2.89487 Test: 0.98371
Epoch: 6560 Train: 3.60482 Test: 1.27503
Epoch: 6640 Train: 0.25408 Test: 0.10515
Epoch: 6720 Train: 0.09726 Test: 0.02231
Epoch: 6800 Train: 1.56907 Test: 0.77225
Epoch: 6880 Train: 0.05997 Test: 0.01628
Epoch: 6960 Train: 0.74958 Test: 0.22012
Epoch: 7040 Train: 0.50520 Test: 0.17119
Epoch: 7120 Train: 1.20852 Test: 0.48799
Epoch: 7200 Train: 0.05731 Test: 0.01675
Epoch: 7280 Train: 0.50046 Test: 0.12637
Epoch: 7360 Train: 0.82595 Test: 0.35411
Epoch: 7440 Train: 0.07830 Test: 0.02947
Epoch: 7520 Train: 0.06548 Test: 0.01770
Epoch: 7600 Train: 3.36045 Test: 1.21753
Epoch: 7680 Train: 0.67629 Test: 0.25084
Epoch: 7760 Train: 0.16569 Test: 0.04016
Epoch: 7840 Train: 0.12029 Test: 0.02790
Epoch 7920: New minimal relative error: 0.10%, model saved.
Epoch: 7920 Train: 0.04767 Test: 0.01553
Epoch: 7999 Train: 0.17181 Test: 0.04370
Training Loss: tensor(0.1718)
Test Loss: tensor(0.0437)
Learned LE: [  0.8638789   -0.03391454 -14.574287  ]
True LE: [ 8.4948552e-01 -5.7180447e-04 -1.4526743e+01]
Relative Error: [0.08563048 0.08697519 0.085232   0.08171596 0.07837848 0.07689437
 0.07763954 0.08051892 0.08541249 0.09133511 0.09653918 0.09849603
 0.09619997 0.08998846 0.08130264 0.07185204 0.06326715 0.05774554
 0.05860339 0.06884588 0.08742946 0.11006049 0.1323449  0.15051705
 0.16242169 0.16800332 0.16828042 0.1653529  0.16143857 0.15850143
 0.15720192 0.15785976 0.16006336 0.16205919 0.16192155 0.15737556
 0.14752544 0.13460131 0.12239429 0.1148619  0.11368486 0.11709428
 0.12064862 0.12064405 0.11476611 0.1031689  0.08741335 0.06996881
 0.05334705 0.03963103 0.03037396 0.02587487 0.02563905 0.02826079
 0.03272118 0.03832967 0.04495528 0.05240952 0.06047171 0.06883975
 0.07695722 0.08371592 0.0882245  0.08954642 0.08766259 0.0837139
 0.07977063 0.07758355 0.0778701  0.08074387 0.08606033 0.09227922
 0.09671621 0.09724909 0.0931178  0.08515754 0.07524596 0.06528588
 0.05667655 0.05165219 0.05395331 0.06645352 0.08708804 0.11090829
 0.13283873 0.14910957 0.15822884 0.16055457 0.15827361 0.15385649
 0.14951405 0.1470271  0.14707544 0.14889708 0.15210965 0.15467077
 0.15451342 0.14928329 0.13852203 0.1253429  0.11444732 0.10967129
 0.11155886 0.11715549 0.12150806 0.12058034 0.11276982 0.0989797
 0.08118452 0.06233837 0.0453248  0.03240702 0.02515912 0.02358482
 0.02577122 0.02993804 0.03498732 0.04074393 0.0471307  0.05414306
 0.06178442 0.0698556  0.07767446 0.08461757 0.08919311 0.09062454
 0.08865467 0.08445112 0.080043   0.07727063 0.07719745 0.08028716
 0.08590668 0.09209783 0.0959546  0.0952117  0.08960263 0.08054496
 0.07012861 0.06018808 0.05197275 0.04737168 0.05074682 0.06518322
 0.08740074 0.11152691 0.13220994 0.1461351  0.15192184 0.15103231
 0.14637907 0.14107679 0.1372841  0.13613546 0.13768822 0.14095502
 0.14505602 0.14807172 0.14768957 0.1414452  0.12970607 0.11641832
 0.10672605 0.10440651 0.10904147 0.11633484 0.12075055 0.11882612
 0.10916363 0.09324954 0.07373315 0.0540352  0.03722908 0.02584578
 0.02143759 0.02281649 0.02679218 0.03173536 0.03698339 0.04252976
 0.04839383 0.05474144 0.06172026 0.06910393 0.07674961 0.08360453
 0.08841991 0.08995035 0.08809323 0.08364396 0.07888646 0.07566249
 0.07541051 0.07864033 0.08462543 0.09085093 0.09416276 0.09255137
 0.08608441 0.0765776  0.06641652 0.05727889 0.04968938 0.04543484
 0.04944262 0.06494334 0.08785481 0.11175194 0.13075827 0.14141892
 0.14353856 0.13954476 0.1332578  0.12784855 0.12516883 0.12593772
 0.12936088 0.13396268 0.13888353 0.14204524 0.1411528  0.13372186
 0.12097235 0.10737848 0.09898286 0.09893527 0.10575622 0.11438492
 0.11881316 0.11581796 0.10443187 0.08682163 0.06622203 0.04630281
 0.03024716 0.02109995 0.02004421 0.0235379  0.02846697 0.03351728
 0.03864685 0.04362587 0.04883141 0.05411544 0.06003183 0.06664351
 0.07379225 0.08076365 0.08554289 0.08741135 0.08571593 0.0814405
 0.07647902 0.07286938 0.07232761 0.07568495 0.08193177 0.08823476
 0.09135411 0.08945467 0.08302692 0.07408439 0.06484227 0.05662482
 0.04963422 0.04551845 0.04966132 0.06567913 0.08878931 0.11170629
 0.12807623 0.13504964 0.13332449 0.12658408 0.11915302 0.11446688
 0.11366144 0.11683965 0.12201271 0.12778379 0.13287663 0.1359719
 0.13436845 0.1258929  0.11202192 0.09849299 0.09114198 0.09334154
 0.10191802 0.11146613 0.11578368 0.1119245  0.09940004 0.08093674
 0.05998174 0.04038656 0.02560495 0.01908771 0.02057962 0.02507817
 0.03005081 0.03494364 0.03958404 0.04384468 0.04803618 0.05233886
 0.05705201 0.06254342 0.06908871 0.07568435 0.08077352 0.08281801
 0.08158743 0.07764696 0.07278129 0.06900982 0.06814159 0.07136063
 0.07766319 0.08411309 0.08743207 0.08600841 0.08047785 0.07284915
 0.06493821 0.05764643 0.05104666 0.04719302 0.05121708 0.0670892
 0.08991187 0.1112739  0.12451452 0.12726852 0.12133546 0.1122097
 0.10485258 0.10176063 0.10328102 0.10861841 0.11538503 0.12149221
 0.12646489 0.1292237  0.12686992 0.11741101 0.10269883 0.08941528
 0.08335202 0.08715525 0.09735505 0.10777008 0.1120678  0.10780624
 0.09500474 0.07645015 0.05582087 0.03699507 0.02365682 0.01909997
 0.02164266 0.02627916 0.03098058 0.03542827 0.03946132 0.0428565
 0.04609664 0.04954303 0.05296406 0.05717963 0.06250988 0.06886525
 0.07405961 0.07650539 0.07579233 0.07253119 0.06808946 0.06423063
 0.06291319 0.06554142 0.07164941 0.07823088 0.08221428 0.082085
 0.07839821 0.07263751 0.06606753 0.05935825 0.05294064 0.04901562
 0.0531356  0.06893002 0.09112617 0.11072642 0.12038981 0.11832309
 0.10818504 0.09693729 0.09030384 0.08973368]
