time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 98.95%, model saved.
Epoch: 0 Train: 9544.94434 Test: 3982.19263
Epoch: 80 Train: 2354.22852 Test: 977.33698
Epoch: 160 Train: 1335.19214 Test: 452.38934
Epoch: 240 Train: 877.49182 Test: 227.96048
Epoch: 320 Train: 430.27133 Test: 147.46576
Epoch 400: New minimal relative error: 26.07%, model saved.
Epoch: 400 Train: 237.58113 Test: 47.19928
Epoch: 480 Train: 404.14008 Test: 69.46600
Epoch 560: New minimal relative error: 10.23%, model saved.
Epoch: 560 Train: 120.00449 Test: 13.37153
Epoch: 640 Train: 124.51785 Test: 20.89194
Epoch 720: New minimal relative error: 9.76%, model saved.
Epoch: 720 Train: 62.73995 Test: 5.51986
Epoch 800: New minimal relative error: 3.98%, model saved.
Epoch: 800 Train: 57.80026 Test: 6.92873
Epoch: 880 Train: 55.07593 Test: 5.69117
Epoch: 960 Train: 50.68927 Test: 6.91931
Epoch: 1040 Train: 39.70849 Test: 8.05361
Epoch: 1120 Train: 41.12684 Test: 3.72319
Epoch: 1200 Train: 52.25304 Test: 20.67183
Epoch: 1280 Train: 25.26235 Test: 0.78537
Epoch: 1360 Train: 46.47802 Test: 3.96012
Epoch 1440: New minimal relative error: 3.14%, model saved.
Epoch: 1440 Train: 22.30660 Test: 0.77994
Epoch: 1520 Train: 25.73217 Test: 2.55218
Epoch: 1600 Train: 21.86490 Test: 1.12265
Epoch 1680: New minimal relative error: 1.74%, model saved.
Epoch: 1680 Train: 22.72859 Test: 0.93152
Epoch: 1760 Train: 18.52530 Test: 0.59034
Epoch: 1840 Train: 19.24037 Test: 2.21045
Epoch: 1920 Train: 19.03736 Test: 0.95124
Epoch: 2000 Train: 21.13584 Test: 0.68385
Epoch: 2080 Train: 17.30978 Test: 1.35415
Epoch: 2160 Train: 15.78177 Test: 0.69299
Epoch 2240: New minimal relative error: 1.48%, model saved.
Epoch: 2240 Train: 14.89648 Test: 0.55602
Epoch: 2320 Train: 14.00926 Test: 0.33911
Epoch: 2400 Train: 14.75246 Test: 0.39330
Epoch: 2480 Train: 18.85410 Test: 3.82307
Epoch: 2560 Train: 18.86386 Test: 6.82571
Epoch: 2640 Train: 11.64041 Test: 0.41568
Epoch: 2720 Train: 11.64781 Test: 0.20171
Epoch: 2800 Train: 16.97300 Test: 1.41418
Epoch: 2880 Train: 12.94304 Test: 2.38818
Epoch: 2960 Train: 14.54225 Test: 4.86062
Epoch: 3040 Train: 10.07611 Test: 0.18814
Epoch 3120: New minimal relative error: 0.97%, model saved.
Epoch: 3120 Train: 10.01522 Test: 0.14630
Epoch: 3200 Train: 9.74481 Test: 0.15959
Epoch: 3280 Train: 9.82018 Test: 0.37343
Epoch: 3360 Train: 9.59285 Test: 0.21285
Epoch: 3440 Train: 9.46814 Test: 0.17640
Epoch: 3520 Train: 9.88229 Test: 0.43158
Epoch: 3600 Train: 9.06164 Test: 0.93395
Epoch: 3680 Train: 8.74262 Test: 0.21998
Epoch: 3760 Train: 8.25086 Test: 0.15329
Epoch: 3840 Train: 8.34921 Test: 0.11178
Epoch: 3920 Train: 9.83165 Test: 0.40310
Epoch: 4000 Train: 8.60760 Test: 0.90586
Epoch: 4080 Train: 7.64843 Test: 0.07071
Epoch: 4160 Train: 7.38947 Test: 0.06815
Epoch: 4240 Train: 7.63080 Test: 0.20011
Epoch: 4320 Train: 9.50432 Test: 0.75363
Epoch: 4400 Train: 6.82050 Test: 0.08220
Epoch: 4480 Train: 6.75132 Test: 0.05660
Epoch: 4560 Train: 7.27960 Test: 0.16855
Epoch: 4640 Train: 7.17048 Test: 0.64109
Epoch: 4720 Train: 6.55445 Test: 0.06254
Epoch: 4800 Train: 6.65983 Test: 0.07725
Epoch: 4880 Train: 7.23115 Test: 0.83899
Epoch: 4960 Train: 6.29059 Test: 0.05989
Epoch: 5040 Train: 6.22338 Test: 0.05580
Epoch: 5120 Train: 7.21310 Test: 0.76846
Epoch: 5200 Train: 6.15008 Test: 0.05671
Epoch: 5280 Train: 6.11671 Test: 0.09496
Epoch: 5360 Train: 6.07889 Test: 0.07375
Epoch: 5440 Train: 6.44844 Test: 0.45049
Epoch: 5520 Train: 6.03599 Test: 0.09996
Epoch: 5600 Train: 5.91454 Test: 0.08122
Epoch: 5680 Train: 5.96412 Test: 0.10307
Epoch: 5760 Train: 5.68879 Test: 0.09089
Epoch: 5840 Train: 5.93232 Test: 0.10804
Epoch: 5920 Train: 6.36985 Test: 0.48132
Epoch: 6000 Train: 8.67943 Test: 1.96707
Epoch: 6080 Train: 5.65621 Test: 0.04607
Epoch: 6160 Train: 5.57382 Test: 0.09384
Epoch: 6240 Train: 6.19776 Test: 0.50523
Epoch: 6320 Train: 5.59740 Test: 0.11615
Epoch: 6400 Train: 5.36311 Test: 0.07938
Epoch: 6480 Train: 5.37238 Test: 0.07026
Epoch: 6560 Train: 5.35157 Test: 0.19164
Epoch: 6640 Train: 6.11690 Test: 1.04786
Epoch: 6720 Train: 6.04472 Test: 0.84292
Epoch: 6800 Train: 5.80212 Test: 0.19487
Epoch: 6880 Train: 5.63089 Test: 0.36819
Epoch: 6960 Train: 5.61118 Test: 0.31030
Epoch: 7040 Train: 6.13403 Test: 0.60778
Epoch: 7120 Train: 5.34968 Test: 0.08813
Epoch: 7200 Train: 5.30546 Test: 0.08925
Epoch: 7280 Train: 5.25874 Test: 0.11557
Epoch: 7360 Train: 5.12847 Test: 0.06542
Epoch: 7440 Train: 5.58376 Test: 0.04032
Epoch: 7520 Train: 5.10729 Test: 0.19727
Epoch: 7600 Train: 5.04057 Test: 0.03742
Epoch: 7680 Train: 6.57473 Test: 1.54039
Epoch: 7760 Train: 4.93644 Test: 0.04414
Epoch: 7840 Train: 5.04009 Test: 0.07194
Epoch: 7920 Train: 4.98864 Test: 0.04979
Epoch: 7999 Train: 4.99437 Test: 0.03447
Training Loss: tensor(4.9944)
Test Loss: tensor(0.0345)
Learned LE: [  0.8186537    0.05098422 -14.540413  ]
True LE: [ 8.5297197e-01  4.6953312e-03 -1.4535044e+01]
Relative Error: [1.0935996  1.1538516  1.3962176  1.5892162  1.8338057  1.9954689
 2.2635357  2.6226826  2.2457225  2.157767   2.1690183  2.090714
 2.097286   2.2113338  2.2004209  2.1744034  2.339997   2.2731059
 1.9995073  1.4420666  1.2832788  1.3323625  1.3617374  1.3937429
 1.4963207  1.7830136  1.8142674  1.8454973  1.8344028  2.1118896
 2.1018112  2.292671   2.313463   2.097228   1.9740651  1.9780931
 1.8762121  1.7199489  1.5164132  1.4672918  1.3708369  1.1828724
 1.081914   1.3891549  1.7829971  2.0608783  2.610854   3.2243657
 3.646546   3.7546966  3.6470964  3.2707882  2.5964897  1.9736816
 1.2846736  1.0136464  0.549659   0.67612684 1.0928168  1.511001
 1.4866368  1.2766665  1.0870211  1.0082635  1.2957013  1.5769296
 1.8528994  2.0551064  2.3114967  2.4947863  2.0909321  1.8710845
 1.8599087  1.8779888  1.798246   1.7182872  1.7349527  1.7273316
 1.9677153  2.0309618  1.817996   1.4243509  1.2526176  1.1120052
 1.1433897  1.2110142  1.3733568  1.7435918  1.6853445  1.5408475
 1.3580921  1.6673025  2.2415605  2.3023481  2.602936   2.236141
 2.043134   2.0427346  2.0219457  1.9079194  1.840602   1.7409345
 1.7038932  1.3870629  0.989712   0.9927159  1.6755475  1.9526569
 2.0803542  2.7848246  3.2278497  3.5100157  3.4316034  3.1416643
 2.6552713  2.025201   1.3903599  1.1109571  0.57512593 0.5626848
 1.1646175  1.6694931  1.6092098  1.3479235  1.1363932  0.9475105
 0.9875546  1.2242857  1.5604848  1.9771625  2.361239   2.505546
 2.0690973  1.8005868  1.6003093  1.6150249  1.5392902  1.4679928
 1.38097    1.3898177  1.6246458  1.8933336  1.8195499  1.553371
 1.1273564  0.9644394  1.0264748  1.0790621  1.2396282  1.6923947
 1.6334554  1.5131139  1.1932588  1.3146782  1.8250864  2.432475
 2.4996724  2.362279   1.9261487  1.7557291  1.817626   1.720203
 1.7898271  1.8884758  1.9613063  1.872257   1.2456083  0.6448847
 1.1835544  1.7549226  1.9092418  2.040001   2.7993536  3.2880082
 3.269881   3.0261338  2.62205    2.0052779  1.4447309  1.141536
 0.5535048  0.397647   0.95275044 1.6089035  1.6822586  1.4461265
 1.1989361  0.9949446  0.802553   0.8637446  1.1503156  1.5396129
 2.0308924  2.4416637  2.1083112  1.8031952  1.635461   1.388161
 1.3246762  1.3482149  1.2437928  1.2267962  1.3592713  1.7282497
 1.9167063  1.8099527  1.3625287  1.0151175  0.97002786 0.97314036
 1.1515741  1.5355276  1.5889744  1.4804478  1.2293638  1.1078209
 1.3728502  1.8163288  2.3766913  2.3670013  1.7393811  1.3778538
 1.3574144  1.379197   1.3781382  1.6178164  1.743957   1.9981539
 2.022545   0.8919229  0.58635306 1.2069563  1.7164804  1.8644571
 2.1934752  2.7631328  2.9856632  2.9247193  2.6143115  1.9914474
 1.4390463  1.1060754  0.5227995  0.2236689  0.7356876  1.3977737
 1.78806    1.5140865  1.2955862  1.0191984  0.7306468  0.63729715
 0.7872802  1.0749266  1.5466995  2.0437913  1.9484471  1.8221544
 1.7134328  1.4776864  1.2663544  1.2754999  1.2885302  1.266011
 1.367198   1.6182939  1.9749132  2.1401775  2.0123377  1.3724967
 1.0772933  1.1103722  1.1008929  1.2561213  1.6184642  1.5324522
 1.1445954  1.0268245  1.0306174  1.1516926  1.6583353  2.123983
 1.9130636  1.2517515  0.9608214  0.9725791  0.9592467  1.1165156
 1.2935153  1.5794303  1.8253269  1.8041362  0.83276165 0.48207507
 1.1243324  1.7032835  1.7707831  2.0590682  2.5411837  2.5452108
 2.4634044  2.0850542  1.473999   0.960864   0.5335261  0.08710179
 0.45850742 1.0730867  1.7038021  1.6519251  1.3478961  1.1113464
 0.79134053 0.5102525  0.6709268  0.77956104 1.1023706  1.5681158
 1.4663267  1.4521861  1.6282512  1.6125662  1.5195254  1.393449
 1.2651446  1.0774628  0.917438   0.9678688  1.0691624  1.4048595
 1.6357297  1.714973   1.4732325  1.4900743  1.421141   1.2920188
 1.4098442  1.498781   1.3115634  0.99674034 0.90317345 0.840723
 0.97413397 1.3715893  1.8822901  1.6381131  1.1328207  0.9534339
 1.0139861  0.8703414  0.986059   1.0567037  1.344621   1.4950445
 1.367101   1.1655308  0.3465051  0.9733241  1.5339831  1.5453426
 1.7108188  2.1058571  2.0597029  1.8759426  1.4700247  1.0290102
 0.5521306  0.36302295 0.07721114 0.65881336 1.2530103  1.8274626
 1.4602011  1.1771395  0.8722457  0.56125426 0.45131063 0.79613316
 0.92555296 1.3001523  1.5629467  1.3042848  1.22977    1.3448061
 1.5152044  1.5838172  1.16324    0.7547897  0.57703924 0.5863627
 0.61570823 0.7005229  0.9535584  1.2610587  1.3829819  1.2723341
 1.2940599  1.5272818  1.7123063  1.6322358 ]
