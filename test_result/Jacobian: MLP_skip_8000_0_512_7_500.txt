time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.23%, model saved.
Epoch: 0 Train: 31187.26562 Test: 4203.67920
Epoch 80: New minimal relative error: 88.33%, model saved.
Epoch: 80 Train: 8356.60254 Test: 1473.05432
Epoch: 160 Train: 6731.51172 Test: 1110.65076
Epoch: 240 Train: 6017.95117 Test: 894.37695
Epoch: 320 Train: 6371.17236 Test: 1009.25067
Epoch 400: New minimal relative error: 58.18%, model saved.
Epoch: 400 Train: 4945.81982 Test: 755.26447
Epoch: 480 Train: 3603.81348 Test: 580.00690
Epoch: 560 Train: 3034.41919 Test: 246.11539
Epoch: 640 Train: 3452.43774 Test: 439.82889
Epoch 720: New minimal relative error: 47.71%, model saved.
Epoch: 720 Train: 1209.52356 Test: 62.96033
Epoch 800: New minimal relative error: 16.94%, model saved.
Epoch: 800 Train: 545.31653 Test: 20.06321
Epoch 880: New minimal relative error: 10.14%, model saved.
Epoch: 880 Train: 453.03336 Test: 9.26070
Epoch: 960 Train: 370.51562 Test: 15.41770
Epoch 1040: New minimal relative error: 5.35%, model saved.
Epoch: 1040 Train: 236.72501 Test: 3.17347
Epoch 1120: New minimal relative error: 4.76%, model saved.
Epoch: 1120 Train: 203.36273 Test: 3.01378
Epoch 1200: New minimal relative error: 4.07%, model saved.
Epoch: 1200 Train: 166.13104 Test: 1.89857
Epoch 1280: New minimal relative error: 3.11%, model saved.
Epoch: 1280 Train: 150.63374 Test: 1.64900
Epoch: 1360 Train: 222.15088 Test: 73.05803
Epoch: 1440 Train: 124.69481 Test: 1.18311
Epoch: 1520 Train: 104.59845 Test: 1.48421
Epoch: 1600 Train: 90.01006 Test: 1.17606
Epoch: 1680 Train: 102.44959 Test: 5.56526
Epoch: 1760 Train: 83.65735 Test: 0.93861
Epoch: 1840 Train: 77.56040 Test: 0.54299
Epoch: 1920 Train: 80.51500 Test: 0.78349
Epoch: 2000 Train: 73.21131 Test: 0.90672
Epoch: 2080 Train: 70.46577 Test: 0.88350
Epoch: 2160 Train: 72.35978 Test: 1.94801
Epoch: 2240 Train: 66.22799 Test: 0.79575
Epoch: 2320 Train: 77.16110 Test: 12.88327
Epoch: 2400 Train: 58.72531 Test: 0.38212
Epoch: 2480 Train: 60.04356 Test: 0.70016
Epoch: 2560 Train: 61.36823 Test: 0.45941
Epoch: 2640 Train: 58.58915 Test: 0.60487
Epoch 2720: New minimal relative error: 2.95%, model saved.
Epoch: 2720 Train: 55.82368 Test: 0.60569
Epoch: 2800 Train: 50.74019 Test: 2.43670
Epoch: 2880 Train: 48.67044 Test: 1.32171
Epoch: 2960 Train: 43.96847 Test: 1.22064
Epoch: 3040 Train: 45.53899 Test: 3.45467
Epoch: 3120 Train: 45.13120 Test: 0.52614
Epoch: 3200 Train: 44.00904 Test: 0.43699
Epoch: 3280 Train: 43.40393 Test: 0.27566
Epoch: 3360 Train: 45.08178 Test: 0.41170
Epoch: 3440 Train: 42.46160 Test: 0.19188
Epoch: 3520 Train: 47.17952 Test: 3.07705
Epoch: 3600 Train: 43.03480 Test: 0.77413
Epoch: 3680 Train: 39.62596 Test: 0.84051
Epoch: 3760 Train: 36.56729 Test: 0.27120
Epoch: 3840 Train: 34.85288 Test: 0.83728
Epoch: 3920 Train: 35.04757 Test: 1.16312
Epoch 4000: New minimal relative error: 2.35%, model saved.
Epoch: 4000 Train: 32.82287 Test: 0.23098
Epoch: 4080 Train: 46.59196 Test: 5.08096
Epoch: 4160 Train: 38.08578 Test: 1.87553
Epoch: 4240 Train: 37.90254 Test: 0.20701
Epoch: 4320 Train: 42.32448 Test: 1.98400
Epoch 4400: New minimal relative error: 1.29%, model saved.
Epoch: 4400 Train: 33.18243 Test: 0.29436
Epoch: 4480 Train: 33.15123 Test: 0.54263
Epoch: 4560 Train: 32.13125 Test: 0.28951
Epoch: 4640 Train: 32.64571 Test: 0.71801
Epoch: 4720 Train: 39.17680 Test: 3.30797
Epoch: 4800 Train: 31.24291 Test: 0.21995
Epoch: 4880 Train: 31.89671 Test: 0.93483
Epoch: 4960 Train: 38.46077 Test: 4.28974
Epoch: 5040 Train: 30.12259 Test: 0.31408
Epoch: 5120 Train: 27.74941 Test: 0.39561
Epoch: 5200 Train: 25.62240 Test: 0.11600
Epoch: 5280 Train: 26.45735 Test: 0.19674
Epoch: 5360 Train: 26.23023 Test: 0.15120
Epoch: 5440 Train: 29.09952 Test: 0.63542
Epoch: 5520 Train: 27.74330 Test: 0.80068
Epoch: 5600 Train: 27.13310 Test: 0.12548
Epoch: 5680 Train: 27.61874 Test: 0.11244
Epoch: 5760 Train: 24.42646 Test: 0.12592
Epoch: 5840 Train: 25.54310 Test: 0.84857
Epoch 5920: New minimal relative error: 0.70%, model saved.
Epoch: 5920 Train: 23.33009 Test: 0.08674
Epoch: 6000 Train: 22.49833 Test: 0.16526
Epoch: 6080 Train: 22.48701 Test: 0.12383
Epoch: 6160 Train: 21.83101 Test: 0.12928
Epoch: 6240 Train: 22.52377 Test: 0.17175
Epoch: 6320 Train: 22.87802 Test: 0.42442
Epoch: 6400 Train: 22.54169 Test: 0.25745
Epoch: 6480 Train: 22.78572 Test: 0.14783
Epoch: 6560 Train: 23.29725 Test: 1.25277
Epoch: 6640 Train: 23.77873 Test: 0.38267
Epoch: 6720 Train: 22.51757 Test: 0.74572
Epoch: 6800 Train: 22.18538 Test: 0.52587
Epoch: 6880 Train: 20.06042 Test: 0.16178
Epoch: 6960 Train: 18.99843 Test: 0.43825
Epoch: 7040 Train: 18.33041 Test: 0.05956
Epoch: 7120 Train: 23.85985 Test: 2.18400
Epoch: 7200 Train: 20.18532 Test: 0.07278
Epoch: 7280 Train: 19.75015 Test: 0.29777
Epoch: 7360 Train: 19.00787 Test: 0.10238
Epoch: 7440 Train: 18.48249 Test: 0.05731
Epoch: 7520 Train: 18.43774 Test: 0.05839
Epoch: 7600 Train: 18.04371 Test: 0.07417
Epoch: 7680 Train: 17.04418 Test: 0.10689
Epoch: 7760 Train: 19.72628 Test: 0.76592
Epoch: 7840 Train: 19.06006 Test: 0.05896
Epoch: 7920 Train: 19.64600 Test: 0.08093
Epoch: 7999 Train: 19.46067 Test: 0.40693
Training Loss: tensor(19.4607)
Test Loss: tensor(0.4069)
Learned LE: [ 8.7520975e-01 -3.9344127e-03 -1.4537840e+01]
True LE: [ 8.7800753e-01 -1.4378878e-03 -1.4547396e+01]
Relative Error: [2.8999038  2.6941001  2.3304563  1.972529   1.7058243  1.5846322
 1.6889907  1.6913241  1.8664399  2.189881   2.3370512  2.3172953
 2.1582937  2.0917048  2.1661363  2.478123   2.7965353  2.9738357
 2.8747904  2.7564929  2.586605   2.3933606  2.3488543  2.265341
 2.367314   2.8022156  3.3836703  3.9383233  4.0471935  4.391152
 4.8795404  5.147876   5.548236   5.9032726  5.9212575  5.7959576
 5.790739   5.7051587  4.9313884  4.1257677  3.873343   3.9735637
 4.6835294  5.5587173  6.6059403  6.905599   6.089654   5.24023
 4.6648135  4.0282593  3.4829125  2.8690925  2.2219436  1.9130428
 1.6699027  1.6895308  1.616285   1.0347433  1.2854384  1.905771
 2.403932   2.6786573  2.7912533  2.724927   2.3956544  1.9017471
 1.4938486  1.1847398  1.0700909  1.1516702  1.2296703  1.4998311
 1.8718425  1.9637972  1.9534097  1.8498043  1.9762231  2.2029252
 2.5717564  2.807948   2.7908769  2.73255    2.68529    2.4725645
 2.2409759  2.06075    2.0173886  2.2323415  2.6389112  3.364687
 3.569346   3.8573687  4.3204126  4.572713   4.953045   5.26648
 5.317815   5.169396   5.0540824  4.7067738  4.28401    3.6155272
 3.0983725  2.925508   3.4273162  4.335196   5.315324   5.848585
 5.910017   5.6564155  4.8514366  4.3177414  3.6792607  3.1134546
 2.421105   1.8214632  1.5514723  1.4130454  1.4918832  1.0151312
 1.0163965  1.5817755  2.1251216  2.6278155  2.7994862  2.8007066
 2.6530683  2.1927693  1.5318568  1.1489743  0.75825727 0.63866615
 0.7987109  0.9822404  1.2454574  1.543138   1.5704302  1.6052129
 1.6192619  1.771538   2.1589708  2.505342   2.641744   2.6707444
 2.6035118  2.5589392  2.4026833  2.166151   1.9816272  2.0458786
 2.2128527  2.6988826  3.2648787  3.397502   3.717353   3.966983
 4.3198442  4.636884   4.603169   4.809886   4.5051236  3.9716144
 3.5472755  3.1153066  2.530485   2.1747632  2.4838538  3.0985045
 3.987919   4.728938   4.838682   5.30652    5.3716507  4.607255
 3.9159882  3.3813124  2.7041297  1.9892027  1.5155307  1.2923163
 1.2146084  1.1872191  0.7580949  1.1997259  1.7803918  2.2365103
 2.6704934  2.8521678  2.753693   2.4363112  1.9917085  1.3143581
 0.8788745  0.45007375 0.31478557 0.6063114  0.8954245  1.1099367
 1.2525692  1.2812548  1.3730775  1.4156553  1.5621011  1.9170734
 2.2665145  2.3869061  2.5416515  2.4395573  2.4633927  2.3797123
 2.1332254  2.0379717  2.0379233  2.1362333  2.4976003  3.0006397
 3.2374513  3.592752   3.6505415  4.023993   4.05952    4.1148825
 4.2447066  3.844754   3.0685515  2.666415   2.322299   1.8022182
 1.5441183  2.0924604  2.7658703  3.5777872  3.966105   4.1161065
 4.7601204  4.9653707  4.341559   3.5447366  3.0012932  2.3602383
 1.6950605  1.3011304  1.0942981  1.0706255  0.8996783  0.689968
 1.3313829  1.83612    2.3003662  2.6108513  2.7048595  2.5215638
 2.21801    1.7731929  1.1477365  0.64098656 0.3118832  0.29121086
 0.7910031  1.3239692  1.2358099  1.146831   1.1435717  1.2265501
 1.3746687  1.360481   1.681498   1.864477   2.0610025  2.2726076
 2.2844996  2.331974   2.3112566  2.158208   2.01297    2.021378
 2.0695071  2.2416453  2.609332   2.9234731  3.1882186  3.31529
 3.571734   3.4649155  3.5326264  3.5460744  3.1931694  2.3837461
 1.9058707  1.5917951  1.0258623  0.97422796 1.748591   2.255557
 3.0648685  3.313189   3.5256414  4.0728154  4.3422065  4.009136
 3.1499114  2.4386399  1.9843754  1.5032686  1.086391   0.89318204
 0.9076926  0.7008439  0.63241386 1.2198448  1.6906807  2.1111734
 2.333002   2.3878047  2.217115   1.8964413  1.4772264  0.9225327
 0.56939465 0.5537765  0.6664716  0.93345326 1.2374834  1.2679573
 1.1467944  1.1898539  1.306944   1.4801956  1.3950486  1.3338554
 1.3977934  1.4902985  1.8821182  2.1037614  2.0472717  2.1220112
 2.078055   2.014169   1.9500773  1.8961502  1.9306166  2.1648672
 2.6953404  2.750646   2.7426572  3.1615753  3.068887   2.9253569
 2.773393   2.2474418  1.726079   1.2547336  0.89053863 0.5666939
 0.5380402  1.0743821  1.4058995  2.192812   2.7547517  2.847426
 3.1655502  3.3427489  3.2893167  2.901521   2.0046186  1.5897791
 1.2943964  0.95583916 0.8124559  0.86195946 0.7023628  0.4580154
 0.9641825  1.3987434  1.8435262  2.063283   2.0421972  1.9376029
 1.6897821  1.2114557  0.74380946 0.51977485 0.5093502  0.73502225
 0.83011997 1.0362432  1.3207008  1.3306684  1.1668781  1.1159254
 1.2528493  1.4237075  1.3176608  1.1940517  0.8808951  1.0399766
 1.5169734  1.740969   1.9193504  2.019738  ]
