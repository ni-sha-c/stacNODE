time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.13%, model saved.
Epoch: 0 Train: 59634.84375 Test: 4266.97217
Epoch: 80 Train: 15809.84570 Test: 1732.14331
Epoch 160: New minimal relative error: 75.10%, model saved.
Epoch: 160 Train: 15156.26172 Test: 1628.29785
Epoch: 240 Train: 13916.85352 Test: 1363.00146
Epoch 320: New minimal relative error: 71.54%, model saved.
Epoch: 320 Train: 14744.78418 Test: 1397.74365
Epoch: 400 Train: 12480.19824 Test: 1171.21045
Epoch 480: New minimal relative error: 70.85%, model saved.
Epoch: 480 Train: 14257.27734 Test: 1350.69971
Epoch: 560 Train: 13374.29004 Test: 1329.00818
Epoch: 640 Train: 13987.94141 Test: 1371.38086
Epoch: 720 Train: 12696.45117 Test: 1191.27173
Epoch: 800 Train: 12058.36621 Test: 1031.11792
Epoch: 880 Train: 10189.78223 Test: 765.21307
Epoch: 960 Train: 8377.30859 Test: 583.06775
Epoch: 1040 Train: 7188.88428 Test: 407.25897
Epoch: 1120 Train: 6741.47656 Test: 359.54529
Epoch: 1200 Train: 5024.38721 Test: 219.79051
Epoch 1280: New minimal relative error: 27.73%, model saved.
Epoch: 1280 Train: 2913.79956 Test: 93.10318
Epoch 1360: New minimal relative error: 26.82%, model saved.
Epoch: 1360 Train: 1979.80151 Test: 49.17617
Epoch 1440: New minimal relative error: 21.11%, model saved.
Epoch: 1440 Train: 1297.50159 Test: 20.11848
Epoch 1520: New minimal relative error: 16.35%, model saved.
Epoch: 1520 Train: 937.57782 Test: 11.98978
Epoch 1600: New minimal relative error: 15.20%, model saved.
Epoch: 1600 Train: 713.63000 Test: 7.51786
Epoch 1680: New minimal relative error: 10.31%, model saved.
Epoch: 1680 Train: 597.69330 Test: 5.40147
Epoch 1760: New minimal relative error: 9.54%, model saved.
Epoch: 1760 Train: 525.88007 Test: 4.20632
Epoch: 1840 Train: 496.60788 Test: 5.40844
Epoch: 1920 Train: 469.28467 Test: 8.64124
Epoch 2000: New minimal relative error: 6.11%, model saved.
Epoch: 2000 Train: 485.92172 Test: 4.12699
Epoch: 2080 Train: 452.40063 Test: 3.72829
Epoch: 2160 Train: 477.16339 Test: 4.65844
Epoch: 2240 Train: 429.12115 Test: 4.68673
Epoch: 2320 Train: 492.31763 Test: 4.96827
Epoch: 2400 Train: 444.56433 Test: 4.31545
Epoch: 2480 Train: 367.40826 Test: 2.52179
Epoch: 2560 Train: 364.26974 Test: 2.88606
Epoch: 2640 Train: 322.00870 Test: 1.64183
Epoch: 2720 Train: 326.53110 Test: 1.98039
Epoch: 2800 Train: 313.08978 Test: 2.82698
Epoch 2880: New minimal relative error: 2.26%, model saved.
Epoch: 2880 Train: 288.64780 Test: 1.80425
Epoch: 2960 Train: 281.59171 Test: 1.57473
Epoch: 3040 Train: 274.61469 Test: 2.52213
Epoch: 3120 Train: 257.19543 Test: 1.39501
Epoch: 3200 Train: 253.59877 Test: 1.09510
Epoch: 3280 Train: 235.80231 Test: 1.53870
Epoch: 3360 Train: 241.65417 Test: 5.04727
Epoch: 3440 Train: 240.76859 Test: 2.12677
Epoch: 3520 Train: 213.25603 Test: 1.35718
Epoch: 3600 Train: 204.57460 Test: 1.08481
Epoch: 3680 Train: 194.70067 Test: 0.60998
Epoch: 3760 Train: 188.15228 Test: 1.10469
Epoch: 3840 Train: 180.00951 Test: 0.51058
Epoch: 3920 Train: 176.50310 Test: 0.56185
Epoch: 4000 Train: 199.45079 Test: 5.28191
Epoch: 4080 Train: 176.25104 Test: 0.74911
Epoch: 4160 Train: 172.51057 Test: 1.11004
Epoch: 4240 Train: 223.22380 Test: 1.26880
Epoch: 4320 Train: 228.74156 Test: 1.07579
Epoch: 4400 Train: 205.82074 Test: 0.94931
Epoch: 4480 Train: 178.76646 Test: 0.74981
Epoch: 4560 Train: 172.73886 Test: 0.58716
Epoch: 4640 Train: 166.23038 Test: 0.47322
Epoch: 4720 Train: 164.94302 Test: 0.47877
Epoch: 4800 Train: 161.65680 Test: 1.22480
Epoch: 4880 Train: 158.61182 Test: 0.40965
Epoch: 4960 Train: 156.86813 Test: 0.46905
Epoch: 5040 Train: 143.55339 Test: 0.75233
Epoch: 5120 Train: 140.48053 Test: 0.38171
Epoch: 5200 Train: 142.95291 Test: 0.42665
Epoch: 5280 Train: 134.68109 Test: 0.27694
Epoch: 5360 Train: 140.62910 Test: 0.32449
Epoch: 5440 Train: 135.12663 Test: 0.35609
Epoch: 5520 Train: 133.13210 Test: 0.37955
Epoch: 5600 Train: 139.77405 Test: 0.54194
Epoch: 5680 Train: 146.72397 Test: 0.72675
Epoch: 5760 Train: 150.23021 Test: 0.72391
Epoch: 5840 Train: 214.38615 Test: 1.99466
Epoch: 5920 Train: 178.31319 Test: 1.10019
Epoch: 6000 Train: 164.30678 Test: 0.79057
Epoch: 6080 Train: 144.66092 Test: 0.52481
Epoch: 6160 Train: 133.29030 Test: 0.48984
Epoch: 6240 Train: 137.38895 Test: 0.63752
Epoch: 6320 Train: 131.01421 Test: 0.47985
Epoch: 6400 Train: 134.73753 Test: 0.38850
Epoch: 6480 Train: 143.96904 Test: 0.42580
Epoch: 6560 Train: 144.38393 Test: 0.63924
Epoch: 6640 Train: 140.01790 Test: 0.38711
Epoch: 6720 Train: 127.06030 Test: 0.28870
Epoch: 6800 Train: 125.60799 Test: 0.26283
Epoch: 6880 Train: 126.91306 Test: 0.34142
Epoch: 6960 Train: 130.00645 Test: 0.45714
Epoch: 7040 Train: 127.98709 Test: 0.46669
Epoch: 7120 Train: 123.33423 Test: 0.30163
Epoch: 7200 Train: 119.82094 Test: 0.26161
Epoch: 7280 Train: 122.71178 Test: 0.29524
Epoch: 7360 Train: 117.17123 Test: 0.34900
Epoch: 7440 Train: 119.82109 Test: 0.30711
Epoch: 7520 Train: 122.18484 Test: 0.35286
Epoch: 7600 Train: 121.99584 Test: 0.32983
Epoch: 7680 Train: 125.48916 Test: 0.33913
Epoch: 7760 Train: 127.74879 Test: 0.33445
Epoch: 7840 Train: 125.16445 Test: 0.33035
Epoch: 7920 Train: 123.32446 Test: 0.27797
Epoch: 7999 Train: 118.48183 Test: 0.25170
Training Loss: tensor(118.4818)
Test Loss: tensor(0.2517)
Learned LE: [ 8.5090047e-01  4.9410405e-04 -1.4553494e+01]
True LE: [ 8.6603558e-01 -2.2113859e-03 -1.4551199e+01]
Relative Error: [ 4.845732   5.5001283  6.504255   7.7014503  8.840195   9.679803
 10.553189  11.295174  11.803417  12.358473  13.054907  13.803702
 14.56627   15.22257   15.679913  16.010256  16.196733  16.269478
 16.223763  16.064175  15.79736   15.436257  15.027957  14.592322
 14.316088  13.971398  13.166836  12.098045  10.738137   9.816081
  9.056803   8.372918   7.9012556  7.54314    7.2244854  7.1224704
  6.6786485  6.143336   6.75676    7.643275   8.20572    8.462713
  8.427683   8.209377   7.7924356  7.238629   6.575587   5.8735304
  5.209162   4.755264   4.553326   4.1806636  3.6514158  3.0448306
  2.7262616  2.510532   2.5326006  2.843344   3.223996   3.6306233
  3.5198696  3.5232525  3.9762926  4.3733444  5.063567   6.0884895
  7.209216   8.056103   8.794011   9.3248825  9.742994  10.373383
 11.128104  11.988727  12.900456  13.70874   14.249012  14.635883
 14.893813  15.035007  15.051013  14.913027  14.639182  14.25576
 13.774226  13.238421  12.687135  12.435792  11.749342  10.703034
  9.363289   8.248672   7.491132   6.7384377  6.202091   5.8838
  5.5427585  5.4609337  5.0580277  4.6429296  5.724451   6.7479634
  7.277456   7.6163263  7.6103187  7.3364334  6.9172835  6.3664937
  5.7354345  5.0502286  4.4203787  4.1596456  4.023954   3.7337754
  3.2808418  2.7611685  2.291581   2.2784412  2.5790374  3.2116823
  4.0537453  4.5538197  4.74035    4.287101   4.051971   4.062948
  4.165307   4.6994743  5.553507   6.470018   7.20642    7.707103
  7.8089314  8.329844   9.130641  10.093898  11.140296  12.077049
 12.68501   13.2075815 13.633011  13.880422  13.942631  13.824177
 13.581808  13.210806  12.70138   12.100394  11.436947  10.83069
 10.360087   9.440121   8.216744   6.843558   6.0606995  5.2739363
  4.634856   4.3014336  3.924185   3.8054056  3.45457    3.2213833
  4.6164103  5.6938777  6.341877   6.7578063  6.7264805  6.512884
  6.247965   5.7773504  5.1517467  4.4493284  3.8505714  3.7151237
  3.5617259  3.2703915  2.9779387  2.7171466  2.5141745  2.6086156
  3.0880668  3.8247035  4.802056   5.791941   6.247148   6.037456
  5.3006115  4.942698   4.498223   4.1920385  4.395499   5.0722203
  5.7887373  6.4015517  6.3619337  6.3649282  7.0462546  8.049394
  9.165545  10.233441  11.125402  11.903641  12.480144  12.8559885
 13.020639  12.977423  12.738855  12.323146  11.804202  11.171952
 10.456771   9.668285   9.039069   8.28989    7.1656213  5.728902
  4.810752   4.0865445  3.3561115  2.877909   2.4963467  2.2031395
  1.882509   1.8211765  3.3905077  4.5892215  5.37239    5.776633
  5.826361   5.856736   5.683694   5.310913   4.7860017  4.168627
  3.58283    3.3713632  3.2394686  2.9860508  2.7917302  2.7814808
  2.9583075  3.3320959  3.952094   4.704559   5.270447   5.9244084
  6.435649   6.0902514  5.325505   4.689984   4.5918136  4.538646
  4.216805   4.558249   4.9411907  5.3709326  5.406116   5.1451488
  5.045502   5.8488097  6.9559283  8.222858   9.48339   10.439861
 11.131125  11.634281  11.942906  12.061847  12.000756  11.690397
 11.158315  10.470035   9.726682   8.907266   8.011986   7.327652
  6.367069   5.094468   3.8841698  3.3307507  3.0695033  3.0330403
  2.9728293  2.5396557  1.7771157  0.9232094  2.1367674  3.4571602
  4.281863   4.71446    5.0173287  5.182003   5.134584   4.890259
  4.5675592  4.137987   3.6706526  3.5167847  3.1989608  2.7625246
  2.5970032  2.738427   3.2986972  4.0868936  4.837148   5.2400775
  5.4547405  5.5417023  5.5409055  5.218187   4.7983785  3.9971092
  3.5250854  3.5222862  3.5748324  4.16985    4.61042    4.996149
  5.027937   4.6407857  4.382374   3.8235168  4.520936   5.9232125
  7.431779   8.580131   9.492776  10.2207    10.725393  10.998095
 11.047335  10.91125   10.611617  10.096353   9.325714   8.454873
  7.5645065  6.611522   5.847266   4.796629   3.5236592  3.1530313
  3.553338   3.582675   3.5003557  3.2711167  2.8453517  1.761041
  1.592754   2.49473    3.1260734  3.6139586  4.1053     4.4162664
  4.5356827  4.5375485  4.38262    4.0565724  3.6772974  3.5328321
  3.3662882  2.9494824  2.4899776  2.5709486  3.2238562  4.2398257
  5.3634653  5.606555   5.8335757  5.252515   4.4645085  4.275137
  3.923627   3.4682698  2.6607645  2.1722422  2.1571615  2.4086242
  3.3832319  4.439864   4.914763   4.717652   4.5365787  3.8868916
  2.7593234  3.2184963  4.7754498  6.2893925  7.499484   8.508858
  9.267171   9.771161  10.026392  10.054695   9.889329   9.566959
  9.086991   8.404106   7.502585   6.5811067]
