time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 101.96%, model saved.
Epoch: 0 Train: 60328.60938 Test: 3865.18677
Epoch: 80 Train: 16521.41797 Test: 1559.07532
Epoch 160: New minimal relative error: 62.57%, model saved.
Epoch: 160 Train: 14980.50879 Test: 1210.25903
Epoch: 240 Train: 17306.57812 Test: 1987.98792
Epoch: 320 Train: 14956.84766 Test: 1277.62720
Epoch: 400 Train: 12739.07520 Test: 1107.00110
Epoch: 480 Train: 13619.56250 Test: 1182.64624
Epoch: 560 Train: 13221.17090 Test: 1124.85559
Epoch: 640 Train: 15179.18457 Test: 1243.38892
Epoch: 720 Train: 14705.02734 Test: 1105.21472
Epoch: 800 Train: 14310.86328 Test: 1181.66162
Epoch: 880 Train: 14453.68652 Test: 2463.28223
Epoch: 960 Train: 14246.44824 Test: 1263.44885
Epoch: 1040 Train: 10920.63086 Test: 1106.43652
Epoch: 1120 Train: 10365.85645 Test: 881.79431
Epoch 1200: New minimal relative error: 52.81%, model saved.
Epoch: 1200 Train: 10156.97168 Test: 804.77380
Epoch: 1280 Train: 9023.55762 Test: 663.35663
Epoch 1360: New minimal relative error: 40.03%, model saved.
Epoch: 1360 Train: 7678.52100 Test: 473.77054
Epoch: 1440 Train: 6162.44092 Test: 289.64145
Epoch: 1520 Train: 5660.13574 Test: 243.48848
Epoch: 1600 Train: 4285.27441 Test: 153.60277
Epoch: 1680 Train: 2499.12622 Test: 155.14195
Epoch: 1760 Train: 2646.02246 Test: 95.45918
Epoch: 1840 Train: 2160.67139 Test: 282.93411
Epoch: 1920 Train: 978.37946 Test: 18.60847
Epoch 2000: New minimal relative error: 37.78%, model saved.
Epoch: 2000 Train: 881.95532 Test: 21.26439
Epoch 2080: New minimal relative error: 21.90%, model saved.
Epoch: 2080 Train: 640.92773 Test: 9.85193
Epoch 2160: New minimal relative error: 15.63%, model saved.
Epoch: 2160 Train: 540.02838 Test: 6.23182
Epoch: 2240 Train: 703.34528 Test: 47.62747
Epoch: 2320 Train: 583.46735 Test: 9.34711
Epoch: 2400 Train: 649.95404 Test: 22.35943
Epoch: 2480 Train: 444.72430 Test: 6.43624
Epoch 2560: New minimal relative error: 7.57%, model saved.
Epoch: 2560 Train: 389.93466 Test: 4.83667
Epoch: 2640 Train: 375.88297 Test: 5.61222
Epoch: 2720 Train: 326.63385 Test: 2.93236
Epoch: 2800 Train: 420.50674 Test: 12.56324
Epoch: 2880 Train: 298.49350 Test: 3.15303
Epoch: 2960 Train: 316.57910 Test: 5.32531
Epoch: 3040 Train: 250.41005 Test: 4.24848
Epoch: 3120 Train: 245.65225 Test: 2.38384
Epoch: 3200 Train: 225.36369 Test: 2.45169
Epoch: 3280 Train: 186.18318 Test: 1.77514
Epoch: 3360 Train: 170.51828 Test: 1.38776
Epoch: 3440 Train: 181.76569 Test: 2.80208
Epoch 3520: New minimal relative error: 6.63%, model saved.
Epoch: 3520 Train: 199.71968 Test: 2.54297
Epoch 3600: New minimal relative error: 5.26%, model saved.
Epoch: 3600 Train: 169.83588 Test: 1.40909
Epoch: 3680 Train: 159.51424 Test: 1.31439
Epoch: 3760 Train: 161.72626 Test: 1.54440
Epoch: 3840 Train: 154.81992 Test: 1.63152
Epoch: 3920 Train: 146.16507 Test: 1.11147
Epoch: 4000 Train: 132.86411 Test: 0.98478
Epoch: 4080 Train: 152.53719 Test: 1.37981
Epoch: 4160 Train: 137.58638 Test: 1.22307
Epoch: 4240 Train: 142.69496 Test: 1.24858
Epoch: 4320 Train: 129.56711 Test: 1.54901
Epoch: 4400 Train: 126.57019 Test: 1.00190
Epoch: 4480 Train: 110.28069 Test: 0.73820
Epoch: 4560 Train: 106.39135 Test: 0.79241
Epoch: 4640 Train: 107.06123 Test: 0.90375
Epoch: 4720 Train: 104.14799 Test: 0.66470
Epoch: 4800 Train: 107.45805 Test: 2.89895
Epoch: 4880 Train: 137.15405 Test: 10.72006
Epoch: 4960 Train: 111.16006 Test: 1.01265
Epoch: 5040 Train: 121.87447 Test: 3.50230
Epoch: 5120 Train: 128.47478 Test: 2.94716
Epoch 5200: New minimal relative error: 4.45%, model saved.
Epoch: 5200 Train: 102.23007 Test: 0.66407
Epoch: 5280 Train: 92.87460 Test: 0.49605
Epoch 5360: New minimal relative error: 2.94%, model saved.
Epoch: 5360 Train: 95.26900 Test: 0.61455
Epoch: 5440 Train: 97.01656 Test: 0.68202
Epoch: 5520 Train: 91.00473 Test: 0.70683
Epoch: 5600 Train: 89.57098 Test: 1.53391
Epoch: 5680 Train: 92.90656 Test: 0.73356
Epoch: 5760 Train: 90.56679 Test: 0.86455
Epoch: 5840 Train: 95.16144 Test: 0.69179
Epoch: 5920 Train: 107.90094 Test: 1.18716
Epoch: 6000 Train: 79.46861 Test: 0.62079
Epoch: 6080 Train: 73.43130 Test: 0.39262
Epoch: 6160 Train: 74.93323 Test: 0.47714
Epoch: 6240 Train: 77.45762 Test: 0.71736
Epoch: 6320 Train: 88.74382 Test: 0.81876
Epoch 6400: New minimal relative error: 2.14%, model saved.
Epoch: 6400 Train: 72.59225 Test: 0.41882
Epoch: 6480 Train: 68.56196 Test: 0.35205
Epoch: 6560 Train: 64.94662 Test: 0.30109
Epoch: 6640 Train: 72.26084 Test: 0.85263
Epoch: 6720 Train: 74.44113 Test: 0.57192
Epoch: 6800 Train: 65.73704 Test: 0.43582
Epoch: 6880 Train: 60.86400 Test: 0.31496
Epoch: 6960 Train: 67.66024 Test: 0.61931
Epoch: 7040 Train: 66.11938 Test: 0.61676
Epoch: 7120 Train: 66.75066 Test: 0.60112
Epoch: 7200 Train: 56.44490 Test: 0.37640
Epoch: 7280 Train: 55.58450 Test: 0.36463
Epoch: 7360 Train: 61.96397 Test: 0.51025
Epoch: 7440 Train: 57.53445 Test: 0.51888
Epoch: 7520 Train: 53.57643 Test: 0.29241
Epoch: 7600 Train: 68.23549 Test: 4.08672
Epoch: 7680 Train: 50.23219 Test: 0.24653
Epoch: 7760 Train: 49.18211 Test: 0.22655
Epoch: 7840 Train: 51.01487 Test: 0.35344
Epoch: 7920 Train: 51.13966 Test: 0.28407
Epoch: 7999 Train: 47.44610 Test: 0.42197
Training Loss: tensor(47.4461)
Test Loss: tensor(0.4220)
Learned LE: [  0.85482514   0.01846481 -14.547264  ]
True LE: [ 8.7830889e-01 -4.4777435e-03 -1.4551396e+01]
Relative Error: [0.6814024  0.6056067  0.56927407 0.5340858  0.52248335 0.53592235
 0.5277013  0.2776911  0.20272851 0.37238368 0.70563924 1.0568534
 1.2255037  1.031425   0.7669906  0.947341   1.5336018  2.3441904
 2.6840239  2.6549063  2.4632757  2.418647   2.6179948  2.9291062
 2.7400916  1.9785544  1.4640995  1.2253737  1.2078955  0.9466613
 1.0684443  1.3764644  1.7060168  2.1161008  2.546422   2.896575
 3.7391214  4.2894545  4.2760105  4.140718   4.1101685  4.0980225
 3.79779    3.2498167  2.751307   2.3005545  1.5332901  1.222342
 1.6502675  2.201742   2.5877564  2.5097265  2.2094939  1.7724024
 1.5407364  1.5476195  1.5593129  1.4802074  1.4093494  1.1317264
 0.88410854 0.7355013  0.66578555 0.6111498  0.69591004 0.7549187
 0.75693077 0.6614822  0.44808435 0.40109515 0.31221384 0.38235033
 0.5585706  0.85729444 1.193814   1.2958984  0.99999523 0.6967266
 0.8343024  1.3710334  2.1363032  2.3317618  2.2146442  2.0857823
 2.1231546  2.3398693  2.7577474  2.3641708  1.6769761  1.2554141
 1.0871671  0.936959   1.0278202  1.3822058  1.5229477  1.964771
 2.3699322  2.6723132  3.1261582  3.938679   4.25763    4.1190467
 4.0375104  4.1319947  4.045499   3.7681475  3.294934   2.8377476
 2.1887631  1.4528705  1.3037547  1.802746   2.2919047  2.4456217
 2.2111652  1.7669467  1.4689301  1.382951   1.3895754  1.3697672
 1.2959862  1.0425055  0.7555413  0.6284517  0.5944081  0.6393614
 0.744171   0.92635995 1.0220526  0.89577323 0.6274814  0.45072827
 0.45498568 0.4369854  0.5169398  0.6996092  0.9981922  1.3664491
 1.239899   0.8561438  0.6185656  0.6376071  1.1312001  1.6516653
 1.7946552  1.7617083  1.7916985  1.8713127  1.9555433  2.3258991
 1.983826   1.492415   1.137018   0.96676576 1.0084215  1.3258473
 1.549406   1.708181   2.0393798  2.4063525  2.7304142  3.1140025
 3.861308   4.041607   3.8910925  3.832953   4.013837   3.951301
 3.7175357  3.3435204  2.9835887  2.1456118  1.4805189  1.3165933
 1.747028   2.1399841  2.2010965  1.9321265  1.4411997  1.2575856
 1.2525083  1.2835727  1.1774374  0.9212704  0.6032858  0.47186726
 0.50014824 0.549441   0.67056674 0.81979597 0.92856866 0.79216564
 0.5033932  0.3705582  0.40574825 0.6720099  0.7309636  0.7210637
 0.79524946 0.9736394  1.376131   1.2191293  0.8533862  0.47189644
 0.48315403 0.70420086 1.0555086  1.3121414  1.398627   1.412342
 1.5990655  1.687579   2.0318763  1.7628025  1.4041295  1.1536236
 0.9916677  1.2175223  1.56245    1.6306207  1.7678659  1.8950516
 2.2804391  2.6170378  2.8749359  3.4914699  3.7019446  3.5594637
 3.5085506  3.74615    3.815529   3.631794   3.3548853  2.992847
 2.2636268  1.563446   1.2038901  1.487341   1.8737379  2.0308254
 1.6403042  1.2006118  1.1150415  1.0305102  1.2148632  1.0036649
 0.5145255  0.28664118 0.32370558 0.454045   0.54789954 0.721445
 0.7514406  0.621744   0.33845884 0.23398131 0.36197388 0.53301674
 0.745916   0.85342973 0.8318996  0.8615555  1.0332714  1.2943423
 1.0895169  0.810027   0.56945246 0.31924275 0.25110102 0.41416124
 0.7759629  0.9448641  1.1066282  1.33546    1.5680872  1.7881877
 1.6727211  1.3572562  1.1566318  0.9987478  1.2471178  1.6115261
 1.7645929  1.8094001  1.8312657  2.0372522  2.3424544  2.5194418
 2.8515651  3.2401898  3.1396382  3.0966482  3.3500645  3.5003397
 3.4521554  3.326553   3.0385396  2.4301765  1.6326144  1.1524198
 1.1160227  1.5993379  1.8916754  1.5364811  1.1470728  0.9905695
 0.9103841  1.106602   0.7251081  0.27315557 0.17934415 0.28113
 0.35797453 0.42143673 0.5116838  0.51998603 0.27995256 0.07918327
 0.15008016 0.31685832 0.43583494 0.6099252  0.68109363 0.7357283
 0.66093624 0.79406977 1.0489666  1.1967874  0.912912   0.6338022
 0.6971067  0.59302616 0.40911412 0.22698458 0.48108268 0.66817325
 0.95927215 1.2266484  1.5344388  1.5610245  1.2584276  1.1300212
 0.9489758  0.9313269  1.4127048  1.7139422  1.9282355  1.7812198
 1.788939   2.013306   2.1176612  2.1958995  2.729108   2.8112278
 2.7132704  2.8292964  2.9421868  3.1055422  3.071197   3.044065
 2.5885668  1.8941065  1.2941773  0.72345924 1.0566516  1.5333831
 1.7017398  1.1013172  0.89106375 0.8040663  0.8963665  0.6044025
 0.18721172 0.14375338 0.22570328 0.31458116 0.3308285  0.32717878
 0.35060737 0.21664901 0.14517829 0.17255852 0.23598625 0.3148271
 0.37998617 0.4362925  0.45241997 0.5661086  0.50103563 0.5849288
 0.8670333  0.93668073 0.8337388  0.8041824  0.888058   0.8925259
 0.6491333  0.27827466 0.32375407 0.47243875]
