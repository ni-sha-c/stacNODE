time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.31%, model saved.
Epoch: 0 Train: 9334.37598 Test: 4299.63672
Epoch: 80 Train: 2691.52930 Test: 1160.20630
Epoch: 160 Train: 2511.41162 Test: 1073.22998
Epoch 240: New minimal relative error: 63.85%, model saved.
Epoch: 240 Train: 2022.17529 Test: 792.11792
Epoch: 320 Train: 2015.69580 Test: 731.45544
Epoch: 400 Train: 1472.34277 Test: 581.92102
Epoch 480: New minimal relative error: 40.58%, model saved.
Epoch: 480 Train: 642.94696 Test: 281.14990
Epoch: 560 Train: 270.45416 Test: 47.55962
Epoch: 640 Train: 287.95441 Test: 478.20975
Epoch 720: New minimal relative error: 17.74%, model saved.
Epoch: 720 Train: 127.20889 Test: 11.83782
Epoch: 800 Train: 96.36622 Test: 34.03299
Epoch 880: New minimal relative error: 15.55%, model saved.
Epoch: 880 Train: 69.98031 Test: 6.02444
Epoch: 960 Train: 96.90146 Test: 32.10218
Epoch 1040: New minimal relative error: 9.87%, model saved.
Epoch: 1040 Train: 60.28978 Test: 11.02350
Epoch: 1120 Train: 45.77581 Test: 13.58042
Epoch: 1200 Train: 50.97219 Test: 9.21476
Epoch 1280: New minimal relative error: 3.54%, model saved.
Epoch: 1280 Train: 31.76737 Test: 0.78561
Epoch: 1360 Train: 48.89730 Test: 20.42932
Epoch: 1440 Train: 31.04544 Test: 3.16851
Epoch: 1520 Train: 24.94184 Test: 1.88688
Epoch: 1600 Train: 30.45458 Test: 7.39941
Epoch: 1680 Train: 33.52455 Test: 7.07896
Epoch: 1760 Train: 20.89974 Test: 0.77222
Epoch: 1840 Train: 27.08957 Test: 4.63426
Epoch: 1920 Train: 24.38309 Test: 4.14893
Epoch: 2000 Train: 28.19022 Test: 5.57728
Epoch: 2080 Train: 19.02784 Test: 0.95247
Epoch: 2160 Train: 17.54212 Test: 0.42186
Epoch: 2240 Train: 17.78562 Test: 2.55035
Epoch: 2320 Train: 16.67629 Test: 0.54882
Epoch: 2400 Train: 22.63820 Test: 4.09172
Epoch: 2480 Train: 18.03281 Test: 1.80694
Epoch: 2560 Train: 20.96720 Test: 6.57762
Epoch: 2640 Train: 14.94483 Test: 0.72505
Epoch: 2720 Train: 19.65122 Test: 2.44731
Epoch: 2800 Train: 16.33722 Test: 2.66880
Epoch: 2880 Train: 13.79560 Test: 1.23282
Epoch: 2960 Train: 13.29772 Test: 1.97476
Epoch: 3040 Train: 14.24967 Test: 1.80169
Epoch: 3120 Train: 12.03376 Test: 0.51123
Epoch: 3200 Train: 10.92381 Test: 0.18575
Epoch: 3280 Train: 11.88805 Test: 0.19908
Epoch: 3360 Train: 11.62744 Test: 1.55916
Epoch: 3440 Train: 11.52058 Test: 0.69763
Epoch: 3520 Train: 12.07851 Test: 0.36895
Epoch: 3600 Train: 11.32998 Test: 1.36194
Epoch: 3680 Train: 10.71839 Test: 1.08335
Epoch: 3760 Train: 15.29241 Test: 2.76712
Epoch: 3840 Train: 10.16540 Test: 0.87062
Epoch: 3920 Train: 10.31426 Test: 0.34512
Epoch: 4000 Train: 9.72144 Test: 0.66526
Epoch: 4080 Train: 11.05771 Test: 0.54909
Epoch: 4160 Train: 12.50539 Test: 3.83076
Epoch: 4240 Train: 8.81730 Test: 0.40018
Epoch 4320: New minimal relative error: 3.51%, model saved.
Epoch: 4320 Train: 9.22788 Test: 0.18028
Epoch: 4400 Train: 8.79685 Test: 0.62298
Epoch: 4480 Train: 9.20245 Test: 0.87246
Epoch: 4560 Train: 9.08828 Test: 1.36283
Epoch: 4640 Train: 9.50381 Test: 0.79727
Epoch: 4720 Train: 15.23343 Test: 4.25703
Epoch 4800: New minimal relative error: 3.17%, model saved.
Epoch: 4800 Train: 10.28555 Test: 0.31083
Epoch: 4880 Train: 11.75219 Test: 2.31392
Epoch: 4960 Train: 7.69840 Test: 0.05217
Epoch: 5040 Train: 8.38058 Test: 0.12736
Epoch: 5120 Train: 8.07481 Test: 0.18482
Epoch: 5200 Train: 7.70543 Test: 0.25235
Epoch 5280: New minimal relative error: 2.48%, model saved.
Epoch: 5280 Train: 7.24518 Test: 0.13863
Epoch: 5360 Train: 7.31550 Test: 0.33054
Epoch: 5440 Train: 7.35521 Test: 0.17816
Epoch: 5520 Train: 7.12745 Test: 0.08217
Epoch: 5600 Train: 7.50784 Test: 0.11108
Epoch: 5680 Train: 7.98384 Test: 0.21923
Epoch: 5760 Train: 7.16871 Test: 0.21626
Epoch: 5840 Train: 6.99550 Test: 0.10236
Epoch: 5920 Train: 7.10777 Test: 0.04959
Epoch: 6000 Train: 7.90418 Test: 0.16783
Epoch: 6080 Train: 6.83053 Test: 0.05119
Epoch: 6160 Train: 6.62553 Test: 0.08243
Epoch: 6240 Train: 6.96665 Test: 0.15778
Epoch: 6320 Train: 8.85197 Test: 1.59690
Epoch: 6400 Train: 7.04997 Test: 0.22889
Epoch 6480: New minimal relative error: 1.14%, model saved.
Epoch: 6480 Train: 6.52213 Test: 0.35667
Epoch: 6560 Train: 6.32569 Test: 0.14871
Epoch: 6640 Train: 6.15350 Test: 0.04533
Epoch: 6720 Train: 6.10073 Test: 0.04733
Epoch: 6800 Train: 6.04180 Test: 0.03685
Epoch: 6880 Train: 5.90456 Test: 0.12536
Epoch: 6960 Train: 5.86595 Test: 0.08136
Epoch: 7040 Train: 6.24930 Test: 0.20777
Epoch: 7120 Train: 6.65978 Test: 0.26207
Epoch: 7200 Train: 6.60345 Test: 0.49965
Epoch: 7280 Train: 6.10011 Test: 0.17316
Epoch: 7360 Train: 5.98248 Test: 0.25032
Epoch: 7440 Train: 5.98627 Test: 0.12877
Epoch: 7520 Train: 6.09941 Test: 0.40113
Epoch: 7600 Train: 6.69122 Test: 0.93551
Epoch: 7680 Train: 5.80547 Test: 0.71244
Epoch: 7760 Train: 5.41528 Test: 0.03072
Epoch: 7840 Train: 6.11793 Test: 0.53828
Epoch: 7920 Train: 5.15361 Test: 0.03629
Epoch: 7999 Train: 5.21493 Test: 0.17763
Training Loss: tensor(5.2149)
Test Loss: tensor(0.1776)
Learned LE: [  0.94828063  -0.09684077 -14.531498  ]
True LE: [ 8.65447342e-01  3.02799465e-03 -1.45385895e+01]
Relative Error: [1.9744165  2.230065   2.4205506  2.3027918  2.2937493  2.3992975
 2.9541886  3.4081566  3.7627232  3.9286587  3.5926006  3.1514485
 2.6184597  2.0602572  1.3514955  1.0316228  1.209017   1.1951977
 1.2714196  1.2839632  1.0092247  0.6447518  0.4924655  0.5322742
 0.53007305 0.6179086  0.809386   1.2355651  1.7749602  2.2746797
 2.789246   3.3543131  3.153709   2.8680882  2.7424777  2.9270227
 3.1818807  3.279299   2.858355   1.9080954  1.2718387  1.5899575
 2.0108905  2.3304417  2.7739437  3.5583298  4.564654   3.943091
 2.871098   2.2203665  1.8802812  1.6398784  1.5588903  1.5546665
 1.4545196  1.2792933  1.1128776  1.007411   1.1577454  1.6229053
 1.788892   1.8762926  1.9822587  2.1946514  2.4592643  2.467581
 2.6006577  2.513525   2.896497   3.210796   3.584744   3.593158
 3.2128997  2.827798   2.3784218  1.8945615  1.1738263  0.90123665
 0.9492496  0.9926155  1.1401157  1.2204462  0.86124355 0.5370641
 0.30984202 0.3029466  0.29394493 0.3407443  0.53257406 0.9430339
 1.2606038  1.911171   2.472989   2.6614964  2.242654   2.085634
 1.9613862  1.9798954  2.2336705  2.3829958  2.0627697  1.3903706
 0.9820995  1.1597685  1.6440598  1.8927628  2.2788262  3.1712432
 3.8703039  3.3487406  2.3253636  1.7272968  1.562315   1.5188495
 1.553139   1.6572737  1.6025748  1.3677467  1.1509395  0.9802769
 0.98461556 1.253364   1.3913146  1.5942196  1.8383049  2.1416504
 2.48903    2.614675   2.6968305  2.4466314  2.6835353  3.1146557
 3.420515   3.4036014  3.0047429  2.64262    2.1635687  1.5532492
 1.1176662  0.8189521  0.691416   0.8408541  0.986035   1.0290582
 0.7858584  0.6413843  0.4612046  0.33180535 0.48474902 0.5205194
 0.46257806 0.73689777 1.0127132  1.4630958  2.0177307  2.0384822
 1.637129   1.4679996  1.3439121  1.1653183  1.3399822  1.4042875
 1.3939267  0.7649429  0.6738503  0.8403739  1.2599784  1.5206639
 1.9422786  2.786266   3.1143122  2.6380339  1.8275937  1.3191463
 1.3898381  1.5183603  1.6137222  1.7724574  1.7993106  1.5720208
 1.333369   1.1218892  1.0007051  1.0169173  1.0825623  1.250187
 1.5634341  1.8689017  2.2084699  2.4539852  2.4214737  2.3665643
 2.397389   2.9429903  3.3825665  3.3161216  2.9603586  2.5853984
 2.1769083  1.4116123  0.8808522  0.9361156  0.50134474 0.60923594
 0.8623317  0.8859459  0.8380787  0.77890134 0.6354423  0.5305723
 0.7857164  0.9242465  0.7930694  0.69990766 0.82825756 1.187039
 1.4195155  1.5669816  1.4483169  0.85151803 0.7981635  0.6739233
 0.55246943 0.5882002  0.6767815  0.36851388 0.6457573  0.52487725
 0.8829607  1.2183948  1.641912   2.434491   2.4959052  1.9853334
 1.42824    1.0482781  1.3207164  1.3925887  1.4355572  1.5306355
 1.5857276  1.5261095  1.4604896  1.5068676  1.3574593  1.1583978
 1.0458242  1.0120994  1.0571146  1.2987741  1.7207377  1.9270589
 1.8460248  1.8905731  1.8991524  2.5237157  3.2503467  3.2986853
 2.986001   2.7022493  2.379538   1.5700555  0.8783561  0.69149745
 0.6617847  0.36048037 0.672801   0.8371062  0.80180585 0.77627265
 0.7628239  0.6746395  0.9630897  1.162666   1.0613469  0.9221806
 0.8669829  0.8959277  1.0458808  1.1121502  1.0916042  0.8116345
 0.5233557  0.5464067  0.47884682 0.27624413 0.26107106 0.35758796
 0.87766397 0.7473671  0.42169756 0.9949283  1.378883   1.9192686
 1.9672064  1.3232636  0.9266558  0.909264   1.1663256  1.2457075
 1.2667279  1.2458115  1.3480065  1.274023   1.1716397  1.3049426
 1.4317908  1.4420449  1.2729974  1.1252286  0.9262152  0.82273585
 1.2047548  1.5150796  1.373865   1.3998566  1.4750925  1.8255824
 2.615277   3.1639283  2.9349782  2.7374518  2.4903076  1.8739806
 1.0974437  0.7076445  0.7336156  0.3892375  0.3486596  0.6767057
 0.7615746  0.68074113 0.7114005  0.7606552  0.9786553  1.259646
 1.3094475  1.1488376  0.98569095 1.0040468  0.89651763 0.93857086
 0.92448664 0.8908509  0.69642687 0.69746256 0.8481622  0.7519771
 0.647159   0.663599   1.2101793  1.2765491  0.8040912  0.39424732
 1.0151178  1.3630056  1.3822671  0.8192458  0.72951496 0.76423186
 0.97467005 1.0753828  1.1028147  1.0030937  0.98992217 1.0062472
 0.91925097 1.0399851  1.1536429  1.2652019  1.4427736  1.3864067
 1.1227593  0.84287    0.6609474  0.9476365  0.98043215 0.95474833
 1.0777469  1.2112087  1.804504   2.529526   2.4777818  2.3522544
 2.1981702  1.8499004  1.1465147  0.7091263  0.628236   0.4774109
 0.2934915  0.35599965 0.52978235 0.6218491  0.6175419  0.63438267
 0.7395506  1.0259604  1.2795279  1.3032783 ]
