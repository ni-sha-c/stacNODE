time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.13%, model saved.
Epoch: 0 Train: 9843.93750 Test: 4034.05420
Epoch 80: New minimal relative error: 98.61%, model saved.
Epoch: 80 Train: 2469.67773 Test: 969.13098
Epoch: 160 Train: 1493.70435 Test: 433.89465
Epoch 240: New minimal relative error: 79.39%, model saved.
Epoch: 240 Train: 1016.69647 Test: 303.92102
Epoch: 320 Train: 565.32324 Test: 107.57198
Epoch 400: New minimal relative error: 24.72%, model saved.
Epoch: 400 Train: 285.91513 Test: 68.93773
Epoch 480: New minimal relative error: 15.75%, model saved.
Epoch: 480 Train: 172.34610 Test: 23.19159
Epoch 560: New minimal relative error: 13.49%, model saved.
Epoch: 560 Train: 115.44937 Test: 16.16530
Epoch: 640 Train: 89.89806 Test: 13.42391
Epoch: 720 Train: 150.13318 Test: 20.32614
Epoch 800: New minimal relative error: 5.09%, model saved.
Epoch: 800 Train: 57.43188 Test: 3.96321
Epoch: 880 Train: 39.37613 Test: 1.78963
Epoch: 960 Train: 35.30978 Test: 1.49813
Epoch: 1040 Train: 33.77507 Test: 2.90464
Epoch: 1120 Train: 29.77528 Test: 1.68977
Epoch: 1200 Train: 38.29795 Test: 10.41530
Epoch: 1280 Train: 34.46321 Test: 2.60982
Epoch: 1360 Train: 24.10635 Test: 0.72933
Epoch 1440: New minimal relative error: 3.26%, model saved.
Epoch: 1440 Train: 20.74276 Test: 0.62352
Epoch: 1520 Train: 19.95197 Test: 1.77271
Epoch: 1600 Train: 26.53775 Test: 0.98041
Epoch: 1680 Train: 16.47777 Test: 0.33029
Epoch: 1760 Train: 16.91772 Test: 0.34720
Epoch: 1840 Train: 16.15888 Test: 0.60741
Epoch: 1920 Train: 16.30404 Test: 0.43015
Epoch: 2000 Train: 12.64486 Test: 0.53416
Epoch: 2080 Train: 12.18770 Test: 0.16166
Epoch: 2160 Train: 12.04295 Test: 0.52221
Epoch: 2240 Train: 15.77570 Test: 2.28056
Epoch: 2320 Train: 14.09137 Test: 5.18594
Epoch: 2400 Train: 10.21817 Test: 0.17625
Epoch: 2480 Train: 15.11763 Test: 2.44527
Epoch: 2560 Train: 11.71350 Test: 1.41917
Epoch: 2640 Train: 9.78223 Test: 0.21223
Epoch: 2720 Train: 8.88002 Test: 0.10655
Epoch: 2800 Train: 10.02232 Test: 0.39781
Epoch 2880: New minimal relative error: 2.83%, model saved.
Epoch: 2880 Train: 9.52758 Test: 0.55598
Epoch: 2960 Train: 9.86484 Test: 0.57452
Epoch: 3040 Train: 9.99563 Test: 0.18486
Epoch: 3120 Train: 8.68458 Test: 0.29259
Epoch: 3200 Train: 8.80691 Test: 0.31379
Epoch 3280: New minimal relative error: 2.71%, model saved.
Epoch: 3280 Train: 7.40452 Test: 0.13556
Epoch: 3360 Train: 7.97612 Test: 0.83583
Epoch: 3440 Train: 8.23743 Test: 1.04328
Epoch: 3520 Train: 8.65045 Test: 1.09877
Epoch: 3600 Train: 7.01999 Test: 0.16931
Epoch: 3680 Train: 13.45744 Test: 5.76917
Epoch: 3760 Train: 9.79995 Test: 2.20773
Epoch: 3840 Train: 8.17125 Test: 1.46873
Epoch: 3920 Train: 6.62513 Test: 0.53908
Epoch: 4000 Train: 6.31355 Test: 0.04686
Epoch 4080: New minimal relative error: 2.56%, model saved.
Epoch: 4080 Train: 6.81672 Test: 0.38258
Epoch: 4160 Train: 6.65462 Test: 0.21546
Epoch: 4240 Train: 11.49694 Test: 4.75835
Epoch: 4320 Train: 5.87303 Test: 0.04126
Epoch: 4400 Train: 5.82453 Test: 0.04315
Epoch: 4480 Train: 5.71206 Test: 0.06363
Epoch: 4560 Train: 5.87914 Test: 0.09331
Epoch: 4640 Train: 5.74399 Test: 0.25297
Epoch: 4720 Train: 5.65194 Test: 0.14242
Epoch: 4800 Train: 6.06688 Test: 0.52007
Epoch: 4880 Train: 6.02049 Test: 0.71384
Epoch: 4960 Train: 6.33466 Test: 1.20791
Epoch: 5040 Train: 5.21481 Test: 0.03395
Epoch: 5120 Train: 5.76076 Test: 0.06535
Epoch: 5200 Train: 5.15171 Test: 0.03350
Epoch: 5280 Train: 5.21962 Test: 0.04571
Epoch: 5360 Train: 5.96938 Test: 0.18673
Epoch: 5440 Train: 6.74196 Test: 1.34365
Epoch 5520: New minimal relative error: 1.54%, model saved.
Epoch: 5520 Train: 5.51996 Test: 0.21709
Epoch: 5600 Train: 5.05463 Test: 0.06730
Epoch: 5680 Train: 5.12489 Test: 0.06322
Epoch: 5760 Train: 5.16639 Test: 0.14012
Epoch: 5840 Train: 5.36115 Test: 0.25519
Epoch: 5920 Train: 4.93248 Test: 0.08868
Epoch: 6000 Train: 6.31496 Test: 1.71094
Epoch: 6080 Train: 4.91053 Test: 0.07979
Epoch: 6160 Train: 5.10956 Test: 0.31452
Epoch: 6240 Train: 4.93657 Test: 0.08357
Epoch: 6320 Train: 5.07892 Test: 0.11379
Epoch: 6400 Train: 5.55880 Test: 0.84592
Epoch: 6480 Train: 4.71037 Test: 0.03947
Epoch: 6560 Train: 4.72528 Test: 0.03668
Epoch: 6640 Train: 4.89155 Test: 0.16929
Epoch: 6720 Train: 7.04119 Test: 1.43958
Epoch: 6800 Train: 4.59959 Test: 0.08725
Epoch: 6880 Train: 5.70437 Test: 0.83155
Epoch: 6960 Train: 4.71673 Test: 0.03653
Epoch: 7040 Train: 4.49894 Test: 0.07335
Epoch: 7120 Train: 4.37216 Test: 0.03794
Epoch: 7200 Train: 4.41942 Test: 0.04357
Epoch: 7280 Train: 4.70356 Test: 0.11352
Epoch: 7360 Train: 4.29333 Test: 0.03704
Epoch: 7440 Train: 4.78113 Test: 0.31904
Epoch: 7520 Train: 4.94870 Test: 0.62465
Epoch: 7600 Train: 4.23838 Test: 0.02450
Epoch: 7680 Train: 4.28294 Test: 0.13380
Epoch: 7760 Train: 4.28126 Test: 0.02414
Epoch: 7840 Train: 4.86162 Test: 0.19060
Epoch: 7920 Train: 4.25814 Test: 0.02344
Epoch: 7999 Train: 4.32811 Test: 0.06705
Training Loss: tensor(4.3281)
Test Loss: tensor(0.0671)
Learned LE: [  0.7964951    0.06585602 -14.539221  ]
True LE: [ 8.5297179e-01  4.6950197e-03 -1.4535046e+01]
Relative Error: [0.6680719  0.54083985 0.5457398  0.64714986 0.7871946  1.0969471
 1.3972068  1.7181495  1.9953926  2.3145313  2.5659983  2.7877579
 2.7872105  2.6199243  2.1095057  1.7480217  1.4782639  1.2938291
 1.1964068  1.0734961  0.8897046  0.9977367  1.2533858  1.4861056
 1.5615793  1.4642808  1.4052905  1.6018984  1.7800676  1.7617904
 1.7716985  1.8065248  1.8239591  1.672337   1.299109   1.0841776
 1.2587746  1.3894324  1.2522874  1.0060736  0.7119719  0.6864611
 0.7587243  0.92192835 0.7898356  0.79821515 0.92960036 0.8790791
 0.7901008  0.8954986  1.0068778  0.8043208  0.67624617 0.6294156
 0.88702625 1.0709344  0.96251583 0.48432717 0.7220205  0.45968467
 0.3360162  0.66272265 0.79934233 0.6600913  0.50476986 0.565603
 0.6438004  0.90189576 1.2516088  1.5159979  1.8911041  2.256615
 2.4397984  2.5545895  2.6105425  2.4476695  2.1639385  1.7094834
 1.4833806  1.1173732  1.130883   0.98080754 0.82348037 0.8313724
 1.137891   1.3077598  1.4185983  1.4407763  1.4109591  1.2844727
 1.4048326  1.4039034  1.3282548  1.4485357  1.5549872  1.4677186
 1.2395531  0.86125946 1.0324886  1.266108   1.2384808  0.97226834
 0.6051719  0.6024412  0.76643574 0.81605625 0.92542654 0.66593003
 0.7926267  0.9052492  0.8151155  0.72490853 0.6869428  0.59589106
 0.554222   0.62902904 0.94505334 1.2189059  1.1840211  0.5382963
 0.7019814  0.6899005  0.23469776 0.66177833 0.84976506 0.7633127
 0.5474643  0.47720528 0.54262614 0.68248004 1.0386689  1.3216702
 1.7329838  2.0225854  2.3697019  2.3773298  2.3224695  2.2494187
 2.1477463  1.855228   1.4100696  1.1246053  0.9200056  0.9547215
 0.80250853 0.58881193 0.88125217 1.1455534  1.2404107  1.3052106
 1.3302764  1.2292262  0.9522262  0.92021245 0.9389014  1.0495869
 1.2834964  1.4278404  1.2975092  0.9431193  0.89760965 1.0860282
 1.1596158  1.0040497  0.6066735  0.47079808 0.70828825 0.77956843
 0.69161975 0.6909348  0.5889406  0.671679   0.633282   0.52658224
 0.38676172 0.51358277 0.49751407 0.72174585 0.94430375 1.2378801
 1.2926695  0.75241464 0.26875308 0.7950236  0.20934932 0.5153081
 0.8285953  0.89710516 0.71429247 0.45347762 0.4666757  0.5574304
 0.73831624 1.0186943  1.503452   1.8031487  2.0017219  2.1711504
 2.1275513  2.0334818  2.0102715  1.8470135  1.4481255  1.0396672
 0.78191245 0.64843327 0.68029565 0.60095954 0.5810284  0.8938882
 0.9922172  1.0396712  1.1146047  1.0964491  0.8955797  0.4152557
 0.54606485 0.7715565  0.9775384  1.2472113  1.3109452  1.0548997
 0.70152444 0.93465    1.0665967  0.9578177  0.7230852  0.44840634
 0.60076404 0.67112416 0.6661463  0.5013349  0.489739   0.35835582
 0.42808834 0.35843578 0.47422546 0.8577959  0.84947616 0.9029897
 1.1385792  1.350888   1.2682588  0.94618034 0.54324526 0.198465
 0.4829085  0.23245618 0.66723305 0.8301766  0.7872255  0.60705787
 0.5254321  0.5177529  0.57851386 0.7207109  1.1819408  1.5645463
 1.7541131  1.8010103  1.8779633  1.8939145  1.7747473  1.6589595
 1.5177133  1.2117779  0.9726546  0.7429895  0.71146274 0.67281705
 0.62353206 0.68549603 0.8563618  0.7542418  0.7688822  0.90763617
 0.77442664 0.5451823  0.38751698 0.6006953  0.7762716  1.0180558
 1.1344719  1.1281574  0.8376721  0.6928167  0.96849865 0.99827063
 0.74169564 0.44911015 0.4787469  0.53899455 0.5904282  0.51037204
 0.30557752 0.21222539 0.51373386 0.5674     0.7121252  1.1195732
 1.3080771  1.1979659  1.4633621  1.5281965  1.4460638  1.109208
 0.80710006 0.60449845 0.36862642 0.31670475 0.34220123 0.60995674
 0.7611902  0.7124453  0.5711774  0.48859763 0.46978673 0.546278
 0.83695966 1.1849139  1.4367756  1.5846295  1.662203   1.5391803
 1.6288658  1.630401   1.4872221  1.4105312  1.3137867  1.1523423
 0.8568227  0.8191286  0.81724685 0.71616864 0.6973778  0.76765513
 0.651152   0.69619787 0.51738733 0.35648853 0.48202324 0.5149552
 0.7153584  0.7874811  1.0205994  1.2236986  1.1010655  0.6992475
 0.63212746 0.96426755 0.98725927 0.6791918  0.5988048  0.42369398
 0.49703586 0.51507574 0.43474483 0.5013187  0.67089367 0.93915045
 0.8550861  1.065177   1.5554427  1.6374823  1.5713403  1.8463131
 2.0134292  1.4782213  0.9972485  0.8298236  0.80377686 0.51154774
 0.26749027 0.28771457 0.55074674 0.6618707  0.59599304 0.48722765
 0.44588032 0.46739945 0.55297947 0.8606143  1.1245426  1.2008154
 1.3651834  1.3487967  1.4264681  1.402208   1.4741157  1.4741193
 1.3506817  1.282349   1.2977657  1.0097901  0.9195333  0.9280245
 0.86993325 0.8201748  0.80807877 0.703059  ]
