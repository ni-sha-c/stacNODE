time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.13%, model saved.
Epoch: 0 Train: 9843.93750 Test: 4034.05420
Epoch 100: New minimal relative error: 98.96%, model saved.
Epoch: 100 Train: 2353.43506 Test: 854.25171
Epoch: 200 Train: 1279.50867 Test: 331.20355
Epoch: 300 Train: 675.95978 Test: 162.04027
Epoch 400: New minimal relative error: 50.24%, model saved.
Epoch: 400 Train: 326.15994 Test: 136.34084
Epoch 500: New minimal relative error: 17.62%, model saved.
Epoch: 500 Train: 160.36018 Test: 25.39407
Epoch 600: New minimal relative error: 14.83%, model saved.
Epoch: 600 Train: 99.54826 Test: 14.54991
Epoch: 700 Train: 118.45580 Test: 9.52344
Epoch: 800 Train: 155.40909 Test: 23.44595
Epoch: 900 Train: 69.68501 Test: 10.75112
Epoch 1000: New minimal relative error: 4.70%, model saved.
Epoch: 1000 Train: 34.28238 Test: 1.41924
Epoch: 1100 Train: 32.21513 Test: 2.22120
Epoch: 1200 Train: 59.92210 Test: 31.83190
Epoch: 1300 Train: 33.64100 Test: 2.71279
Epoch: 1400 Train: 20.54384 Test: 0.75233
Epoch: 1500 Train: 21.04392 Test: 6.93284
Epoch: 1600 Train: 21.77432 Test: 1.95040
Epoch: 1700 Train: 39.67248 Test: 8.54911
Epoch 1800: New minimal relative error: 4.52%, model saved.
Epoch: 1800 Train: 23.05115 Test: 2.01888
Epoch 1900: New minimal relative error: 4.44%, model saved.
Epoch: 1900 Train: 15.70158 Test: 0.41474
Epoch: 2000 Train: 14.35383 Test: 0.27333
Epoch: 2100 Train: 14.16714 Test: 1.45037
Epoch: 2200 Train: 14.23300 Test: 0.84061
Epoch: 2300 Train: 16.95465 Test: 0.53116
Epoch: 2400 Train: 15.02135 Test: 0.70107
Epoch: 2500 Train: 17.58258 Test: 4.84276
Epoch: 2600 Train: 10.31340 Test: 0.22530
Epoch: 2700 Train: 19.17624 Test: 8.59039
Epoch: 2800 Train: 11.55687 Test: 0.93916
Epoch: 2900 Train: 11.05018 Test: 0.70346
Epoch: 3000 Train: 16.78558 Test: 5.29151
Epoch 3100: New minimal relative error: 3.80%, model saved.
Epoch: 3100 Train: 8.86743 Test: 0.26006
Epoch: 3200 Train: 19.33409 Test: 5.99313
Epoch: 3300 Train: 9.98386 Test: 1.31529
Epoch: 3400 Train: 9.05740 Test: 1.22168
Epoch: 3500 Train: 7.40976 Test: 0.28595
Epoch 3600: New minimal relative error: 3.13%, model saved.
Epoch: 3600 Train: 6.73619 Test: 0.09326
Epoch: 3700 Train: 7.46485 Test: 0.32970
Epoch: 3800 Train: 6.60140 Test: 0.06541
Epoch: 3900 Train: 6.55475 Test: 0.08566
Epoch: 4000 Train: 6.72664 Test: 0.35259
Epoch: 4100 Train: 7.72138 Test: 0.97191
Epoch: 4200 Train: 11.50306 Test: 4.11819
Epoch: 4300 Train: 5.99885 Test: 0.04141
Epoch: 4400 Train: 6.11757 Test: 0.14659
Epoch: 4500 Train: 6.06630 Test: 0.12633
Epoch: 4600 Train: 6.04071 Test: 0.59669
Epoch 4700: New minimal relative error: 2.99%, model saved.
Epoch: 4700 Train: 5.85971 Test: 0.09300
Epoch: 4800 Train: 5.27566 Test: 0.07025
Epoch: 4900 Train: 5.03610 Test: 0.07223
Epoch: 5000 Train: 5.22689 Test: 0.27084
Epoch: 5100 Train: 18.89683 Test: 10.78235
Epoch: 5200 Train: 4.90602 Test: 0.14039
Epoch: 5300 Train: 4.74597 Test: 0.08943
Epoch: 5400 Train: 12.33197 Test: 6.27655
Epoch: 5500 Train: 5.92137 Test: 0.87661
Epoch: 5600 Train: 4.52788 Test: 0.04636
Epoch: 5700 Train: 4.30273 Test: 0.03276
Epoch: 5800 Train: 4.75490 Test: 0.13031
Epoch: 5900 Train: 4.29605 Test: 0.07546
Epoch 6000: New minimal relative error: 2.82%, model saved.
Epoch: 6000 Train: 4.07179 Test: 0.05666
Epoch: 6100 Train: 3.90486 Test: 0.03992
Epoch: 6200 Train: 3.89599 Test: 0.02944
Epoch: 6300 Train: 4.44737 Test: 0.41149
Epoch: 6400 Train: 4.12323 Test: 0.14915
Epoch: 6500 Train: 4.72120 Test: 0.82149
Epoch: 6600 Train: 3.84149 Test: 0.13525
Epoch: 6700 Train: 5.05568 Test: 1.04446
Epoch: 6800 Train: 3.65881 Test: 0.01912
Epoch 6900: New minimal relative error: 1.43%, model saved.
Epoch: 6900 Train: 3.69877 Test: 0.02029
Epoch: 7000 Train: 3.63219 Test: 0.02862
Epoch: 7100 Train: 5.66013 Test: 1.60619
Epoch: 7200 Train: 4.06391 Test: 0.32512
Epoch: 7300 Train: 3.54334 Test: 0.03085
Epoch: 7400 Train: 3.50711 Test: 0.02980
Epoch: 7500 Train: 3.52222 Test: 0.02747
Epoch: 7600 Train: 3.78201 Test: 0.50815
Epoch: 7700 Train: 4.15428 Test: 0.63481
Epoch: 7800 Train: 4.87986 Test: 1.15427
Epoch: 7900 Train: 3.14466 Test: 0.05807
Epoch: 8000 Train: 3.11727 Test: 0.01826
Epoch: 8100 Train: 3.11821 Test: 0.06200
Epoch: 8200 Train: 3.15746 Test: 0.08336
Epoch: 8300 Train: 3.10275 Test: 0.03611
Epoch: 8400 Train: 3.10187 Test: 0.02773
Epoch 8500: New minimal relative error: 1.31%, model saved.
Epoch: 8500 Train: 3.05497 Test: 0.02461
Epoch: 8600 Train: 3.19631 Test: 0.14927
Epoch: 8700 Train: 3.04085 Test: 0.04062
Epoch: 8800 Train: 2.88182 Test: 0.02773
Epoch: 8900 Train: 2.85761 Test: 0.01821
Epoch: 9000 Train: 2.99505 Test: 0.14135
Epoch: 9100 Train: 3.26268 Test: 0.31466
Epoch: 9200 Train: 2.84425 Test: 0.08138
Epoch: 9300 Train: 2.96201 Test: 0.08585
Epoch: 9400 Train: 2.71236 Test: 0.03128
Epoch: 9500 Train: 2.64053 Test: 0.02406
Epoch 9600: New minimal relative error: 1.16%, model saved.
Epoch: 9600 Train: 2.65115 Test: 0.01295
Epoch: 9700 Train: 2.55794 Test: 0.01396
Epoch: 9800 Train: 2.58216 Test: 0.01523
Epoch: 9900 Train: 2.62451 Test: 0.05999
Epoch: 9999 Train: 2.49226 Test: 0.01307
Training Loss: tensor(2.4923)
Test Loss: tensor(0.0131)
Learned LE: [ 8.5006070e-01  1.0236624e-02 -1.4536617e+01]
True LE: [ 8.5297197e-01  4.6953312e-03 -1.4535044e+01]
Relative Error: [1.5581375  1.5631367  1.3622609  1.0496922  0.72409505 0.5872619
 0.27492768 0.28188446 0.4708705  0.2870212  0.11339734 0.17082453
 0.18487006 0.19125822 0.2898776  0.14409542 0.19160698 0.3471934
 0.5596733  0.7260846  1.0517962  0.97234315 0.8477365  0.70023984
 0.56402665 0.5457083  0.6622901  0.8002114  0.9479043  1.0407959
 0.99454767 0.9793932  0.7726047  0.71951586 0.554996   0.41451055
 0.56963336 0.7770665  0.9111737  0.84981555 0.67810196 0.5031351
 0.6812333  0.7369089  0.94115025 1.0563823  1.185414   1.038689
 1.0192038  1.09769    1.4610227  1.7302009  1.4671601  1.1584756
 0.6783575  0.08959452 0.7981608  0.88708603 0.5315484  0.13191842
 0.5648525  1.0786511  1.3685638  1.3466358  1.1636574  0.86885643
 0.5642663  0.4350576  0.1715089  0.40645227 0.5052631  0.39456812
 0.21910141 0.17015988 0.14926518 0.02642597 0.05019021 0.10675978
 0.1975262  0.46650237 0.71293575 0.67434835 0.8306533  0.80220073
 0.7713381  0.75586396 0.5459965  0.42664742 0.4797736  0.5226594
 0.6186682  0.7859768  0.84546226 0.9629937  0.9086859  0.68276787
 0.6536223  0.44566345 0.60774976 0.8302712  0.96322167 0.8451921
 0.6428166  0.3491711  0.45646107 0.5676286  0.6957426  0.9658452
 0.9050972  0.93884915 1.0473971  1.1616197  1.476389   1.787545
 1.6799433  1.3807153  0.9762198  0.30979398 0.44616103 1.0621336
 0.97876424 0.42098746 0.2692038  0.7794129  1.0625454  1.1008546
 0.9341309  0.7033802  0.38668928 0.32777044 0.19202836 0.4595009
 0.6228842  0.5378161  0.35678402 0.2599171  0.19520389 0.17467758
 0.16466293 0.13426094 0.16053884 0.28872156 0.48156396 0.43299693
 0.52121747 0.6716495  0.5201015  0.48616025 0.4331089  0.39963117
 0.48050871 0.3635771  0.41213384 0.64120233 0.7868363  0.8935392
 0.93999785 0.6818127  0.57098544 0.4281435  0.5132641  0.80403376
 1.0396705  1.0812781  0.7484936  0.30367082 0.23631968 0.3881223
 0.47354203 0.6224431  0.7972069  0.8242865  0.90743136 1.1651728
 1.2979356  1.719234   1.7412434  1.5098523  1.2033248  0.652197
 0.05591041 0.55405354 1.1068939  0.9461628  0.34486046 0.30825648
 0.6578405  0.8638122  0.7811519  0.59045887 0.24316514 0.2216297
 0.14461772 0.41813818 0.6456389  0.5937554  0.45449412 0.41764352
 0.33969885 0.2735266  0.30356568 0.2889955  0.20770837 0.18135837
 0.2559904  0.36513022 0.3882213  0.48401067 0.42981935 0.36734185
 0.33143508 0.26024586 0.33279365 0.42704308 0.25831068 0.45882472
 0.65635645 0.7198995  0.7625733  0.83783966 0.6333866  0.45692614
 0.3258293  0.5375032  0.91458243 1.112903   1.0502642  0.6253449
 0.21516572 0.18691744 0.3570755  0.34680203 0.5223784  0.7564199
 0.76708037 0.9630316  1.0689938  1.458023   1.7160926  1.5437642
 1.3103989  0.8905984  0.42929816 0.0575259  0.45646486 1.119902
 0.89462334 0.45117894 0.25680944 0.612024   0.64005035 0.48831627
 0.24929139 0.25023416 0.20976707 0.3390758  0.63202375 0.63311785
 0.5381336  0.32756284 0.27164778 0.25385112 0.16306002 0.2229383
 0.0575854  0.12575904 0.23760246 0.25379762 0.249526   0.29740524
 0.28796273 0.23066638 0.21478091 0.24591361 0.1526064  0.24960253
 0.13749176 0.27356786 0.59812146 0.5517316  0.5884358  0.7352706
 0.799047   0.5054038  0.37340185 0.2031612  0.5194805  0.92375875
 1.0577387  0.9002365  0.46125814 0.21270262 0.30617797 0.3309676
 0.10202385 0.5224037  0.61454844 0.6742398  0.87391484 1.0013988
 1.3461058  1.4139442  1.3149307  1.0150776  0.71810526 0.3284917
 0.13852432 0.39633316 1.1244804  1.0111778  0.44709682 0.21051425
 0.41517162 0.36541808 0.24980293 0.32917517 0.26047283 0.35479906
 0.53370136 0.67121965 0.5219185  0.22101954 0.13221578 0.15570247
 0.22625148 0.11917913 0.0094302  0.1751179  0.2735065  0.43586475
 0.24929503 0.25772017 0.24604781 0.22834006 0.2379729  0.2446491
 0.131387   0.15085255 0.17016551 0.16212913 0.3284629  0.4721639
 0.4531349  0.581238   0.68626755 0.7354976  0.40575    0.22675619
 0.13913906 0.46119854 0.8454273  0.90322727 0.72906387 0.47800153
 0.3143312  0.5228606  0.21539661 0.23867193 0.42391047 0.32995558
 0.4357561  0.6191882  0.73068655 1.2320464  1.234845   1.0729105
 0.8155076  0.51065975 0.34386876 0.2529503  0.30283308 1.0015838
 1.1799197  0.50505704 0.15092903 0.24976735 0.16426231 0.21028207
 0.17708628 0.18249255 0.40624806 0.5970341  0.4389544  0.22843596
 0.2139     0.15369883 0.11180373 0.20064224 0.14619026 0.08342367
 0.23099647 0.44862983 0.51482606 0.26469117 0.19828384 0.18223527
 0.2192982  0.23068248 0.39397496 0.30452448]
