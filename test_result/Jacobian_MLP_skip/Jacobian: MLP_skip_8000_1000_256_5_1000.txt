time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.74%, model saved.
Epoch: 0 Train: 59735.99609 Test: 4019.53760
Epoch 80: New minimal relative error: 93.71%, model saved.
Epoch: 80 Train: 16281.70508 Test: 1527.52771
Epoch 160: New minimal relative error: 92.80%, model saved.
Epoch: 160 Train: 14861.82422 Test: 1414.27551
Epoch: 240 Train: 13726.65137 Test: 1094.55505
Epoch 320: New minimal relative error: 80.89%, model saved.
Epoch: 320 Train: 14502.54297 Test: 1258.49585
Epoch 400: New minimal relative error: 70.49%, model saved.
Epoch: 400 Train: 14093.06445 Test: 1295.76282
Epoch: 480 Train: 14608.02930 Test: 1218.65808
Epoch: 560 Train: 12904.20312 Test: 1357.32178
Epoch: 640 Train: 13814.34668 Test: 1303.54761
Epoch: 720 Train: 13824.03711 Test: 1194.68372
Epoch: 800 Train: 12021.91113 Test: 972.57788
Epoch: 880 Train: 11829.32812 Test: 907.85974
Epoch: 960 Train: 9305.71875 Test: 630.75116
Epoch: 1040 Train: 8323.75195 Test: 490.87039
Epoch: 1120 Train: 6596.83301 Test: 317.09698
Epoch: 1200 Train: 4400.74756 Test: 194.16399
Epoch: 1280 Train: 2921.04077 Test: 104.81200
Epoch 1360: New minimal relative error: 63.78%, model saved.
Epoch: 1360 Train: 2492.58789 Test: 56.71001
Epoch 1440: New minimal relative error: 39.13%, model saved.
Epoch: 1440 Train: 1859.97522 Test: 30.99336
Epoch: 1520 Train: 1891.22949 Test: 64.28553
Epoch 1600: New minimal relative error: 25.89%, model saved.
Epoch: 1600 Train: 1475.76160 Test: 27.57905
Epoch 1680: New minimal relative error: 13.31%, model saved.
Epoch: 1680 Train: 1271.93079 Test: 16.63750
Epoch: 1760 Train: 1170.17688 Test: 15.61125
Epoch: 1840 Train: 1087.62390 Test: 17.18299
Epoch: 1920 Train: 1044.48193 Test: 31.83416
Epoch: 2000 Train: 932.82544 Test: 17.28406
Epoch: 2080 Train: 877.14221 Test: 12.36732
Epoch: 2160 Train: 785.02539 Test: 9.53523
Epoch: 2240 Train: 766.41980 Test: 9.81211
Epoch: 2320 Train: 775.80841 Test: 13.82818
Epoch 2400: New minimal relative error: 8.09%, model saved.
Epoch: 2400 Train: 669.88385 Test: 7.89851
Epoch: 2480 Train: 655.19641 Test: 8.03486
Epoch: 2560 Train: 622.95105 Test: 7.32349
Epoch: 2640 Train: 545.53125 Test: 5.66148
Epoch: 2720 Train: 529.00061 Test: 9.31286
Epoch: 2800 Train: 501.43710 Test: 5.72622
Epoch: 2880 Train: 614.57379 Test: 8.16693
Epoch: 2960 Train: 510.53638 Test: 5.66767
Epoch: 3040 Train: 462.52518 Test: 5.51522
Epoch: 3120 Train: 416.95178 Test: 4.14345
Epoch: 3200 Train: 385.58307 Test: 3.44599
Epoch: 3280 Train: 358.43295 Test: 3.06799
Epoch 3360: New minimal relative error: 5.71%, model saved.
Epoch: 3360 Train: 331.69238 Test: 2.87915
Epoch: 3440 Train: 313.87524 Test: 2.70401
Epoch: 3520 Train: 295.92020 Test: 2.46001
Epoch: 3600 Train: 286.65387 Test: 2.66571
Epoch: 3680 Train: 283.71436 Test: 2.36004
Epoch: 3760 Train: 285.65372 Test: 2.49708
Epoch: 3840 Train: 295.01822 Test: 4.88362
Epoch: 3920 Train: 375.86127 Test: 6.22250
Epoch: 4000 Train: 371.21530 Test: 4.08278
Epoch: 4080 Train: 326.04919 Test: 2.83119
Epoch: 4160 Train: 441.07391 Test: 8.56841
Epoch: 4240 Train: 310.46008 Test: 2.87087
Epoch: 4320 Train: 332.58151 Test: 4.05976
Epoch: 4400 Train: 295.57507 Test: 3.15413
Epoch: 4480 Train: 355.85672 Test: 4.71875
Epoch: 4560 Train: 291.36826 Test: 4.49627
Epoch: 4640 Train: 282.42435 Test: 2.78563
Epoch: 4720 Train: 380.54932 Test: 4.68965
Epoch: 4800 Train: 300.90384 Test: 3.09954
Epoch: 4880 Train: 339.24200 Test: 3.82304
Epoch: 4960 Train: 300.25092 Test: 2.94586
Epoch: 5040 Train: 311.95251 Test: 4.10940
Epoch: 5120 Train: 305.08157 Test: 3.35142
Epoch: 5200 Train: 365.50574 Test: 5.28366
Epoch: 5280 Train: 313.29141 Test: 3.07276
Epoch: 5360 Train: 356.97443 Test: 4.02283
Epoch: 5440 Train: 284.81384 Test: 2.72313
Epoch: 5520 Train: 261.66898 Test: 2.19466
Epoch: 5600 Train: 247.49863 Test: 2.39991
Epoch: 5680 Train: 237.07603 Test: 3.12392
Epoch: 5760 Train: 242.74506 Test: 2.23131
Epoch: 5840 Train: 256.00946 Test: 2.91519
Epoch: 5920 Train: 231.79088 Test: 1.94444
Epoch: 6000 Train: 221.92177 Test: 1.72569
Epoch: 6080 Train: 207.82607 Test: 1.65669
Epoch: 6160 Train: 205.62064 Test: 1.85394
Epoch: 6240 Train: 213.25272 Test: 1.98218
Epoch: 6320 Train: 221.83492 Test: 2.26634
Epoch: 6400 Train: 215.38403 Test: 2.34243
Epoch: 6480 Train: 191.65866 Test: 1.63475
Epoch: 6560 Train: 192.63647 Test: 1.50332
Epoch 6640: New minimal relative error: 5.03%, model saved.
Epoch: 6640 Train: 183.24870 Test: 1.48287
Epoch: 6720 Train: 177.52615 Test: 1.40150
Epoch: 6800 Train: 171.99683 Test: 1.41531
Epoch: 6880 Train: 178.35300 Test: 1.54664
Epoch: 6960 Train: 219.50189 Test: 1.93376
Epoch: 7040 Train: 241.47586 Test: 2.98534
Epoch: 7120 Train: 249.59921 Test: 2.85482
Epoch: 7200 Train: 222.06763 Test: 2.50453
Epoch: 7280 Train: 222.47626 Test: 1.98632
Epoch: 7360 Train: 231.79437 Test: 1.98800
Epoch: 7440 Train: 237.31552 Test: 2.35174
Epoch 7520: New minimal relative error: 4.71%, model saved.
Epoch: 7520 Train: 235.24123 Test: 2.25736
Epoch: 7600 Train: 192.12830 Test: 1.53031
Epoch: 7680 Train: 194.69099 Test: 1.58914
Epoch: 7760 Train: 226.02858 Test: 2.14841
Epoch: 7840 Train: 246.23270 Test: 2.74803
Epoch: 7920 Train: 198.12582 Test: 1.65498
Epoch: 7999 Train: 201.10541 Test: 2.62369
Training Loss: tensor(201.1054)
Test Loss: tensor(2.6237)
Learned LE: [ 8.8809812e-01 -8.0361934e-03 -1.4533348e+01]
True LE: [ 8.7128067e-01 -6.1654863e-03 -1.4541613e+01]
Relative Error: [2.4535575  2.8985317  3.348401   3.648399   3.8095708  3.7587192
 3.4673645  3.0499103  2.5972252  1.9390849  1.4017633  1.2389553
 1.0337784  1.1331394  1.2620153  1.2886801  1.1727883  1.4755414
 2.235797   2.944791   3.6094542  4.1625543  4.5553823  4.685147
 4.6111403  4.8135843  5.1681304  5.6827483  6.088097   6.498479
 6.84493    7.034707   7.078714   6.759095   6.313078   5.7402263
 4.948444   4.622309   4.2755594  3.7710931  3.0860407  3.4891572
 4.0336637  4.4602     4.6353145  4.69806    4.7146583  4.652254
 4.312574   4.030685   3.5912879  3.1385713  3.1269088  3.104928
 2.990783   2.8478618  2.6882677  2.4081044  2.0301707  1.7272456
 1.6118239  1.8696431  1.9715648  2.1702268  2.6054258  2.9300294
 3.121722   3.1140897  2.9591556  2.6873565  2.21798    1.6588078
 1.2055547  1.0129148  1.0179292  1.1155262  1.1417971  1.1042912
 1.0052954  1.6967148  2.3758168  2.9906025  3.5653949  4.00339
 4.3734407  4.4495897  4.4220133  4.3170037  4.6180916  5.013079
 5.394761   5.86965    6.302543   6.6179695  6.6498303  6.4564137
 6.1842756  5.7175274  5.036959   4.1923165  3.904314   3.4320045
 2.7920017  2.6554017  3.2246535  3.6783702  3.999179   4.070618
 4.0369587  3.975388   3.6901581  3.4320486  3.2480755  3.1467054
 3.114897   2.995881   2.8040867  2.5143254  2.2590888  1.9542662
 1.5005236  1.3902757  1.3707415  1.4192129  1.4779766  1.5509412
 1.8984847  2.2686815  2.4744534  2.5073318  2.4081078  2.2588177
 1.9433588  1.3940809  1.0629375  0.8536702  1.0378757  1.1227764
 1.0812845  0.9747062  1.0024424  1.7983934  2.4746368  3.008184
 3.4847572  3.8204665  4.1177707  4.161302   4.064549   4.0416627
 3.939106   4.3356113  4.618914   5.079194   5.6171083  6.025806
 6.1435943  6.1319776  6.0219345  5.6991906  5.149628   4.3632436
 3.5273347  3.1192214  2.5367064  1.8240067  2.377122   2.8551607
 3.2445755  3.4927104  3.4818847  3.4068267  3.2118256  3.0734587
 3.116022   3.300722   3.2503598  3.0796006  2.8388066  2.442449
 2.0688796  1.6653932  0.9581003  0.8566182  0.88538706 0.8915908
 1.005334   1.1715691  1.2592313  1.5958538  1.8514696  1.9534975
 1.9466445  1.7867882  1.4730486  1.2186036  1.2290003  0.9867135
 1.0418352  1.140866   1.149453   0.97257507 1.0915976  1.7674539
 2.4320214  2.9489653  3.3444908  3.5969093  3.8094711  3.8094468
 3.6353576  3.5471036  3.5443022  3.4432337  3.7792537  4.2036595
 4.7396636  5.2208505  5.507159   5.7022595  5.777527   5.6354184
 5.251907   4.582046   3.7107117  2.916607   2.460007   1.8625568
 1.6691492  2.0550585  2.4087164  2.7359538  2.9888985  2.989789
 2.89055    2.914146   2.8662677  3.0385127  3.0881937  3.0486448
 3.1062956  2.7254765  2.2668848  1.7561629  0.9203648  0.4378678
 0.5291634  0.4389959  0.5547041  0.8328433  0.95341307 0.96839166
 1.2351811  1.4402626  1.4989936  1.3283631  1.0802372  0.903188
 1.3754958  1.1144799  1.0714902  1.1297454  1.179723   1.0392276
 1.1487163  1.6832664  2.2274394  2.7264738  3.1195786  3.3220985
 3.4220338  3.383024   3.1946976  2.9753273  2.9388824  2.9204388
 2.7026193  3.1929693  3.7605672  4.1934767  4.6655426  5.0748787
 5.3752813  5.4536095  5.2584367  4.7905726  4.1944966  3.273695
 2.513707   2.0681524  1.3654692  1.5542754  1.8593671  2.0858293
 2.3077705  2.5762575  2.7668965  2.5490303  2.333871   2.4731863
 2.5893152  2.4874609  2.4211411  2.5964332  2.8714585  2.326748
 1.6778119  1.093719   0.73785794 0.68398315 0.38190427 0.5216395
 0.71721053 0.6787423  0.62918097 0.861325   0.9996919  1.0075272
 0.8755     0.76039875 1.1399084  1.2724524  1.1526417  1.1888689
 1.1837394  1.1336908  1.0961854  1.5193802  2.0467591  2.4389572
 2.7415037  2.9907854  2.9835289  2.955036   2.6420274  2.3879008
 2.184357   2.2101836  2.0499694  1.9568778  2.5297213  3.059598
 3.525524   4.1409225  4.706854   5.0225325  5.0602727  4.9239573
 4.6537647  4.01724    2.9805553  2.2661452  1.7036855  1.0803638
 1.3717364  1.6715378  1.9365996  2.169201   2.3601859  2.2850919
 2.0234869  2.0635378  2.1405306  2.1351275  1.9411093  1.9590182
 2.255314   2.7261353  2.75647    2.2301378  1.6631372  1.3150715
 0.92269677 0.48919162 0.4987546  0.51134264 0.46657357 0.58139074
 0.62818754 0.72359514 0.76768273 0.79884005 0.99335957 1.3474067
 1.2073433  1.3150324  1.2405322  1.1660041  1.119256   1.3003591
 1.6249765  2.0688133  2.34593    2.5021303  2.5383878  2.4019525
 2.1610708  1.7261473  1.447786   1.5347428 ]
