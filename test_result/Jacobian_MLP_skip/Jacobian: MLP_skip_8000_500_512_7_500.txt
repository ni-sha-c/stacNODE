time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.05%, model saved.
Epoch: 0 Train: 31351.90625 Test: 4055.84106
Epoch 80: New minimal relative error: 88.54%, model saved.
Epoch: 80 Train: 8583.72656 Test: 1436.17334
Epoch: 160 Train: 7698.32324 Test: 1191.11450
Epoch: 240 Train: 8161.82129 Test: 1401.82288
Epoch 320: New minimal relative error: 70.63%, model saved.
Epoch: 320 Train: 7030.38770 Test: 1020.62396
Epoch: 400 Train: 6515.73682 Test: 981.79755
Epoch 480: New minimal relative error: 68.16%, model saved.
Epoch: 480 Train: 6102.35254 Test: 1036.61169
Epoch: 560 Train: 4747.94971 Test: 644.36517
Epoch: 640 Train: 4496.33057 Test: 496.90353
Epoch 720: New minimal relative error: 67.69%, model saved.
Epoch: 720 Train: 3658.38770 Test: 412.94031
Epoch: 800 Train: 2657.47998 Test: 296.20099
Epoch: 880 Train: 1442.09839 Test: 167.49931
Epoch 960: New minimal relative error: 18.85%, model saved.
Epoch: 960 Train: 488.79749 Test: 17.67312
Epoch 1040: New minimal relative error: 18.18%, model saved.
Epoch: 1040 Train: 341.76785 Test: 9.61798
Epoch: 1120 Train: 311.22668 Test: 13.68927
Epoch: 1200 Train: 304.35379 Test: 24.66843
Epoch 1280: New minimal relative error: 6.31%, model saved.
Epoch: 1280 Train: 179.61040 Test: 2.33341
Epoch: 1360 Train: 159.02124 Test: 8.36215
Epoch: 1440 Train: 129.35503 Test: 1.01930
Epoch: 1520 Train: 122.75748 Test: 7.93843
Epoch: 1600 Train: 109.07240 Test: 1.60390
Epoch: 1680 Train: 111.75323 Test: 1.13262
Epoch 1760: New minimal relative error: 4.58%, model saved.
Epoch: 1760 Train: 93.21924 Test: 3.34618
Epoch: 1840 Train: 83.25043 Test: 4.51080
Epoch: 1920 Train: 86.89204 Test: 9.74705
Epoch: 2000 Train: 76.62163 Test: 1.64249
Epoch: 2080 Train: 65.38129 Test: 1.32175
Epoch: 2160 Train: 63.49771 Test: 1.26198
Epoch: 2240 Train: 64.63579 Test: 0.69152
Epoch: 2320 Train: 70.55084 Test: 4.93839
Epoch: 2400 Train: 72.28194 Test: 3.00682
Epoch: 2480 Train: 62.46156 Test: 3.32730
Epoch: 2560 Train: 60.98518 Test: 2.47207
Epoch: 2640 Train: 59.57328 Test: 2.09141
Epoch 2720: New minimal relative error: 4.13%, model saved.
Epoch: 2720 Train: 51.75238 Test: 2.27882
Epoch 2800: New minimal relative error: 3.21%, model saved.
Epoch: 2800 Train: 48.31026 Test: 0.24811
Epoch: 2880 Train: 74.13683 Test: 5.42484
Epoch: 2960 Train: 43.58311 Test: 0.64501
Epoch: 3040 Train: 45.97638 Test: 1.90678
Epoch: 3120 Train: 52.41381 Test: 3.37944
Epoch 3200: New minimal relative error: 2.19%, model saved.
Epoch: 3200 Train: 38.52735 Test: 0.20664
Epoch: 3280 Train: 39.63628 Test: 0.22726
Epoch: 3360 Train: 55.82602 Test: 5.47833
Epoch: 3440 Train: 37.74833 Test: 0.42278
Epoch: 3520 Train: 37.57553 Test: 0.23172
Epoch: 3600 Train: 35.94244 Test: 0.14512
Epoch: 3680 Train: 37.21091 Test: 0.90136
Epoch: 3760 Train: 35.66973 Test: 0.85768
Epoch: 3840 Train: 38.02804 Test: 3.83026
Epoch: 3920 Train: 35.38886 Test: 1.10183
Epoch: 4000 Train: 30.58261 Test: 0.14906
Epoch: 4080 Train: 30.41485 Test: 0.23688
Epoch: 4160 Train: 29.61237 Test: 0.21946
Epoch: 4240 Train: 32.81406 Test: 1.45644
Epoch: 4320 Train: 30.40088 Test: 0.45690
Epoch 4400: New minimal relative error: 1.59%, model saved.
Epoch: 4400 Train: 27.30393 Test: 0.33477
Epoch: 4480 Train: 29.42494 Test: 0.41862
Epoch: 4560 Train: 27.43621 Test: 0.55453
Epoch: 4640 Train: 31.09916 Test: 1.54424
Epoch: 4720 Train: 28.39839 Test: 0.86658
Epoch: 4800 Train: 30.72354 Test: 0.65823
Epoch: 4880 Train: 29.33109 Test: 0.66350
Epoch: 4960 Train: 31.25602 Test: 1.16454
Epoch: 5040 Train: 24.97460 Test: 0.05788
Epoch: 5120 Train: 27.06160 Test: 1.33653
Epoch: 5200 Train: 26.03366 Test: 0.42708
Epoch: 5280 Train: 24.90524 Test: 0.04987
Epoch: 5360 Train: 26.47745 Test: 0.08393
Epoch: 5440 Train: 27.72641 Test: 1.45875
Epoch: 5520 Train: 27.93878 Test: 1.55928
Epoch: 5600 Train: 24.82139 Test: 0.84180
Epoch: 5680 Train: 22.62481 Test: 0.05823
Epoch: 5760 Train: 22.21849 Test: 0.26015
Epoch: 5840 Train: 23.15298 Test: 0.12288
Epoch 5920: New minimal relative error: 0.98%, model saved.
Epoch: 5920 Train: 21.59920 Test: 0.08823
Epoch: 6000 Train: 21.33496 Test: 0.05512
Epoch: 6080 Train: 21.03347 Test: 0.03464
Epoch: 6160 Train: 20.30412 Test: 0.09671
Epoch: 6240 Train: 21.18690 Test: 0.35327
Epoch: 6320 Train: 20.00806 Test: 0.08762
Epoch: 6400 Train: 20.02800 Test: 0.10573
Epoch: 6480 Train: 20.13368 Test: 0.03785
Epoch: 6560 Train: 21.02234 Test: 0.26996
Epoch: 6640 Train: 19.84274 Test: 0.19114
Epoch: 6720 Train: 19.63192 Test: 0.09436
Epoch: 6800 Train: 19.13853 Test: 0.13313
Epoch: 6880 Train: 19.00512 Test: 0.11994
Epoch: 6960 Train: 21.60857 Test: 0.30974
Epoch: 7040 Train: 20.24522 Test: 0.05977
Epoch: 7120 Train: 20.14286 Test: 0.03826
Epoch: 7200 Train: 19.89906 Test: 0.07840
Epoch: 7280 Train: 21.67707 Test: 0.24278
Epoch: 7360 Train: 21.35119 Test: 0.75326
Epoch: 7440 Train: 24.71996 Test: 1.12826
Epoch: 7520 Train: 20.21027 Test: 0.09384
Epoch: 7600 Train: 23.26524 Test: 2.81622
Epoch: 7680 Train: 20.83539 Test: 0.07327
Epoch: 7760 Train: 18.88888 Test: 0.06630
Epoch: 7840 Train: 20.02854 Test: 0.15442
Epoch: 7920 Train: 18.44159 Test: 0.04145
Epoch: 7999 Train: 18.63866 Test: 0.05471
Training Loss: tensor(18.6387)
Test Loss: tensor(0.0547)
Learned LE: [  0.89437646  -0.03545999 -14.539777  ]
True LE: [ 8.7800753e-01 -1.4378878e-03 -1.4547396e+01]
Relative Error: [1.4731659  1.7994245  1.9844588  2.015763   2.1857195  2.3926969
 2.3105624  2.1838644  2.156064   2.3544338  2.6936076  2.9002433
 2.5731049  1.9852422  1.47201    1.2436179  1.2201349  1.245028
 1.1391039  1.1042637  1.4122798  1.7883636  2.0110533  2.0995908
 1.97984    1.9475175  1.6695591  1.3248043  0.8972418  0.9906951
 1.3912017  1.7807612  2.112559   2.5974848  3.4040506  3.9247296
 4.546414   4.85825    4.5746126  4.5072274  3.8277314  2.4898832
 1.1649641  0.54818976 1.3606912  1.6220136  1.6640649  1.5123194
 1.5201529  1.6559008  1.7782563  2.007205   2.3287275  2.7525692
 2.8236463  2.6239588  2.1936114  1.7594917  1.3190933  0.9220105
 0.64688426 0.9311131  1.3511064  1.6119133  1.8167933  1.844841
 2.01031    2.214656   2.331163   2.2121427  2.040144   2.1117525
 2.4272172  2.6803339  2.6338406  2.2192354  1.5885125  1.1756219
 1.1702139  1.3164254  1.306643   1.1047497  0.94412744 1.3082541
 1.6766975  1.8980241  1.9629741  1.8884678  1.7817491  1.3505313
 0.8488488  0.76493037 1.0479933  1.364451   1.8418386  1.9501071
 2.2347846  3.0821996  3.6084776  4.109205   4.085491   3.9223063
 3.9041905  3.00062    1.5830252  0.19621937 1.0679322  1.3854889
 1.5508258  1.4188825  1.3499135  1.4032389  1.3188835  1.4664057
 1.7874119  2.2256503  2.4871032  2.422571   2.1125968  1.7338606
 1.3956637  1.0306712  0.5944833  0.6399401  1.0969671  1.4488235
 1.6432526  1.7425765  1.9031687  2.0025659  2.1349628  2.1751106
 2.0994403  1.9300656  1.9993268  2.2366755  2.4756646  2.3802755
 1.9719012  1.2789564  0.9767256  1.1897485  1.394823   1.2718927
 1.1012965  0.8801612  1.1724138  1.5684584  1.7226655  1.771567
 1.7890136  1.517031   0.9739972  0.46112412 0.67278713 0.949197
 1.3662889  1.7011948  1.7278645  1.8248794  2.5495927  3.0259347
 3.4767394  3.3060935  3.1031826  3.0705156  2.3825283  0.9848611
 0.6848542  1.1390387  1.3512259  1.467639   1.322347   1.1622193
 1.1719685  1.0736216  1.3372632  1.6330564  1.8850936  2.0120761
 1.8774656  1.5231978  1.3262849  1.1337827  0.84343946 0.41713622
 0.6378581  1.0029452  1.3088394  1.582711   1.930378   2.1355844
 2.1330404  2.1775723  2.1084437  1.9824895  1.7332606  1.7514755
 1.9360553  2.1517282  2.1453981  1.819285   1.1096742  0.82373804
 1.1607513  1.3018647  1.2516235  1.133385   0.9263701  1.0198059
 1.3305693  1.5829265  1.6828309  1.6660727  1.3092971  0.70387393
 0.29331258 0.5314273  0.8180243  1.4078691  1.5246769  1.4796084
 1.3371636  1.8666099  2.4111545  2.8109975  2.732555   2.4673235
 2.4109893  1.8697782  0.76388687 0.906567   1.1866624  1.3411803
 1.3497593  1.1310337  0.98376936 0.9529561  0.96787846 1.082633
 1.3421696  1.4480746  1.4608871  1.3172102  1.0326433  0.9634467
 0.9775683  0.639877   0.44582942 0.51712525 0.8330426  1.1068074
 1.6059302  1.9851589  2.2767348  2.1378334  2.0232196  1.9285265
 1.8497107  1.6280373  1.4591644  1.580032   1.8994216  1.9229265
 1.7713988  1.0762082  0.76064736 0.9617441  1.161445   1.1560519
 1.1423658  1.0806712  0.93281794 1.085292   1.411218   1.6263756
 1.6411562  1.3281701  0.7150909  0.16148399 0.36584428 0.7109685
 1.2861611  1.3220756  1.2965711  1.223567   1.3273237  1.8627958
 2.2439766  2.3788216  1.9821992  1.972577   1.3981274  0.6526104
 0.76724046 1.0839416  1.2732128  1.1029557  0.8713552  0.66556317
 0.7494206  0.9144469  0.88290906 1.0058385  1.024032   1.0048105
 1.0182714  0.85855556 0.82879823 0.78296375 0.5538176  0.6127162
 0.47067165 0.5894679  1.0185477  1.4707347  1.7930841  1.9758383
 1.9700216  1.8223826  1.8414501  1.7614056  1.6115159  1.411764
 1.1842316  1.329355   1.6819888  1.6286256  1.1818233  0.69446295
 0.82724    0.8351448  0.9835436  1.012914   1.1291727  0.9097573
 0.88000417 1.1136006  1.5138655  1.5389326  1.4316977  0.947516
 0.37753913 0.1948981  0.54722404 1.12686    1.2105235  1.2143505
 1.2686205  1.2766865  1.3736522  1.8002391  2.174286   1.8172437
 1.6061641  1.2637956  0.6355164  0.57308733 0.78223085 1.1027074
 0.947414   0.6227258  0.4084332  0.6029748  0.8533996  0.864248
 0.7622486  0.74734455 0.7243908  0.7397748  0.69044065 0.67968017
 0.59656274 0.48545036 0.601926   0.58422714 0.5143539  0.8266674
 1.1549139  1.3601614  1.4120208  1.465318   1.4829408  1.3600676
 1.3600805  1.3883455  1.3599708  1.2289442  0.923615   0.90404814
 1.1437165  1.2077475  0.87210816 0.64725417 0.77891535 0.7577737
 0.8054277  0.8917825  0.924066   0.8205102 ]
