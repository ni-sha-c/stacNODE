time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.82%, model saved.
Epoch: 0 Train: 61572.83594 Test: 4000.00732
Epoch 80: New minimal relative error: 92.02%, model saved.
Epoch: 80 Train: 16871.06641 Test: 1489.89124
Epoch 160: New minimal relative error: 70.21%, model saved.
Epoch: 160 Train: 14071.87402 Test: 1164.82800
Epoch: 240 Train: 13012.49121 Test: 1599.09216
Epoch: 320 Train: 14554.10352 Test: 1468.66333
Epoch: 400 Train: 13051.23438 Test: 1147.50635
Epoch: 480 Train: 13976.23438 Test: 1466.40808
Epoch: 560 Train: 14295.87988 Test: 1268.72668
Epoch 640: New minimal relative error: 64.18%, model saved.
Epoch: 640 Train: 14480.55859 Test: 1236.03467
Epoch: 720 Train: 13864.00684 Test: 1364.40405
Epoch: 800 Train: 13432.09961 Test: 1154.04382
Epoch: 880 Train: 11863.64258 Test: 964.70642
Epoch: 960 Train: 11861.79395 Test: 1005.94702
Epoch: 1040 Train: 10431.17383 Test: 921.23096
Epoch: 1120 Train: 8283.97363 Test: 506.79974
Epoch: 1200 Train: 7070.61572 Test: 344.14075
Epoch: 1280 Train: 5671.57178 Test: 221.71259
Epoch: 1360 Train: 3525.98633 Test: 106.13033
Epoch 1440: New minimal relative error: 42.21%, model saved.
Epoch: 1440 Train: 1938.00745 Test: 51.08679
Epoch 1520: New minimal relative error: 26.10%, model saved.
Epoch: 1520 Train: 1579.07166 Test: 24.32478
Epoch: 1600 Train: 1345.14807 Test: 24.74194
Epoch: 1680 Train: 1217.17383 Test: 23.25829
Epoch: 1760 Train: 1294.29138 Test: 62.64653
Epoch 1840: New minimal relative error: 22.13%, model saved.
Epoch: 1840 Train: 998.79468 Test: 12.05539
Epoch: 1920 Train: 1157.52832 Test: 38.47553
Epoch 2000: New minimal relative error: 16.08%, model saved.
Epoch: 2000 Train: 770.71460 Test: 8.59756
Epoch: 2080 Train: 898.18207 Test: 15.33267
Epoch: 2160 Train: 686.32642 Test: 8.26929
Epoch: 2240 Train: 629.07770 Test: 10.28027
Epoch: 2320 Train: 585.20026 Test: 56.11332
Epoch: 2400 Train: 517.08160 Test: 5.10532
Epoch: 2480 Train: 503.00272 Test: 10.38134
Epoch: 2560 Train: 500.15860 Test: 17.39052
Epoch: 2640 Train: 451.95367 Test: 6.47581
Epoch: 2720 Train: 442.63571 Test: 7.32506
Epoch: 2800 Train: 437.59372 Test: 10.19655
Epoch 2880: New minimal relative error: 14.83%, model saved.
Epoch: 2880 Train: 383.02539 Test: 7.40715
Epoch: 2960 Train: 359.99896 Test: 2.75239
Epoch: 3040 Train: 365.94073 Test: 9.39163
Epoch: 3120 Train: 310.57492 Test: 2.00936
Epoch 3200: New minimal relative error: 12.57%, model saved.
Epoch: 3200 Train: 277.85956 Test: 1.54674
Epoch: 3280 Train: 268.12686 Test: 1.58347
Epoch 3360: New minimal relative error: 7.89%, model saved.
Epoch: 3360 Train: 252.97548 Test: 1.46013
Epoch 3440: New minimal relative error: 6.45%, model saved.
Epoch: 3440 Train: 255.37418 Test: 1.76212
Epoch: 3520 Train: 236.18874 Test: 1.64308
Epoch: 3600 Train: 218.98235 Test: 2.84998
Epoch: 3680 Train: 219.03055 Test: 10.64064
Epoch: 3760 Train: 204.38812 Test: 1.02146
Epoch: 3840 Train: 192.03262 Test: 0.87286
Epoch: 3920 Train: 203.14926 Test: 1.13480
Epoch: 4000 Train: 188.03586 Test: 2.88468
Epoch: 4080 Train: 175.50577 Test: 3.94113
Epoch: 4160 Train: 158.47144 Test: 0.59432
Epoch: 4240 Train: 242.77249 Test: 12.17654
Epoch: 4320 Train: 157.86977 Test: 0.57905
Epoch: 4400 Train: 138.66055 Test: 0.41574
Epoch: 4480 Train: 135.42654 Test: 0.42965
Epoch: 4560 Train: 134.65067 Test: 0.55179
Epoch: 4640 Train: 131.30215 Test: 0.48322
Epoch: 4720 Train: 129.68292 Test: 0.55487
Epoch: 4800 Train: 126.73207 Test: 0.49480
Epoch: 4880 Train: 125.08383 Test: 1.10235
Epoch: 4960 Train: 124.85517 Test: 2.92399
Epoch: 5040 Train: 112.36438 Test: 0.41598
Epoch: 5120 Train: 108.90267 Test: 0.57147
Epoch: 5200 Train: 107.21762 Test: 0.88949
Epoch: 5280 Train: 96.37970 Test: 0.34229
Epoch: 5360 Train: 89.14291 Test: 0.75585
Epoch 5440: New minimal relative error: 6.19%, model saved.
Epoch: 5440 Train: 92.69707 Test: 0.33484
Epoch: 5520 Train: 94.22620 Test: 0.36878
Epoch 5600: New minimal relative error: 4.48%, model saved.
Epoch: 5600 Train: 89.32748 Test: 0.25732
Epoch: 5680 Train: 84.80257 Test: 0.20584
Epoch: 5760 Train: 84.70776 Test: 0.21716
Epoch: 5840 Train: 82.53027 Test: 0.31633
Epoch: 5920 Train: 83.94097 Test: 0.24716
Epoch: 6000 Train: 87.87269 Test: 2.59678
Epoch: 6080 Train: 75.24496 Test: 0.19853
Epoch: 6160 Train: 76.16090 Test: 0.22426
Epoch: 6240 Train: 79.09245 Test: 0.26729
Epoch: 6320 Train: 76.85773 Test: 0.37906
Epoch: 6400 Train: 74.18345 Test: 0.27441
Epoch: 6480 Train: 74.20234 Test: 0.31322
Epoch: 6560 Train: 72.09647 Test: 0.24160
Epoch: 6640 Train: 71.57974 Test: 0.24288
Epoch: 6720 Train: 75.96201 Test: 0.24768
Epoch 6800: New minimal relative error: 4.39%, model saved.
Epoch: 6800 Train: 76.19575 Test: 0.24543
Epoch: 6880 Train: 73.98249 Test: 0.33347
Epoch: 6960 Train: 77.91836 Test: 0.31484
Epoch: 7040 Train: 73.58636 Test: 0.25622
Epoch: 7120 Train: 80.88477 Test: 0.65414
Epoch: 7200 Train: 76.73709 Test: 0.31590
Epoch: 7280 Train: 76.57700 Test: 0.29355
Epoch: 7360 Train: 70.68309 Test: 0.21172
Epoch: 7440 Train: 73.58548 Test: 0.26362
Epoch: 7520 Train: 82.83272 Test: 0.40415
Epoch: 7600 Train: 79.73571 Test: 0.39525
Epoch: 7680 Train: 74.13515 Test: 0.36414
Epoch: 7760 Train: 67.35970 Test: 0.21943
Epoch: 7840 Train: 72.39863 Test: 0.30016
Epoch: 7920 Train: 72.70470 Test: 0.24869
Epoch: 7999 Train: 70.08085 Test: 0.22771
Training Loss: tensor(70.0808)
Test Loss: tensor(0.2277)
Learned LE: [ 8.81405056e-01 -1.21965772e-03 -1.45489435e+01]
True LE: [ 8.7599623e-01 -1.8101576e-04 -1.4552428e+01]
Relative Error: [0.85048676 0.9386718  1.2824185  1.8194865  2.2825909  2.6370864
 2.6709464  3.010238   3.1835313  3.3074036  3.3545792  3.320143
 3.243292   3.0460374  2.8013768  2.3276374  1.3052577  0.54273593
 0.9331642  1.5962383  2.0277667  2.170369   2.091619   2.1371455
 2.4130676  2.4048173  2.1434731  1.7035726  1.3190063  1.3343322
 1.2404644  1.0807793  0.99352586 1.1035733  1.5016954  1.8514124
 2.1036472  2.2351925  2.296281   1.9042999  1.7784003  1.6393187
 1.5252303  1.4801453  1.5589722  1.5476629  1.3558687  1.3952649
 1.6146394  1.8022653  1.880307   2.0974784  2.0814657  2.2288663
 2.3828049  2.6078844  3.112429   3.1316028  2.268543   1.5008087
 1.1902419  1.1176865  1.0142157  1.0251243  1.2219702  1.6850978
 2.2663724  2.502733   2.5959868  2.8619254  3.1609683  3.1112616
 3.1580448  3.2126849  3.3068771  3.0701811  2.6542318  2.2131252
 1.7386595  0.91587037 0.59598917 1.3410234  1.8989308  2.1604319
 2.1661785  2.0455422  2.4615607  2.5266263  2.276938   1.780566
 1.2111834  1.2877154  1.1762698  1.0339724  1.0031308  1.0966474
 1.2999997  1.7817581  2.231174   2.4260163  2.137753   1.5564303
 1.0345855  0.9981352  0.91880727 0.91858405 1.0734     1.2873477
 1.3221313  1.395265   1.625421   1.7629197  1.7462887  1.761953
 1.9923029  2.1146667  2.1750216  2.1647735  2.3634949  2.7954435
 2.545381   1.9581784  1.4961624  1.3683078  1.2779305  1.2398995
 1.3583722  1.6011046  2.1734567  2.4912758  2.5654085  2.6687274
 2.7684174  2.8785727  2.845522   2.8682246  2.9121728  2.863453
 2.6823041  2.3206723  1.9634496  1.4810492  0.6395551  0.97129476
 1.6996338  2.090342   2.1867595  2.0441492  2.2107794  2.4430108
 2.4061413  1.9607576  1.2288102  1.1022983  1.1741359  1.1456096
 1.104603   1.1425958  1.3491102  1.7190542  2.2535172  2.5018067
 1.9622161  1.2888951  0.86032516 0.53485566 0.5347341  0.56788117
 0.67134976 0.8277661  1.0087812  1.2890507  1.5872141  1.9336293
 1.857757   1.7069755  1.7132034  1.8928918  1.9902964  1.9111348
 1.8569572  2.0200198  2.4007938  2.1562452  1.7984936  1.6097637
 1.5667902  1.660449   1.5534048  1.6461263  1.9894712  2.4491484
 2.631156   2.5845828  2.414335   2.582066   2.7373688  2.7019906
 2.7058964  2.5713925  2.3037786  2.284081   2.322468   1.918656
 1.1646004  0.63765305 1.2311491  1.9221147  2.1707437  2.133726
 1.8788971  2.2130663  2.2762494  2.039129   1.5075232  0.9658145
 1.1619651  1.1176261  1.0359552  1.1023859  1.2003329  1.585379
 2.0854483  2.294011   2.177037   1.4602323  0.77888644 0.43592247
 0.39446273 0.45693672 0.5951619  0.6961359  0.8880915  1.2063148
 1.6265385  1.8234663  2.0726523  1.894134   1.6874233  1.7485522
 1.8732008  1.852301   1.658581   1.637299   1.5943811  1.9734666
 1.8258989  1.6836576  1.6414542  1.7967072  1.9722012  1.8646412
 1.9520789  2.354089   2.570574   2.636666   2.2911696  2.0039182
 2.2156012  2.50843    2.7086706  2.572631   2.2699804  1.8994656
 2.115296   2.3067095  1.7636915  0.9152434  0.7525286  1.3244623
 1.8597951  2.177754   2.0568523  1.8253764  2.0743668  2.0415943
 1.6849515  1.069382   1.0378107  1.1185485  0.9097492  0.884152
 0.91082996 1.16384    1.6154538  1.9415615  2.2452984  1.7608037
 1.2236305  0.6374235  0.19494902 0.22680065 0.47309184 0.74559736
 0.94527435 1.1288667  1.5164893  1.9197845  2.170919   2.2292
 1.9824879  1.7277529  1.8492056  1.8684436  1.8085989  1.6153088
 1.4504466  1.243669   1.5324378  1.5310975  1.6676757  1.7057575
 1.8585224  2.0931473  2.2929418  2.3696454  2.5148687  2.6665025
 2.2905962  1.8137124  1.3640338  1.4304427  1.7709699  2.1107473
 2.233844   2.175135   1.8703328  2.0093017  2.1491058  1.7327831
 0.87269133 0.7710198  1.1821789  1.7744293  2.0289898  1.9762695
 1.7601326  1.988565   1.9018971  1.4820391  0.8526658  0.920383
 1.0296066  0.79014003 0.6893429  0.70503837 1.0408595  1.3274798
 1.8002387  1.9863589  1.6567478  1.3310747  0.88524073 0.3886886
 0.39087692 0.7240786  0.83117354 1.0010471  1.3107164  1.6846193
 2.0856426  2.4195352  2.548896   2.338803   1.9813832  1.806081
 1.8682712  1.8798733  1.6919649  1.3798801  0.8577327  0.9923109
 1.2600585  1.4120445  1.654227   1.9598427  2.2663345  2.5566268
 2.7148783  2.794103   2.6313078  2.267313   1.988616   1.4328449
 1.1518536  1.2504355  1.4502112  1.6451869  1.6559435  2.0171297
 1.9960815  1.984836   1.4744331  1.0747933  0.6464775  1.0878531
 1.471351   1.8050799  1.8232588  1.5332254 ]
