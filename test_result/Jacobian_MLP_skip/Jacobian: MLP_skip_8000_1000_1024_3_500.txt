time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.73%, model saved.
Epoch: 0 Train: 32769.50781 Test: 4130.89551
Epoch 80: New minimal relative error: 74.58%, model saved.
Epoch: 80 Train: 7224.91699 Test: 1192.39844
Epoch 160: New minimal relative error: 54.25%, model saved.
Epoch: 160 Train: 6495.35449 Test: 1043.06653
Epoch: 240 Train: 7202.78955 Test: 971.97882
Epoch: 320 Train: 5975.61035 Test: 785.18243
Epoch: 400 Train: 4815.79346 Test: 541.75073
Epoch: 480 Train: 3222.62427 Test: 306.74704
Epoch: 560 Train: 2401.29956 Test: 131.93187
Epoch 640: New minimal relative error: 23.13%, model saved.
Epoch: 640 Train: 1380.68115 Test: 94.49203
Epoch: 720 Train: 838.25018 Test: 145.62524
Epoch: 800 Train: 409.95132 Test: 9.48385
Epoch: 880 Train: 321.02774 Test: 10.84650
Epoch: 960 Train: 297.17413 Test: 24.51637
Epoch 1040: New minimal relative error: 7.77%, model saved.
Epoch: 1040 Train: 237.07877 Test: 5.25803
Epoch: 1120 Train: 305.25372 Test: 11.76704
Epoch: 1200 Train: 222.44325 Test: 5.59158
Epoch: 1280 Train: 160.70477 Test: 3.31083
Epoch: 1360 Train: 158.17168 Test: 2.13357
Epoch: 1440 Train: 136.33377 Test: 1.57648
Epoch: 1520 Train: 144.22148 Test: 1.66369
Epoch: 1600 Train: 134.40881 Test: 4.03421
Epoch: 1680 Train: 132.11630 Test: 8.43466
Epoch: 1760 Train: 113.41267 Test: 1.71527
Epoch: 1840 Train: 119.29778 Test: 2.60239
Epoch: 1920 Train: 117.69216 Test: 8.98220
Epoch: 2000 Train: 129.61398 Test: 4.60692
Epoch: 2080 Train: 107.91988 Test: 5.71428
Epoch: 2160 Train: 101.58580 Test: 1.30000
Epoch: 2240 Train: 141.68248 Test: 45.87417
Epoch: 2320 Train: 111.76987 Test: 1.41377
Epoch: 2400 Train: 91.81937 Test: 0.73582
Epoch: 2480 Train: 83.54217 Test: 0.75274
Epoch: 2560 Train: 88.75263 Test: 0.84859
Epoch: 2640 Train: 81.09076 Test: 0.93225
Epoch: 2720 Train: 112.38514 Test: 17.64973
Epoch: 2800 Train: 82.55285 Test: 0.62866
Epoch: 2880 Train: 111.53173 Test: 1.22677
Epoch: 2960 Train: 78.48020 Test: 0.60063
Epoch: 3040 Train: 70.65721 Test: 0.52176
Epoch 3120: New minimal relative error: 2.79%, model saved.
Epoch: 3120 Train: 72.79475 Test: 0.64750
Epoch: 3200 Train: 91.66113 Test: 6.24391
Epoch: 3280 Train: 70.57056 Test: 0.51315
Epoch: 3360 Train: 86.03632 Test: 2.53774
Epoch: 3440 Train: 61.70916 Test: 0.40922
Epoch: 3520 Train: 68.03134 Test: 9.99449
Epoch: 3600 Train: 57.96301 Test: 1.56850
Epoch: 3680 Train: 55.41366 Test: 2.47136
Epoch: 3760 Train: 51.44040 Test: 0.47979
Epoch: 3840 Train: 51.50183 Test: 0.27351
Epoch: 3920 Train: 50.90038 Test: 0.32523
Epoch: 4000 Train: 60.08055 Test: 10.29941
Epoch: 4080 Train: 46.88763 Test: 0.21677
Epoch: 4160 Train: 45.95014 Test: 0.22884
Epoch: 4240 Train: 43.82121 Test: 0.21162
Epoch: 4320 Train: 47.17871 Test: 0.24917
Epoch: 4400 Train: 43.48382 Test: 0.18488
Epoch: 4480 Train: 43.75648 Test: 0.97708
Epoch: 4560 Train: 41.60151 Test: 0.36643
Epoch: 4640 Train: 41.22590 Test: 0.27276
Epoch: 4720 Train: 41.28897 Test: 0.17385
Epoch: 4800 Train: 44.28326 Test: 0.94319
Epoch: 4880 Train: 37.96112 Test: 0.13989
Epoch: 4960 Train: 36.46761 Test: 0.15096
Epoch: 5040 Train: 36.60809 Test: 0.10194
Epoch: 5120 Train: 35.74265 Test: 0.11967
Epoch: 5200 Train: 40.14955 Test: 3.78085
Epoch: 5280 Train: 34.53888 Test: 0.30309
Epoch: 5360 Train: 36.84856 Test: 0.61955
Epoch: 5440 Train: 42.28448 Test: 3.93259
Epoch: 5520 Train: 33.13288 Test: 0.09473
Epoch: 5600 Train: 33.62462 Test: 0.11176
Epoch: 5680 Train: 32.63580 Test: 0.09616
Epoch: 5760 Train: 33.04889 Test: 0.14615
Epoch: 5840 Train: 35.36898 Test: 3.05304
Epoch: 5920 Train: 33.83036 Test: 0.16072
Epoch: 6000 Train: 32.88056 Test: 0.33023
Epoch: 6080 Train: 33.49621 Test: 0.22418
Epoch: 6160 Train: 32.34059 Test: 0.15563
Epoch: 6240 Train: 34.81913 Test: 1.80733
Epoch: 6320 Train: 30.51597 Test: 0.10156
Epoch 6400: New minimal relative error: 2.36%, model saved.
Epoch: 6400 Train: 30.43509 Test: 0.12557
Epoch 6480: New minimal relative error: 1.71%, model saved.
Epoch: 6480 Train: 29.36988 Test: 0.08588
Epoch: 6560 Train: 29.48001 Test: 0.08822
Epoch: 6640 Train: 28.68039 Test: 0.09068
Epoch: 6720 Train: 28.48892 Test: 0.19172
Epoch: 6800 Train: 27.92167 Test: 0.07737
Epoch: 6880 Train: 27.99590 Test: 0.17580
Epoch: 6960 Train: 27.60412 Test: 0.08702
Epoch: 7040 Train: 28.43258 Test: 0.29192
Epoch: 7120 Train: 27.20864 Test: 0.07461
Epoch: 7200 Train: 26.58443 Test: 0.17222
Epoch: 7280 Train: 26.09580 Test: 0.08518
Epoch: 7360 Train: 26.48734 Test: 0.11312
Epoch: 7440 Train: 26.75545 Test: 0.07755
Epoch: 7520 Train: 26.39846 Test: 0.19851
Epoch: 7600 Train: 26.11131 Test: 0.28340
Epoch: 7680 Train: 27.24523 Test: 0.67284
Epoch: 7760 Train: 26.14454 Test: 0.07395
Epoch: 7840 Train: 25.22375 Test: 0.07208
Epoch: 7920 Train: 26.13494 Test: 0.12650
Epoch: 7999 Train: 25.64579 Test: 0.08291
Training Loss: tensor(25.6458)
Test Loss: tensor(0.0829)
Learned LE: [  0.8308598    0.05580449 -14.549673  ]
True LE: [ 8.7465066e-01  4.0824357e-03 -1.4560905e+01]
Relative Error: [6.7280264  6.5990357  6.720772   7.05942    7.544226   8.140562
 8.609064   8.614931   8.411582   8.487686   8.189253   7.1381555
 5.9634542  5.105683   4.5326443  4.204322   4.20879    4.250203
 3.9557557  3.4834697  3.093111   3.0249755  3.2464862  3.369858
 3.4453063  3.4318175  3.223468   3.1958997  3.3736315  3.6215298
 3.891129   3.878627   3.4429529  3.482867   3.4353285  3.539126
 3.5443072  3.3546834  3.6508613  3.8106267  3.6316962  2.9294264
 2.1608555  1.6217014  0.8489014  0.6065459  0.8852059  1.4343251
 2.2077641  3.0537498  3.694011   3.6827724  3.591644   3.6063511
 3.603165   3.6776443  3.8773298  4.1816463  4.4774284  4.7103157
 5.061805   5.363892   5.6474886  5.638313   5.617023   5.8793983
 6.364021   6.9668584  7.5367794  7.9188795  7.719717   7.733394
 7.626781   6.7561336  5.6373887  4.7371974  4.1113696  3.8108203
 3.7278955  3.8713834  3.6857646  3.1723282  2.8035088  2.695907
 2.886977   3.0312276  3.1036842  3.087917   2.954669   2.7977035
 2.9137275  3.0117218  3.213922   3.3749607  2.990429   2.9134083
 2.8825028  3.0651035  3.157736   2.9679651  3.393894   3.7615232
 3.7077003  3.048071   2.2278116  1.6233054  0.8905151  0.39980876
 0.6371138  1.0494089  1.8129466  2.7082644  3.2848399  3.1554613
 3.0406334  3.028261   2.9590776  2.847205   2.9543483  3.277947
 3.5457947  3.7058094  3.9396517  4.34428    4.6288133  4.9293404
 4.7907057  4.8873606  5.1917725  5.7288113  6.325777   6.901431
 7.139071   7.00216    6.999322   6.295684   5.523301   4.4647665
 3.8272388  3.4505022  3.3023233  3.3219318  3.2803946  2.8844845
 2.4925861  2.2283168  2.5193825  2.7665849  2.8543475  2.784816
 2.672832   2.517079   2.5200737  2.517794   2.5393353  2.6949027
 2.5311432  2.2382526  2.29171    2.5130463  2.7963245  2.7785933
 3.2125325  3.7766747  3.8228922  3.314671   2.573042   1.6333703
 1.086374   0.31029087 0.47337702 0.88600785 1.5639677  2.4372666
 2.8201432  2.6689458  2.5878365  2.59546    2.4741962  2.2313926
 2.1671507  2.2172077  2.29304    2.5238645  2.812865   3.0920596
 3.6543746  4.070229   4.1819224  4.133694   4.3175273  4.6194754
 5.1412325  5.639096   6.1893067  6.441287   6.369094   5.7552004
 5.1767936  4.445361   3.5781887  3.1151752  2.8438325  2.6779497
 2.7473972  2.6212676  2.2998428  1.7893771  1.9035171  2.297692
 2.586632   2.6324697  2.4358099  2.2917027  2.2202847  2.1525388
 2.032321   2.0518632  2.0241272  1.6430104  1.7321281  1.9832757
 2.3546977  2.507779   2.8743436  3.5530589  3.804624   3.475199
 2.894406   2.0342903  1.2941262  0.64416444 0.32446647 0.7115697
 1.3573197  2.1230984  2.451999   2.2609913  2.1657329  2.198688
 1.9973096  1.741723   1.6062326  1.4534055  1.3999399  1.4905165
 1.6908265  1.9507209  2.3350353  2.9740567  3.4693856  3.586783
 3.592841   3.755857   4.0898914  4.541355   4.986766   5.3675284
 5.9078193  5.329493   4.7276163  4.3034053  3.6231165  2.9466026
 2.4334316  2.202714   2.1604166  2.0063548  2.0148134  1.6649636
 1.3608006  1.7136002  2.1084292  2.3186164  2.3625     2.126093
 2.0450804  1.8935313  1.7390503  1.5337747  1.4564563  1.1679125
 1.093222   1.2574807  1.6839561  2.044831   2.2874346  3.0313632
 3.5665863  3.474696   3.1017745  2.474298   1.7474476  0.95046693
 0.10742464 0.4931941  1.0833472  1.8020458  2.3527634  2.0238993
 1.9230593  1.792512   1.6265448  1.3992598  1.2238952  0.9817302
 0.95155126 1.0584085  1.1683211  1.2715402  1.426008   1.6770747
 2.375426   2.820074   2.9428773  3.0989456  3.2344909  3.5265617
 3.9971628  4.2176566  4.538404   5.0989304  4.4224734  3.993945
 3.6789508  3.1589098  2.2263749  1.7361425  1.6616685  1.5531117
 1.4064549  1.4998136  1.0942442  1.1072379  1.5821855  1.9191593
 2.04127    2.046228   1.9955376  1.7707484  1.547511   1.3949748
 1.2543931  1.0434974  0.6798858  0.74763256 0.84050524 1.2434895
 1.5235443  2.2261608  2.9450166  3.1785066  3.0809708  2.783654
 2.3215604  1.4667863  0.57580864 0.16170475 0.66593176 1.4145423
 2.1722546  2.1566753  1.772224   1.5570263  1.4526927  1.1846081
 0.98885113 0.7293623  0.53958845 0.79594123 1.258438   1.36878
 1.2418375  1.1194644  1.1785009  1.617124   2.2462347  2.4523432
 2.6747072  2.7963045  3.0661612  3.3810382  3.447904   3.757043
 3.9698143  3.831952   3.4132109  3.2653277  2.5558252  1.5683935
 1.2364316  1.3121969  1.1525574  1.0103947  1.2348739  0.86473346
 1.0640337  1.5259963  1.7692674  1.8138995 ]
