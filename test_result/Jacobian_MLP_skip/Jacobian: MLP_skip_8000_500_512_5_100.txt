time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.09%, model saved.
Epoch: 0 Train: 9503.37402 Test: 4255.02783
Epoch: 80 Train: 2607.28442 Test: 1128.01660
Epoch: 160 Train: 2224.88232 Test: 937.81836
Epoch: 240 Train: 1645.00391 Test: 469.24231
Epoch: 320 Train: 536.90552 Test: 117.33237
Epoch 400: New minimal relative error: 61.95%, model saved.
Epoch: 400 Train: 345.37408 Test: 79.18793
Epoch 480: New minimal relative error: 28.68%, model saved.
Epoch: 480 Train: 160.34726 Test: 20.84031
Epoch 560: New minimal relative error: 26.03%, model saved.
Epoch: 560 Train: 117.58139 Test: 15.56523
Epoch: 640 Train: 105.32977 Test: 24.07189
Epoch: 720 Train: 65.92239 Test: 7.46370
Epoch: 800 Train: 55.98672 Test: 5.76203
Epoch 880: New minimal relative error: 15.74%, model saved.
Epoch: 880 Train: 75.21847 Test: 7.74044
Epoch 960: New minimal relative error: 6.17%, model saved.
Epoch: 960 Train: 42.91708 Test: 2.50775
Epoch: 1040 Train: 36.65733 Test: 1.73398
Epoch 1120: New minimal relative error: 5.42%, model saved.
Epoch: 1120 Train: 35.90513 Test: 2.39417
Epoch: 1200 Train: 50.98177 Test: 16.37825
Epoch: 1280 Train: 35.62656 Test: 3.15732
Epoch: 1360 Train: 27.67634 Test: 1.17349
Epoch: 1440 Train: 27.60743 Test: 1.74093
Epoch: 1520 Train: 56.09264 Test: 17.82770
Epoch: 1600 Train: 32.10137 Test: 9.54869
Epoch: 1680 Train: 30.03774 Test: 5.95414
Epoch: 1760 Train: 22.29325 Test: 1.16499
Epoch: 1840 Train: 29.18049 Test: 4.94431
Epoch: 1920 Train: 21.55239 Test: 3.11829
Epoch: 2000 Train: 24.31623 Test: 4.33749
Epoch: 2080 Train: 34.34187 Test: 9.04706
Epoch: 2160 Train: 20.51884 Test: 3.13041
Epoch 2240: New minimal relative error: 2.17%, model saved.
Epoch: 2240 Train: 19.18139 Test: 1.23773
Epoch: 2320 Train: 17.40815 Test: 1.23281
Epoch: 2400 Train: 16.73050 Test: 1.54971
Epoch: 2480 Train: 18.78081 Test: 5.07420
Epoch: 2560 Train: 17.21487 Test: 1.61957
Epoch: 2640 Train: 17.35401 Test: 2.45021
Epoch: 2720 Train: 17.28588 Test: 2.63081
Epoch: 2800 Train: 13.23180 Test: 1.30545
Epoch: 2880 Train: 13.49226 Test: 0.47857
Epoch: 2960 Train: 12.51183 Test: 1.33754
Epoch: 3040 Train: 17.90702 Test: 3.74853
Epoch: 3120 Train: 11.45212 Test: 0.25023
Epoch: 3200 Train: 13.48270 Test: 1.65110
Epoch: 3280 Train: 11.28425 Test: 0.26737
Epoch: 3360 Train: 10.97297 Test: 0.23243
Epoch: 3440 Train: 12.00178 Test: 1.65811
Epoch: 3520 Train: 14.10863 Test: 2.37991
Epoch: 3600 Train: 11.15519 Test: 0.20761
Epoch: 3680 Train: 10.20420 Test: 0.57106
Epoch: 3760 Train: 13.03665 Test: 1.63437
Epoch: 3840 Train: 11.06398 Test: 1.04911
Epoch: 3920 Train: 9.70988 Test: 0.26996
Epoch: 4000 Train: 10.09539 Test: 0.36182
Epoch: 4080 Train: 10.39112 Test: 1.13591
Epoch: 4160 Train: 11.04613 Test: 1.61297
Epoch: 4240 Train: 14.23918 Test: 2.95949
Epoch: 4320 Train: 9.08521 Test: 0.78511
Epoch: 4400 Train: 9.24784 Test: 0.40366
Epoch: 4480 Train: 9.13469 Test: 1.22436
Epoch: 4560 Train: 9.06203 Test: 0.58027
Epoch: 4640 Train: 8.26702 Test: 0.39208
Epoch: 4720 Train: 10.15362 Test: 1.01860
Epoch: 4800 Train: 8.43237 Test: 0.20926
Epoch: 4880 Train: 10.75948 Test: 0.64392
Epoch: 4960 Train: 7.81030 Test: 0.09299
Epoch: 5040 Train: 8.25609 Test: 0.39189
Epoch: 5120 Train: 9.33460 Test: 1.21344
Epoch: 5200 Train: 7.39653 Test: 0.25636
Epoch: 5280 Train: 8.48246 Test: 1.16689
Epoch: 5360 Train: 7.16319 Test: 0.29229
Epoch: 5440 Train: 7.83504 Test: 0.65231
Epoch: 5520 Train: 7.94488 Test: 0.56036
Epoch: 5600 Train: 7.59171 Test: 0.39469
Epoch: 5680 Train: 7.74264 Test: 0.46318
Epoch: 5760 Train: 8.09641 Test: 0.46150
Epoch: 5840 Train: 7.84934 Test: 0.93271
Epoch: 5920 Train: 7.08742 Test: 0.13852
Epoch: 6000 Train: 6.97970 Test: 0.10299
Epoch: 6080 Train: 7.14632 Test: 0.17818
Epoch: 6160 Train: 7.55186 Test: 0.30123
Epoch: 6240 Train: 7.33906 Test: 0.33488
Epoch: 6320 Train: 6.88925 Test: 0.10568
Epoch: 6400 Train: 6.80238 Test: 0.09562
Epoch: 6480 Train: 6.68293 Test: 0.09592
Epoch: 6560 Train: 6.70726 Test: 0.14518
Epoch: 6640 Train: 6.47992 Test: 0.09564
Epoch: 6720 Train: 7.09307 Test: 0.45256
Epoch 6800: New minimal relative error: 2.09%, model saved.
Epoch: 6800 Train: 6.38012 Test: 0.09235
Epoch: 6880 Train: 7.52803 Test: 0.67677
Epoch: 6960 Train: 6.21979 Test: 0.07055
Epoch: 7040 Train: 6.33192 Test: 0.10941
Epoch: 7120 Train: 6.52978 Test: 0.30528
Epoch: 7200 Train: 6.11516 Test: 0.09556
Epoch: 7280 Train: 6.43247 Test: 0.25975
Epoch: 7360 Train: 6.08460 Test: 0.07898
Epoch: 7440 Train: 6.04397 Test: 0.31104
Epoch: 7520 Train: 5.80804 Test: 0.10375
Epoch: 7600 Train: 5.88376 Test: 0.22762
Epoch: 7680 Train: 5.70551 Test: 0.32306
Epoch: 7760 Train: 5.62889 Test: 0.26001
Epoch: 7840 Train: 5.77075 Test: 0.18424
Epoch: 7920 Train: 5.51496 Test: 0.08240
Epoch: 7999 Train: 5.51214 Test: 0.13692
Training Loss: tensor(5.5121)
Test Loss: tensor(0.1369)
Learned LE: [  0.82740104   0.02282763 -14.526142  ]
True LE: [ 8.6082995e-01  3.3862288e-03 -1.4547920e+01]
Relative Error: [2.7955863  2.9101374  2.8709092  2.779123   2.7161264  3.0573225
 3.1087017  2.7385197  2.68609    2.8755336  3.142566   3.5388763
 4.0563765  3.483204   3.0793877  2.9334543  2.6405687  2.329712
 1.9575672  1.8400679  1.4620969  1.1485405  1.2599801  1.1075628
 0.8401846  0.68331265 0.57422477 0.4795442  0.6128097  1.0013242
 1.0471886  0.8189862  0.51146734 0.7442711  0.794876   1.0567206
 1.265126   1.374417   1.7651346  2.3371625  2.8811831  3.2871206
 3.4342475  3.488867   3.609999   3.4508235  3.0819025  3.085177
 3.4708648  3.7988915  3.0603404  2.4850655  2.0553167  1.7820636
 1.6750958  1.5382652  1.4738351  1.513518   1.5604639  1.7834412
 2.1378973  2.354701   2.5694613  2.7879596  2.8759277  2.7205734
 2.686142   2.7903962  3.0660744  2.7977061  2.5008743  2.531342
 2.6225584  2.8145242  3.2023787  3.44178    2.828589   2.5403566
 2.4151852  2.3453562  2.0498114  1.8341858  1.5000558  1.1873935
 1.0659087  0.9617087  0.66204774 0.6970428  0.6645871  0.5620766
 0.49508503 0.66836107 0.9769792  0.99493134 0.86388475 0.7347868
 0.8339044  0.8015431  0.9898186  1.0645933  1.1748588  1.5970001
 2.0673294  2.578492   3.046356   3.110689   3.138764   3.0420036
 2.5668364  2.3233716  2.5229726  3.0456395  2.634688   2.0498822
 1.6948534  1.5162296  1.3069218  1.306999   1.2751203  1.3262419
 1.4085578  1.4447683  1.7733358  2.1231885  2.4376998  2.6512628
 2.836995   2.8217072  2.6837788  2.6201     2.887594   3.1701345
 2.651771   2.4559953  2.3923569  2.340042   2.504423   2.683278
 2.9320734  2.3764331  2.3220448  2.0898383  2.0773318  1.9072688
 1.71171    1.2888161  1.0686715  1.0646913  0.60099924 0.49251676
 0.67473674 0.6422958  0.5796121  0.47853887 0.6065353  0.8565176
 1.0321476  0.91855454 0.96361536 0.76997566 0.7885198  0.965935
 0.971873   1.0048891  1.3482952  1.7307864  2.256025   2.5652456
 2.6998417  2.380931   2.1623719  1.7475995  1.7179384  2.0446305
 2.3502636  1.763923   1.1900004  1.0363522  1.1125746  1.0916656
 1.1420413  1.16159    1.207736   1.3025833  1.47142    1.837432
 2.158097   2.3106456  2.3552248  2.4749599  2.5773253  2.4755802
 2.335014   2.6027744  2.9732554  2.7498102  2.5346189  2.2301471
 2.088758   2.1132405  2.193767   2.542701   2.1668622  2.1086833
 1.8209059  1.8518785  1.8809153  1.540176   1.1336253  1.0107391
 0.9907671  0.5141306  0.4846586  0.56638557 0.5567241  0.46860242
 0.37240437 0.46944976 0.80942893 1.1553576  1.0333422  1.0410743
 0.67765963 0.7386887  0.94029206 0.74232554 0.72998905 0.905581
 1.2243651  1.6814822  2.1472802  1.960096   1.6279893  1.5374341
 1.1994176  1.2439893  1.6708136  1.6920539  1.189575   0.727789
 0.57937706 0.7374075  0.87072814 0.86109287 0.9538235  1.111204
 1.2368164  1.445663   1.6237257  1.7512692  1.9454409  2.0015044
 2.0933602  2.2989016  2.312191   2.1393747  2.415747   2.6782134
 2.6095471  2.5698318  2.2850292  1.9450381  1.808999   1.7999042
 2.0811076  2.0833092  1.8831259  1.6092558  1.6001545  1.7470546
 1.3919871  1.0497028  0.9467463  0.880422   0.488197   0.42921594
 0.42475402 0.42013443 0.3302168  0.2546727  0.39234477 0.74069
 1.0945233  1.1168287  1.0242984  0.6896151  0.6922689  0.7822687
 0.5536945  0.37192604 0.4935876  0.62313575 1.0008299  1.3681837
 1.3869594  1.0612429  1.0420711  0.75584936 0.91015434 1.5261886
 1.4321678  1.0340539  0.67194873 0.5872452  0.43192524 0.386095
 0.4535473  0.6897861  0.8908216  1.0454086  1.063274   1.2272193
 1.3923156  1.6812875  1.8581394  1.8535557  2.0932174  2.1347513
 2.0030246  2.2052758  2.6070607  2.628096   2.4749365  2.1894047
 1.966638   1.6460928  1.5044266  1.6290795  2.021945   1.6054044
 1.4128568  1.3410442  1.5739225  1.5138355  1.055322   0.82704705
 0.6998491  0.5010923  0.35481635 0.3459071  0.30496615 0.34003475
 0.19496062 0.2738534  0.46391422 0.78879046 1.070681   0.9125554
 0.64586663 0.5445285  0.7972519  0.5561285  0.35497734 0.34506646
 0.3804811  0.35007286 0.587188   1.0694621  0.73491186 0.7233559
 0.54427207 0.6141248  1.1702734  1.1861931  0.9543229  0.65539646
 0.44254476 0.35264078 0.2504302  0.17937317 0.27746925 0.5288051
 0.739681   0.71220547 0.8723022  1.0639744  1.3314719  1.5349016
 1.694233   1.8575542  2.196064   2.117706   1.9442496  2.2012343
 2.3622355  2.5650203  2.1587918  1.7778525  1.546371   1.4353949
 1.2538012  1.2399005  1.6044064  1.3938153  1.1714859  1.3037403
 1.584815   1.4371102  1.0354357  0.77642673]
