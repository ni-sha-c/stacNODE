time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.74%, model saved.
Epoch: 0 Train: 32290.82422 Test: 4097.68506
Epoch 80: New minimal relative error: 90.80%, model saved.
Epoch: 80 Train: 7062.95166 Test: 1179.76709
Epoch 160: New minimal relative error: 72.71%, model saved.
Epoch: 160 Train: 6186.26416 Test: 889.69580
Epoch: 240 Train: 6548.77637 Test: 912.11682
Epoch: 320 Train: 4979.70410 Test: 637.29108
Epoch 400: New minimal relative error: 63.68%, model saved.
Epoch: 400 Train: 4206.67480 Test: 475.50491
Epoch: 480 Train: 2849.31055 Test: 246.21288
Epoch: 560 Train: 1818.83643 Test: 107.89087
Epoch: 640 Train: 931.93347 Test: 153.28987
Epoch 720: New minimal relative error: 22.52%, model saved.
Epoch: 720 Train: 543.96863 Test: 21.07966
Epoch: 800 Train: 471.96552 Test: 77.14621
Epoch: 880 Train: 557.79059 Test: 208.54022
Epoch 960: New minimal relative error: 20.47%, model saved.
Epoch: 960 Train: 272.70831 Test: 6.98490
Epoch 1040: New minimal relative error: 9.01%, model saved.
Epoch: 1040 Train: 238.92030 Test: 6.06292
Epoch: 1120 Train: 305.82123 Test: 62.93158
Epoch: 1200 Train: 202.31062 Test: 3.26472
Epoch 1280: New minimal relative error: 8.10%, model saved.
Epoch: 1280 Train: 169.42137 Test: 2.24626
Epoch: 1360 Train: 155.22267 Test: 2.29058
Epoch 1440: New minimal relative error: 7.78%, model saved.
Epoch: 1440 Train: 147.57547 Test: 2.48871
Epoch: 1520 Train: 180.30769 Test: 15.58865
Epoch: 1600 Train: 145.77063 Test: 5.53763
Epoch: 1680 Train: 115.25117 Test: 1.29157
Epoch: 1760 Train: 112.04005 Test: 1.31391
Epoch: 1840 Train: 120.46924 Test: 2.40192
Epoch: 1920 Train: 96.92278 Test: 3.46382
Epoch 2000: New minimal relative error: 6.30%, model saved.
Epoch: 2000 Train: 93.85610 Test: 0.79317
Epoch: 2080 Train: 88.59753 Test: 5.01552
Epoch: 2160 Train: 88.70226 Test: 7.51915
Epoch: 2240 Train: 103.76285 Test: 12.69458
Epoch: 2320 Train: 77.00919 Test: 2.81229
Epoch: 2400 Train: 74.24253 Test: 0.61201
Epoch: 2480 Train: 91.90852 Test: 9.05904
Epoch: 2560 Train: 68.06242 Test: 0.42278
Epoch: 2640 Train: 67.19213 Test: 0.55392
Epoch: 2720 Train: 70.97983 Test: 7.83620
Epoch: 2800 Train: 60.44925 Test: 1.58146
Epoch: 2880 Train: 61.34791 Test: 2.56517
Epoch: 2960 Train: 63.39861 Test: 1.56484
Epoch: 3040 Train: 57.55084 Test: 0.71098
Epoch: 3120 Train: 57.99978 Test: 3.38549
Epoch: 3200 Train: 76.88390 Test: 6.22462
Epoch 3280: New minimal relative error: 1.13%, model saved.
Epoch: 3280 Train: 54.33439 Test: 0.31964
Epoch: 3360 Train: 50.52153 Test: 0.41126
Epoch: 3440 Train: 48.43221 Test: 0.25736
Epoch: 3520 Train: 53.31437 Test: 0.54734
Epoch: 3600 Train: 47.77295 Test: 0.33704
Epoch: 3680 Train: 45.11427 Test: 0.43944
Epoch: 3760 Train: 44.99991 Test: 0.67603
Epoch: 3840 Train: 62.68518 Test: 10.77925
Epoch: 3920 Train: 44.94524 Test: 0.34117
Epoch: 4000 Train: 43.56474 Test: 0.23262
Epoch: 4080 Train: 42.78743 Test: 0.70853
Epoch: 4160 Train: 43.79336 Test: 0.85672
Epoch: 4240 Train: 39.42139 Test: 0.19485
Epoch: 4320 Train: 39.74353 Test: 0.74095
Epoch: 4400 Train: 38.90057 Test: 0.43296
Epoch: 4480 Train: 37.54311 Test: 0.20221
Epoch: 4560 Train: 40.51038 Test: 0.31991
Epoch: 4640 Train: 36.67493 Test: 0.18917
Epoch: 4720 Train: 35.77061 Test: 0.13445
Epoch: 4800 Train: 35.32098 Test: 0.17231
Epoch: 4880 Train: 34.77985 Test: 0.12475
Epoch: 4960 Train: 37.68213 Test: 0.38114
Epoch: 5040 Train: 33.37679 Test: 0.11738
Epoch: 5120 Train: 36.59411 Test: 0.27008
Epoch: 5200 Train: 34.98618 Test: 0.51818
Epoch: 5280 Train: 32.62665 Test: 0.21667
Epoch: 5360 Train: 32.49201 Test: 0.12411
Epoch: 5440 Train: 33.24870 Test: 1.97116
Epoch: 5520 Train: 30.42193 Test: 0.11992
Epoch: 5600 Train: 30.25129 Test: 0.14761
Epoch: 5680 Train: 30.28479 Test: 0.11008
Epoch: 5760 Train: 30.74577 Test: 0.18502
Epoch: 5840 Train: 29.56989 Test: 0.14268
Epoch: 5920 Train: 30.06955 Test: 0.14983
Epoch: 6000 Train: 28.84118 Test: 0.59872
Epoch: 6080 Train: 28.41833 Test: 0.11097
Epoch: 6160 Train: 27.15384 Test: 0.09918
Epoch: 6240 Train: 28.16019 Test: 0.09367
Epoch: 6320 Train: 26.71121 Test: 0.10753
Epoch 6400: New minimal relative error: 0.91%, model saved.
Epoch: 6400 Train: 26.99284 Test: 0.15985
Epoch: 6480 Train: 25.46166 Test: 0.11769
Epoch: 6560 Train: 25.07167 Test: 0.37528
Epoch: 6640 Train: 26.55506 Test: 0.95634
Epoch: 6720 Train: 25.91552 Test: 0.64648
Epoch: 6800 Train: 25.60918 Test: 0.07098
Epoch: 6880 Train: 25.43892 Test: 0.07255
Epoch: 6960 Train: 25.41851 Test: 0.17656
Epoch: 7040 Train: 24.97193 Test: 0.10051
Epoch: 7120 Train: 24.36022 Test: 0.06885
Epoch: 7200 Train: 23.91972 Test: 0.09752
Epoch: 7280 Train: 24.58345 Test: 0.67102
Epoch: 7360 Train: 24.26366 Test: 0.06634
Epoch: 7440 Train: 24.45405 Test: 0.10185
Epoch: 7520 Train: 24.48737 Test: 0.16950
Epoch: 7600 Train: 24.70193 Test: 0.11492
Epoch: 7680 Train: 24.74111 Test: 0.48760
Epoch: 7760 Train: 23.17804 Test: 0.08484
Epoch: 7840 Train: 22.57800 Test: 0.07532
Epoch: 7920 Train: 22.20229 Test: 0.05631
Epoch: 7999 Train: 22.01496 Test: 0.24582
Training Loss: tensor(22.0150)
Test Loss: tensor(0.2458)
Learned LE: [ 8.7821996e-01  8.8988207e-03 -1.4550930e+01]
True LE: [ 8.7465066e-01  4.0821671e-03 -1.4560904e+01]
Relative Error: [1.135003   1.160362   1.005573   1.1642987  1.554979   2.084414
 2.4599621  2.5875058  2.5722256  2.5006719  2.5800712  2.298766
 2.0092766  1.7616311  1.7147806  1.7421937  1.6240586  1.4838685
 1.4842365  1.2790865  1.6657054  2.1114106  2.3238685  2.4666586
 2.6295857  2.3218117  1.5602467  0.9458025  1.002624   0.8406386
 0.537658   0.52075255 0.6864766  0.8330433  0.73606807 0.3960473
 1.2784951  2.2609699  2.8414726  3.2422879  3.7277334  4.03725
 4.0387673  3.7916996  3.3683152  2.754944   2.0939958  1.6216308
 1.3197644  1.1017704  1.4953028  1.9533012  2.043582   1.8896086
 1.671469   1.3979883  1.2396845  1.0107971  0.7438462  0.6701896
 0.8288059  1.2134116  1.3596016  1.4398519  1.1588188  1.0732329
 1.3257833  1.7840309  2.3629882  2.857695   2.8082745  2.6186907
 2.6493537  2.307421   2.0849235  1.8031471  1.7004112  1.6220487
 1.5903066  1.5320603  1.4496713  1.4866121  1.7809331  2.199219
 2.434896   2.445176   2.3323307  2.263497   1.6097652  0.9647995
 0.92485845 1.2684326  1.0745591  1.1222138  1.4503042  1.5805331
 1.5843056  0.9932376  0.39776903 1.274995   1.9593388  2.4793892
 3.104011   3.652321   3.8710854  3.7267756  3.3749084  2.8278995
 2.042044   1.6025372  1.3646662  1.1254933  1.5088382  2.012249
 2.1850455  2.1087532  1.9111246  1.5749362  1.4113811  1.1736815
 0.8187629  0.65893924 0.7900039  1.1910015  1.29516    1.3400807
 1.2309043  0.9996209  0.9396053  1.2101924  1.6750693  2.1771643
 2.6024244  2.6024716  2.587357   2.2431903  2.0502439  1.7249717
 1.5098584  1.4840978  1.4478272  1.4323215  1.3823416  1.5411656
 1.7903218  2.1996818  2.4388108  2.5128813  2.3645353  1.9547541
 1.5600438  0.998363   0.8156558  1.6812807  1.5683569  1.1661406
 1.0934978  1.2983565  1.7939327  1.6297543  0.83017415 0.4053439
 0.9242001  1.5984943  2.3824925  3.1206381  3.5701275  3.5692897
 3.3430538  2.8823116  2.2176397  1.5881289  1.4660139  1.1536205
 1.5026778  2.07295    2.309566   2.3230648  2.1598208  1.8300824
 1.6394509  1.3865982  0.98814154 0.6616198  0.7847569  0.8857325
 1.113717   1.2900498  1.3791004  1.186344   0.89127487 0.86336255
 1.2481139  1.5784124  2.0720818  2.112525   1.9589614  1.9011759
 2.1253767  1.7339882  1.4163679  1.3546603  1.2604184  1.1734236
 1.2139826  1.3343579  1.5491732  1.9360489  2.284118   2.5059104
 2.3573887  2.000825   1.3372025  0.85598445 0.727849   1.3552862
 1.7700019  1.4260968  0.95829797 0.87912863 1.1578183  1.309064
 1.134455   0.5501796  0.4798932  0.9406805  1.5392536  2.369811
 2.9906886  3.2939396  3.3000722  2.9726658  2.4468946  1.6955471
 1.5316209  1.2748244  1.5875304  2.1012669  2.4032521  2.4764943
 2.3226044  2.1279     1.9787142  1.6896682  1.2385874  0.77707493
 0.79890835 0.8798793  0.99785084 1.2060848  1.3420354  1.4168726
 1.1602001  0.8270241  0.8534524  1.1682388  1.5601649  1.5694333
 1.5414393  1.3230335  1.5582182  1.7129322  1.5065672  1.192415
 1.1031349  0.9441683  1.0013824  1.1001552  1.2966341  1.6085128
 1.951731   2.1490996  2.2269952  2.0504928  1.4588867  0.6824471
 0.58719885 0.91041744 1.4041895  1.6270674  1.3168075  0.9262635
 0.8086104  0.8916784  0.70879054 0.40697557 0.6557742  0.6731274
 1.1269455  1.71779    2.2440257  2.7007453  3.1087666  3.0547905
 2.7065794  2.079637   1.508987   1.4266267  1.5158211  1.9781226
 2.3674996  2.5323026  2.4192345  2.2896342  2.1391685  2.0098937
 1.5942501  1.3051158  1.297524   1.2132285  1.086767   1.0345577
 1.1660832  1.3462687  1.3584627  1.1644233  0.79620254 0.92435086
 1.1526015  1.3963321  1.1562312  0.9477364  1.0327734  1.1861953
 1.2851161  1.1492361  0.90356565 0.7505174  0.5939988  0.7919853
 1.1705352  1.350804   1.8184209  2.0965095  2.0597863  1.834673
 1.5608641  0.84424776 0.46967855 0.6865074  0.93954355 1.3301638
 1.613647   1.3166767  0.8505796  0.5761331  0.4833191  0.39177307
 0.25477192 0.7426376  0.76361936 1.2662706  1.7650541  2.0104644
 2.5916505  2.902163   2.8153484  2.439079   1.723095   1.4177252
 1.2528911  1.7387878  2.0982099  2.4543025  2.5314224  2.3943906
 2.1922493  2.067797   1.7794539  1.7655272  1.8529333  1.7113501
 1.4759588  1.2180679  1.0378218  1.0602739  1.195915   1.3106961
 1.0771354  0.6007747  0.77033603 0.8919736  0.92398685 0.7259804
 0.52113163 0.75568265 0.9466926  0.9655125  0.75675726 0.6175349
 0.35279292 0.36825708 0.8322815  1.2721738  1.4613006  1.8471466
 2.1322548  2.0473254  1.56813    0.99148834]
