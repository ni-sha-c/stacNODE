time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 97.39%, model saved.
Epoch: 0 Train: 60031.92578 Test: 3733.95898
Epoch: 100 Train: 11701.17090 Test: 1080.24988
Epoch 200: New minimal relative error: 92.67%, model saved.
Epoch: 200 Train: 13146.26855 Test: 1050.32544
Epoch: 300 Train: 11782.25391 Test: 1247.43323
Epoch 400: New minimal relative error: 55.68%, model saved.
Epoch: 400 Train: 10129.84570 Test: 836.09796
Epoch: 500 Train: 10876.60352 Test: 854.43237
Epoch 600: New minimal relative error: 53.43%, model saved.
Epoch: 600 Train: 9331.86133 Test: 659.97571
Epoch: 700 Train: 7698.08643 Test: 501.93774
Epoch: 800 Train: 6894.21143 Test: 392.18634
Epoch: 900 Train: 4794.87402 Test: 199.02585
Epoch: 1000 Train: 1967.11548 Test: 54.61206
Epoch 1100: New minimal relative error: 39.86%, model saved.
Epoch: 1100 Train: 1257.83142 Test: 26.16709
Epoch 1200: New minimal relative error: 26.84%, model saved.
Epoch: 1200 Train: 990.03699 Test: 24.97389
Epoch: 1300 Train: 830.04126 Test: 11.85715
Epoch: 1400 Train: 752.84460 Test: 15.92846
Epoch 1500: New minimal relative error: 8.50%, model saved.
Epoch: 1500 Train: 621.74323 Test: 10.12180
Epoch: 1600 Train: 596.46002 Test: 8.30324
Epoch: 1700 Train: 574.76245 Test: 18.41819
Epoch: 1800 Train: 578.33759 Test: 27.74391
Epoch 1900: New minimal relative error: 5.87%, model saved.
Epoch: 1900 Train: 539.09949 Test: 16.34764
Epoch: 2000 Train: 569.44666 Test: 13.48561
Epoch: 2100 Train: 574.84998 Test: 48.67283
Epoch: 2200 Train: 418.75702 Test: 4.20097
Epoch: 2300 Train: 395.85599 Test: 4.20125
Epoch: 2400 Train: 463.91721 Test: 22.39744
Epoch: 2500 Train: 405.69299 Test: 7.52902
Epoch: 2600 Train: 396.86353 Test: 8.95992
Epoch: 2700 Train: 391.29004 Test: 33.92251
Epoch: 2800 Train: 339.72470 Test: 5.38766
Epoch: 2900 Train: 321.49850 Test: 6.88749
Epoch: 3000 Train: 360.04059 Test: 9.24117
Epoch: 3100 Train: 620.78326 Test: 58.78349
Epoch: 3200 Train: 381.10056 Test: 10.67500
Epoch: 3300 Train: 336.51648 Test: 6.16826
Epoch: 3400 Train: 330.78867 Test: 6.23250
Epoch: 3500 Train: 297.87317 Test: 5.34702
Epoch: 3600 Train: 308.32550 Test: 5.25614
Epoch: 3700 Train: 315.89310 Test: 7.81326
Epoch: 3800 Train: 324.18933 Test: 9.15995
Epoch: 3900 Train: 307.05353 Test: 3.56390
Epoch: 4000 Train: 312.46399 Test: 4.21524
Epoch: 4100 Train: 264.47156 Test: 2.37877
Epoch 4200: New minimal relative error: 3.59%, model saved.
Epoch: 4200 Train: 287.39526 Test: 3.23938
Epoch: 4300 Train: 298.13956 Test: 2.89120
Epoch: 4400 Train: 311.48035 Test: 3.64690
Epoch: 4500 Train: 282.99161 Test: 3.85427
Epoch: 4600 Train: 283.54663 Test: 3.85562
Epoch: 4700 Train: 305.40939 Test: 5.76885
Epoch: 4800 Train: 282.79590 Test: 2.81246
Epoch: 4900 Train: 273.48071 Test: 3.59451
Epoch: 5000 Train: 277.38821 Test: 2.67378
Epoch: 5100 Train: 261.07434 Test: 3.07787
Epoch: 5200 Train: 216.82935 Test: 1.40767
Epoch: 5300 Train: 215.73822 Test: 1.42628
Epoch: 5400 Train: 231.07767 Test: 3.69013
Epoch: 5500 Train: 246.85495 Test: 6.61975
Epoch: 5600 Train: 219.08362 Test: 1.78400
Epoch: 5700 Train: 205.52959 Test: 3.33077
Epoch: 5800 Train: 238.88498 Test: 2.23713
Epoch: 5900 Train: 190.99409 Test: 1.34474
Epoch: 6000 Train: 182.76183 Test: 0.96148
Epoch: 6100 Train: 194.40135 Test: 1.29043
Epoch: 6200 Train: 195.69826 Test: 1.95996
Epoch: 6300 Train: 175.62378 Test: 2.83359
Epoch: 6400 Train: 157.85547 Test: 2.03350
Epoch: 6500 Train: 167.10709 Test: 10.74985
Epoch: 6600 Train: 138.88417 Test: 0.83407
Epoch: 6700 Train: 124.37539 Test: 0.72346
Epoch: 6800 Train: 143.91595 Test: 1.64236
Epoch: 6900 Train: 138.38200 Test: 0.97845
Epoch: 7000 Train: 132.06583 Test: 0.76901
Epoch: 7100 Train: 129.72386 Test: 0.87118
Epoch: 7200 Train: 133.90424 Test: 1.03932
Epoch 7300: New minimal relative error: 3.20%, model saved.
Epoch: 7300 Train: 129.21016 Test: 0.96388
Epoch: 7400 Train: 130.81392 Test: 1.33109
Epoch: 7500 Train: 134.60350 Test: 1.63192
Epoch: 7600 Train: 110.15116 Test: 0.54781
Epoch: 7700 Train: 117.33968 Test: 1.02044
Epoch: 7800 Train: 110.18114 Test: 0.62109
Epoch: 7900 Train: 100.78576 Test: 0.53103
Epoch: 8000 Train: 99.34449 Test: 0.47036
Epoch: 8100 Train: 97.92603 Test: 0.60351
Epoch: 8200 Train: 98.26192 Test: 0.59021
Epoch: 8300 Train: 93.51644 Test: 0.51355
Epoch: 8400 Train: 108.72462 Test: 0.85298
Epoch: 8500 Train: 113.59135 Test: 1.07792
Epoch: 8600 Train: 98.31860 Test: 0.72882
Epoch: 8700 Train: 84.82908 Test: 0.41523
Epoch: 8800 Train: 84.87195 Test: 0.42490
Epoch 8900: New minimal relative error: 2.51%, model saved.
Epoch: 8900 Train: 89.68257 Test: 0.51538
Epoch: 9000 Train: 96.57232 Test: 0.59189
Epoch: 9100 Train: 95.64098 Test: 0.66863
Epoch: 9200 Train: 82.68665 Test: 0.87438
Epoch: 9300 Train: 90.86160 Test: 0.82469
Epoch: 9400 Train: 89.53818 Test: 0.74813
Epoch: 9500 Train: 81.61806 Test: 1.18583
Epoch: 9600 Train: 88.24446 Test: 0.75534
Epoch: 9700 Train: 95.23251 Test: 0.84265
Epoch: 9800 Train: 85.98499 Test: 0.72440
Epoch: 9900 Train: 87.13802 Test: 0.57507
Epoch: 9999 Train: 85.22628 Test: 0.52695
Training Loss: tensor(85.2263)
Test Loss: tensor(0.5269)
Learned LE: [ 8.7463129e-01  7.0944517e-03 -1.4523977e+01]
True LE: [ 8.6866802e-01  1.0186301e-02 -1.4557983e+01]
Relative Error: [ 6.443131   6.6303663  6.839613   7.068064   7.203076   7.250856
  7.6300106  8.113716   8.575203   8.882988   9.165977   9.192758
  9.050062   8.889182   8.706415   8.925736   9.2120075  9.647252
 10.128907  10.541708  10.983126  11.385377  11.873774  12.186852
 12.377881  12.455839  12.4386015 12.354391  12.307437  12.292056
 12.265662  12.213334  12.120522  11.976452  12.502001  12.99787
 13.439785  13.849254  14.0922575 14.251425  14.665445  14.99923
 14.902025  14.545099  14.217355  13.9358635 13.56983   13.100277
 12.655709  12.348166  11.925022  11.331541  10.673295   9.899887
  8.994418   7.9557486  7.1812925  6.7492433  6.2455134  5.6664653
  5.4094577  5.5209117  5.7623787  6.1033463  6.3693633  6.576176
  6.6775136  6.848273   7.0175843  7.478313   7.9176784  8.244792
  8.556753   8.636542   8.516838   8.379332   8.16081    8.132117
  8.504071   8.897984   9.332662   9.672887  10.130448  10.407809
 10.859076  11.228728  11.466179  11.621736  11.6322975 11.568172
 11.539222  11.613541  11.581677  11.4397335 11.351254  11.394366
 11.920925  12.341463  12.697655  13.045009  13.252134  13.392888
 13.724187  14.036433  13.934666  13.6173725 13.324263  13.124178
 12.715322  12.212931  11.84819   11.617706  11.232195  10.71539
 10.080142   9.34311    8.46197    7.412178   6.4594164  5.9689546
  5.3793154  4.8284526  4.7064705  4.8218966  5.0997996  5.5098033
  5.841215   6.0561347  6.2705026  6.471809   6.440699   6.8248878
  7.31398    7.658282   7.983111   8.082296   7.96872    7.8647456
  7.713215   7.689297   7.8318434  8.188059   8.504244   8.921209
  9.196769   9.492709   9.866331  10.313571  10.661662  10.8622055
 10.900383  10.864402  10.846452  10.935555  10.946418  10.759578
 10.581684  10.729103  11.318978  11.635991  11.943995  12.216171
 12.394956  12.509269  12.752851  13.05319   12.9934635 12.702992
 12.450241  12.250734  11.8614025 11.368906  11.111188  10.973919
 10.633538  10.168515   9.593825   8.982727   8.124624   7.05308
  5.8890095  5.314374   4.656528   4.1338716  4.0436735  4.228191
  4.5340786  4.8615174  5.281299   5.7243576  6.0053644  6.1522865
  6.1812763  6.382519   6.782717   7.040299   7.3063164  7.4375944
  7.374158   7.2991705  7.18629    7.258474   7.3921347  7.503969
  7.7319107  8.091261   8.287003   8.598188   8.880327   9.463457
  9.8757105 10.102923  10.158087  10.156247  10.161184  10.276472
 10.271607  10.122002   9.852184  10.011535  10.57691   10.928276
 11.183537  11.367848  11.527523  11.602642  11.744114  12.050262
 12.050679  11.814303  11.608615  11.361474  11.007462  10.576137
 10.42274   10.305019  10.061323   9.751792   9.330026   8.700105
  7.8513565  6.779647   5.5069246  4.834621   4.0595984  3.6075704
  3.5379858  3.6920722  3.980234   4.3102756  4.8224416  5.3214808
  5.591516   5.8351483  5.9397154  5.893069   6.1977882  6.5073295
  6.69691    6.7378774  6.7041373  6.660912   6.571445   6.762265
  6.9859877  6.980043   7.0286164  7.1310244  7.4756527  7.694395
  8.116812   8.555322   9.022561   9.339706   9.482633   9.555158
  9.651205   9.703229   9.611422   9.453726   9.222996   9.249427
  9.77676   10.199674  10.344148  10.489002  10.619612  10.681046
 10.701392  11.022265  11.080916  10.917098  10.748367  10.48944
 10.149417   9.833204   9.759797   9.649619   9.617803   9.48701
  8.963282   8.306252   7.42876    6.380729   5.2026043  4.410784
  3.768658   3.3659086  3.239049   3.3619273  3.5547917  3.8886125
  4.3085485  4.700227   5.128498   5.400343   5.532145   5.4956822
  5.559162   5.9743223  6.215728   6.3848457  6.195189   6.1282973
  6.047925   6.2596045  6.483441   6.517251   6.4727635  6.3667283
  6.58632    6.9580703  7.253949   7.5882463  8.207195   8.690663
  8.929782   9.07181    9.25268    9.301777   9.151893   8.874864
  8.561153   8.470946   8.937442   9.359917   9.56831    9.658413
  9.676318   9.71883    9.73001    9.949448  10.10396    9.988624
  9.864208   9.620678   9.281921   9.128598   9.056935   9.06234
  9.244593   9.008147   8.518494   7.8683643  7.1288004  6.1777587
  4.9892144  4.0026784  3.4706306  3.1347568  3.1721349  3.3256452
  3.3892055  3.5596464  3.7887242  4.101632   4.5062666  4.8318377
  4.9760127  5.1014442  5.073594   5.312648   5.6125574  5.8773446
  5.781524   5.580813   5.4638567  5.6826396  5.9954624  6.1151404
  5.9949126  5.8000164  5.805789   6.1312437  6.4840093  6.842441
  7.3929243  7.954616   8.2888775  8.515765 ]
