time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.85%, model saved.
Epoch: 0 Train: 31604.97070 Test: 3597.42407
Epoch 80: New minimal relative error: 90.10%, model saved.
Epoch: 80 Train: 8315.19922 Test: 1242.16870
Epoch: 160 Train: 7670.05762 Test: 1002.57086
Epoch 240: New minimal relative error: 76.12%, model saved.
Epoch: 240 Train: 7088.76758 Test: 983.03662
Epoch: 320 Train: 6544.09082 Test: 874.02490
Epoch 400: New minimal relative error: 66.17%, model saved.
Epoch: 400 Train: 5717.95020 Test: 728.23596
Epoch: 480 Train: 3792.98462 Test: 412.45605
Epoch 560: New minimal relative error: 59.72%, model saved.
Epoch: 560 Train: 1812.00427 Test: 121.34404
Epoch 640: New minimal relative error: 30.21%, model saved.
Epoch: 640 Train: 944.56146 Test: 41.91033
Epoch 720: New minimal relative error: 26.63%, model saved.
Epoch: 720 Train: 670.70300 Test: 35.90705
Epoch: 800 Train: 495.31223 Test: 17.04356
Epoch 880: New minimal relative error: 11.01%, model saved.
Epoch: 880 Train: 379.05045 Test: 15.11110
Epoch: 960 Train: 317.46243 Test: 8.62462
Epoch: 1040 Train: 271.62143 Test: 15.52153
Epoch: 1120 Train: 235.82707 Test: 4.78062
Epoch: 1200 Train: 224.38092 Test: 5.35355
Epoch 1280: New minimal relative error: 10.12%, model saved.
Epoch: 1280 Train: 202.12039 Test: 4.12341
Epoch: 1360 Train: 192.47046 Test: 7.54622
Epoch 1440: New minimal relative error: 5.81%, model saved.
Epoch: 1440 Train: 201.13452 Test: 3.90270
Epoch 1520: New minimal relative error: 3.67%, model saved.
Epoch: 1520 Train: 167.92305 Test: 2.64438
Epoch: 1600 Train: 167.26709 Test: 4.95047
Epoch: 1680 Train: 151.72231 Test: 2.34704
Epoch: 1760 Train: 183.26355 Test: 52.12267
Epoch 1840: New minimal relative error: 2.53%, model saved.
Epoch: 1840 Train: 144.31544 Test: 2.21482
Epoch: 1920 Train: 135.75484 Test: 2.52607
Epoch: 2000 Train: 139.89404 Test: 2.15487
Epoch: 2080 Train: 123.85978 Test: 1.77133
Epoch: 2160 Train: 116.63776 Test: 1.57374
Epoch: 2240 Train: 108.74501 Test: 1.38827
Epoch: 2320 Train: 106.01853 Test: 1.47455
Epoch: 2400 Train: 101.19502 Test: 1.50753
Epoch: 2480 Train: 95.77699 Test: 1.55637
Epoch: 2560 Train: 104.94367 Test: 10.94399
Epoch: 2640 Train: 90.96143 Test: 1.01640
Epoch: 2720 Train: 89.77425 Test: 1.04177
Epoch: 2800 Train: 88.96428 Test: 1.68801
Epoch: 2880 Train: 103.22910 Test: 4.75230
Epoch: 2960 Train: 100.12767 Test: 3.40469
Epoch: 3040 Train: 87.69709 Test: 1.01852
Epoch: 3120 Train: 97.41225 Test: 1.32843
Epoch: 3200 Train: 104.63082 Test: 1.64551
Epoch: 3280 Train: 91.40652 Test: 1.51319
Epoch: 3360 Train: 94.25897 Test: 1.44135
Epoch: 3440 Train: 85.14483 Test: 1.06399
Epoch: 3520 Train: 78.98275 Test: 0.92126
Epoch: 3600 Train: 76.92725 Test: 0.90506
Epoch: 3680 Train: 74.70631 Test: 0.77016
Epoch: 3760 Train: 72.20158 Test: 0.98167
Epoch 3840: New minimal relative error: 2.08%, model saved.
Epoch: 3840 Train: 69.41088 Test: 0.69153
Epoch: 3920 Train: 70.95438 Test: 0.71526
Epoch: 4000 Train: 94.74226 Test: 8.87097
Epoch: 4080 Train: 79.30080 Test: 1.01850
Epoch: 4160 Train: 76.08643 Test: 0.96209
Epoch: 4240 Train: 71.59515 Test: 0.81471
Epoch: 4320 Train: 67.72343 Test: 0.73250
Epoch: 4400 Train: 67.10368 Test: 1.12358
Epoch: 4480 Train: 65.51919 Test: 0.67602
Epoch: 4560 Train: 65.45432 Test: 0.68259
Epoch 4640: New minimal relative error: 1.81%, model saved.
Epoch: 4640 Train: 64.86443 Test: 0.71500
Epoch: 4720 Train: 70.03192 Test: 1.89613
Epoch: 4800 Train: 67.65199 Test: 0.75552
Epoch: 4880 Train: 66.80166 Test: 0.76223
Epoch: 4960 Train: 65.27579 Test: 0.73605
Epoch: 5040 Train: 64.38811 Test: 0.69860
Epoch 5120: New minimal relative error: 1.73%, model saved.
Epoch: 5120 Train: 62.38670 Test: 0.67063
Epoch: 5200 Train: 59.63766 Test: 0.59455
Epoch: 5280 Train: 57.25256 Test: 0.55247
Epoch: 5360 Train: 56.95350 Test: 0.57494
Epoch: 5440 Train: 56.99212 Test: 2.08021
Epoch: 5520 Train: 55.70185 Test: 0.55361
Epoch 5600: New minimal relative error: 1.68%, model saved.
Epoch: 5600 Train: 56.36713 Test: 0.55589
Epoch: 5680 Train: 55.72446 Test: 0.50987
Epoch: 5760 Train: 55.55591 Test: 0.50104
Epoch: 5840 Train: 53.11043 Test: 0.45680
Epoch: 5920 Train: 52.47095 Test: 0.49331
Epoch: 6000 Train: 51.23471 Test: 0.43699
Epoch: 6080 Train: 51.86963 Test: 0.61800
Epoch: 6160 Train: 50.34968 Test: 0.60572
Epoch: 6240 Train: 48.85397 Test: 0.44068
Epoch: 6320 Train: 48.85613 Test: 0.54409
Epoch: 6400 Train: 51.49383 Test: 1.96670
Epoch: 6480 Train: 49.93113 Test: 0.40769
Epoch: 6560 Train: 51.05934 Test: 1.55047
Epoch 6640: New minimal relative error: 0.83%, model saved.
Epoch: 6640 Train: 50.16603 Test: 0.43659
Epoch: 6720 Train: 50.28164 Test: 0.51543
Epoch: 6800 Train: 50.96257 Test: 0.48719
Epoch: 6880 Train: 51.40079 Test: 0.51976
Epoch: 6960 Train: 50.88061 Test: 0.55044
Epoch: 7040 Train: 51.96876 Test: 1.53797
Epoch: 7120 Train: 49.08573 Test: 0.40301
Epoch: 7200 Train: 47.28837 Test: 0.38536
Epoch: 7280 Train: 46.10259 Test: 0.46065
Epoch: 7360 Train: 45.07568 Test: 0.37042
Epoch: 7440 Train: 46.14496 Test: 0.35120
Epoch: 7520 Train: 47.33273 Test: 0.40443
Epoch: 7600 Train: 47.67030 Test: 2.05646
Epoch: 7680 Train: 44.86586 Test: 0.33434
Epoch: 7760 Train: 43.90465 Test: 0.34017
Epoch: 7840 Train: 44.48523 Test: 0.32976
Epoch: 7920 Train: 46.23352 Test: 0.36311
Epoch: 7999 Train: 46.08887 Test: 0.41484
Training Loss: tensor(46.0889)
Test Loss: tensor(0.4148)
Learned LE: [  0.8901616   -0.02214805 -14.555115  ]
True LE: [ 8.7361157e-01 -1.4430428e-03 -1.4543825e+01]
Relative Error: [0.7008216  0.9063605  1.1226115  1.216137   1.0382396  0.7771562
 0.7607663  0.7380227  0.6313352  0.7845186  0.57381856 0.8439369
 0.74535364 0.22133654 0.20742793 0.23974411 0.47639352 0.8315088
 1.1732231  1.8923966  2.2871583  1.2769983  0.3331201  0.23279528
 0.49513647 0.6435159  0.46598768 0.28455168 0.15971452 0.4759667
 1.1707726  1.414164   1.429505   1.4808222  1.672806   1.7227533
 1.5259991  1.3186727  1.3423109  1.294482   0.98885036 0.8397685
 0.9473751  1.5294447  2.0534377  2.146711   1.4092706  0.96361095
 0.9122656  1.0800633  1.5216199  1.7956698  2.290176   2.6594067
 2.447106   2.555535   2.1368868  1.7294129  1.0438021  0.5692496
 0.33258307 0.38964307 0.5834459  0.43613622 0.7000905  0.9923107
 1.1176013  1.0904357  0.8856749  0.7225383  0.815583   0.73909706
 0.7665546  0.7220825  0.877204   1.076412   0.69920266 0.27837828
 0.20041531 0.50153196 0.64059585 0.7758595  1.2556766  2.254408
 1.70291    0.4151879  0.3024004  0.5068088  0.5260406  0.41620517
 0.22738776 0.5259834  0.31846416 0.7431806  1.2736067  1.3564156
 1.2316192  1.2430161  1.4224753  1.1807665  1.0656861  1.1815976
 1.088444   0.78268826 0.37346607 0.33314902 0.85153794 1.3761578
 1.6942885  1.8203549  1.0779016  0.82425    0.84275293 1.2020969
 1.5162373  2.0149226  2.3861506  2.4591181  2.4742444  1.8575419
 1.4526646  0.8482529  0.475578   0.38699478 0.5730573  0.586543
 0.4118908  0.4059098  0.5268291  0.6147149  0.67427003 0.6181037
 0.58899534 0.73135585 0.86723405 0.81305915 0.64006454 0.61282253
 0.8114624  0.8327447  0.56000984 0.10479964 0.16020833 0.4460819
 0.48388556 0.5782197  1.1550812  1.9809713  1.162999   0.02663805
 0.5816117  0.6559143  0.4173259  0.1647435  0.47385362 0.29661947
 0.3904226  0.62475437 1.1877414  1.4408723  1.1861047  1.0448236
 1.1404669  0.7394672  0.8028055  1.0214207  0.70495725 0.18717259
 0.22252758 0.32294357 0.6550184  0.98125935 1.3328379  1.7319156
 0.99116635 0.6093001  0.73103786 1.2026544  1.5089194  1.8888488
 2.2832518  2.2175622  1.7519126  1.2718421  0.88750935 0.43281773
 0.42367586 0.6938082  0.86861813 0.66580975 0.455944   0.31660283
 0.27354613 0.44573763 0.5758549  0.82905674 0.9801519  1.3876605
 1.4542414  1.4691262  1.0552691  1.0108877  0.4641851  0.29451057
 0.23008482 0.11783363 0.16378327 0.25395095 0.36025766 0.34707072
 0.6463936  1.6825083  1.2483714  0.11821026 0.6216157  0.7468748
 0.3388396  0.22478104 0.5354039  0.21315953 0.27049437 0.6588175
 1.0596552  1.2795119  1.1665125  0.97996306 1.1211364  0.9067298
 0.89539325 1.0642422  0.6637572  0.5558474  0.5119506  0.3281696
 0.2545303  0.5847344  1.0844359  1.4212265  1.0352745  0.7210175
 0.7610401  1.0876653  1.3367912  1.5876445  2.0228076  1.7683611
 1.2581664  0.9305249  0.6238301  0.45724803 0.5958415  0.95999336
 1.075548   1.1169896  1.1931814  1.0695281  0.7406689  0.47924328
 0.4774583  0.59196025 0.89600235 1.1844661  1.5194061  1.6394677
 1.9675324  2.1249774  1.6819129  0.9071246  0.3985744  0.29430312
 0.5006208  0.24583001 0.36975557 0.599277   0.36509266 0.61133283
 1.5721097  0.7399092  0.36847916 0.7128955  0.5940229  0.06971326
 0.63909817 0.38117212 0.32977882 0.7839176  0.8682225  1.0240638
 1.0567378  1.0615283  0.68930197 0.5444902  0.60488236 1.1224843
 1.3939682  1.3326243  1.1793292  0.8343094  0.5635599  0.48002255
 0.69231445 1.0229232  1.2287321  0.90314466 0.7713232  0.8591138
 0.8883686  1.160288   1.5723381  1.5184165  1.1103601  0.7700035
 0.6320401  0.39964283 0.5886184  1.0658803  1.4221829  1.5933342
 1.2699555  0.64406776 0.17821717 0.18406346 0.42629758 0.4885706
 0.30838034 0.16418216 0.45426822 0.94773805 1.371502   1.7558572
 1.687534   1.615279   1.5581764  1.2792569  0.52225274 0.37251547
 0.46089533 0.52174044 0.6589254  0.5820423  0.40480396 0.90587384
 0.80653876 0.27064854 0.5548663  0.66579324 0.21705839 0.7852055
 0.4234641  0.3888431  1.0680665  1.0102717  0.87203765 0.94762015
 0.76149404 0.30596107 0.5847633  0.5679704  0.6836808  0.5851419
 0.2075779  0.8245281  1.3448168  1.0013694  0.78046817 0.8380956
 0.749788   0.74767077 0.9438998  0.6124605  0.6164901  0.4975602
 0.7427828  1.201262   1.0744627  0.9174629  0.67042273 0.6441939
 0.6035844  0.9926985  1.5667433  1.6079292  1.2297746  0.71171623
 0.3202358  0.15593009 0.27853718 0.37161842 0.32117277 0.17622533
 0.15498179 0.15598707 0.17950603 0.22629437 0.5663335  0.71491706
 0.8706306  1.1591908  1.2700845  1.3101634 ]
