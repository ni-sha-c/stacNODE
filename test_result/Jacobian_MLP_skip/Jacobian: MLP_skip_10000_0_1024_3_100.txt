time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 98.95%, model saved.
Epoch: 0 Train: 9544.94434 Test: 3982.19263
Epoch: 100 Train: 2251.10083 Test: 846.72076
Epoch: 200 Train: 1146.22314 Test: 385.40576
Epoch: 300 Train: 819.52563 Test: 158.09685
Epoch 400: New minimal relative error: 50.97%, model saved.
Epoch: 400 Train: 241.82095 Test: 48.95664
Epoch 500: New minimal relative error: 38.26%, model saved.
Epoch: 500 Train: 255.98923 Test: 29.41081
Epoch: 600 Train: 150.15613 Test: 41.85090
Epoch 700: New minimal relative error: 7.90%, model saved.
Epoch: 700 Train: 62.48299 Test: 5.59234
Epoch: 800 Train: 83.13791 Test: 29.26414
Epoch: 900 Train: 61.50759 Test: 12.14658
Epoch: 1000 Train: 44.75399 Test: 4.54895
Epoch 1100: New minimal relative error: 3.62%, model saved.
Epoch: 1100 Train: 33.04126 Test: 2.08732
Epoch: 1200 Train: 35.01301 Test: 2.77911
Epoch: 1300 Train: 28.04709 Test: 5.82285
Epoch: 1400 Train: 27.88818 Test: 8.23050
Epoch: 1500 Train: 20.77881 Test: 0.86084
Epoch: 1600 Train: 28.44715 Test: 3.94293
Epoch: 1700 Train: 23.00301 Test: 1.52134
Epoch: 1800 Train: 21.85053 Test: 2.07865
Epoch: 1900 Train: 17.59872 Test: 0.55124
Epoch: 2000 Train: 20.08729 Test: 4.95097
Epoch 2100: New minimal relative error: 1.34%, model saved.
Epoch: 2100 Train: 16.78106 Test: 0.41802
Epoch: 2200 Train: 15.46133 Test: 0.34205
Epoch: 2300 Train: 17.37880 Test: 5.29674
Epoch: 2400 Train: 12.55107 Test: 0.32911
Epoch: 2500 Train: 22.03355 Test: 4.92877
Epoch: 2600 Train: 12.08942 Test: 0.92539
Epoch: 2700 Train: 22.81590 Test: 11.72093
Epoch: 2800 Train: 10.70381 Test: 0.20215
Epoch: 2900 Train: 10.21392 Test: 0.21495
Epoch: 3000 Train: 11.38109 Test: 0.71335
Epoch: 3100 Train: 9.56517 Test: 0.18259
Epoch: 3200 Train: 9.68614 Test: 0.20493
Epoch: 3300 Train: 12.10152 Test: 2.20355
Epoch: 3400 Train: 8.82863 Test: 0.17636
Epoch: 3500 Train: 11.56820 Test: 1.79151
Epoch: 3600 Train: 9.22476 Test: 1.74193
Epoch: 3700 Train: 8.59857 Test: 0.50026
Epoch: 3800 Train: 9.39254 Test: 1.39633
Epoch: 3900 Train: 7.78291 Test: 0.46905
Epoch: 4000 Train: 7.44264 Test: 0.28555
Epoch: 4100 Train: 8.09053 Test: 0.94029
Epoch: 4200 Train: 7.49029 Test: 0.81629
Epoch: 4300 Train: 10.32979 Test: 1.47506
Epoch 4400: New minimal relative error: 1.11%, model saved.
Epoch: 4400 Train: 8.65863 Test: 0.17336
Epoch: 4500 Train: 7.26747 Test: 0.49023
Epoch: 4600 Train: 9.00636 Test: 3.78049
Epoch: 4700 Train: 7.02465 Test: 0.83868
Epoch: 4800 Train: 6.09327 Test: 0.05319
Epoch: 4900 Train: 8.41564 Test: 2.32807
Epoch: 5000 Train: 7.93271 Test: 0.78315
Epoch: 5100 Train: 5.76769 Test: 0.12768
Epoch: 5200 Train: 6.03772 Test: 0.52380
Epoch: 5300 Train: 5.75154 Test: 0.22944
Epoch: 5400 Train: 5.83395 Test: 0.49491
Epoch: 5500 Train: 6.43775 Test: 0.77660
Epoch: 5600 Train: 5.43060 Test: 0.16374
Epoch: 5700 Train: 6.49812 Test: 1.06811
Epoch: 5800 Train: 8.96891 Test: 1.06747
Epoch: 5900 Train: 5.23406 Test: 0.17650
Epoch: 6000 Train: 5.12266 Test: 0.12740
Epoch: 6100 Train: 5.10247 Test: 0.09189
Epoch: 6200 Train: 6.14667 Test: 0.52325
Epoch: 6300 Train: 5.14776 Test: 0.10525
Epoch: 6400 Train: 5.55003 Test: 0.55652
Epoch: 6500 Train: 5.07198 Test: 0.40215
Epoch: 6600 Train: 5.01286 Test: 0.23844
Epoch: 6700 Train: 5.47182 Test: 0.32756
Epoch: 6800 Train: 5.56137 Test: 0.58545
Epoch: 6900 Train: 4.82928 Test: 0.03496
Epoch: 7000 Train: 6.16707 Test: 1.35287
Epoch: 7100 Train: 4.73549 Test: 0.13323
Epoch: 7200 Train: 4.89969 Test: 0.21953
Epoch: 7300 Train: 4.49709 Test: 0.05234
Epoch: 7400 Train: 8.36002 Test: 4.06960
Epoch 7500: New minimal relative error: 0.95%, model saved.
Epoch: 7500 Train: 4.34206 Test: 0.03013
Epoch: 7600 Train: 4.61479 Test: 0.04384
Epoch: 7700 Train: 6.52981 Test: 0.86799
Epoch: 7800 Train: 4.47270 Test: 0.10239
Epoch: 7900 Train: 4.37844 Test: 0.14177
Epoch: 8000 Train: 4.87911 Test: 0.26317
Epoch: 8100 Train: 4.30171 Test: 0.04075
Epoch: 8200 Train: 4.31490 Test: 0.05238
Epoch: 8300 Train: 4.23789 Test: 0.05510
Epoch: 8400 Train: 4.97470 Test: 0.69494
Epoch: 8500 Train: 4.23057 Test: 0.04153
Epoch 8600: New minimal relative error: 0.87%, model saved.
Epoch: 8600 Train: 4.19365 Test: 0.05098
Epoch: 8700 Train: 4.38415 Test: 0.23908
Epoch: 8800 Train: 4.37186 Test: 0.09036
Epoch: 8900 Train: 4.28971 Test: 0.26847
Epoch: 9000 Train: 4.06259 Test: 0.03856
Epoch: 9100 Train: 4.38637 Test: 0.13112
Epoch: 9200 Train: 3.93953 Test: 0.03151
Epoch: 9300 Train: 3.82175 Test: 0.03747
Epoch: 9400 Train: 3.87833 Test: 0.12786
Epoch: 9500 Train: 3.88830 Test: 0.03630
Epoch: 9600 Train: 3.77633 Test: 0.04532
Epoch: 9700 Train: 4.06053 Test: 0.33052
Epoch: 9800 Train: 4.25789 Test: 0.19164
Epoch: 9900 Train: 3.90331 Test: 0.15816
Epoch: 9999 Train: 3.84184 Test: 0.08384
Training Loss: tensor(3.8418)
Test Loss: tensor(0.0838)
Learned LE: [ 8.7634867e-01 -1.3235633e-02 -1.4529274e+01]
True LE: [ 8.5297197e-01  4.6953312e-03 -1.4535044e+01]
Relative Error: [0.36301076 0.39903694 0.6217577  0.73295224 0.6698105  0.43663993
 0.43309098 0.5060469  0.3880304  0.32183182 0.23322499 0.301174
 0.49079025 0.9559987  1.5989854  2.3293483  2.671541   2.5152066
 2.1363075  1.8508203  1.6146598  1.4831123  1.4894267  1.61155
 1.8461605  1.6043303  1.5371203  1.4452852  1.3760322  1.2876692
 1.2819908  1.3306341  1.2965239  1.0802691  0.74379456 0.72048914
 1.021023   1.2333506  1.3555472  1.3540176  1.2784467  1.1137027
 1.1701778  1.3365015  1.4362402  1.6359689  1.8800898  1.8955973
 1.9108679  2.1728818  2.4422114  2.6499104  2.6600163  2.3965635
 1.9341406  1.7282238  1.5649214  1.3344396  0.9624627  0.482148
 0.14783366 0.22649038 0.44079828 0.41086063 0.49969143 0.6807986
 0.672699   0.5118483  0.47530302 0.41344002 0.37107715 0.36312315
 0.22894177 0.27918917 0.5117618  0.94956803 1.5437462  1.9885007
 2.1562319  2.0129771  1.8311279  1.3666404  1.0717961  0.97330225
 0.9852653  0.98988277 1.223763   1.267375   1.6061796  1.6264126
 1.3727225  1.1763585  1.0873051  1.052212   1.0383737  0.97196287
 0.69774294 0.6538859  0.94535005 1.178827   1.1548727  1.0342448
 0.95823455 0.9563513  1.0350037  1.1824518  1.2698336  1.3665224
 1.5638096  1.7637517  1.825295   2.2292132  2.3764017  2.6836426
 2.804583   2.6056254  2.0601492  1.5320857  1.3808994  1.1481969
 0.970435   0.5424803  0.29200295 0.1993376  0.39094266 0.51327425
 0.33034992 0.62105495 0.6755826  0.61609626 0.57164526 0.4331815
 0.342552   0.41389298 0.39608875 0.21947561 0.41286027 0.9008264
 1.3718171  1.4499375  1.6083101  1.6453613  1.5469736  1.1877154
 0.8175353  0.49404794 0.6007592  0.57449985 0.78524446 0.7393027
 1.0293111  1.5638033  1.5623164  1.1819097  0.9445224  0.8438107
 0.77493733 0.77197206 0.723691   0.6953683  0.86299276 1.097436
 1.1553754  1.0439628  0.7010199  0.6634547  0.8593586  0.96783537
 1.0768061  1.1255528  1.3433355  1.5126648  1.6904927  2.0236907
 2.310252   2.5599976  2.83763    2.7744467  2.2545145  1.5916655
 1.102253   1.07741    0.87003314 0.7575078  0.41852057 0.2238572
 0.26306316 0.44461793 0.16679452 0.4845655  0.66254234 0.6654004
 0.642453   0.5263852  0.33483773 0.44232324 0.46488965 0.23026603
 0.2334987  0.8285024  1.0093856  1.0300639  1.0771157  1.2479632
 1.2466407  1.0823052  0.6938227  0.41973954 0.27264437 0.40740365
 0.5487511  0.46983612 0.5380613  0.90436774 1.2523506  1.3813207
 0.9724931  0.7082625  0.644285   0.6023682  0.6209654  0.6623495
 0.68972373 0.9061108  1.0304005  0.9481529  0.7307091  0.4935679
 0.66259104 0.77116454 0.77687556 0.87045896 1.0254272  1.3713061
 1.4473604  1.724661   2.1511517  2.3159986  2.6927402  2.816889
 2.4392705  1.9006348  1.1334933  0.7922964  0.9681703  0.7907522
 0.655846   0.3736285  0.21851014 0.2521521  0.34422028 0.18931624
 0.5025594  0.5418111  0.59911764 0.5629033  0.4390997  0.38343418
 0.4318161  0.28961834 0.10787313 0.4750885  0.705784   0.59950405
 0.58024853 0.66255534 0.8804879  0.8788021  0.7555909  0.58703303
 0.539196   0.42798123 0.31879696 0.4658491  0.3322737  0.3964112
 0.61596245 0.8163235  1.0007281  0.7218343  0.47286502 0.5179683
 0.42754692 0.51846087 0.64783627 0.7199438  0.862082   0.82110465
 0.6123135  0.37811318 0.38250312 0.6296457  0.6292489  0.4807152
 0.6569894  0.92922235 1.2697079  1.3407705  1.7170528  1.9914333
 2.2643912  2.5338314  2.4675035  2.145421   1.5581795  0.7001976
 0.57014775 0.9621134  0.7052824  0.6804037  0.3477259  0.16846013
 0.26554582 0.19458559 0.21259743 0.3789851  0.44777566 0.4612357
 0.43344688 0.30626947 0.27551168 0.20885295 0.1450454  0.33376947
 0.34988743 0.27966776 0.20088297 0.33713663 0.41891053 0.7071418
 0.7151043  0.74516606 0.7032226  0.71176535 0.6146356  0.34445274
 0.37789464 0.2810329  0.32123938 0.2667169  0.40843883 0.5674782
 0.60032433 0.38303417 0.41860923 0.34798485 0.45196474 0.63666636
 0.7609169  0.8666859  0.72263205 0.31189683 0.21476398 0.559131
 0.4733978  0.4705534  0.48188922 0.6902138  0.85795873 1.0491487
 1.0993125  1.4944922  1.7335057  2.0923185  2.2232614  2.1116676
 1.8888301  1.1884309  0.48130107 0.44117892 0.95201635 0.6783592
 0.54854167 0.1975295  0.13885792 0.33612224 0.1353417  0.12849237
 0.22924407 0.37887344 0.4336718  0.29015547 0.14633776 0.09469633
 0.2953927  0.41587824 0.167865   0.20810233 0.24552162 0.36703297
 0.46048075 0.3112819  0.5447202  0.6494965  0.74598914 0.7626162
 0.6785271  0.6200848  0.32139075 0.39504078]
