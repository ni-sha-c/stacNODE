time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.18%, model saved.
Epoch: 0 Train: 31359.39258 Test: 3888.17432
Epoch 100: New minimal relative error: 97.69%, model saved.
Epoch: 100 Train: 8567.99805 Test: 1374.37854
Epoch: 200 Train: 7277.26416 Test: 1538.21448
Epoch 300: New minimal relative error: 72.87%, model saved.
Epoch: 300 Train: 6994.31055 Test: 1045.57458
Epoch: 400 Train: 6990.70117 Test: 1102.90503
Epoch 500: New minimal relative error: 67.86%, model saved.
Epoch: 500 Train: 5840.19531 Test: 790.02063
Epoch: 600 Train: 2958.53491 Test: 248.21802
Epoch 700: New minimal relative error: 16.64%, model saved.
Epoch: 700 Train: 1430.64893 Test: 136.64359
Epoch: 800 Train: 856.49176 Test: 43.07509
Epoch: 900 Train: 676.70331 Test: 23.69697
Epoch 1000: New minimal relative error: 15.35%, model saved.
Epoch: 1000 Train: 555.02423 Test: 21.44286
Epoch 1100: New minimal relative error: 8.00%, model saved.
Epoch: 1100 Train: 451.91046 Test: 11.30391
Epoch 1200: New minimal relative error: 6.68%, model saved.
Epoch: 1200 Train: 408.30289 Test: 10.06750
Epoch: 1300 Train: 358.25244 Test: 10.01782
Epoch: 1400 Train: 347.18805 Test: 16.73089
Epoch: 1500 Train: 348.39911 Test: 11.70861
Epoch: 1600 Train: 345.01962 Test: 13.17074
Epoch: 1700 Train: 271.08527 Test: 4.60407
Epoch: 1800 Train: 248.97401 Test: 4.23023
Epoch: 1900 Train: 228.94366 Test: 4.64794
Epoch: 2000 Train: 218.64465 Test: 6.53412
Epoch: 2100 Train: 214.72849 Test: 9.98317
Epoch: 2200 Train: 184.15088 Test: 3.07761
Epoch: 2300 Train: 167.88957 Test: 2.96649
Epoch: 2400 Train: 161.11082 Test: 4.19832
Epoch: 2500 Train: 143.49397 Test: 2.05437
Epoch 2600: New minimal relative error: 5.61%, model saved.
Epoch: 2600 Train: 140.77609 Test: 1.89998
Epoch 2700: New minimal relative error: 5.08%, model saved.
Epoch: 2700 Train: 167.63318 Test: 3.03423
Epoch: 2800 Train: 144.66676 Test: 4.52787
Epoch: 2900 Train: 132.75967 Test: 2.30265
Epoch: 3000 Train: 136.08807 Test: 3.59366
Epoch 3100: New minimal relative error: 2.66%, model saved.
Epoch: 3100 Train: 124.50997 Test: 1.45100
Epoch: 3200 Train: 129.81412 Test: 11.49452
Epoch: 3300 Train: 106.57268 Test: 1.07632
Epoch 3400: New minimal relative error: 1.94%, model saved.
Epoch: 3400 Train: 96.35115 Test: 0.97955
Epoch: 3500 Train: 100.58561 Test: 1.03150
Epoch: 3600 Train: 94.89030 Test: 0.98075
Epoch: 3700 Train: 92.08398 Test: 0.97465
Epoch: 3800 Train: 91.40549 Test: 1.06006
Epoch: 3900 Train: 89.55074 Test: 1.14129
Epoch: 4000 Train: 85.16175 Test: 2.18209
Epoch: 4100 Train: 77.67414 Test: 0.72808
Epoch: 4200 Train: 79.60069 Test: 1.86217
Epoch: 4300 Train: 77.95123 Test: 1.27927
Epoch: 4400 Train: 71.40629 Test: 0.68083
Epoch: 4500 Train: 72.02542 Test: 1.11986
Epoch: 4600 Train: 71.97270 Test: 0.77269
Epoch: 4700 Train: 66.08013 Test: 0.61725
Epoch: 4800 Train: 70.76408 Test: 0.64753
Epoch: 4900 Train: 73.33270 Test: 1.07085
Epoch: 5000 Train: 74.98536 Test: 2.00153
Epoch: 5100 Train: 72.37713 Test: 1.57890
Epoch: 5200 Train: 66.37347 Test: 1.12235
Epoch: 5300 Train: 61.41262 Test: 0.56246
Epoch 5400: New minimal relative error: 1.33%, model saved.
Epoch: 5400 Train: 58.36244 Test: 0.40777
Epoch: 5500 Train: 57.12977 Test: 0.61056
Epoch: 5600 Train: 56.18671 Test: 0.41775
Epoch: 5700 Train: 54.44663 Test: 0.36435
Epoch: 5800 Train: 54.17031 Test: 0.43604
Epoch: 5900 Train: 54.81922 Test: 1.12348
Epoch: 6000 Train: 53.62525 Test: 0.56784
Epoch: 6100 Train: 51.48997 Test: 0.35311
Epoch: 6200 Train: 51.01086 Test: 0.34123
Epoch: 6300 Train: 50.55658 Test: 0.31623
Epoch: 6400 Train: 48.10920 Test: 0.33609
Epoch: 6500 Train: 47.93432 Test: 0.48486
Epoch: 6600 Train: 47.75657 Test: 0.81224
Epoch: 6700 Train: 51.72136 Test: 0.65379
Epoch: 6800 Train: 51.88422 Test: 0.75612
Epoch: 6900 Train: 53.76488 Test: 0.53454
Epoch: 7000 Train: 51.25522 Test: 0.47886
Epoch: 7100 Train: 52.90953 Test: 0.56569
Epoch: 7200 Train: 60.14162 Test: 0.84553
Epoch: 7300 Train: 54.00178 Test: 0.65249
Epoch: 7400 Train: 52.51419 Test: 0.81063
Epoch: 7500 Train: 52.79154 Test: 0.73468
Epoch: 7600 Train: 49.43613 Test: 0.48484
Epoch: 7700 Train: 47.76151 Test: 0.44731
Epoch: 7800 Train: 47.94740 Test: 0.67571
Epoch: 7900 Train: 50.11313 Test: 0.69128
Epoch: 8000 Train: 49.13617 Test: 0.49057
Epoch: 8100 Train: 48.56116 Test: 0.46277
Epoch: 8200 Train: 56.32273 Test: 1.75247
Epoch: 8300 Train: 49.56281 Test: 0.46240
Epoch: 8400 Train: 47.46033 Test: 0.51321
Epoch: 8500 Train: 46.19671 Test: 0.76895
Epoch: 8600 Train: 45.35590 Test: 0.39596
Epoch: 8700 Train: 44.16866 Test: 0.37658
Epoch: 8800 Train: 44.71663 Test: 0.56894
Epoch: 8900 Train: 43.47181 Test: 0.52487
Epoch: 9000 Train: 43.98921 Test: 0.34273
Epoch: 9100 Train: 43.27498 Test: 0.34081
Epoch: 9200 Train: 43.35905 Test: 0.47817
Epoch: 9300 Train: 41.99071 Test: 0.34047
Epoch: 9400 Train: 43.83389 Test: 0.36141
Epoch: 9500 Train: 41.53047 Test: 0.35520
Epoch: 9600 Train: 42.48816 Test: 0.38179
Epoch: 9700 Train: 44.10228 Test: 0.69103
Epoch: 9800 Train: 42.04859 Test: 0.40232
Epoch: 9900 Train: 42.65380 Test: 0.36922
Epoch: 9999 Train: 40.76313 Test: 0.35786
Training Loss: tensor(40.7631)
Test Loss: tensor(0.3579)
Learned LE: [  0.8768716   -0.01962449 -14.534213  ]
True LE: [ 8.8015002e-01 -4.0863231e-03 -1.4551449e+01]
Relative Error: [1.3686874  1.2599645  1.1545501  1.083659   1.078711   0.95534575
 0.8173441  0.79121643 0.85258794 0.38766366 0.43820515 0.5286927
 0.46634915 0.42417216 0.85405993 1.3068724  1.4902955  1.3659046
 1.1527652  1.087935   1.2153842  1.263016   1.076755   0.99406594
 0.86212254 0.673938   0.5820059  0.49454504 0.43855816 0.34772125
 0.45385277 0.5380881  0.6811792  0.71541    0.7720755  0.6191952
 0.5155162  0.62565374 0.6446808  0.81239575 1.1573296  1.5855085
 1.7607487  1.5248523  0.9531307  0.386274   0.22947983 0.42584273
 0.63606524 0.8204069  0.98279727 1.0933897  1.0298117  1.0145745
 1.1407461  1.268692   1.5780691  1.8215172  2.0247283  2.0321667
 1.8461525  1.5533733  1.3654203  1.2817135  1.3260698  1.4176491
 1.2644956  1.1386681  0.8569015  0.823774   0.9128908  0.6888403
 0.43068406 0.46780318 0.42733285 0.30609882 0.46737698 1.0356364
 1.3827647  1.2805569  1.0768306  1.034235   1.1693466  1.2513316
 1.0986224  1.10888    0.980049   0.81123745 0.68198085 0.5932723
 0.48545665 0.48387855 0.50305396 0.56259876 0.7281461  0.7278258
 0.7793371  0.7477885  0.5445524  0.51623213 0.44701    0.7165331
 1.0012007  1.4354532  1.6062601  1.3793563  0.9957648  0.38775405
 0.20573272 0.45301542 0.6094037  0.7290727  0.93853986 1.1200187
 1.213303   1.3547964  1.475862   1.5319873  1.8413994  2.2313018
 2.0784016  1.9309921  1.8227617  1.5361068  1.2698388  1.0620868
 1.0221045  1.0140113  1.108081   1.3174698  1.0883167  0.95961857
 1.046794   1.0984302  0.61246276 0.4058004  0.48333263 0.41809744
 0.18385544 0.69425523 1.0232426  1.1506732  0.999437   0.9270562
 1.0973518  1.2887074  1.115608   1.1565566  1.1127658  1.0173577
 0.85508686 0.7998201  0.6128361  0.67325    0.6823096  0.70394784
 0.8564718  0.8605987  0.79260993 0.9650399  0.71229464 0.52291954
 0.39570177 0.596202   0.82361364 1.0702611  1.5378357  1.3341578
 1.0982128  0.42697978 0.25768805 0.38457635 0.5119158  0.6657783
 0.96050173 1.164121   1.2297604  1.4108472  1.6029892  1.7650217
 1.930346   1.853207   1.6910615  1.5762964  1.5189799  1.5463428
 1.3998425  1.1386405  0.9600172  0.8335877  0.7692834  0.8127403
 0.9839677  1.2503618  1.304232   1.251255   1.0457693  0.5056968
 0.47648606 0.56238544 0.3182152  0.24605526 0.7403189  1.0316923
 0.9535823  0.8373163  0.9651865  1.2742792  1.1740276  1.1612238
 1.2297871  1.2199183  1.1823193  1.0666986  0.8779337  0.8712518
 0.7850308  0.80636024 0.98932225 1.1100252  1.0064021  0.9749199
 1.015865   0.6503565  0.44516096 0.52288777 0.6708948  0.7462018
 1.237431   1.3383919  1.1621293  0.532276   0.21892422 0.3300165
 0.4298082  0.5466341  0.6996843  0.94977486 1.1384931  1.4270494
 1.5355529  1.6655471  1.7937078  1.4087716  1.2259215  1.3044045
 1.3399999  1.3028915  1.2561361  1.1760216  1.022795   0.9254714
 0.71828693 0.593656   0.57115513 0.6933721  1.244021   1.5724559
 1.4153788  0.9304671  0.44215423 0.5221558  0.49090615 0.18391117
 0.48443842 0.7273631  0.88057977 0.828436   0.82899725 1.0446228
 1.1592175  1.2049755  1.3559514  1.316779   1.2949523  1.351239
 1.086486   0.7384485  0.6102397  0.56275874 0.61935824 0.8312679
 0.9171383  1.1270559  1.1063697  0.9774955  0.61469585 0.5385693
 0.55836195 0.5934279  0.8966932  1.3172952  1.1831186  0.7773188
 0.17528358 0.24521965 0.41835666 0.3962988  0.41702315 0.75168806
 0.99941695 1.2        1.5104022  1.7410173  1.4994719  1.2584306
 0.9515794  0.7408245  0.8480234  1.0590208  1.236714   1.1324625
 1.1151817  0.9330736  0.8547554  0.72945154 0.5367366  0.423769
 0.7082349  1.1840878  1.6597437  1.4398911  0.81095594 0.2855156
 0.44400212 0.3521968  0.22355181 0.5643234  0.6909195  0.9012442
 0.86674494 0.9511414  1.125977   1.1485059  1.3105698  1.2368445
 1.2033275  1.317051   1.0042268  0.49628228 0.29683644 0.42632544
 0.5340612  0.6256791  0.63712937 0.6889316  0.9031614  1.2476927
 1.0090356  0.66232413 0.56195235 0.5875446  0.68674564 0.9451227
 1.2634228  1.0907418  0.423114   0.10536207 0.3642723  0.43129373
 0.28988555 0.5254406  0.795117   0.95233685 1.3141329  1.7612139
 1.3490691  0.8188026  0.60658765 0.56994545 0.4470517  0.34464464
 0.4448936  0.6629236  0.8029171  1.0676917  1.0283555  0.8655952
 0.85367924 0.629318   0.42648947 0.68281114 0.96193683 1.1522713
 1.4186158  0.75119066 0.3087766  0.3443526  0.2538668  0.3440768
 0.54084575 0.6613092  0.95420116 0.950801   1.0865593  1.2500575
 1.1036366  1.0749795  1.1172863  1.2224364 ]
