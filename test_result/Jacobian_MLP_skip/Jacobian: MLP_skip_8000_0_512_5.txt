time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.36%, model saved.
Epoch: 0 Train: 170483.09375 Test: 4448.14600
Epoch: 80 Train: 42970.30469 Test: 1881.82239
Epoch: 160 Train: 38106.25000 Test: 1189.31641
Epoch: 240 Train: 32443.28516 Test: 1330.95715
Epoch: 320 Train: 34859.39844 Test: 1522.81616
Epoch: 400 Train: 37495.47266 Test: 1424.04785
Epoch 480: New minimal relative error: 86.22%, model saved.
Epoch: 480 Train: 37049.12109 Test: 1423.80151
Epoch 560: New minimal relative error: 76.67%, model saved.
Epoch: 560 Train: 32982.69531 Test: 1358.01270
Epoch: 640 Train: 38143.42188 Test: 1463.18396
Epoch: 720 Train: 38567.12500 Test: 1531.06067
Epoch: 800 Train: 38768.67578 Test: 1561.13843
Epoch: 880 Train: 34383.77344 Test: 1358.22131
Epoch: 960 Train: 39407.75000 Test: 1575.14160
Epoch 1040: New minimal relative error: 63.29%, model saved.
Epoch: 1040 Train: 35667.04688 Test: 1524.00879
Epoch: 1120 Train: 38029.37109 Test: 1680.01624
Epoch 1200: New minimal relative error: 57.55%, model saved.
Epoch: 1200 Train: 37211.87109 Test: 1378.96838
Epoch: 1280 Train: 39249.48438 Test: 1539.30420
Epoch: 1360 Train: 34648.66797 Test: 1394.41504
Epoch 1440: New minimal relative error: 57.25%, model saved.
Epoch: 1440 Train: 39334.64453 Test: 1576.75000
Epoch: 1520 Train: 36074.78125 Test: 1598.59607
Epoch: 1600 Train: 37934.94141 Test: 1497.32458
Epoch: 1680 Train: 36496.30859 Test: 1881.60864
Epoch: 1760 Train: 35859.10547 Test: 1509.65588
Epoch: 1840 Train: 38037.87891 Test: 1766.20886
Epoch: 1920 Train: 32711.81055 Test: 1332.05994
Epoch: 2000 Train: 36269.94531 Test: 1524.43994
Epoch: 2080 Train: 34652.20312 Test: 1468.13025
Epoch: 2160 Train: 35619.44531 Test: 1399.49902
Epoch 2240: New minimal relative error: 55.78%, model saved.
Epoch: 2240 Train: 34570.12109 Test: 1421.77271
Epoch: 2320 Train: 34016.44531 Test: 1282.78003
Epoch: 2400 Train: 35700.27734 Test: 1404.21033
Epoch: 2480 Train: 32586.56641 Test: 1324.63086
Epoch: 2560 Train: 35235.40625 Test: 1317.76807
Epoch: 2640 Train: 30439.47266 Test: 1240.03906
Epoch: 2720 Train: 33483.04297 Test: 1259.38550
Epoch: 2800 Train: 31093.03320 Test: 1290.72327
Epoch: 2880 Train: 31772.24805 Test: 1119.33459
Epoch: 2960 Train: 30188.74219 Test: 1214.59534
Epoch: 3040 Train: 29235.93164 Test: 939.97131
Epoch: 3120 Train: 29623.98047 Test: 1051.92310
Epoch: 3200 Train: 27882.29102 Test: 1021.25848
Epoch: 3280 Train: 27234.48633 Test: 824.84155
Epoch: 3360 Train: 27028.15430 Test: 913.15088
Epoch: 3440 Train: 26239.36914 Test: 840.89410
Epoch: 3520 Train: 25132.45312 Test: 874.06366
Epoch: 3600 Train: 24300.85352 Test: 659.44006
Epoch: 3680 Train: 24667.14648 Test: 601.37262
Epoch: 3760 Train: 24072.42188 Test: 670.81799
Epoch: 3840 Train: 23622.07617 Test: 645.04956
Epoch: 3920 Train: 22833.16211 Test: 588.13599
Epoch: 4000 Train: 22116.86133 Test: 551.26233
Epoch: 4080 Train: 19950.83594 Test: 441.37286
Epoch: 4160 Train: 19619.49609 Test: 427.54996
Epoch: 4240 Train: 18698.02734 Test: 388.43948
Epoch: 4320 Train: 16756.77539 Test: 323.65613
Epoch: 4400 Train: 15286.95020 Test: 258.97275
Epoch: 4480 Train: 13971.52539 Test: 213.58897
Epoch: 4560 Train: 12295.45117 Test: 160.66521
Epoch: 4640 Train: 10541.98242 Test: 126.71307
Epoch: 4720 Train: 8555.62402 Test: 90.87370
Epoch 4800: New minimal relative error: 44.96%, model saved.
Epoch: 4800 Train: 8503.22754 Test: 84.90614
Epoch 4880: New minimal relative error: 36.97%, model saved.
Epoch: 4880 Train: 7961.73682 Test: 81.94873
Epoch: 4960 Train: 7159.39160 Test: 75.68555
Epoch 5040: New minimal relative error: 33.69%, model saved.
Epoch: 5040 Train: 6911.69141 Test: 79.97664
Epoch: 5120 Train: 7251.02686 Test: 84.45903
Epoch 5200: New minimal relative error: 26.33%, model saved.
Epoch: 5200 Train: 5995.82422 Test: 62.42614
Epoch: 5280 Train: 5148.31201 Test: 50.25023
Epoch: 5360 Train: 4753.15234 Test: 41.72233
Epoch: 5440 Train: 4251.65723 Test: 55.83282
Epoch: 5520 Train: 4023.12915 Test: 25.12403
Epoch 5600: New minimal relative error: 23.66%, model saved.
Epoch: 5600 Train: 3865.95776 Test: 22.74186
Epoch: 5680 Train: 3516.08447 Test: 19.37861
Epoch: 5760 Train: 3333.83472 Test: 20.04787
Epoch 5840: New minimal relative error: 22.33%, model saved.
Epoch: 5840 Train: 3011.56006 Test: 20.60737
Epoch: 5920 Train: 2734.67188 Test: 11.77157
Epoch 6000: New minimal relative error: 9.95%, model saved.
Epoch: 6000 Train: 2593.10181 Test: 10.52972
Epoch: 6080 Train: 2382.05615 Test: 9.28952
Epoch: 6160 Train: 2274.70581 Test: 8.09785
Epoch: 6240 Train: 2336.73120 Test: 11.41369
Epoch: 6320 Train: 2794.50537 Test: 30.88737
Epoch: 6400 Train: 2666.43726 Test: 22.49996
Epoch: 6480 Train: 2238.00073 Test: 12.53532
Epoch: 6560 Train: 2165.58813 Test: 7.45082
Epoch: 6640 Train: 2053.71924 Test: 7.25596
Epoch: 6720 Train: 2012.48145 Test: 5.82317
Epoch: 6800 Train: 1968.97156 Test: 7.25400
Epoch: 6880 Train: 1970.28162 Test: 5.71015
Epoch: 6960 Train: 1889.55164 Test: 6.09023
Epoch: 7040 Train: 1939.06580 Test: 12.37563
Epoch: 7120 Train: 2145.42944 Test: 24.06162
Epoch: 7200 Train: 2147.41602 Test: 12.55744
Epoch: 7280 Train: 1952.14844 Test: 7.54422
Epoch: 7360 Train: 1853.18848 Test: 12.95624
Epoch 7440: New minimal relative error: 9.07%, model saved.
Epoch: 7440 Train: 1803.97327 Test: 6.71569
Epoch: 7520 Train: 1731.77356 Test: 12.57474
Epoch: 7600 Train: 1703.71802 Test: 13.18887
Epoch: 7680 Train: 1815.48877 Test: 7.77693
Epoch: 7760 Train: 1722.22778 Test: 9.09516
Epoch: 7840 Train: 1661.84900 Test: 4.79938
Epoch 7920: New minimal relative error: 7.38%, model saved.
Epoch: 7920 Train: 1591.65332 Test: 7.19471
Epoch: 7999 Train: 1765.44482 Test: 7.54001
Training Loss: tensor(1765.4448)
Test Loss: tensor(7.5400)
Learned LE: [  1.0512167   -0.21802485 -14.5221815 ]
True LE: [ 8.5669029e-01  5.9722243e-03 -1.4537118e+01]
Relative Error: [10.234875   10.667912   11.286693   11.946976   12.730281   13.415238
 13.871286   14.281401   14.62569    14.602886   14.301764   13.724862
 13.12178    12.367374   11.431795   10.36899     9.579595    8.919295
  8.217967    7.8820186   7.592249    6.260626    3.9004986   1.8415384
  0.45902696  1.3118335   2.4219134   3.270815    3.83307     4.1218467
  4.1767025   3.9792736   3.6254532   3.4454708   3.257509    3.0814981
  2.9236732   2.7308643   2.5242264   1.8410726   1.688627    2.612546
  2.798706    3.2606623   3.928686    4.509152    4.9299965   5.1841426
  5.325639    5.39437     5.2040477   4.8070674   4.201527    3.3295722
  2.2069178   1.1465015   1.6879165   3.5477786   5.7272973   8.074517
  9.073413    9.472531    9.912292   10.260518   10.643438   11.317786
 12.106335   12.96895    13.455275   12.875207   12.629056   12.417797
 12.084666   11.845886   11.692549   11.6187315  11.67954    10.686823
  9.761072    8.964941    8.084988    7.597792    7.1071215   6.9190555
  4.7346673   2.5013766   0.6439706   0.96920496  2.1399434   2.9971366
  3.6026678   3.9467592   4.009319    3.8521132   3.4529989   2.9887931
  2.7617345   2.632753    2.5097833   2.3954809   2.1919172   1.6708436
  1.4092196   2.3280447   2.6769426   2.9439335   3.42761     3.8841784
  4.4193597   4.703515    4.913603    5.1458583   5.0616856   4.83766
  4.3311605   3.5207953   2.4916887   1.531395    1.7660038   3.3430073
  5.528454    7.8888187   8.476963    9.137976    9.50503     9.842689
 10.26189    10.824928   11.5980215  11.875352   11.447026   10.923323
 10.509942   10.306102   10.132125    9.929691    9.680268    9.775943
  9.749885    9.639886   10.069726    9.375202    8.347177    7.624948
  6.980174    6.5858827   5.9245253   3.490138    1.4350799   0.351202
  1.6752611   2.6569242   3.3081915   3.7014697   3.846636    3.7153146
  3.3379145   2.737773    2.2014143   2.075404    2.2293363   2.3008697
  2.2751186   2.0579581   1.7505957   2.4909337   2.8961117   3.0163321
  3.1693783   3.3291762   3.7181325   4.1675415   4.4026256   4.8178635
  4.8734446   4.8043714   4.42453     3.7341409   2.8519635   1.9697812
  1.8473691   3.0511775   4.9786158   7.203562    7.775105    8.404954
  8.959448    9.527933   10.022975   10.559953   10.886015   10.113693
  9.613025    9.229824    8.760866    8.459881    8.345122    8.09895
  7.8896112   7.930883    8.103779    7.950012    8.07193     8.641429
  9.023862    8.1197815   7.254038    6.617051    6.2582517   4.9952154
  2.7264628   0.9575526   0.88381666  2.0402098   2.854059    3.3839889
  3.6264226   3.6273208   3.2820928   2.707145    1.8924618   1.2499785
  1.4841055   2.078545    2.5451937   2.6546013   2.548242    3.0894024
  3.4668996   3.5438848   3.3719916   3.0998137   3.1015134   3.5102804
  3.790804    4.320516    4.5584464   4.6721954   4.449013    3.9560754
  3.248175    2.487402    2.0367866   2.5557942   4.137723    6.3124576
  6.829392    7.407154    8.073986    8.911177    9.785358   10.432076
  9.800251    8.893362    8.164303    7.717815    7.3597603   6.9905877
  6.705568    6.5513597   6.429409    6.4021544   6.4078918   6.495441
  6.408585    6.784504    7.202922    7.971635    7.937671    7.0288734
  6.38714     6.112453    4.540141    2.4614108   0.95402735  1.2508545
  2.2019446   3.0123947   3.3719137   3.42208     3.2901816   2.7589447
  1.9580044   0.97626346  0.45434007  1.6092248   2.5395203   3.1971123
  3.4595568   3.8812644   4.2943206   4.3651276   4.229893    3.6021068
  2.9850926   2.813795    3.1630423   3.6761515   4.0268984   4.357758
  4.3428044   4.0697284   3.65828     3.0866945   2.363482    2.088038
  3.0248811   4.964573    5.6448836   6.126098    6.721434    7.6031356
  8.71227    10.047049    9.169706    8.216078    7.397927    6.641428
  6.195254    5.854789    5.559299    5.2728047   5.19765     5.1212792
  5.252393    5.125284    4.9618635   5.283736    5.630884    6.058066
  6.6464534   7.4392905   6.990201    6.415644    6.247541    4.64027
  2.3872766   0.9923551   1.4191344   2.4060874   3.1890247   3.402224
  3.2118526   2.8183413   2.2546325   1.3474509   0.42253345  1.0780292
  2.4203649   3.4869843   4.2324505   4.5200644   5.250036    5.4681168
  5.358333    4.9155474   3.913574    3.0053868   2.4366546   2.9215512
  3.2340276   3.7375867   3.98589     4.008771    3.9167035   3.565561
  2.9115686   2.0766041   2.1183758   3.4208446   4.600793    4.7968516
  5.27909     6.130314    7.2352266   8.505972    7.9827695   7.54349
  6.827127    6.0165143   5.2249312   4.6038294   4.334128    4.229373
  4.1991463   4.160025    4.120557    4.245979    4.137778    3.6880474
  4.2777157   4.6890965   5.0010495   5.41011     6.0791373   7.1375566
  6.7495713   6.514468    5.180944    2.8162122 ]
