time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 107.01%, model saved.
Epoch: 0 Train: 171579.25000 Test: 3629.32153
Epoch: 80 Train: 34477.76562 Test: 1852.19153
Epoch: 160 Train: 32283.54883 Test: 1789.68164
Epoch 240: New minimal relative error: 100.19%, model saved.
Epoch: 240 Train: 30171.04688 Test: 1049.24475
Epoch: 320 Train: 31185.40625 Test: 1211.96423
Epoch: 400 Train: 35365.69531 Test: 1193.84241
Epoch: 480 Train: 32080.64453 Test: 1163.71985
Epoch 560: New minimal relative error: 55.32%, model saved.
Epoch: 560 Train: 31018.16797 Test: 1070.09973
Epoch: 640 Train: 34094.02344 Test: 1246.28650
Epoch: 720 Train: 34553.67188 Test: 1424.60754
Epoch: 800 Train: 36124.63281 Test: 1126.92480
Epoch: 880 Train: 39508.95312 Test: 1300.34204
Epoch: 960 Train: 36783.72266 Test: 1418.00269
Epoch: 1040 Train: 38109.31250 Test: 1337.81714
Epoch: 1120 Train: 36100.59375 Test: 1170.28540
Epoch: 1200 Train: 37555.64844 Test: 1290.67773
Epoch: 1280 Train: 34730.16406 Test: 1250.50024
Epoch: 1360 Train: 37117.24219 Test: 1198.27783
Epoch: 1440 Train: 36383.38281 Test: 1037.07312
Epoch: 1520 Train: 37840.36719 Test: 1259.56250
Epoch: 1600 Train: 38517.86719 Test: 1470.13342
Epoch: 1680 Train: 37141.12500 Test: 1231.61438
Epoch: 1760 Train: 36193.55469 Test: 1192.09119
Epoch: 1840 Train: 39329.83984 Test: 1248.41858
Epoch 1920: New minimal relative error: 54.90%, model saved.
Epoch: 1920 Train: 34474.10156 Test: 1142.00525
Epoch: 2000 Train: 41180.27734 Test: 1376.51770
Epoch: 2080 Train: 37403.38281 Test: 1230.30835
Epoch: 2160 Train: 35789.92188 Test: 1085.47571
Epoch: 2240 Train: 37592.06641 Test: 1228.96606
Epoch: 2320 Train: 34847.67188 Test: 1192.93762
Epoch: 2400 Train: 38464.43359 Test: 1218.47144
Epoch: 2480 Train: 35323.33594 Test: 1146.05518
Epoch: 2560 Train: 37156.61328 Test: 1101.50122
Epoch: 2640 Train: 34611.25000 Test: 1134.14111
Epoch: 2720 Train: 36351.21484 Test: 1134.78040
Epoch: 2800 Train: 35705.95312 Test: 1170.05591
Epoch: 2880 Train: 34489.55859 Test: 1071.85449
Epoch: 2960 Train: 34558.83984 Test: 1095.24463
Epoch: 3040 Train: 33634.40234 Test: 1090.76196
Epoch: 3120 Train: 35803.21484 Test: 1132.31885
Epoch: 3200 Train: 32449.53516 Test: 1001.94043
Epoch: 3280 Train: 33607.20312 Test: 975.18158
Epoch: 3360 Train: 33575.23828 Test: 1022.55762
Epoch: 3440 Train: 32110.61133 Test: 971.51434
Epoch: 3520 Train: 32126.96094 Test: 1000.18353
Epoch: 3600 Train: 33160.53906 Test: 1026.53748
Epoch: 3680 Train: 31652.95898 Test: 1014.82434
Epoch: 3760 Train: 31299.79883 Test: 943.31091
Epoch: 3840 Train: 32380.52930 Test: 1000.20142
Epoch: 3920 Train: 29929.41016 Test: 901.28650
Epoch: 4000 Train: 29535.84180 Test: 888.35321
Epoch: 4080 Train: 31135.23828 Test: 935.18024
Epoch: 4160 Train: 29258.73242 Test: 886.30237
Epoch: 4240 Train: 28503.74414 Test: 860.05927
Epoch: 4320 Train: 29457.96094 Test: 797.16821
Epoch: 4400 Train: 28190.98828 Test: 786.30359
Epoch: 4480 Train: 26833.75000 Test: 741.69037
Epoch: 4560 Train: 25982.50781 Test: 699.49127
Epoch: 4640 Train: 27313.28906 Test: 743.68921
Epoch: 4720 Train: 26900.38281 Test: 742.92670
Epoch: 4800 Train: 25185.57617 Test: 661.88293
Epoch: 4880 Train: 24330.00195 Test: 595.03979
Epoch: 4960 Train: 24862.11523 Test: 622.70166
Epoch: 5040 Train: 24455.95508 Test: 614.71204
Epoch: 5120 Train: 24051.08789 Test: 619.68750
Epoch: 5200 Train: 23815.52734 Test: 588.71265
Epoch: 5280 Train: 23187.07617 Test: 548.07532
Epoch: 5360 Train: 22376.74805 Test: 527.54138
Epoch: 5440 Train: 22022.73828 Test: 484.06177
Epoch: 5520 Train: 21738.15234 Test: 481.38293
Epoch: 5600 Train: 21732.05469 Test: 489.70636
Epoch: 5680 Train: 21528.05859 Test: 475.39993
Epoch: 5760 Train: 20891.05469 Test: 450.60229
Epoch: 5840 Train: 20139.59180 Test: 417.51358
Epoch: 5920 Train: 19305.81055 Test: 389.26624
Epoch: 6000 Train: 19641.63672 Test: 378.35709
Epoch: 6080 Train: 18726.57812 Test: 356.02362
Epoch: 6160 Train: 18207.58594 Test: 359.98184
Epoch: 6240 Train: 17752.44531 Test: 320.52963
Epoch: 6320 Train: 17057.41992 Test: 305.71332
Epoch: 6400 Train: 16173.91504 Test: 273.18869
Epoch: 6480 Train: 15584.41406 Test: 244.28841
Epoch: 6560 Train: 14909.66016 Test: 221.38255
Epoch: 6640 Train: 13403.19824 Test: 192.21556
Epoch: 6720 Train: 12236.78418 Test: 161.89223
Epoch: 6800 Train: 9862.67871 Test: 122.16922
Epoch 6880: New minimal relative error: 29.32%, model saved.
Epoch: 6880 Train: 4243.94287 Test: 53.14745
Epoch 6960: New minimal relative error: 10.86%, model saved.
Epoch: 6960 Train: 2680.64795 Test: 18.29216
Epoch: 7040 Train: 2020.66199 Test: 17.41199
Epoch: 7120 Train: 1706.14856 Test: 11.79038
Epoch: 7200 Train: 1459.56995 Test: 10.04101
Epoch 7280: New minimal relative error: 8.02%, model saved.
Epoch: 7280 Train: 1239.23962 Test: 5.95227
Epoch: 7360 Train: 1111.10547 Test: 5.33575
Epoch: 7440 Train: 1039.61841 Test: 14.90863
Epoch: 7520 Train: 908.86963 Test: 6.64870
Epoch: 7600 Train: 862.64093 Test: 4.06257
Epoch 7680: New minimal relative error: 7.86%, model saved.
Epoch: 7680 Train: 835.37476 Test: 4.76567
Epoch 7760: New minimal relative error: 7.02%, model saved.
Epoch: 7760 Train: 786.17926 Test: 2.09389
Epoch 7840: New minimal relative error: 3.86%, model saved.
Epoch: 7840 Train: 759.54700 Test: 2.97998
Epoch: 7920 Train: 705.00659 Test: 3.58246
Epoch: 7999 Train: 676.12604 Test: 2.02878
Training Loss: tensor(676.1260)
Test Loss: tensor(2.0288)
Learned LE: [  0.9862734   -0.11276307 -14.540092  ]
True LE: [ 8.6200601e-01  9.4578583e-03 -1.4542118e+01]
Relative Error: [1.3831049  1.14916    0.8757232  1.0648633  1.5755938  2.2974315
 2.8301346  3.369671   3.4458704  3.4971886  3.8324468  4.133881
 4.152618   3.7909076  3.4957767  3.601379   3.508383   3.2467008
 3.0106618  2.8616529  2.901208   3.1642747  3.3332765  3.6537535
 4.082014   4.403552   4.3279963  4.215718   4.269638   4.0321507
 3.7268653  3.0606496  2.7366195  2.6489434  2.7052522  2.9211307
 3.4130168  4.0826154  4.6818767  4.6809015  4.66934    4.821122
 4.2321076  4.3974614  3.877159   3.2921157  2.6361194  2.5940406
 2.27724    1.962379   1.8456396  1.715618   1.3784066  1.1328341
 0.7644278  0.41249558 0.24125126 0.5605726  0.8908992  0.90293336
 1.1596073  1.4382966  1.5943248  1.5583934  1.2390313  1.0138844
 1.3342257  1.9616978  2.7891452  3.4259703  3.930535   3.9332418
 3.8644633  4.201417   4.352269   4.52087    4.132385   3.9070327
 3.8711283  3.7585218  3.5857663  3.3147051  3.0698142  2.9068842
 3.0839667  3.525202   3.7982886  4.1972456  4.2502856  3.9831512
 3.913896   3.8616889  3.67909    3.1846204  2.7730656  2.4738529
 2.7128675  2.5634     2.8963962  3.7693102  4.355582   5.054203
 4.9600224  4.937358   4.64775    4.1068683  4.2072835  3.5926666
 3.0834656  2.7658265  2.5281885  1.8764217  1.6946795  1.4267157
 1.2587695  0.8948959  0.67819947 0.42107087 0.2445937  0.25684422
 0.58287567 0.7232773  0.9423102  1.287086   1.4999111  1.5033838
 1.3490742  1.2263173  1.2150902  1.8747404  2.6645005  3.4582253
 3.9644346  4.378761   4.415836   4.172535   4.5615907  4.6138353
 4.56236    4.430759   4.2986183  4.168602   3.9788132  3.8394358
 3.6666718  3.387197   2.9131103  3.0985565  3.5216956  3.8697503
 4.092057   4.033118   3.716549   3.5585382  3.460785   3.211591
 2.6803215  2.5858457  2.4568217  2.5900512  2.7710733  3.1005445
 3.9992397  4.7674937  5.4319124  5.4716325  5.2734213  4.8791676
 4.141163   3.9191115  3.348897   2.916334   2.8489168  2.2800272
 1.6838305  1.4043984  1.1229734  0.86294234 0.74237084 0.5397664
 0.39026815 0.2161411  0.23807205 0.4562955  0.5630578  0.90319926
 1.1857667  1.2590343  1.2937319  1.3467406  1.3566579  1.7268728
 2.5032477  3.1392918  3.782432   4.2528872  4.558601   4.7794394
 4.466072   4.5776496  4.756058   4.5835075  4.5150485  4.5086784
 4.3746157  4.2161674  4.03936    3.9949238  3.723761   3.2806358
 3.0001493  3.4535737  3.9304166  3.8307996  3.826581   3.5509448
 3.3313081  3.2862995  2.9487915  2.2761314  2.2666965  2.2343516
 2.5241432  2.8330321  3.5487983  4.0929165  4.86206    5.449375
 5.910416   5.3562922  5.279003   4.468961   4.059426   3.1177466
 2.7726855  2.9213033  2.0860224  1.584357   1.2520779  0.9605368
 0.8221343  0.8066292  0.6665912  0.48616162 0.24325591 0.2997017
 0.5803153  0.48970646 0.72669095 0.7894723  0.91706085 1.0999473
 1.3615342  1.7462085  2.2207887  2.7499025  3.3122852  3.8315978
 4.228444   4.405952   4.603235   4.801636   4.513168   4.6534386
 4.5682874  4.385132   4.4733014  4.577435   4.3644094  4.146229
 4.2242427  3.9408922  3.684644   3.3214965  3.2220643  3.6637776
 3.6264067  3.5536344  3.508277   3.0965762  3.1680868  3.066578
 2.2974563  1.9515138  1.7027988  2.2109978  2.8735754  3.6570032
 4.138911   4.6224923  5.284533   5.7539864  5.566109   5.403518
 4.6993947  4.47485    3.4661856  2.6944003  2.7675817  2.2254925
 1.649023   1.2873169  0.9847455  0.9901048  1.0714045  0.9617392
 0.7681224  0.469828   0.37710208 0.6834323  0.52945805 0.32724148
 0.37171835 0.5679952  0.88535297 1.3142259  1.7241716  2.20248
 2.7707462  3.0810747  3.3475046  3.7755563  3.9793677  4.012605
 4.2930927  4.6654334  4.361437   4.497384   4.317715   4.104523
 4.2598805  4.4082727  4.3114023  4.195267   4.1111574  3.9468489
 3.7904136  3.3678665  3.299568   3.3722825  3.2591488  3.2381728
 3.0490522  2.812667   2.96734    2.7043736  2.004063   1.7650667
 1.754222   2.3423193  3.3895173  4.11228    4.5307665  4.7283063
 5.2758565  5.64066    5.292536   5.1595917  4.399265   4.0778475
 3.2899554  2.7701833  2.2756853  2.002835   1.4686326  1.2140574
 1.3550284  1.4802948  1.3669009  1.1123714  0.8265063  0.5484547
 0.64433956 0.6469151  0.17757973 0.20270252 0.385991   0.58630097
 1.1083584  1.5759972  2.0596273  2.4383702  2.6121552  2.941352
 3.080409   3.2081656  3.433654   3.4348738  3.648654   4.0551944
 4.0371623  4.1848397  4.132741   3.83621    3.9922397  4.124503
 4.288642   4.3043265  3.9689012  3.9843214 ]
