time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.19%, model saved.
Epoch: 0 Train: 9367.31641 Test: 4104.77539
Epoch: 80 Train: 2702.42163 Test: 1183.72559
Epoch: 160 Train: 2583.01416 Test: 1148.60925
Epoch: 240 Train: 2201.87256 Test: 916.76367
Epoch 320: New minimal relative error: 99.85%, model saved.
Epoch: 320 Train: 1723.29016 Test: 696.91168
Epoch 400: New minimal relative error: 71.05%, model saved.
Epoch: 400 Train: 855.20349 Test: 130.20895
Epoch: 480 Train: 437.07129 Test: 86.96657
Epoch 560: New minimal relative error: 20.13%, model saved.
Epoch: 560 Train: 217.05225 Test: 30.59944
Epoch: 640 Train: 188.97050 Test: 25.55196
Epoch: 720 Train: 134.11336 Test: 12.02968
Epoch: 800 Train: 130.52060 Test: 42.27700
Epoch: 880 Train: 100.45980 Test: 9.70902
Epoch: 960 Train: 76.31953 Test: 11.18652
Epoch 1040: New minimal relative error: 15.77%, model saved.
Epoch: 1040 Train: 88.41062 Test: 15.27824
Epoch 1120: New minimal relative error: 4.98%, model saved.
Epoch: 1120 Train: 65.86993 Test: 4.60139
Epoch: 1200 Train: 130.15422 Test: 81.52844
Epoch: 1280 Train: 59.04409 Test: 11.24687
Epoch: 1360 Train: 50.30492 Test: 8.79592
Epoch: 1440 Train: 78.24081 Test: 13.34601
Epoch: 1520 Train: 42.52879 Test: 2.10564
Epoch: 1600 Train: 47.64177 Test: 4.24374
Epoch: 1680 Train: 39.43194 Test: 5.36347
Epoch: 1760 Train: 35.46858 Test: 2.23774
Epoch: 1840 Train: 34.30151 Test: 3.37200
Epoch: 1920 Train: 34.88492 Test: 1.92779
Epoch: 2000 Train: 31.70868 Test: 2.16467
Epoch: 2080 Train: 38.40607 Test: 7.22583
Epoch: 2160 Train: 30.42373 Test: 2.82439
Epoch: 2240 Train: 28.76426 Test: 1.77300
Epoch: 2320 Train: 31.01882 Test: 3.23861
Epoch: 2400 Train: 27.08911 Test: 2.67116
Epoch: 2480 Train: 28.61500 Test: 2.07272
Epoch 2560: New minimal relative error: 2.69%, model saved.
Epoch: 2560 Train: 25.73224 Test: 1.20588
Epoch: 2640 Train: 25.13251 Test: 2.31174
Epoch: 2720 Train: 25.00921 Test: 1.59119
Epoch: 2800 Train: 23.72481 Test: 0.87010
Epoch: 2880 Train: 27.37270 Test: 1.47367
Epoch: 2960 Train: 24.85089 Test: 1.09141
Epoch: 3040 Train: 27.71625 Test: 1.38156
Epoch 3120: New minimal relative error: 1.86%, model saved.
Epoch: 3120 Train: 23.74032 Test: 0.97996
Epoch: 3200 Train: 31.89829 Test: 6.96874
Epoch: 3280 Train: 23.29069 Test: 3.20596
Epoch: 3360 Train: 23.39323 Test: 3.18013
Epoch: 3440 Train: 22.65060 Test: 0.89317
Epoch: 3520 Train: 33.16457 Test: 18.97965
Epoch: 3600 Train: 20.79557 Test: 0.69849
Epoch: 3680 Train: 21.14364 Test: 1.47834
Epoch: 3760 Train: 19.73764 Test: 1.43961
Epoch: 3840 Train: 18.79989 Test: 0.88661
Epoch: 3920 Train: 18.43004 Test: 0.70564
Epoch: 4000 Train: 19.99118 Test: 1.14812
Epoch: 4080 Train: 20.31055 Test: 1.40595
Epoch: 4160 Train: 20.37662 Test: 1.96431
Epoch 4240: New minimal relative error: 1.23%, model saved.
Epoch: 4240 Train: 17.02925 Test: 0.51574
Epoch: 4320 Train: 16.52705 Test: 0.78605
Epoch: 4400 Train: 16.54040 Test: 0.47712
Epoch: 4480 Train: 15.99882 Test: 0.46106
Epoch: 4560 Train: 16.75286 Test: 0.99735
Epoch: 4640 Train: 15.29309 Test: 0.47143
Epoch: 4720 Train: 14.96785 Test: 0.39646
Epoch: 4800 Train: 14.99863 Test: 0.42187
Epoch: 4880 Train: 15.36341 Test: 0.91452
Epoch: 4960 Train: 16.07349 Test: 0.99376
Epoch: 5040 Train: 19.82500 Test: 6.94109
Epoch: 5120 Train: 14.31253 Test: 0.44030
Epoch: 5200 Train: 14.03341 Test: 0.35708
Epoch: 5280 Train: 15.28173 Test: 2.28608
Epoch: 5360 Train: 13.82297 Test: 0.40664
Epoch: 5440 Train: 15.52581 Test: 1.59230
Epoch: 5520 Train: 13.30103 Test: 0.46667
Epoch: 5600 Train: 12.70435 Test: 0.49627
Epoch: 5680 Train: 12.40006 Test: 0.31241
Epoch: 5760 Train: 11.99937 Test: 0.34724
Epoch: 5840 Train: 11.78022 Test: 0.55143
Epoch: 5920 Train: 11.83151 Test: 0.31605
Epoch: 6000 Train: 11.96083 Test: 0.42167
Epoch: 6080 Train: 11.89620 Test: 0.45963
Epoch: 6160 Train: 11.84162 Test: 0.54950
Epoch: 6240 Train: 11.68093 Test: 0.49376
Epoch: 6320 Train: 11.07352 Test: 0.28382
Epoch: 6400 Train: 12.48827 Test: 0.75967
Epoch: 6480 Train: 10.78727 Test: 0.26080
Epoch 6560: New minimal relative error: 1.14%, model saved.
Epoch: 6560 Train: 10.60503 Test: 0.30425
Epoch: 6640 Train: 10.48089 Test: 0.46376
Epoch: 6720 Train: 10.48970 Test: 0.28377
Epoch 6800: New minimal relative error: 1.02%, model saved.
Epoch: 6800 Train: 10.24070 Test: 0.25056
Epoch: 6880 Train: 13.21272 Test: 3.06365
Epoch: 6960 Train: 10.12474 Test: 0.26788
Epoch: 7040 Train: 9.84935 Test: 0.23409
Epoch: 7120 Train: 10.27554 Test: 0.38001
Epoch: 7200 Train: 9.68646 Test: 0.30781
Epoch: 7280 Train: 9.61936 Test: 0.41742
Epoch: 7360 Train: 9.38526 Test: 0.22271
Epoch: 7440 Train: 9.34904 Test: 0.43908
Epoch: 7520 Train: 9.85040 Test: 0.82189
Epoch: 7600 Train: 9.30741 Test: 0.28674
Epoch: 7680 Train: 9.26628 Test: 0.22549
Epoch: 7760 Train: 9.32081 Test: 0.24124
Epoch: 7840 Train: 9.17456 Test: 0.25450
Epoch: 7920 Train: 9.28138 Test: 0.23464
Epoch: 7999 Train: 9.12633 Test: 0.27112
Training Loss: tensor(9.1263)
Test Loss: tensor(0.2711)
Learned LE: [ 8.5184854e-01  1.1899064e-02 -1.4497493e+01]
True LE: [ 8.6566323e-01 -2.7890876e-03 -1.4537115e+01]
Relative Error: [0.8400336  0.8088864  0.7687416  0.78075004 0.8240774  0.85515493
 0.84068745 0.7454292  0.6260896  0.32655793 0.2670733  0.4033746
 0.47625965 0.36078626 0.32982558 0.21669227 0.18066101 0.3519876
 0.7602627  1.1860192  1.2200778  1.0180126  0.80156636 0.58761835
 0.52692986 0.5171544  0.68830776 0.9041445  0.8115636  0.8413507
 1.0581162  1.1460356  1.0338618  0.8274284  0.6557132  0.7107194
 0.90767306 1.134097   1.3415087  1.3316828  1.1730053  1.1358523
 1.0327904  0.67600894 0.41371667 0.35846597 0.60528636 0.76908803
 0.98184484 0.99651617 1.2014861  1.6629872  1.5959531  1.3313105
 1.01186    0.8535701  0.70970005 0.47540075 0.48232442 0.5920217
 0.657824   0.74761724 0.7362741  0.67619425 0.6344947  0.67780745
 0.8252477  0.81620824 0.8239152  0.8134135  0.6995877  0.42966738
 0.15828617 0.12338405 0.28733155 0.22798595 0.177496   0.17965357
 0.06238538 0.21504624 0.6079297  1.0646498  1.1521665  1.0109776
 0.8482421  0.6902206  0.48710158 0.43471208 0.6012439  0.7809407
 0.72543633 0.7716522  0.9358654  1.0670274  0.90689045 0.7369653
 0.6206215  0.63783526 0.9389208  1.2240647  1.2644444  1.2496312
 1.1146159  1.1027236  1.0594131  0.71250826 0.40177387 0.33452213
 0.5313706  0.67776245 0.9081554  0.8331486  0.9743157  1.3910285
 1.5933605  1.2752175  0.8948444  0.67629427 0.6463014  0.43028462
 0.42512634 0.4966898  0.5218642  0.6026048  0.62773883 0.5863364
 0.59058565 0.66244453 0.79039496 0.72961605 0.7299568  0.8598416
 0.78589886 0.6184862  0.43269554 0.0187863  0.23963882 0.3174523
 0.22359823 0.25909993 0.15755582 0.15920548 0.3435378  0.8482516
 1.0132064  0.94847345 0.8388912  0.7713289  0.54583806 0.42087817
 0.4505073  0.6374305  0.6219083  0.7104451  0.90550333 1.0544254
 0.7963715  0.62102103 0.4858557  0.41546836 0.7690944  1.0718639
 1.2259709  1.2569394  1.134705   1.1109529  1.1334703  0.77754587
 0.42574197 0.32603672 0.49539953 0.5665264  0.8184291  0.7159964
 0.8005018  1.1601596  1.3200235  1.2380574  0.90035087 0.6553585
 0.70003724 0.4249249  0.3627174  0.37950796 0.3593193  0.41872734
 0.4713983  0.537382   0.58243304 0.66087556 0.7292096  0.6293988
 0.59447354 0.8185775  0.8823198  0.7905097  0.51543397 0.27295825
 0.11739942 0.29014897 0.2550357  0.24496138 0.43534064 0.24782214
 0.27717486 0.64278054 0.7858121  0.80889165 0.74392015 0.7272838
 0.6181158  0.4434648  0.4522224  0.47676563 0.5457547  0.65489733
 0.8712898  1.0635148  0.8487125  0.3983595  0.27987164 0.16086255
 0.42095754 0.6951794  0.91098464 1.1380574  1.2671245  1.1929253
 1.2017751  0.9353189  0.5881983  0.39610028 0.56737626 0.5560154
 0.77774876 0.6404952  0.6128729  0.94725853 1.073277   1.1908094
 1.0283183  0.7016581  0.64117026 0.5055303  0.3486902  0.26629505
 0.21897365 0.26843414 0.3640593  0.48410025 0.5680751  0.64223564
 0.68183404 0.6054975  0.48505795 0.7290454  0.81606853 0.701514
 0.5745137  0.49833    0.34071136 0.18539642 0.27915502 0.23437117
 0.30559292 0.42851478 0.33993432 0.47224528 0.64307106 0.5834681
 0.56900734 0.61829376 0.6346366  0.60566163 0.4333978  0.32052436
 0.5419163  0.6135865  0.8047609  0.97223353 0.79736936 0.30448064
 0.22336237 0.13171469 0.26908576 0.37877983 0.64789647 0.88134456
 1.1200429  1.2482884  1.2697142  0.99951345 0.64110833 0.33586586
 0.38725883 0.5674675  0.8174335  0.69968903 0.52502644 0.6898081
 0.9235248  0.9704453  1.1439298  0.8129276  0.66699713 0.639258
 0.39573005 0.21553266 0.13981369 0.18071158 0.34586793 0.43632272
 0.49664104 0.5361501  0.5976772  0.58359194 0.47890592 0.66730875
 0.6042526  0.6732262  0.663287   0.6110065  0.5623301  0.3621503
 0.2746943  0.2367991  0.2639097  0.4308697  0.5240677  0.52955127
 0.61856145 0.5079186  0.38320965 0.40354705 0.47457087 0.6596717
 0.45203316 0.18455197 0.46111575 0.616231   0.7348826  0.7821927
 0.7220286  0.27199572 0.2202994  0.23388508 0.22498292 0.34980312
 0.4281433  0.74185276 0.9129862  1.0889415  1.067352   1.0985845
 0.86176693 0.52392304 0.37935594 0.4792817  0.6685956  0.92312044
 0.60364765 0.5766288  0.81985754 0.8039483  0.9785377  1.0715678
 0.7605886  0.6796454  0.48609987 0.27390105 0.07596845 0.1801212
 0.2663261  0.36193475 0.49389842 0.38669926 0.4117411  0.41822717
 0.45088497 0.60754895 0.48801363 0.5179252  0.6652301  0.6969464
 0.7087746  0.7087947  0.39397338 0.2821522  0.34783754 0.56037444
 0.6292771  0.6665851  0.79274774 0.67750454 0.51673603 0.32904255
 0.24667948 0.47448003 0.5278201  0.2251269 ]
