time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.04%, model saved.
Epoch: 0 Train: 59500.10938 Test: 3857.05322
Epoch: 100 Train: 15031.14746 Test: 1748.23779
Epoch: 200 Train: 12536.55859 Test: 1360.22986
Epoch: 300 Train: 13873.13672 Test: 1695.52917
Epoch: 400 Train: 11480.47363 Test: 1542.74683
Epoch: 500 Train: 11084.13379 Test: 1044.03149
Epoch: 600 Train: 17898.60547 Test: 1787.41748
Epoch 700: New minimal relative error: 90.83%, model saved.
Epoch: 700 Train: 12168.26855 Test: 1209.25647
Epoch: 800 Train: 11316.22754 Test: 863.69476
Epoch 900: New minimal relative error: 57.67%, model saved.
Epoch: 900 Train: 9211.41406 Test: 767.85083
Epoch: 1000 Train: 8260.71777 Test: 558.65594
Epoch: 1100 Train: 6537.66748 Test: 356.19495
Epoch: 1200 Train: 5739.20020 Test: 270.15115
Epoch 1300: New minimal relative error: 40.39%, model saved.
Epoch: 1300 Train: 4596.26465 Test: 202.15958
Epoch: 1400 Train: 2585.94507 Test: 98.09389
Epoch 1500: New minimal relative error: 39.39%, model saved.
Epoch: 1500 Train: 1785.05029 Test: 54.64219
Epoch: 1600 Train: 3270.26831 Test: 133.20557
Epoch 1700: New minimal relative error: 5.98%, model saved.
Epoch: 1700 Train: 757.46844 Test: 12.56949
Epoch: 1800 Train: 527.65002 Test: 15.56431
Epoch: 1900 Train: 443.27176 Test: 9.38049
Epoch: 2000 Train: 327.43704 Test: 3.86106
Epoch: 2100 Train: 378.18057 Test: 27.43999
Epoch: 2200 Train: 278.01050 Test: 4.21968
Epoch: 2300 Train: 272.08005 Test: 4.84767
Epoch: 2400 Train: 247.79211 Test: 3.37988
Epoch: 2500 Train: 222.65678 Test: 6.93119
Epoch: 2600 Train: 205.34378 Test: 8.33679
Epoch: 2700 Train: 220.30867 Test: 4.79772
Epoch: 2800 Train: 186.92096 Test: 2.42948
Epoch: 2900 Train: 190.63928 Test: 5.04154
Epoch 3000: New minimal relative error: 2.02%, model saved.
Epoch: 3000 Train: 159.78430 Test: 1.27313
Epoch: 3100 Train: 157.39278 Test: 2.25954
Epoch: 3200 Train: 171.77428 Test: 12.86810
Epoch: 3300 Train: 151.21899 Test: 4.07266
Epoch: 3400 Train: 124.56738 Test: 2.48381
Epoch: 3500 Train: 118.81754 Test: 0.94518
Epoch: 3600 Train: 138.16455 Test: 5.14550
Epoch: 3700 Train: 113.93829 Test: 6.11804
Epoch: 3800 Train: 118.54704 Test: 1.16770
Epoch: 3900 Train: 122.44487 Test: 2.92518
Epoch: 4000 Train: 119.91890 Test: 2.01643
Epoch: 4100 Train: 128.44426 Test: 4.17445
Epoch: 4200 Train: 113.18691 Test: 4.13703
Epoch: 4300 Train: 115.58859 Test: 3.05970
Epoch: 4400 Train: 91.38094 Test: 1.74887
Epoch: 4500 Train: 90.36531 Test: 2.03271
Epoch: 4600 Train: 95.93659 Test: 0.84539
Epoch: 4700 Train: 90.62032 Test: 2.77517
Epoch: 4800 Train: 113.50391 Test: 5.02356
Epoch: 4900 Train: 86.71723 Test: 1.90878
Epoch: 5000 Train: 85.67828 Test: 1.47836
Epoch: 5100 Train: 89.08949 Test: 0.91674
Epoch: 5200 Train: 94.32224 Test: 1.48397
Epoch: 5300 Train: 96.17133 Test: 2.33187
Epoch: 5400 Train: 103.17533 Test: 3.53784
Epoch: 5500 Train: 96.38717 Test: 2.19846
Epoch: 5600 Train: 98.00887 Test: 1.74891
Epoch: 5700 Train: 96.57261 Test: 1.54512
Epoch: 5800 Train: 100.26551 Test: 2.39775
Epoch: 5900 Train: 110.43871 Test: 2.43083
Epoch: 6000 Train: 101.41106 Test: 2.34848
Epoch: 6100 Train: 94.29184 Test: 0.93996
Epoch: 6200 Train: 96.17949 Test: 2.92458
Epoch: 6300 Train: 102.37691 Test: 4.45148
Epoch: 6400 Train: 93.68779 Test: 2.54725
Epoch: 6500 Train: 82.70229 Test: 1.92063
Epoch: 6600 Train: 81.60153 Test: 0.55713
Epoch 6700: New minimal relative error: 1.33%, model saved.
Epoch: 6700 Train: 83.27979 Test: 0.56290
Epoch: 6800 Train: 79.49024 Test: 0.48666
Epoch: 6900 Train: 85.67845 Test: 0.60826
Epoch: 7000 Train: 86.51192 Test: 0.60120
Epoch: 7100 Train: 93.82883 Test: 1.03317
Epoch: 7200 Train: 93.15952 Test: 0.85029
Epoch: 7300 Train: 95.13194 Test: 0.74423
Epoch: 7400 Train: 93.60246 Test: 0.78715
Epoch: 7500 Train: 91.58613 Test: 0.74866
Epoch: 7600 Train: 89.15018 Test: 2.72174
Epoch: 7700 Train: 85.09789 Test: 0.71398
Epoch: 7800 Train: 80.41920 Test: 0.52937
Epoch: 7900 Train: 93.53925 Test: 1.68095
Epoch: 8000 Train: 96.72961 Test: 1.14828
Epoch: 8100 Train: 101.44316 Test: 2.40984
Epoch: 8200 Train: 95.07678 Test: 0.84558
Epoch: 8300 Train: 85.21680 Test: 0.66769
Epoch: 8400 Train: 96.62026 Test: 1.06752
Epoch: 8500 Train: 90.92770 Test: 1.59047
Epoch: 8600 Train: 82.92133 Test: 0.94128
Epoch: 8700 Train: 80.01398 Test: 0.94086
Epoch: 8800 Train: 82.17742 Test: 1.53624
Epoch: 8900 Train: 85.94324 Test: 0.78759
Epoch: 9000 Train: 87.85825 Test: 1.09591
Epoch: 9100 Train: 89.55632 Test: 1.08796
Epoch: 9200 Train: 90.77273 Test: 0.65636
Epoch: 9300 Train: 83.21387 Test: 1.01864
Epoch: 9400 Train: 79.44392 Test: 0.62147
Epoch: 9500 Train: 82.82258 Test: 1.13193
Epoch: 9600 Train: 82.31908 Test: 0.78745
Epoch: 9700 Train: 83.30001 Test: 0.88918
Epoch: 9800 Train: 84.97359 Test: 0.96107
Epoch: 9900 Train: 86.68630 Test: 0.77352
Epoch: 9999 Train: 82.58346 Test: 0.87034
Training Loss: tensor(82.5835)
Test Loss: tensor(0.8703)
Learned LE: [ 8.5803217e-01  4.9578743e-03 -1.4550959e+01]
True LE: [ 8.7830925e-01 -4.4778632e-03 -1.4551398e+01]
Relative Error: [0.6141331  0.703439   0.8295727  0.92166626 1.0787297  1.1780086
 1.1267877  0.96734035 0.82138515 0.7594414  0.71021616 0.72184384
 0.7855513  0.84114003 0.93422186 0.9281688  0.96533865 1.0569412
 0.95625407 0.83209777 0.54012555 0.38380837 0.3869228  0.5055679
 0.79105294 1.0214394  0.90423244 0.94656163 0.9239309  0.8881802
 0.94705725 0.8984841  0.71542686 0.6529127  0.6423089  0.6157066
 0.6084709  0.77988994 0.97287476 0.81181896 0.7402645  0.76675266
 0.6165265  0.54525715 0.45642135 0.44857746 0.41034025 0.2579424
 0.28758523 0.58530045 0.7066977  0.7909518  0.8002784  0.86041105
 1.090751   0.5819802  0.86904454 1.1196301  1.0541054  0.7625087
 0.5590957  0.53561974 0.61826783 0.72926885 0.77337766 0.7346374
 0.9348136  1.0549498  1.0314682  0.9177777  0.7628583  0.7400449
 0.7969721  0.7243681  0.733109   0.77367026 0.9911481  0.9643775
 0.9438416  1.0472561  1.0203445  0.86136776 0.6671926  0.34823626
 0.34542134 0.44648597 0.5896403  0.8271598  0.8266722  0.7621619
 0.8728601  0.927368   0.894423   1.0118433  0.70636344 0.61928153
 0.65522814 0.70283765 0.7829648  0.8990738  1.2267286  1.1968642
 0.99418646 0.9977448  0.94603074 0.7957698  0.73618096 0.41441828
 0.41650096 0.49304384 0.3755203  0.2944325  0.51799846 0.61155194
 0.6850925  0.76772374 0.764765   0.57418036 0.70764965 1.0670608
 1.2161379  1.0003594  0.6612091  0.5014473  0.55955607 0.7108814
 0.7749893  0.69262874 0.74350667 0.9834104  1.1319575  1.0961876
 0.9438997  0.8847882  0.9565315  1.0834421  1.1241482  1.0537238
 1.0891371  1.1007785  1.0921053  1.0938641  1.098179   0.9758969
 0.81435496 0.6015423  0.29484048 0.3407101  0.49643043 0.5901163
 0.79804593 0.6802754  0.6430395  0.7794674  0.8826905  1.0346837
 0.83139426 0.6488572  0.627865   0.68109614 0.8248077  1.0222224
 1.2639279  1.536995   1.549683   1.3494636  1.4302537  1.2826214
 1.0481013  0.75061876 0.27983767 0.38397178 0.6031313  0.4875901
 0.43456447 0.5214962  0.5955169  0.64721394 0.5865758  0.7043849
 0.36313283 0.7360098  1.0669639  1.0912746  0.9275014  0.59593695
 0.5117241  0.5440478  0.69923025 0.6791805  0.5981478  0.738197
 1.0358613  1.1659262  1.1483634  1.0161473  0.9765299  1.1100302
 1.3148011  1.4198252  1.375351   1.4133971  1.3999048  1.3844919
 1.3016381  1.257568   1.1591122  0.9358899  0.62435013 0.30556917
 0.18944965 0.45272434 0.47454584 0.6237751  0.46512234 0.44587114
 0.6962922  0.861025   0.885226   0.63237536 0.51278335 0.4918713
 0.61396605 0.85644794 1.1676799  1.5404407  1.6499048  1.6622791
 1.749406   1.9469593  1.7555445  1.3924499  0.85980994 0.14983086
 0.39291027 0.65410346 0.67525727 0.5928438  0.5956031  0.5939118
 0.6001466  0.56012326 0.52625036 0.32592705 0.71279335 0.9654288
 1.0392466  0.79890096 0.47407034 0.41719702 0.43149632 0.4638441
 0.4883247  0.53032476 0.7392896  1.034106   1.1191503  1.1188897
 1.0662192  1.1432599  1.2382525  1.4544758  1.4844991  1.4413205
 1.5712651  1.6562032  1.7354859  1.5574371  1.3974574  1.3898454
 1.1538054  0.811501   0.44442862 0.15198867 0.3738168  0.49062204
 0.5284057  0.39358753 0.24657767 0.4631774  0.6650315  0.70914304
 0.30868906 0.3475847  0.47045475 0.59786946 0.817714   0.9850842
 1.250698   1.3508506  1.4787095  1.595373   1.855581   2.0489016
 1.8523505  1.2136033  0.47108823 0.3035825  0.5652077  0.72003794
 0.7439791  0.5804396  0.60260904 0.6028796  0.5895213  0.4162691
 0.3617996  0.7124638  0.93968713 1.0042794  0.7616653  0.41439086
 0.30969408 0.34627834 0.39628017 0.44194996 0.4931243  0.6170679
 0.84873056 1.084883   1.1275419  1.1335691  1.1594425  1.1369504
 1.2944179  1.4177983  1.4907473  1.6652272  1.7771939  1.8813267
 1.8574148  1.595681   1.5359426  1.3574866  1.0226486  0.7471839
 0.43756142 0.22915189 0.46365714 0.5025662  0.5736476  0.42251632
 0.34500587 0.5912835  0.7177985  0.23113815 0.15811141 0.36635333
 0.64394253 0.6252618  0.8752485  1.019544   0.8078511  0.9396341
 1.003844   1.3870914  1.6181078  1.8083984  1.634007   1.068226
 0.24651116 0.3230221  0.6774782  0.80370146 0.7223132  0.65481526
 0.6155884  0.49280232 0.47228304 0.30231538 0.60097563 0.861955
 0.89948446 0.6544594  0.38445204 0.3059313  0.23135784 0.29430956
 0.3970521  0.41115814 0.5753553  0.847051   1.1641104  1.3752226
 1.3244841  1.2722237  1.227436   0.98583716 1.0028256  1.096603
 1.3242621  1.654425   1.8097907  1.8657552  1.7301812  1.5989217
 1.5175512  1.2936649  0.9019903  0.65279907]
