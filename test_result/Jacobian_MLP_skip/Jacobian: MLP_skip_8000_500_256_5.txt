time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.65%, model saved.
Epoch: 0 Train: 171910.17188 Test: 4010.80737
Epoch: 80 Train: 43908.08984 Test: 2171.74878
Epoch: 160 Train: 42452.71484 Test: 1962.50439
Epoch: 240 Train: 44233.46094 Test: 1660.32178
Epoch: 320 Train: 38475.39453 Test: 2021.68127
Epoch 400: New minimal relative error: 93.43%, model saved.
Epoch: 400 Train: 41570.14062 Test: 1644.62683
Epoch: 480 Train: 36280.24219 Test: 1471.49683
Epoch 560: New minimal relative error: 82.87%, model saved.
Epoch: 560 Train: 42565.66016 Test: 1797.52234
Epoch 640: New minimal relative error: 65.07%, model saved.
Epoch: 640 Train: 37285.28125 Test: 1510.54712
Epoch: 720 Train: 39964.02734 Test: 1621.97412
Epoch: 800 Train: 40973.95312 Test: 1720.85510
Epoch: 880 Train: 40648.71875 Test: 1514.81311
Epoch: 960 Train: 41450.73047 Test: 1486.18921
Epoch: 1040 Train: 41627.52734 Test: 1631.71509
Epoch: 1120 Train: 41591.00391 Test: 1636.45996
Epoch: 1200 Train: 40575.03906 Test: 1620.38416
Epoch: 1280 Train: 38327.63281 Test: 1515.38452
Epoch: 1360 Train: 40659.41016 Test: 1363.59717
Epoch: 1440 Train: 41416.42578 Test: 1659.24597
Epoch: 1520 Train: 40060.92969 Test: 1525.38818
Epoch: 1600 Train: 41261.59375 Test: 1591.31641
Epoch: 1680 Train: 38728.17188 Test: 1530.64136
Epoch: 1760 Train: 40914.30469 Test: 1571.68652
Epoch: 1840 Train: 38613.14844 Test: 1591.68750
Epoch: 1920 Train: 39561.89844 Test: 1587.81628
Epoch: 2000 Train: 40074.90234 Test: 1546.36145
Epoch: 2080 Train: 37583.87109 Test: 1432.71570
Epoch: 2160 Train: 39663.92578 Test: 1543.75244
Epoch: 2240 Train: 37477.36719 Test: 1513.39160
Epoch: 2320 Train: 38682.52344 Test: 1513.98364
Epoch: 2400 Train: 37822.38281 Test: 1420.03223
Epoch: 2480 Train: 38874.30469 Test: 1477.68481
Epoch: 2560 Train: 36785.07812 Test: 1328.84412
Epoch: 2640 Train: 38792.06641 Test: 1467.70862
Epoch: 2720 Train: 38788.67188 Test: 1950.80188
Epoch: 2800 Train: 36087.94141 Test: 1370.65271
Epoch: 2880 Train: 38224.55078 Test: 1476.05884
Epoch 2960: New minimal relative error: 61.93%, model saved.
Epoch: 2960 Train: 35787.06641 Test: 1419.16748
Epoch: 3040 Train: 35985.40625 Test: 1382.22217
Epoch: 3120 Train: 38166.66406 Test: 1411.00525
Epoch: 3200 Train: 34636.85938 Test: 1325.05078
Epoch: 3280 Train: 34494.66406 Test: 1290.25439
Epoch: 3360 Train: 35940.95703 Test: 1329.98645
Epoch: 3440 Train: 34599.08984 Test: 1280.33069
Epoch: 3520 Train: 33857.74609 Test: 1211.76855
Epoch: 3600 Train: 34494.98438 Test: 1260.90417
Epoch: 3680 Train: 35403.63672 Test: 1312.95508
Epoch: 3760 Train: 33555.36328 Test: 1255.79358
Epoch: 3840 Train: 32582.67773 Test: 1211.82263
Epoch: 3920 Train: 34409.04297 Test: 1210.59863
Epoch: 4000 Train: 34052.54297 Test: 1176.87280
Epoch: 4080 Train: 34134.60547 Test: 1199.85388
Epoch: 4160 Train: 30604.17578 Test: 1055.97351
Epoch: 4240 Train: 31183.79102 Test: 1071.43628
Epoch: 4320 Train: 32447.28516 Test: 1108.39160
Epoch: 4400 Train: 32533.88477 Test: 1144.42554
Epoch: 4480 Train: 30638.91797 Test: 1043.27625
Epoch: 4560 Train: 28528.05078 Test: 892.45917
Epoch: 4640 Train: 28279.07031 Test: 866.10284
Epoch: 4720 Train: 28646.18359 Test: 926.17670
Epoch: 4800 Train: 29230.92578 Test: 931.78406
Epoch: 4880 Train: 28673.46484 Test: 946.27405
Epoch: 4960 Train: 27280.16016 Test: 826.81610
Epoch: 5040 Train: 27472.18945 Test: 864.68579
Epoch: 5120 Train: 29088.15625 Test: 803.97992
Epoch: 5200 Train: 28363.74805 Test: 803.58618
Epoch: 5280 Train: 26703.04883 Test: 778.52625
Epoch: 5360 Train: 27054.64844 Test: 748.06647
Epoch: 5440 Train: 27805.23828 Test: 802.52832
Epoch: 5520 Train: 27222.29492 Test: 780.42822
Epoch: 5600 Train: 26700.22070 Test: 756.81329
Epoch: 5680 Train: 24772.25195 Test: 702.22174
Epoch: 5760 Train: 25271.88086 Test: 659.39331
Epoch: 5840 Train: 26618.83203 Test: 709.73535
Epoch: 5920 Train: 24922.07617 Test: 637.64496
Epoch: 6000 Train: 25253.48828 Test: 620.00842
Epoch: 6080 Train: 24604.89258 Test: 582.06281
Epoch: 6160 Train: 23362.28320 Test: 560.38214
Epoch: 6240 Train: 23421.30664 Test: 531.91370
Epoch: 6320 Train: 21767.78516 Test: 493.62738
Epoch: 6400 Train: 22800.00391 Test: 535.13171
Epoch: 6480 Train: 23340.52539 Test: 559.32721
Epoch: 6560 Train: 23448.07812 Test: 550.70343
Epoch: 6640 Train: 22548.67773 Test: 518.67584
Epoch 6720: New minimal relative error: 54.72%, model saved.
Epoch: 6720 Train: 22039.54297 Test: 482.78815
Epoch: 6800 Train: 20521.66602 Test: 415.60181
Epoch: 6880 Train: 19512.37109 Test: 369.86343
Epoch: 6960 Train: 17713.38477 Test: 317.89594
Epoch: 7040 Train: 18144.06641 Test: 319.56766
Epoch: 7120 Train: 16413.29297 Test: 279.12964
Epoch: 7200 Train: 14317.41309 Test: 234.29970
Epoch: 7280 Train: 12783.08984 Test: 208.09154
Epoch: 7360 Train: 10338.70312 Test: 198.34613
Epoch: 7440 Train: 9144.22949 Test: 168.95149
Epoch 7520: New minimal relative error: 50.73%, model saved.
Epoch: 7520 Train: 7810.43945 Test: 143.56798
Epoch: 7600 Train: 6804.03027 Test: 108.55625
Epoch 7680: New minimal relative error: 40.40%, model saved.
Epoch: 7680 Train: 5900.46436 Test: 72.49337
Epoch 7760: New minimal relative error: 34.84%, model saved.
Epoch: 7760 Train: 5249.97266 Test: 52.43508
Epoch 7840: New minimal relative error: 28.50%, model saved.
Epoch: 7840 Train: 4645.85254 Test: 42.64037
Epoch: 7920 Train: 4275.63525 Test: 37.89432
Epoch: 7999 Train: 3629.53418 Test: 26.40882
Training Loss: tensor(3629.5342)
Test Loss: tensor(26.4088)
Learned LE: [  1.7071102  -0.8188702 -14.454417 ]
True LE: [ 8.75296712e-01  8.37123953e-03 -1.45607605e+01]
Relative Error: [27.159983  28.425661  29.92129   31.62975   32.963375  34.520687
 36.196587  36.822617  36.546047  36.45747   36.570774  36.402702
 36.209362  36.05663   35.898163  35.85112   36.02992   35.93436
 33.746964  30.435291  26.623001  20.606224  15.264752  10.6468935
  6.9350233  4.7675977  5.1211414  7.1231422  9.401332  11.540739
 13.460311  16.53741   21.240847  24.986525  27.904373  30.084442
 30.68015   30.748888  30.152802  28.84422   26.79784   24.019455
 20.748709  20.409246  20.108482  19.984587  18.919199  18.024418
 17.865158  17.591103  17.082342  16.58052   16.01321   15.899627
 16.448408  18.22718   20.117798  21.261147  22.731033  24.440245
 24.650547  24.120504  24.867977  26.00716   27.43357   28.649675
 29.939121  31.402874  32.999466  34.292896  33.61291   33.239967
 33.33644   33.179333  32.94439   32.74148   32.619247  32.853535
 33.33421   33.35474   33.021236  30.009806  26.362503  20.88885
 15.378388  10.586833   6.694175   4.3834734  4.8262496  6.995241
  9.373843  11.546801  13.469221  16.233957  20.989174  25.300554
 28.349308  30.636465  31.394154  31.630531  31.25737   30.185457
 28.412306  25.860294  22.770735  22.451982  22.206064  21.601377
 20.104578  19.022717  18.53841   18.288576  17.73099   16.78698
 15.950388  15.544128  15.892341  17.186049  19.266386  20.482027
 21.868223  23.261364  23.385765  22.565529  22.872355  23.840397
 25.083572  25.961395  27.089817  28.455425  29.925465  30.928091
 30.792973  30.31958   30.391418  30.273125  29.989004  29.72092
 29.518785  29.67338   30.065256  30.678017  30.564219  30.035097
 26.36215   21.429382  16.073006  11.029525   6.846954   4.1507773
  4.371976   6.611823   9.08246   11.344668  13.286122  15.395479
 20.226192  24.840385  28.326664  30.786547  31.843477  32.276688
 32.14773   31.35821   29.88919   27.653206  24.803547  24.23911
 24.147198  22.83119   21.325798  20.09821   19.170374  18.853474
 18.082146  16.909647  15.808667  15.209929  15.300044  16.318272
 18.315723  19.548588  21.059948  22.399122  22.12166   21.261099
 21.08617   21.910835  22.868643  23.558144  24.50891   25.71123
 26.637268  27.55703   28.376495  27.705326  27.75875   27.70444
 27.371143  27.020418  26.6993    26.70506   26.969517  27.448729
 27.732927  28.062222  26.91387   22.589025  17.134462  12.062834
  7.464556   4.1560516  3.7661638  5.9601808  8.504905  10.838122
 12.869172  14.580424  18.838701  23.546877  27.810213  30.571001
 31.959831  32.64709   32.77389   32.30555   31.185207  29.353056
 26.821148  25.694931  25.57018   24.233553  22.577595  21.233862
 19.93414   19.270342  18.389835  17.023424  15.86704   15.03585
 14.78443   15.422034  17.08352   19.118254  19.976007  21.436663
 21.128496  20.2936    19.562813  20.229206  20.911343  21.51758
 22.266287  23.096285  23.660246  24.449759  25.35555   25.594204
 25.455973  25.368713  24.971256  24.237652  23.662693  23.465416
 23.708006  24.228025  25.00835   25.089308  25.523476  24.188583
 18.785915  13.608604   8.891908   4.816681   3.1044939  4.9411287
  7.644191  10.085752  12.1595745 13.944345  16.765013  21.544502
 26.07964   29.783218  32.023052  32.636913  33.09328   32.94829
 32.230293  30.867285  28.761772  26.70608   26.774874  25.688854
 23.879519  22.284044  20.766     19.690533  18.600754  17.415552
 16.186642  15.15798   14.627073  14.828056  15.979746  18.143736
 19.715136  20.73441   20.268328  19.412136  18.715689  18.798244
 19.374058  19.555199  20.218243  20.740547  21.03011   21.580196
 22.389572  23.333715  23.418278  23.040424  22.409956  21.61976
 20.911268  20.404388  20.427263  20.70223   21.311361  22.000578
 22.52392   22.875668  21.641603  15.914769  10.965982   6.820623
  3.5266511  3.6184137  6.176553   8.881289  11.14286   13.008113
 14.530258  18.722239  23.400719  27.76564   30.99812   32.437706
 32.957253  33.24035   32.912983  32.032528  30.49204   28.272686
 27.698235  26.968632  25.26137   23.286951  21.69854   20.3522
 18.873716  17.950504  16.88854   15.792491  14.87768   14.490454
 14.864331  16.306602  18.854729  20.120628  19.774052  18.698723
 18.04895   17.37905   17.364357  17.498053  17.640097  17.885199
 18.434753  19.132984  19.597948  20.29448   21.034021  21.007694
 20.369785  19.697712  18.851791  18.130358  17.75234   17.72479
 17.989767  18.623669  19.140917  19.650427  20.241936  19.357498
 14.385406   9.88977    6.1453533  3.3194702]
