time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.89%, model saved.
Epoch: 0 Train: 31727.33789 Test: 3553.10303
Epoch 80: New minimal relative error: 89.76%, model saved.
Epoch: 80 Train: 8553.47363 Test: 1240.12610
Epoch: 160 Train: 7960.89209 Test: 1058.05945
Epoch 240: New minimal relative error: 77.36%, model saved.
Epoch: 240 Train: 8032.69678 Test: 1035.17322
Epoch: 320 Train: 7693.09961 Test: 911.52875
Epoch: 400 Train: 5789.26416 Test: 750.55920
Epoch: 480 Train: 4112.56787 Test: 442.86166
Epoch: 560 Train: 2643.74268 Test: 239.89400
Epoch: 640 Train: 1375.09790 Test: 93.39178
Epoch 720: New minimal relative error: 61.75%, model saved.
Epoch: 720 Train: 880.78137 Test: 72.90833
Epoch 800: New minimal relative error: 45.11%, model saved.
Epoch: 800 Train: 543.75482 Test: 33.44724
Epoch: 880 Train: 560.79254 Test: 129.54538
Epoch 960: New minimal relative error: 7.14%, model saved.
Epoch: 960 Train: 304.10071 Test: 9.00632
Epoch: 1040 Train: 263.35126 Test: 17.28608
Epoch: 1120 Train: 248.45956 Test: 21.12523
Epoch: 1200 Train: 194.05022 Test: 4.47114
Epoch 1280: New minimal relative error: 5.45%, model saved.
Epoch: 1280 Train: 173.51930 Test: 3.77333
Epoch: 1360 Train: 162.00906 Test: 7.84578
Epoch: 1440 Train: 174.54282 Test: 13.08927
Epoch: 1520 Train: 142.09196 Test: 3.37017
Epoch: 1600 Train: 137.11061 Test: 2.77865
Epoch: 1680 Train: 130.17609 Test: 3.75987
Epoch: 1760 Train: 120.66206 Test: 4.41599
Epoch: 1840 Train: 122.91996 Test: 12.87779
Epoch: 1920 Train: 122.40938 Test: 9.44611
Epoch: 2000 Train: 109.97165 Test: 10.09333
Epoch: 2080 Train: 98.61668 Test: 1.74692
Epoch 2160: New minimal relative error: 3.33%, model saved.
Epoch: 2160 Train: 105.85247 Test: 2.05509
Epoch: 2240 Train: 95.19989 Test: 1.67830
Epoch: 2320 Train: 95.16815 Test: 1.72763
Epoch: 2400 Train: 93.12687 Test: 3.85709
Epoch: 2480 Train: 87.97575 Test: 1.74852
Epoch: 2560 Train: 82.06329 Test: 2.25209
Epoch: 2640 Train: 86.44313 Test: 1.56757
Epoch: 2720 Train: 78.62998 Test: 1.21980
Epoch: 2800 Train: 77.99712 Test: 2.21100
Epoch: 2880 Train: 77.60291 Test: 3.26803
Epoch: 2960 Train: 73.81913 Test: 1.44607
Epoch: 3040 Train: 68.84739 Test: 1.19852
Epoch 3120: New minimal relative error: 2.72%, model saved.
Epoch: 3120 Train: 68.11433 Test: 1.14764
Epoch: 3200 Train: 69.06615 Test: 2.13432
Epoch: 3280 Train: 65.20157 Test: 1.84288
Epoch: 3360 Train: 65.21600 Test: 1.23642
Epoch: 3440 Train: 63.03106 Test: 1.03462
Epoch: 3520 Train: 61.32069 Test: 0.90176
Epoch: 3600 Train: 61.40001 Test: 1.73553
Epoch: 3680 Train: 60.08899 Test: 0.81270
Epoch: 3760 Train: 60.39330 Test: 0.92630
Epoch: 3840 Train: 66.61710 Test: 7.46046
Epoch: 3920 Train: 56.40732 Test: 0.70325
Epoch: 4000 Train: 55.88733 Test: 0.69623
Epoch: 4080 Train: 55.31985 Test: 1.02943
Epoch: 4160 Train: 55.14566 Test: 1.06690
Epoch 4240: New minimal relative error: 2.48%, model saved.
Epoch: 4240 Train: 54.20145 Test: 0.67424
Epoch: 4320 Train: 55.90119 Test: 0.95277
Epoch: 4400 Train: 55.25274 Test: 0.82323
Epoch: 4480 Train: 54.03956 Test: 0.75213
Epoch: 4560 Train: 53.37180 Test: 0.73351
Epoch 4640: New minimal relative error: 1.97%, model saved.
Epoch: 4640 Train: 54.83041 Test: 0.77916
Epoch: 4720 Train: 53.53051 Test: 1.21967
Epoch: 4800 Train: 55.94884 Test: 2.00305
Epoch: 4880 Train: 52.85819 Test: 0.72120
Epoch: 4960 Train: 52.97943 Test: 1.31430
Epoch: 5040 Train: 50.39367 Test: 0.68097
Epoch: 5120 Train: 49.01052 Test: 0.70430
Epoch: 5200 Train: 48.86101 Test: 0.67462
Epoch: 5280 Train: 50.60729 Test: 1.05842
Epoch: 5360 Train: 49.66355 Test: 0.70163
Epoch: 5440 Train: 49.02358 Test: 0.69099
Epoch: 5520 Train: 49.67402 Test: 0.74377
Epoch: 5600 Train: 49.12125 Test: 0.62090
Epoch: 5680 Train: 50.15488 Test: 0.67002
Epoch: 5760 Train: 50.60718 Test: 0.69956
Epoch: 5840 Train: 51.88423 Test: 0.93757
Epoch: 5920 Train: 52.08449 Test: 0.75464
Epoch: 6000 Train: 56.73759 Test: 4.11584
Epoch: 6080 Train: 51.02545 Test: 0.71383
Epoch: 6160 Train: 50.52143 Test: 1.88142
Epoch 6240: New minimal relative error: 1.63%, model saved.
Epoch: 6240 Train: 47.46659 Test: 0.58366
Epoch: 6320 Train: 46.78532 Test: 0.56243
Epoch: 6400 Train: 47.00875 Test: 0.59661
Epoch: 6480 Train: 50.61935 Test: 0.70189
Epoch: 6560 Train: 51.77258 Test: 1.33200
Epoch: 6640 Train: 52.25827 Test: 0.72772
Epoch: 6720 Train: 50.65207 Test: 0.66797
Epoch: 6800 Train: 49.15143 Test: 0.61689
Epoch: 6880 Train: 49.23296 Test: 0.68590
Epoch: 6960 Train: 48.72003 Test: 0.62997
Epoch: 7040 Train: 48.28439 Test: 0.62578
Epoch: 7120 Train: 48.04999 Test: 0.60732
Epoch: 7200 Train: 47.38321 Test: 0.57242
Epoch: 7280 Train: 47.68029 Test: 0.59972
Epoch: 7360 Train: 48.36528 Test: 0.64304
Epoch: 7440 Train: 46.29995 Test: 0.58509
Epoch: 7520 Train: 46.03716 Test: 1.67729
Epoch: 7600 Train: 44.63303 Test: 0.51317
Epoch: 7680 Train: 45.93866 Test: 0.59455
Epoch: 7760 Train: 43.08346 Test: 0.56089
Epoch: 7840 Train: 41.20612 Test: 0.54442
Epoch: 7920 Train: 41.53603 Test: 0.48529
Epoch: 7999 Train: 41.59094 Test: 0.58961
Training Loss: tensor(41.5909)
Test Loss: tensor(0.5896)
Learned LE: [  0.8999073   -0.02792918 -14.5419445 ]
True LE: [ 8.7361157e-01 -1.4430428e-03 -1.4543825e+01]
Relative Error: [0.86006343 1.1139783  1.242283   1.3119662  1.3280742  1.312746
 1.1187214  0.86278105 0.59677607 0.5456701  0.43101668 0.5058835
 0.3018264  0.40399814 0.31573778 0.2855175  0.4853877  0.59264624
 0.6286211  1.3932154  1.8030322  1.8957376  1.7805666  1.3264699
 0.4327211  1.0589155  2.1591172  2.0779595  2.4656     2.1576755
 1.8161138  1.635359   1.6578324  1.7926695  1.861458   1.6043627
 1.2231055  1.5768628  1.7930657  1.6830726  1.362166   0.9734112
 0.664324   0.38648826 0.6211609  0.6728138  0.47719026 0.20342593
 0.46633923 0.6715721  0.6611595  0.89870656 1.8218564  2.429853
 2.705262   2.656236   2.3228319  1.6499532  1.1951247  1.151725
 1.0313768  0.8418776  0.43994108 0.5461563  0.81258744 0.9278584
 1.0693545  1.2029017  1.1686119  1.063966   0.9208093  0.5087365
 0.23181324 0.45661804 0.89077103 0.7423295  0.59002167 0.4310368
 0.28765643 0.31833953 0.4857988  0.4358428  0.68807954 1.3520414
 1.7235925  1.8397678  1.6266581  0.9464534  0.6617678  1.76382
 1.7189181  2.116825   1.8199582  1.4085668  1.5884516  1.6654024
 1.7190238  1.8200576  1.7458389  1.231797   1.2005637  1.5256108
 1.5630454  1.3777838  1.0383735  0.956942   0.5706668  0.4556024
 0.7267328  0.7473718  0.7102191  0.53582764 0.6849255  0.6324291
 0.44744644 1.3486209  2.0464702  2.3416011  2.3079848  1.9353403
 1.2560289  1.1925559  1.2861181  1.0862399  0.74942935 0.43435377
 0.19886228 0.55162585 1.0936229  1.3783687  1.4349302  1.3860743
 1.2177787  0.9097339  0.85051006 0.57791436 0.395494   0.19365089
 0.6280346  0.72158647 0.7256952  0.53784555 0.3568333  0.10325308
 0.38718516 0.5950739  0.4218995  1.200517   1.6293648  1.7359087
 1.3732086  0.7418395  1.094086   1.3563486  1.5060278  1.8076274
 1.0698941  1.1000292  1.4577162  1.6238799  1.5366054  1.5547646
 1.3476778  0.7640044  0.84338945 1.1211994  1.1873797  1.0648934
 1.0833104  1.0477989  0.6047462  0.3936194  0.73914707 1.058105
 0.77698797 0.60386187 0.46925008 0.35178417 0.50590694 1.3783875
 1.8740304  1.9696338  1.6480151  0.99036944 0.72196835 0.8242253
 0.7303141  0.5952797  0.44066018 0.48832083 0.7967408  1.1105827
 1.4111245  1.5947627  1.52421    1.2432567  1.0383701  0.8349467
 0.83413094 0.74904084 0.6302809  0.58174527 0.6419393  0.6462384
 0.61515546 0.6188673  0.4879363  0.04550867 0.20928325 0.42735553
 0.29556778 0.63987976 1.2182996  1.4505366  1.1589932  0.57442373
 1.0574791  1.1406902  1.2627242  1.4774847  0.708075   0.70927304
 1.1520197  1.3545473  1.1447227  1.0681487  0.83957726 0.27798653
 0.08465734 0.57183176 0.859477   0.9913683  1.179812   1.2234219
 0.8557063  0.09238774 0.746646   1.1518486  0.9014754  0.39537153
 0.14412922 0.3782918  0.23469448 1.0736467  1.4920437  1.463122
 0.9732472  0.28326833 0.37486225 0.3002413  0.17154913 0.27561787
 0.45714933 0.93857795 1.2468681  1.2790865  1.2404544  1.2084029
 1.2149186  0.869577   0.5407973  0.50961626 0.7233511  0.83949
 0.467641   0.4362527  0.5652308  0.7454333  0.7231145  0.46513188
 0.3561785  0.510226   0.30289572 0.32698122 0.45332363 0.42607653
 0.5635161  1.1414325  1.1488382  0.8496427  0.56220037 1.2006619
 0.94610834 1.4452673  0.97554976 0.3604372  0.5046721  0.77721995
 0.79960775 0.46082017 0.28568393 0.4618716  0.17043732 0.37994337
 0.87360173 1.2437627  1.2876552  1.2548263  1.0017073  0.40370515
 0.25360873 0.95568234 1.0468941  0.4887201  0.05658503 0.35975522
 0.445138   0.40831393 1.0757397  1.2382988  0.9019231  0.15541914
 0.23001073 0.13794608 0.39768332 0.23965682 0.43946677 0.9010517
 1.0622786  0.95152485 0.892283   0.6696631  0.45050657 0.39812064
 0.31980863 0.07183669 0.36612678 0.38929635 0.22619466 0.32854876
 0.16455062 0.36971122 0.5733269  0.62320733 0.3186673  0.13989654
 0.23616359 0.5789436  0.47006476 0.44278434 0.4795365  0.19571672
 1.0294373  1.2710185  1.0166099  0.4069679  0.98267084 0.7909612
 1.2036046  0.972744   0.3012915  0.18260056 0.15602769 0.33161196
 0.36752218 0.5814004  0.852988   0.6639505  0.58092195 1.0862318
 1.2741106  1.4522572  1.4210336  1.0952445  0.59554315 0.17921214
 0.50682884 0.835999   0.642363   0.05118187 0.40134078 0.6700459
 0.3481453  0.8729026  0.93019336 0.4791143  0.37101477 0.16315903
 0.3044589  0.23197597 0.3833642  0.8063786  1.0690734  1.0881866
 0.86652887 0.37750113 0.7312902  0.8525764  0.8536155  0.84270376
 0.8343064  0.71639585 0.6765341  0.49560866 0.36818168 0.4841667
 0.26206997 0.37834182 0.46097925 0.5817088 ]
