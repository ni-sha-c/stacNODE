time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.02%, model saved.
Epoch: 0 Train: 32467.77344 Test: 4264.41064
Epoch 80: New minimal relative error: 91.39%, model saved.
Epoch: 80 Train: 8772.50000 Test: 1576.48816
Epoch: 160 Train: 8465.15234 Test: 1475.75964
Epoch 240: New minimal relative error: 85.36%, model saved.
Epoch: 240 Train: 7909.51611 Test: 1176.88281
Epoch: 320 Train: 6985.43018 Test: 1018.94995
Epoch: 400 Train: 6397.06836 Test: 803.13702
Epoch: 480 Train: 4786.46094 Test: 483.43710
Epoch 560: New minimal relative error: 81.33%, model saved.
Epoch: 560 Train: 4483.70361 Test: 426.71460
Epoch: 640 Train: 4075.85327 Test: 335.99472
Epoch: 720 Train: 3001.99268 Test: 227.14589
Epoch: 800 Train: 1423.81970 Test: 74.25887
Epoch 880: New minimal relative error: 20.78%, model saved.
Epoch: 880 Train: 802.71259 Test: 29.07644
Epoch: 960 Train: 512.16162 Test: 13.34962
Epoch 1040: New minimal relative error: 13.30%, model saved.
Epoch: 1040 Train: 372.52164 Test: 6.88556
Epoch: 1120 Train: 341.98294 Test: 6.22517
Epoch: 1200 Train: 287.31827 Test: 4.39634
Epoch: 1280 Train: 249.48944 Test: 3.25918
Epoch: 1360 Train: 228.39110 Test: 2.81202
Epoch: 1440 Train: 215.44476 Test: 2.70566
Epoch: 1520 Train: 204.36749 Test: 2.50302
Epoch: 1600 Train: 190.85959 Test: 2.31943
Epoch: 1680 Train: 192.07124 Test: 9.02559
Epoch: 1760 Train: 169.84807 Test: 2.37002
Epoch 1840: New minimal relative error: 5.82%, model saved.
Epoch: 1840 Train: 164.46196 Test: 1.96186
Epoch: 1920 Train: 160.64365 Test: 1.87699
Epoch: 2000 Train: 162.49489 Test: 1.80427
Epoch: 2080 Train: 156.11015 Test: 1.74309
Epoch: 2160 Train: 157.95221 Test: 2.89197
Epoch: 2240 Train: 149.55533 Test: 1.52811
Epoch: 2320 Train: 144.17125 Test: 1.40039
Epoch 2400: New minimal relative error: 5.60%, model saved.
Epoch: 2400 Train: 137.08591 Test: 1.40314
Epoch: 2480 Train: 133.19299 Test: 1.31237
Epoch: 2560 Train: 126.39622 Test: 1.23546
Epoch: 2640 Train: 121.98394 Test: 1.19448
Epoch: 2720 Train: 117.97140 Test: 1.36078
Epoch: 2800 Train: 117.14894 Test: 0.95118
Epoch: 2880 Train: 110.36562 Test: 0.85370
Epoch: 2960 Train: 106.87736 Test: 1.23215
Epoch: 3040 Train: 102.71834 Test: 0.80631
Epoch: 3120 Train: 100.96429 Test: 0.70396
Epoch 3200: New minimal relative error: 4.29%, model saved.
Epoch: 3200 Train: 98.01978 Test: 0.63209
Epoch: 3280 Train: 99.22619 Test: 2.09570
Epoch: 3360 Train: 91.75496 Test: 0.60651
Epoch 3440: New minimal relative error: 3.95%, model saved.
Epoch: 3440 Train: 91.95261 Test: 0.71421
Epoch: 3520 Train: 89.62977 Test: 1.42116
Epoch: 3600 Train: 84.48100 Test: 1.26440
Epoch: 3680 Train: 78.06009 Test: 2.67820
Epoch 3760: New minimal relative error: 2.48%, model saved.
Epoch: 3760 Train: 74.38056 Test: 0.41585
Epoch: 3840 Train: 70.92406 Test: 0.38732
Epoch: 3920 Train: 67.26842 Test: 0.40934
Epoch: 4000 Train: 67.19990 Test: 0.36899
Epoch: 4080 Train: 67.56794 Test: 0.43281
Epoch 4160: New minimal relative error: 1.72%, model saved.
Epoch: 4160 Train: 66.36637 Test: 0.37206
Epoch: 4240 Train: 66.52482 Test: 1.37648
Epoch: 4320 Train: 63.51952 Test: 0.27843
Epoch: 4400 Train: 65.35310 Test: 0.53007
Epoch: 4480 Train: 59.38442 Test: 0.27678
Epoch: 4560 Train: 57.12513 Test: 0.30154
Epoch: 4640 Train: 56.82301 Test: 0.21540
Epoch: 4720 Train: 54.25251 Test: 0.22122
Epoch: 4800 Train: 53.76588 Test: 0.22170
Epoch: 4880 Train: 52.67116 Test: 0.21427
Epoch: 4960 Train: 51.94024 Test: 0.23112
Epoch 5040: New minimal relative error: 1.53%, model saved.
Epoch: 5040 Train: 50.83900 Test: 0.19483
Epoch: 5120 Train: 51.10186 Test: 0.23379
Epoch: 5200 Train: 50.93732 Test: 0.18030
Epoch: 5280 Train: 50.91497 Test: 0.19750
Epoch 5360: New minimal relative error: 1.37%, model saved.
Epoch: 5360 Train: 49.96954 Test: 0.21875
Epoch 5440: New minimal relative error: 1.07%, model saved.
Epoch: 5440 Train: 50.38270 Test: 0.18643
Epoch: 5520 Train: 49.99545 Test: 0.17545
Epoch: 5600 Train: 49.47898 Test: 0.19468
Epoch: 5680 Train: 48.25071 Test: 0.17349
Epoch: 5760 Train: 48.75568 Test: 0.18325
Epoch: 5840 Train: 49.10046 Test: 0.19331
Epoch: 5920 Train: 49.22474 Test: 0.19663
Epoch: 6000 Train: 49.50848 Test: 0.19846
Epoch: 6080 Train: 49.65747 Test: 0.26694
Epoch: 6160 Train: 48.09384 Test: 0.29125
Epoch: 6240 Train: 47.37918 Test: 0.21428
Epoch: 6320 Train: 49.68578 Test: 0.22437
Epoch: 6400 Train: 47.45241 Test: 0.21065
Epoch: 6480 Train: 45.27295 Test: 0.20590
Epoch: 6560 Train: 45.22097 Test: 0.19877
Epoch: 6640 Train: 43.76497 Test: 0.17252
Epoch: 6720 Train: 43.05403 Test: 0.17979
Epoch: 6800 Train: 42.77294 Test: 0.17628
Epoch: 6880 Train: 41.17132 Test: 0.17340
Epoch: 6960 Train: 41.38615 Test: 0.19003
Epoch: 7040 Train: 41.21285 Test: 0.18301
Epoch: 7120 Train: 41.16385 Test: 0.33486
Epoch: 7200 Train: 40.85727 Test: 0.17824
Epoch 7280: New minimal relative error: 0.96%, model saved.
Epoch: 7280 Train: 40.82569 Test: 0.16668
Epoch: 7360 Train: 39.81905 Test: 0.24558
Epoch: 7440 Train: 38.75881 Test: 0.14806
Epoch: 7520 Train: 38.66782 Test: 0.15032
Epoch: 7600 Train: 38.09293 Test: 0.14034
Epoch: 7680 Train: 38.79735 Test: 0.14504
Epoch: 7760 Train: 38.82928 Test: 0.14481
Epoch: 7840 Train: 38.21627 Test: 0.12942
Epoch: 7920 Train: 38.82950 Test: 0.18359
Epoch: 7999 Train: 39.67595 Test: 0.16093
Training Loss: tensor(39.6760)
Test Loss: tensor(0.1609)
Learned LE: [  0.85521966  -0.0245923  -14.511652  ]
True LE: [ 8.4026521e-01  5.7190098e-03 -1.4515159e+01]
Relative Error: [0.5536143  0.6198844  0.697939   0.63131016 0.44836012 0.31362224
 0.32593098 0.432036   0.68378454 1.0120984  1.584165   1.7574826
 1.1081426  0.66368514 0.5653846  0.67548764 0.50701743 0.3211437
 0.3406039  0.34554082 0.18484165 0.16786054 0.21167107 0.40490696
 0.64751136 0.67012024 0.7678485  1.0081621  1.1695038  1.1055979
 0.9578882  1.0632486  0.9553939  0.7250922  0.43769222 0.3706603
 0.35690248 0.6495141  0.40241814 0.45803678 0.5017035  0.58829385
 0.9005232  1.0369447  0.65558267 0.6418707  0.8013041  0.5137739
 0.40207127 0.5524715  0.75105417 0.8718792  0.91756135 0.79871064
 0.74624187 0.8059243  0.8996586  0.975713   0.953191   0.83639723
 0.6764141  0.48898804 0.5629254  0.61994445 0.6241092  0.6540644
 0.6796917  0.39103138 0.31055894 0.3335799  0.44028604 0.74833375
 1.107541   1.6707759  1.436988   0.8783071  0.65494454 0.62446314
 0.48595852 0.3520455  0.30168426 0.36290458 0.22440475 0.21088773
 0.4677808  0.73104066 0.81715435 0.55115557 0.4348349  0.65323305
 0.8640753  0.7019857  0.59441787 0.51162994 0.68375075 0.9166686
 0.5986479  0.35565212 0.2308509  0.44390988 0.42723072 0.4278568
 0.51079106 0.55637205 0.77126646 0.9860668  0.61498165 0.5834362
 0.68564636 0.6461089  0.36215025 0.46625054 0.6762085  0.7933327
 0.85262126 0.8317633  0.698759   0.7674697  0.8791463  1.0097629
 1.1047541  1.1072999  1.0587546  0.8793621  0.8015965  0.7996084
 0.7254693  0.579637   0.5493733  0.65022236 0.40568584 0.30875146
 0.33261606 0.47207353 0.8035388  1.2921404  1.6585456  1.2811122
 0.8925027  0.7420962  0.7038981  0.3380461  0.14707412 0.30606776
 0.38016516 0.3986471  0.70882815 1.0476063  1.0035924  0.76068854
 0.4750318  0.3810327  0.4749573  0.41584855 0.36561042 0.18754071
 0.26659715 0.47240487 0.7259285  0.4922018  0.27720767 0.2416295
 0.51522017 0.38751712 0.3893871  0.50435853 0.66531986 0.9589045
 0.6804585  0.5293577  0.525866   0.7792364  0.44447935 0.46560532
 0.63195515 0.706425   0.74729246 0.75514233 0.75549465 0.8364076
 0.9891533  1.1237667  1.2979332  1.4777821  1.5087647  1.4753387
 1.3799887  1.2397463  1.0378686  0.7857737  0.5909968  0.48057422
 0.60817087 0.4671908  0.29687947 0.3010515  0.5149032  0.86411107
 1.4661276  1.5060201  1.330342   0.998474   0.8611716  0.49145967
 0.21610926 0.20652081 0.46099132 0.6067745  0.75627357 1.0354793
 0.9156552  0.8005914  0.6618428  0.45888653 0.379843   0.2965083
 0.2097564  0.12717615 0.00909753 0.20588301 0.44877142 0.5762911
 0.48522586 0.2589328  0.31347084 0.42076352 0.37245202 0.2570043
 0.52001345 0.8682467  0.79787636 0.45728818 0.4054535  0.54029876
 0.6332944  0.4907594  0.42880908 0.45134813 0.5090161  0.54100996
 0.62619317 0.8404644  1.0042468  1.2711649  1.5059608  1.7166641
 2.0135782  2.1451979  2.1512856  2.0003545  1.7415338  1.4541031
 1.0403454  0.72078884 0.4855494  0.49837753 0.49069798 0.25071487
 0.2013454  0.39039084 0.81316435 1.4381435  1.5492352  1.3635706
 1.0763887  0.90657735 0.3525964  0.23768958 0.51414806 0.6555475
 0.5865531  0.6654629  0.5634067  0.46015233 0.40870336 0.37435588
 0.43787366 0.53530705 0.47507757 0.2947682  0.1067411  0.03400522
 0.2693972  0.5362905  0.54893327 0.5250488  0.37411213 0.35929033
 0.43768972 0.27749237 0.2410591  0.6364324  0.95084393 0.5346624
 0.29652476 0.30614832 0.5396144  0.63594174 0.4055101  0.3251212
 0.38131014 0.4692043  0.5902241  0.8351079  1.0164378  1.2322466
 1.3938965  1.6235888  1.9775845  2.2888136  2.5207188  2.4079301
 2.36897    2.296761   1.8846827  1.2756817  0.7196327  0.4301658
 0.32822496 0.35163254 0.22427666 0.20201772 0.22165495 0.6374056
 1.3203791  1.5452706  1.346926   1.1710986  0.84361315 0.35180706
 0.45064896 0.55112964 0.5275004  0.5195412  0.4975226  0.20218673
 0.25983918 0.16863316 0.23205136 0.3212692  0.6774617  0.72921693
 0.5477919  0.280817   0.05886105 0.33743322 0.6657044  0.6710817
 0.5182412  0.49756822 0.4601608  0.37370104 0.23082201 0.3196405
 0.6136699  0.90770495 0.33708647 0.19674358 0.24179393 0.5737727
 0.6318578  0.32819778 0.3180369  0.45010522 0.5682581  0.821056
 1.2484366  1.2201327  1.3354951  1.4284185  1.4929364  1.6098058
 1.9903194  2.3474443  2.320629   2.2528265  2.311287   2.2613328
 1.7911589  1.0433531  0.5219435  0.46626174 0.3054537  0.2604257
 0.25051287 0.19481759 0.43712276 1.0174183  1.368424   1.2864851
 1.2017765  0.95559156 0.45055455 0.47385824 0.41100505 0.50576806
 0.5440603  0.2752756  0.1993837  0.2620312 ]
