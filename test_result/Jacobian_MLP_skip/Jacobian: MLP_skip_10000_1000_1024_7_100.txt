time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.45%, model saved.
Epoch: 0 Train: 9758.63477 Test: 4341.82422
Epoch: 100 Train: 2632.99512 Test: 1097.94189
Epoch: 200 Train: 2078.59814 Test: 834.47357
Epoch: 300 Train: 1891.52100 Test: 667.79144
Epoch: 400 Train: 1041.65039 Test: 294.65457
Epoch 500: New minimal relative error: 17.73%, model saved.
Epoch: 500 Train: 316.03452 Test: 45.28720
Epoch: 600 Train: 142.38261 Test: 71.20200
Epoch: 700 Train: 127.94479 Test: 32.65922
Epoch: 800 Train: 59.21561 Test: 13.19833
Epoch: 900 Train: 61.77174 Test: 4.79524
Epoch: 1000 Train: 30.91410 Test: 2.56945
Epoch 1100: New minimal relative error: 3.95%, model saved.
Epoch: 1100 Train: 48.19781 Test: 2.37154
Epoch: 1200 Train: 27.89352 Test: 3.29158
Epoch: 1300 Train: 78.40385 Test: 12.71709
Epoch: 1400 Train: 34.66890 Test: 11.58049
Epoch: 1500 Train: 22.39976 Test: 6.27968
Epoch: 1600 Train: 25.90330 Test: 7.23269
Epoch: 1700 Train: 31.49038 Test: 20.34584
Epoch: 1800 Train: 16.54220 Test: 3.85402
Epoch: 1900 Train: 12.98805 Test: 1.86464
Epoch: 2000 Train: 24.90029 Test: 10.50656
Epoch: 2100 Train: 11.52732 Test: 0.28984
Epoch: 2200 Train: 12.70981 Test: 2.20546
Epoch: 2300 Train: 10.20342 Test: 0.98481
Epoch: 2400 Train: 10.52630 Test: 1.29746
Epoch 2500: New minimal relative error: 3.80%, model saved.
Epoch: 2500 Train: 11.63685 Test: 2.44838
Epoch: 2600 Train: 12.19524 Test: 1.52269
Epoch: 2700 Train: 10.37416 Test: 1.18488
Epoch: 2800 Train: 13.55189 Test: 4.65068
Epoch 2900: New minimal relative error: 3.04%, model saved.
Epoch: 2900 Train: 13.49523 Test: 1.15444
Epoch: 3000 Train: 17.22248 Test: 4.01067
Epoch: 3100 Train: 8.81453 Test: 1.12531
Epoch: 3200 Train: 9.24271 Test: 1.88818
Epoch: 3300 Train: 8.51536 Test: 0.73827
Epoch: 3400 Train: 8.78681 Test: 2.34992
Epoch: 3500 Train: 11.66063 Test: 3.73621
Epoch: 3600 Train: 5.70934 Test: 0.10800
Epoch: 3700 Train: 15.85519 Test: 8.85883
Epoch: 3800 Train: 8.53086 Test: 2.40046
Epoch: 3900 Train: 6.02568 Test: 0.39111
Epoch: 4000 Train: 18.12757 Test: 8.10323
Epoch: 4100 Train: 5.43450 Test: 0.05171
Epoch: 4200 Train: 6.74950 Test: 1.64411
Epoch: 4300 Train: 5.84567 Test: 0.68247
Epoch: 4400 Train: 5.58080 Test: 0.92390
Epoch: 4500 Train: 5.72242 Test: 0.22066
Epoch: 4600 Train: 5.71225 Test: 0.85122
Epoch: 4700 Train: 4.67432 Test: 0.17632
Epoch: 4800 Train: 6.76700 Test: 1.67046
Epoch: 4900 Train: 9.22375 Test: 2.72172
Epoch: 5000 Train: 5.20404 Test: 1.24111
Epoch: 5100 Train: 4.26661 Test: 0.31802
Epoch 5200: New minimal relative error: 3.02%, model saved.
Epoch: 5200 Train: 4.22733 Test: 0.32416
Epoch: 5300 Train: 6.02636 Test: 0.98560
Epoch: 5400 Train: 3.77495 Test: 0.04609
Epoch: 5500 Train: 3.60776 Test: 0.02074
Epoch: 5600 Train: 3.86062 Test: 0.09624
Epoch: 5700 Train: 3.51371 Test: 0.13211
Epoch: 5800 Train: 3.88064 Test: 0.17352
Epoch: 5900 Train: 3.51684 Test: 0.06696
Epoch: 6000 Train: 4.00475 Test: 0.55758
Epoch: 6100 Train: 10.62578 Test: 2.35903
Epoch 6200: New minimal relative error: 1.31%, model saved.
Epoch: 6200 Train: 3.13492 Test: 0.02294
Epoch: 6300 Train: 3.03014 Test: 0.02734
Epoch: 6400 Train: 3.63258 Test: 0.41098
Epoch: 6500 Train: 3.35521 Test: 0.35692
Epoch: 6600 Train: 5.00790 Test: 0.93418
Epoch: 6700 Train: 3.17517 Test: 0.22757
Epoch: 6800 Train: 2.85533 Test: 0.03567
Epoch: 6900 Train: 3.17359 Test: 0.18818
Epoch: 7000 Train: 3.09249 Test: 0.26126
Epoch: 7100 Train: 2.71406 Test: 0.03540
Epoch: 7200 Train: 2.77735 Test: 0.07423
Epoch: 7300 Train: 3.20692 Test: 0.47614
Epoch: 7400 Train: 7.28483 Test: 4.45036
Epoch: 7500 Train: 2.60223 Test: 0.02186
Epoch: 7600 Train: 2.91879 Test: 0.25205
Epoch: 7700 Train: 8.26064 Test: 5.30509
Epoch: 7800 Train: 2.49924 Test: 0.01076
Epoch: 7900 Train: 2.68878 Test: 0.17761
Epoch: 8000 Train: 2.59602 Test: 0.04074
Epoch: 8100 Train: 2.53403 Test: 0.03568
Epoch: 8200 Train: 2.55596 Test: 0.01513
Epoch: 8300 Train: 2.54745 Test: 0.04750
Epoch: 8400 Train: 2.66447 Test: 0.01447
Epoch: 8500 Train: 2.96497 Test: 0.17692
Epoch: 8600 Train: 2.71604 Test: 0.02414
Epoch: 8700 Train: 2.58275 Test: 0.01889
Epoch: 8800 Train: 2.61297 Test: 0.03153
Epoch: 8900 Train: 4.05095 Test: 0.88947
Epoch: 9000 Train: 3.07941 Test: 0.03444
Epoch: 9100 Train: 2.45520 Test: 0.07342
Epoch: 9200 Train: 2.78902 Test: 0.24489
Epoch: 9300 Train: 2.69027 Test: 0.23506
Epoch: 9400 Train: 3.09038 Test: 0.38142
Epoch: 9500 Train: 2.45651 Test: 0.10997
Epoch: 9600 Train: 2.36942 Test: 0.04191
Epoch: 9700 Train: 2.52847 Test: 0.03445
Epoch: 9800 Train: 2.42074 Test: 0.02153
Epoch: 9900 Train: 3.04382 Test: 0.31415
Epoch: 9999 Train: 4.81592 Test: 1.86965
Training Loss: tensor(4.8159)
Test Loss: tensor(1.8696)
Learned LE: [  0.8158386    0.03109608 -14.522354  ]
True LE: [ 8.4911156e-01 -2.8352875e-03 -1.4525710e+01]
Relative Error: [3.1295848  3.21498    3.1588137  3.107824   2.879365   2.6006143
 2.1998112  1.7053478  1.2206225  0.9359257  0.9782682  1.1156085
 0.9011996  0.94725126 1.0687358  1.0797552  1.037965   1.0245451
 1.1756086  1.4389986  1.6753482  1.9648936  2.226109   2.505709
 2.8015196  3.1626348  3.4387183  3.4958384  3.8579996  4.1039286
 4.4504123  4.6899405  4.9346714  4.843403   4.5892844  3.8562272
 2.8406234  2.0236058  1.4244354  1.1642662  1.3782766  1.5820918
 1.6224995  1.5774246  1.59385    1.5009773  1.4518626  1.5596923
 1.6067277  1.623626   1.719486   1.7400093  1.7572064  1.9423041
 2.1683495  2.496328   2.9340234  3.384877   3.2658312  2.9759023
 2.7999623  2.83551    2.943159   3.005751   2.9845276  2.9981353
 2.854812   2.5947752  2.1690586  1.663503   1.2110809  0.8317436
 0.72980267 0.7873496  0.765785   0.8256071  0.9837066  1.032224
 0.9569408  0.88849384 1.0138781  1.2969395  1.3029773  1.5845809
 2.105582   2.4619937  2.7624435  2.9995818  3.3096642  3.3311088
 3.6718607  4.161133   4.624058   4.8383455  5.079575   5.088975
 4.8298006  4.0061407  3.0226357  2.1352794  1.476996   1.1419545
 1.1981882  1.4136924  1.5638864  1.5283178  1.6427273  1.6668525
 1.5889943  1.6287692  1.6546471  1.5826904  1.5755243  1.663516
 1.8187152  1.907835   2.019772   2.2547772  2.6726234  3.1277802
 3.2625713  3.0678093  2.7450943  2.621566   2.701521   2.81976
 2.839294   2.9051461  2.8347368  2.6149695  2.2034671  1.6941369
 1.2169719  0.8031527  0.5545534  0.47148418 0.62518764 0.6319094
 0.78629273 0.9572054  0.9027912  0.7756822  0.85658234 1.1288893
 1.0669678  1.2285563  1.6337503  2.391976   2.8250568  2.7846434
 3.0093145  3.0753558  3.2690718  3.9040942  4.4402785  4.8430257
 5.2806163  5.273485   5.018624   4.1185493  3.0673776  2.3013008
 1.5512875  1.1594212  1.1325638  1.2094558  1.3899137  1.5420651
 1.6108987  1.7498648  1.6797552  1.7933164  1.726668   1.6538596
 1.5620857  1.5483454  1.5178328  1.8011001  1.9566536  2.1047502
 2.436065   2.8928504  3.1059942  3.1333022  2.967312   2.654027
 2.49681    2.6071193  2.7241268  2.7375028  2.7766752  2.617896
 2.2937717  1.8246593  1.2726347  0.814101   0.5887585  0.3317178
 0.3153116  0.52459806 0.65358377 0.86892015 0.9486707  0.84035516
 0.7758495  0.9225687  0.98282593 1.0054185  1.3004054  1.9256167
 2.6243935  2.694206   2.7416174  2.951      3.043502   3.548271
 4.11971    4.690826   5.1868668  5.349179   5.210264   4.4724073
 3.2732878  2.3684638  1.6929399  1.195274   1.1688995  1.0882571
 1.235563   1.3903816  1.5118035  1.7511616  1.7889841  1.8846965
 1.8050196  1.6807189  1.5366321  1.4885269  1.3375102  1.3577715
 1.6262062  1.9584248  2.1636145  2.5092816  2.7779524  2.7893066
 2.669277   2.5078738  2.336159   2.409699   2.5805798  2.6220474
 2.6222396  2.5435717  2.3114522  1.9228419  1.4476182  0.92925245
 0.67559505 0.52652925 0.24458092 0.34709117 0.56790954 0.71565735
 0.8341539  0.85688245 0.84391075 0.8815979  1.0121212  0.9686416
 1.083853   1.4201149  1.9952747  2.6279082  2.6761928  2.7515461
 2.9059298  3.0114396  3.6041758  4.1556053  4.901609   5.203263
 5.210403   4.958474   3.7935722  2.662929   1.8146539  1.3418609
 1.1743063  1.0263973  1.0918564  1.3159025  1.5329772  1.6927503
 1.8716475  1.9591408  1.9649245  1.7360088  1.5552033  1.3975123
 1.2185427  1.183544   1.2038171  1.4127133  1.7570982  2.088336
 2.3435674  2.497938   2.4551575  2.2911255  2.143169   2.037936
 2.191339   2.4790514  2.5940895  2.5297234  2.3214524  1.9787369
 1.6197     1.1122751  0.87115216 0.6989976  0.6327934  0.37218353
 0.37930837 0.5367994  0.6543581  0.7013942  0.745492   0.8586534
 0.9040945  0.98179144 1.0649716  1.1696869  1.4102122  2.0262835
 2.5982342  2.6230173  2.442503   2.4913056  2.8054035  3.4077108
 4.125027   4.654221   4.9834123  4.9057007  4.219903   3.257538
 2.2296865  1.5081153  1.2523658  0.9144881  0.88904685 1.1479226
 1.4851803  1.6534722  1.8103738  1.9749591  2.0166779  1.8237962
 1.5424626  1.3041341  1.0762777  0.9498146  0.9545153  1.0061806
 1.1412137  1.4284687  1.7616104  2.044894   2.2013118  2.2385669
 2.0848114  1.8916868  1.8226434  1.9753711  2.2880175  2.472811
 2.336408   2.0809953  1.7087253  1.2964442  1.048619   0.8944315
 0.77542526 0.65659213 0.49249902 0.37385604 0.5048582  0.6061722
 0.64414865 0.6343473  0.7911689  0.91355544 0.9784529  1.1492308
 1.1872     1.4514794  1.9307564  2.3311307 ]
