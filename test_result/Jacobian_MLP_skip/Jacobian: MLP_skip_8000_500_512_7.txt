time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.28%, model saved.
Epoch: 0 Train: 170830.76562 Test: 4065.52466
Epoch: 80 Train: 46080.64844 Test: 1878.64917
Epoch: 160 Train: 41143.77344 Test: 1679.58606
Epoch: 240 Train: 32417.22070 Test: 1545.86316
Epoch: 320 Train: 38084.15234 Test: 1725.37964
Epoch 400: New minimal relative error: 81.59%, model saved.
Epoch: 400 Train: 35755.67969 Test: 1142.64258
Epoch: 480 Train: 42166.70703 Test: 2084.82422
Epoch: 560 Train: 38414.17188 Test: 1449.16638
Epoch: 640 Train: 40646.03906 Test: 2110.98901
Epoch: 720 Train: 41527.86328 Test: 1789.79333
Epoch 800: New minimal relative error: 76.08%, model saved.
Epoch: 800 Train: 42141.33984 Test: 1594.76208
Epoch 880: New minimal relative error: 59.20%, model saved.
Epoch: 880 Train: 38749.48828 Test: 1387.11145
Epoch: 960 Train: 42754.28906 Test: 1417.80627
Epoch: 1040 Train: 42621.29297 Test: 1470.73865
Epoch: 1120 Train: 41028.12109 Test: 1399.47546
Epoch: 1200 Train: 38973.29297 Test: 1281.99426
Epoch: 1280 Train: 41158.37500 Test: 1391.94446
Epoch: 1360 Train: 46418.94922 Test: 1552.44739
Epoch: 1440 Train: 41064.45312 Test: 1437.58459
Epoch: 1520 Train: 37941.15625 Test: 1367.52771
Epoch: 1600 Train: 39049.21875 Test: 1349.47083
Epoch: 1680 Train: 37599.34766 Test: 1430.27783
Epoch: 1760 Train: 38252.32812 Test: 1271.33911
Epoch 1840: New minimal relative error: 58.81%, model saved.
Epoch: 1840 Train: 36383.50000 Test: 1230.14441
Epoch: 1920 Train: 38021.73828 Test: 1386.58240
Epoch: 2000 Train: 37988.47656 Test: 1379.59375
Epoch 2080: New minimal relative error: 54.90%, model saved.
Epoch: 2080 Train: 38509.42578 Test: 1310.25085
Epoch: 2160 Train: 38388.71875 Test: 1353.32898
Epoch: 2240 Train: 38023.41797 Test: 1277.45032
Epoch: 2320 Train: 36851.69922 Test: 1270.21863
Epoch: 2400 Train: 36459.37500 Test: 1225.86072
Epoch: 2480 Train: 35976.94141 Test: 1289.05725
Epoch: 2560 Train: 35946.30859 Test: 1212.51086
Epoch: 2640 Train: 36535.64062 Test: 1295.89270
Epoch: 2720 Train: 36374.41406 Test: 1173.07861
Epoch: 2800 Train: 35329.17188 Test: 1435.31714
Epoch: 2880 Train: 34524.12109 Test: 1194.58984
Epoch: 2960 Train: 34070.14844 Test: 1104.41064
Epoch: 3040 Train: 33832.87891 Test: 1191.03479
Epoch: 3120 Train: 33013.57031 Test: 1034.97375
Epoch: 3200 Train: 33926.70312 Test: 1110.52490
Epoch: 3280 Train: 31286.05078 Test: 969.40295
Epoch: 3360 Train: 30615.73242 Test: 1023.93842
Epoch: 3440 Train: 27963.09766 Test: 833.04077
Epoch: 3520 Train: 28611.96289 Test: 853.95245
Epoch: 3600 Train: 28091.17578 Test: 696.77191
Epoch: 3680 Train: 27283.05664 Test: 722.51282
Epoch: 3760 Train: 26655.29688 Test: 730.68323
Epoch: 3840 Train: 26069.07812 Test: 680.15747
Epoch: 3920 Train: 25804.42773 Test: 619.04510
Epoch: 4000 Train: 23145.33594 Test: 536.80701
Epoch: 4080 Train: 20991.74609 Test: 442.19916
Epoch: 4160 Train: 23585.53906 Test: 520.62866
Epoch: 4240 Train: 19624.18164 Test: 381.15955
Epoch: 4320 Train: 19029.29883 Test: 362.91260
Epoch: 4400 Train: 17330.26172 Test: 316.35355
Epoch: 4480 Train: 17687.58008 Test: 340.43906
Epoch: 4560 Train: 14651.75879 Test: 240.76907
Epoch: 4640 Train: 14800.44238 Test: 204.75870
Epoch: 4720 Train: 10810.71973 Test: 119.63432
Epoch: 4800 Train: 4511.77148 Test: 61.62015
Epoch 4880: New minimal relative error: 13.98%, model saved.
Epoch: 4880 Train: 2557.41284 Test: 14.97447
Epoch 4960: New minimal relative error: 9.77%, model saved.
Epoch: 4960 Train: 1951.55908 Test: 8.97089
Epoch: 5040 Train: 1656.81873 Test: 9.75534
Epoch 5120: New minimal relative error: 9.77%, model saved.
Epoch: 5120 Train: 1546.30908 Test: 6.18123
Epoch: 5200 Train: 1500.64709 Test: 6.82890
Epoch: 5280 Train: 1346.82874 Test: 15.33244
Epoch: 5360 Train: 1327.11865 Test: 16.03779
Epoch: 5440 Train: 1072.56360 Test: 4.49846
Epoch: 5520 Train: 1014.33014 Test: 14.45063
Epoch: 5600 Train: 997.98602 Test: 4.28980
Epoch: 5680 Train: 973.88989 Test: 11.53033
Epoch 5760: New minimal relative error: 9.03%, model saved.
Epoch: 5760 Train: 894.53400 Test: 4.23630
Epoch 5840: New minimal relative error: 6.97%, model saved.
Epoch: 5840 Train: 752.46198 Test: 2.70091
Epoch: 5920 Train: 709.04126 Test: 10.27672
Epoch: 6000 Train: 699.63635 Test: 5.83268
Epoch: 6080 Train: 661.65808 Test: 12.62601
Epoch: 6160 Train: 639.13879 Test: 5.47008
Epoch: 6240 Train: 632.88965 Test: 3.22508
Epoch 6320: New minimal relative error: 6.63%, model saved.
Epoch: 6320 Train: 666.92145 Test: 2.66463
Epoch: 6400 Train: 598.14893 Test: 4.01408
Epoch: 6480 Train: 563.86182 Test: 3.94910
Epoch: 6560 Train: 548.46576 Test: 1.38868
Epoch: 6640 Train: 596.10840 Test: 15.57617
Epoch: 6720 Train: 571.86145 Test: 3.29308
Epoch: 6800 Train: 531.88574 Test: 3.71468
Epoch: 6880 Train: 505.58057 Test: 1.67587
Epoch 6960: New minimal relative error: 4.42%, model saved.
Epoch: 6960 Train: 494.16403 Test: 1.08708
Epoch: 7040 Train: 451.87469 Test: 2.57202
Epoch: 7120 Train: 477.48965 Test: 1.56790
Epoch 7200: New minimal relative error: 4.24%, model saved.
Epoch: 7200 Train: 415.57205 Test: 0.84979
Epoch: 7280 Train: 507.95508 Test: 4.07256
Epoch: 7360 Train: 407.24213 Test: 0.72037
Epoch: 7440 Train: 409.97397 Test: 1.04357
Epoch: 7520 Train: 385.29010 Test: 0.83847
Epoch: 7600 Train: 370.58530 Test: 1.02165
Epoch 7680: New minimal relative error: 3.90%, model saved.
Epoch: 7680 Train: 364.43100 Test: 0.62066
Epoch 7760: New minimal relative error: 2.36%, model saved.
Epoch: 7760 Train: 344.53131 Test: 0.72699
Epoch: 7840 Train: 355.24225 Test: 0.85282
Epoch: 7920 Train: 364.25687 Test: 0.98078
Epoch: 7999 Train: 344.46439 Test: 0.61737
Training Loss: tensor(344.4644)
Test Loss: tensor(0.6174)
Learned LE: [  0.9248871   -0.08783124 -14.5187435 ]
True LE: [ 8.6754274e-01 -9.8538981e-04 -1.4544727e+01]
Relative Error: [3.1934593  3.4261823  3.5212226  3.69841    3.400163   2.925247
 2.4921014  1.9426949  1.24862    1.1951783  1.640045   1.93137
 1.9389546  1.7320315  1.370581   0.63152564 1.3138856  2.990302
 4.929951   6.0166316  5.967166   5.4995937  5.477958   5.2201924
 4.954815   4.8990498  4.910752   4.3466988  3.7785306  3.4759502
 3.7118568  4.298506   4.9185624  5.3443685  5.5281115  5.277494
 4.7631564  4.302691   4.0786686  4.351615   4.7940397  5.1409893
 4.9194856  4.5356336  3.9929588  3.2782655  2.0059407  0.5423868
 1.5963812  0.7502602  0.3885151  0.5982403  0.59193945 1.8506179
 2.2725086  2.0927892  1.9006871  1.7765135  1.8596772  2.191079
 2.479265   2.7114532  2.9169154  3.099746   3.2066114  3.3126752
 3.2965086  2.990308   2.5854928  2.2337482  1.6081659  1.2566749
 1.4331045  1.7819657  2.0075538  1.9212093  1.6325377  0.9383659
 0.85058165 2.2616115  3.9785187  5.3725247  5.610527   5.1292787
 4.8611894  4.6597915  4.754677   4.771537   4.7498884  4.2088265
 3.6776607  3.7574875  3.8632553  4.038597   4.5105343  4.9525695
 5.171688   5.0431433  4.699449   4.414646   4.1705856  3.7789583
 4.0388036  4.4534903  4.676304   4.3450093  3.9137905  3.3867233
 2.2398272  0.34835154 1.2417345  0.64583427 0.3839111  0.6713013
 0.6134584  1.63005    2.0363045  1.9211371  1.8532666  1.8138485
 1.823259   2.06396    2.4417396  2.6436138  2.788379   2.9305124
 3.1940553  3.0669074  3.085421   2.9572446  2.6547112  2.2869642
 2.0591185  1.6178819  1.506308   1.5742462  1.8809084  2.0442123
 1.9088461  1.433027   0.7545517  1.3565613  2.9431407  4.6329436
 5.2257886  4.822817   4.2720847  4.4192014  4.5096273  4.549149
 4.5722785  4.112701   3.9901361  4.1069818  4.188257   4.364138
 4.6032367  4.7420716  4.9528217  4.694025   4.539428   4.3926115
 4.3389916  4.074383   3.5070848  3.7809265  4.1739693  4.2436085
 3.90817    3.547944   2.6530135  1.1138619  0.5018233  0.5060578
 0.4295316  0.79470557 0.65879446 1.2993942  1.8148956  1.7510248
 1.7697166  1.865834   1.90091    2.0885532  2.2979     2.6390576
 2.8731608  2.972106   3.11481    3.185667   2.924405   2.833899
 2.7399101  2.459568   2.2611995  2.0286627  1.7632525  1.6702093
 1.7682056  1.9635955  1.947668   1.6437945  1.0623144  0.7540814
 1.9738797  3.553333   4.6551185  4.588248   4.1312957  4.237478
 4.3318377  4.334807   4.2976704  4.191639   4.229768   4.3633523
 4.3784466  4.5161133  4.757012   4.914099   4.6351876  4.524566
 4.2337627  4.194624   4.331765   4.2951074  3.886119   3.2706368
 3.542163   3.9663463  3.953515   3.6570578  3.1164336  2.1288874
 0.38008183 0.6004833  0.56194425 0.92942107 0.93527913 0.9178331
 1.565279   1.5403073  1.6349945  1.8474473  1.879507   1.9495434
 2.008888   2.216761   2.5223067  2.7949317  2.9093995  2.9554574
 2.9900155  2.7246819  2.5846913  2.6390693  2.412964   2.234473
 1.9842716  1.9533256  1.8094269  1.8099318  1.7117606  1.6135616
 1.3917047  0.88143533 1.1546615  2.398703   3.9891675  4.34735
 4.0875163  3.8329706  4.197595   4.1828027  4.0519094  4.137905
 4.265926   4.5591474  4.525939   4.4812083  4.675105   4.937392
 4.9465027  4.347012   3.9821765  3.783712   3.9143455  4.250856
 4.1068816  3.6930676  3.0998363  3.3574514  3.7987313  3.7440636
 3.4453433  2.7777734  1.6145308  0.28108644 0.61139697 0.9925751
 1.1217705  0.4890961  1.16782    1.4078921  1.4952264  1.6398513
 1.742322   1.8136964  1.8560307  1.93996    2.0886226  2.3307052
 2.5757847  2.6035252  2.6451375  2.5585938  2.4803796  2.5206344
 2.5227065  2.2971988  2.1632514  2.0009131  2.0831792  1.7863691
 1.5637536  1.5269473  1.4337908  1.3644934  0.89119655 1.3717082
 2.5790625  3.8381948  3.8671856  3.878041   3.7319024  4.073903
 3.9039738  3.9594297  4.170885   4.454166   4.6602144  4.388059
 4.452316   4.691259   4.910128   4.775737   4.075719   3.4055195
 3.492056   3.620023   3.8084407  3.816055   3.5056255  3.0033686
 3.0392556  3.619501   3.6382098  3.358097   2.685618   1.2728562
 0.5682142  0.91924417 1.1368779  0.7643839  0.569815   1.2628492
 1.2198128  1.2723405  1.4718186  1.6402336  1.7032498  1.728208
 1.7964386  1.8885331  2.0934525  2.3181221  2.3138306  2.2511146
 2.1632147  2.2415552  2.199254   2.392623   2.1426458  2.079813
 2.0037477  2.0217388  1.7179393  1.3146757  1.3082683  1.3811429
 1.3282087  0.97974503 1.3598716  2.433172   3.431234   3.552267
 3.5075176  3.671185   3.876071   3.7819653 ]
