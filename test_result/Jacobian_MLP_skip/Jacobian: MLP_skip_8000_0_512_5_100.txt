time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.12%, model saved.
Epoch: 0 Train: 9400.63672 Test: 4207.93164
Epoch: 80 Train: 2543.94629 Test: 1106.86731
Epoch: 160 Train: 2085.46265 Test: 781.49786
Epoch: 240 Train: 921.59418 Test: 245.73354
Epoch: 320 Train: 462.61649 Test: 90.97724
Epoch 400: New minimal relative error: 93.25%, model saved.
Epoch: 400 Train: 253.34126 Test: 55.52915
Epoch 480: New minimal relative error: 33.42%, model saved.
Epoch: 480 Train: 189.31535 Test: 28.18290
Epoch: 560 Train: 150.26666 Test: 36.57236
Epoch 640: New minimal relative error: 7.51%, model saved.
Epoch: 640 Train: 85.50830 Test: 9.47861
Epoch: 720 Train: 58.73509 Test: 8.44210
Epoch: 800 Train: 51.30611 Test: 5.55466
Epoch: 880 Train: 61.20938 Test: 25.63959
Epoch: 960 Train: 47.28390 Test: 13.47929
Epoch: 1040 Train: 34.48708 Test: 1.78646
Epoch 1120: New minimal relative error: 4.13%, model saved.
Epoch: 1120 Train: 30.97641 Test: 2.22007
Epoch: 1200 Train: 36.70335 Test: 8.49242
Epoch: 1280 Train: 35.65453 Test: 5.83645
Epoch: 1360 Train: 29.14145 Test: 3.10448
Epoch: 1440 Train: 29.03561 Test: 8.10181
Epoch: 1520 Train: 27.44711 Test: 4.67658
Epoch: 1600 Train: 38.64833 Test: 11.20854
Epoch: 1680 Train: 24.65116 Test: 3.06455
Epoch: 1760 Train: 20.20449 Test: 1.34500
Epoch 1840: New minimal relative error: 2.94%, model saved.
Epoch: 1840 Train: 19.31609 Test: 2.59368
Epoch 1920: New minimal relative error: 2.84%, model saved.
Epoch: 1920 Train: 18.82876 Test: 1.77729
Epoch 2000: New minimal relative error: 1.64%, model saved.
Epoch: 2000 Train: 16.56009 Test: 0.39880
Epoch: 2080 Train: 19.60529 Test: 5.10584
Epoch: 2160 Train: 30.29456 Test: 4.36893
Epoch: 2240 Train: 17.74644 Test: 1.69378
Epoch: 2320 Train: 18.24356 Test: 4.12792
Epoch: 2400 Train: 14.86848 Test: 1.01439
Epoch: 2480 Train: 14.26754 Test: 0.69100
Epoch: 2560 Train: 13.68960 Test: 1.09617
Epoch: 2640 Train: 18.77448 Test: 2.57044
Epoch: 2720 Train: 20.64746 Test: 6.98612
Epoch: 2800 Train: 13.55415 Test: 2.52517
Epoch: 2880 Train: 12.41656 Test: 2.63410
Epoch: 2960 Train: 14.42879 Test: 3.58312
Epoch: 3040 Train: 12.35678 Test: 1.18831
Epoch: 3120 Train: 13.98716 Test: 2.20565
Epoch: 3200 Train: 13.98234 Test: 2.06985
Epoch: 3280 Train: 16.65225 Test: 6.31012
Epoch: 3360 Train: 10.05545 Test: 0.35503
Epoch: 3440 Train: 10.02584 Test: 0.44272
Epoch: 3520 Train: 9.80906 Test: 0.32041
Epoch: 3600 Train: 11.89261 Test: 1.16116
Epoch: 3680 Train: 21.62378 Test: 7.76371
Epoch: 3760 Train: 9.14227 Test: 0.14215
Epoch 3840: New minimal relative error: 1.59%, model saved.
Epoch: 3840 Train: 8.98292 Test: 0.15746
Epoch: 3920 Train: 9.31891 Test: 0.69599
Epoch: 4000 Train: 12.10041 Test: 1.59239
Epoch: 4080 Train: 10.68761 Test: 1.80373
Epoch 4160: New minimal relative error: 1.55%, model saved.
Epoch: 4160 Train: 7.97033 Test: 0.10923
Epoch 4240: New minimal relative error: 0.86%, model saved.
Epoch: 4240 Train: 7.89270 Test: 0.13203
Epoch: 4320 Train: 8.19233 Test: 0.34093
Epoch: 4400 Train: 10.36717 Test: 3.14897
Epoch: 4480 Train: 7.50468 Test: 0.13755
Epoch: 4560 Train: 7.63356 Test: 0.23503
Epoch: 4640 Train: 7.18085 Test: 0.08995
Epoch: 4720 Train: 7.26387 Test: 0.33014
Epoch: 4800 Train: 7.24122 Test: 0.21731
Epoch: 4880 Train: 7.38274 Test: 0.22534
Epoch: 4960 Train: 6.80708 Test: 0.09477
Epoch: 5040 Train: 7.59931 Test: 0.38518
Epoch: 5120 Train: 7.61805 Test: 1.51657
Epoch: 5200 Train: 6.77361 Test: 0.12126
Epoch: 5280 Train: 6.56873 Test: 0.08743
Epoch: 5360 Train: 6.78809 Test: 0.56062
Epoch: 5440 Train: 7.31532 Test: 1.39109
Epoch: 5520 Train: 6.59893 Test: 0.22685
Epoch 5600: New minimal relative error: 0.83%, model saved.
Epoch: 5600 Train: 6.31517 Test: 0.10955
Epoch: 5680 Train: 7.53447 Test: 1.16179
Epoch: 5760 Train: 6.55667 Test: 0.31501
Epoch: 5840 Train: 6.50233 Test: 0.30659
Epoch: 5920 Train: 6.79148 Test: 0.63396
Epoch: 6000 Train: 6.93873 Test: 0.82047
Epoch: 6080 Train: 6.14830 Test: 0.20199
Epoch: 6160 Train: 6.00170 Test: 0.12327
Epoch: 6240 Train: 5.96500 Test: 0.14259
Epoch: 6320 Train: 6.29975 Test: 0.70617
Epoch: 6400 Train: 5.84233 Test: 0.08871
Epoch: 6480 Train: 6.28528 Test: 0.31983
Epoch: 6560 Train: 5.68917 Test: 0.06321
Epoch: 6640 Train: 5.82403 Test: 0.09059
Epoch: 6720 Train: 6.69886 Test: 0.42735
Epoch: 6800 Train: 6.20681 Test: 0.82043
Epoch: 6880 Train: 5.70148 Test: 0.35281
Epoch: 6960 Train: 5.79446 Test: 0.39073
Epoch: 7040 Train: 5.51889 Test: 0.11594
Epoch: 7120 Train: 5.57929 Test: 0.20552
Epoch: 7200 Train: 5.72187 Test: 0.42215
Epoch: 7280 Train: 5.37866 Test: 0.09177
Epoch: 7360 Train: 5.38447 Test: 0.06077
Epoch: 7440 Train: 5.36910 Test: 0.05679
Epoch: 7520 Train: 5.39950 Test: 0.08701
Epoch: 7600 Train: 5.40192 Test: 0.08705
Epoch: 7680 Train: 5.78029 Test: 0.22720
Epoch 7760: New minimal relative error: 0.63%, model saved.
Epoch: 7760 Train: 5.29166 Test: 0.05385
Epoch: 7840 Train: 6.46645 Test: 0.76308
Epoch: 7920 Train: 5.24952 Test: 0.05238
Epoch: 7999 Train: 5.13799 Test: 0.05146
Training Loss: tensor(5.1380)
Test Loss: tensor(0.0515)
Learned LE: [ 8.4663683e-01  8.4084906e-03 -1.4531361e+01]
True LE: [ 8.6082995e-01  3.3862288e-03 -1.4547920e+01]
Relative Error: [4.6373544  4.890755   4.954383   4.8355136  4.6796975  4.244671
 4.30042    4.5303235  4.5072007  3.7940476  3.3650246  2.7796185
 2.1006255  1.5432329  1.1337093  0.87473106 0.9636826  1.1060795
 1.2628949  1.3381954  1.5579109  1.568038   1.3499911  1.0385115
 0.87047553 0.8146327  1.0254457  1.128663   1.3236848  0.9588223
 0.7742438  0.95891154 1.4771221  1.9072738  2.397186   2.0938625
 1.7188675  1.6292344  1.621249   1.6064433  1.579375   1.8091617
 1.9469516  1.9218793  1.7840813  1.4654843  1.136737   1.1105322
 1.3318212  1.5315526  1.8263087  2.2996674  2.5109646  2.7898324
 3.21735    3.6132708  3.594702   3.7496004  3.9848971  4.1134963
 4.0433574  3.8674426  3.9228916  4.202601   4.3635464  4.241263
 4.1185675  3.9332051  3.7448187  3.9222984  3.9623759  4.0139055
 3.316519   3.1791308  2.7224252  2.0350268  1.4078066  0.91898626
 0.71227497 0.9444372  1.0920254  1.2970803  1.627459   1.7395861
 1.6449925  1.3257817  1.099413   0.911327   0.83750415 0.85304546
 0.9924283  1.0047888  0.61196727 0.597877   0.96204674 1.276694
 1.8306056  1.9816482  1.4172066  1.329037   1.52596    1.4316647
 1.2247115  1.3627434  1.5057156  1.5405906  1.52078    1.3362051
 0.9816887  0.8314427  0.9613637  1.2423637  1.5573797  2.07782
 2.2366395  2.5678997  2.9701884  3.2151713  3.2051213  3.213619
 3.5018916  3.6516006  3.6664948  3.713068   3.6234863  3.6744244
 3.773808   3.7334123  3.563367   3.4265347  3.0826504  3.0975516
 3.3099632  3.1636205  3.1218991  2.905463   2.6779068  2.533256
 1.9396653  1.3757749  0.8281243  0.6249141  0.952388   1.0700338
 1.3494462  1.7758827  1.7665956  1.6205039  1.3232661  1.0852027
 0.977794   0.90408677 0.8412048  0.8618461  0.82939696 0.6087398
 0.61679554 0.9668519  1.175715   1.7339382  1.518421   1.0957997
 1.0749354  1.3660469  1.0892024  0.9241761  1.1263616  1.2181588
 1.1903093  1.1538931  1.0049292  0.64388204 0.6919122  0.8137538
 1.0111036  1.5046269  1.780577   2.1672013  2.6345913  2.8508956
 2.8018692  2.846023   3.05106    3.197597   3.3121283  3.455376
 3.5853477  3.4686122  3.2586834  3.2037091  3.1112838  2.8120697
 2.558409   2.1201508  2.3331773  2.490441   2.3859217  2.320876
 2.5472143  2.1007667  2.0449     1.9839259  1.4723209  0.73748046
 0.47864342 0.7574068  0.87661827 1.2665291  1.7173846  1.6941868
 1.4617774  1.3120563  1.0257578  0.8292736  0.8887943  0.9582024
 0.9472259  0.8862136  0.7415346  0.6763152  0.9855465  1.1347339
 1.6347988  1.1766777  0.8952856  0.8769449  0.9650429  0.8786599
 0.7788993  0.907712   0.89305526 0.9250637  0.95831466 0.7511174
 0.61922276 0.73474854 0.67480767 0.780647   1.1225806  1.5159453
 2.1133072  2.4004526  2.4419546  2.4600358  2.605401   2.6871703
 2.8565307  3.0402126  3.1377878  3.25794    3.0995338  2.9123101
 2.7911253  2.5303454  2.1624455  1.7535372  1.2817243  1.4676898
 1.6464027  1.6606145  1.6288528  1.8923242  1.8439962  1.6302192
 1.7814026  1.58312    0.9338723  0.32431957 0.5125605  0.69213194
 1.0899122  1.4688764  1.5203904  1.3439665  1.208781   1.0222037
 0.7589563  0.6930842  0.9876614  1.0336066  1.0514457  1.0122247
 0.8903452  1.0685716  1.1845177  1.4801824  0.996411   0.71188354
 0.5367493  0.5174666  0.5861747  0.5882593  0.6494545  0.65483415
 0.80568033 0.8638465  0.78524846 0.80650467 0.8780127  0.59192556
 0.49092472 0.6465133  1.1377918  1.7514858  1.9371595  1.9434594
 2.0590463  2.262293   2.456761   2.5608187  2.28847    2.2037017
 2.203913   2.1835434  2.161692   1.8557631  1.9923494  1.8244737
 1.3332871  0.87011105 0.81309164 0.90303886 0.9282859  1.11154
 1.2014264  1.5617067  1.4946398  1.4814849  1.5785532  1.1942666
 0.50995314 0.24544008 0.49675825 0.8001879  1.1296568  1.2759342
 1.2257519  1.1703911  1.0676118  0.7345694  0.5293634  0.7251543
 1.1199964  0.99551445 1.0031643  0.89325476 0.84479165 1.037672
 1.5649866  1.1202323  0.94766617 0.33143836 0.25005722 0.37599617
 0.36377603 0.36558685 0.580759   0.68539387 0.88716567 0.97281617
 0.85233253 0.97164977 0.7351139  0.4583419  0.42578393 0.74776804
 1.3490316  1.5889382  1.4713664  1.5600811  1.7732205  1.6681213
 1.4476842  1.2462571  1.2231345  1.3097682  1.4933307  1.3393276
 0.9302035  0.73336476 0.76069874 0.7862914  0.78354    0.5781445
 0.6381638  0.67085207 0.736041   0.8552325  0.86226416 1.2576547
 1.3607198  1.2959545  1.2582296  1.0537341  0.49133202 0.22200334
 0.40809762 0.76029104 0.92751247 0.9943326 ]
