time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 5000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 2
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 24.569692612 Test: 16.489576340
Epoch 0: New minimal relative error: 16.49%, model saved.
Epoch: 50 Train: 3.506473541 Test: 3.507340670
Epoch 50: New minimal relative error: 3.51%, model saved.
Epoch: 100 Train: 3.458041191 Test: 3.458879471
Epoch 100: New minimal relative error: 3.46%, model saved.
Epoch: 150 Train: 3.457373619 Test: 3.458261013
Epoch 150: New minimal relative error: 3.46%, model saved.
Epoch: 200 Train: 3.450680017 Test: 3.449796677
Epoch 200: New minimal relative error: 3.45%, model saved.
Epoch: 250 Train: 3.422507286 Test: 3.430387974
Epoch 250: New minimal relative error: 3.43%, model saved.
Epoch: 300 Train: 3.422417164 Test: 3.429966450
Epoch 300: New minimal relative error: 3.43%, model saved.
Epoch: 350 Train: 3.430556297 Test: 3.438832045
Epoch: 400 Train: 3.428710222 Test: 3.438911915
Epoch: 450 Train: 3.429122210 Test: 3.442909718
Epoch: 500 Train: 3.425641060 Test: 3.439836025
Epoch: 550 Train: 3.433769703 Test: 3.448976517
Epoch: 600 Train: 3.420797825 Test: 3.437260389
Epoch: 650 Train: 3.411794186 Test: 3.435707569
Epoch: 700 Train: 3.399830341 Test: 3.416983128
Epoch 700: New minimal relative error: 3.42%, model saved.
Epoch: 750 Train: 3.404143333 Test: 3.411530256
Epoch 750: New minimal relative error: 3.41%, model saved.
Epoch: 800 Train: 3.396716118 Test: 3.405330658
Epoch 800: New minimal relative error: 3.41%, model saved.
Epoch: 850 Train: 3.395418644 Test: 3.405722618
Epoch: 900 Train: 3.418760300 Test: 3.430179119
Epoch: 950 Train: 3.465685368 Test: 3.476674080
Epoch: 1000 Train: 3.434473515 Test: 3.435592651
Epoch: 1050 Train: 3.423414230 Test: 3.433473110
Epoch: 1100 Train: 3.432734966 Test: 3.441393375
Epoch: 1150 Train: 3.448805094 Test: 3.453400612
Epoch: 1200 Train: 3.457887173 Test: 3.461122036
Epoch: 1250 Train: 3.465097427 Test: 3.481340885
Epoch: 1300 Train: 3.478063822 Test: 3.492653608
Epoch: 1350 Train: 3.467434406 Test: 3.479305267
Epoch: 1400 Train: 3.452638149 Test: 3.471567154
Epoch: 1450 Train: 3.441946268 Test: 3.460055113
Epoch: 1500 Train: 3.443432808 Test: 3.460038424
Epoch: 1550 Train: 3.438045979 Test: 3.456371307
Epoch: 1600 Train: 3.425788164 Test: 3.444700480
Epoch: 1650 Train: 3.419466019 Test: 3.437691689
Epoch: 1700 Train: 3.415371418 Test: 3.433448315
Epoch: 1750 Train: 3.414068222 Test: 3.437063217
Epoch: 1800 Train: 3.411713600 Test: 3.437711477
Epoch: 1850 Train: 3.414366245 Test: 3.433278084
Epoch: 1900 Train: 3.413675785 Test: 3.434864998
Epoch: 1950 Train: 3.404477119 Test: 3.428196430
Epoch: 2000 Train: 3.404305220 Test: 3.427891493
Epoch: 2050 Train: 3.402423859 Test: 3.426410913
Epoch: 2100 Train: 3.398838997 Test: 3.421224117
Epoch: 2150 Train: 3.399750710 Test: 3.418846607
Epoch: 2200 Train: 3.400614262 Test: 3.421282768
Epoch: 2250 Train: 3.401696205 Test: 3.422643900
Epoch: 2300 Train: 3.402039051 Test: 3.420970678
Epoch: 2350 Train: 3.409142494 Test: 3.429998398
Epoch: 2400 Train: 3.411537170 Test: 3.431312084
Epoch: 2450 Train: 3.409729958 Test: 3.424036503
Epoch: 2500 Train: 3.412320614 Test: 3.429617643
Epoch: 2550 Train: 3.416751862 Test: 3.436784744
Epoch: 2600 Train: 3.427205324 Test: 3.445922852
Epoch: 2650 Train: 3.435288906 Test: 3.452009916
Epoch: 2700 Train: 3.439247370 Test: 3.451458216
Epoch: 2750 Train: 3.435510635 Test: 3.451501131
Epoch: 2800 Train: 3.436452389 Test: 3.456040382
Epoch: 2850 Train: 3.443101883 Test: 3.457707644
Epoch: 2900 Train: 3.445003986 Test: 3.464016199
Epoch: 2950 Train: 3.451612949 Test: 3.467893600
Epoch: 3000 Train: 3.455497742 Test: 3.468472242
Epoch: 3050 Train: 3.450767040 Test: 3.462973595
Epoch: 3100 Train: 3.443887234 Test: 3.456881046
Epoch: 3150 Train: 3.445016861 Test: 3.455703020
Epoch: 3200 Train: 3.449792385 Test: 3.467800140
Epoch: 3250 Train: 3.449617386 Test: 3.466973782
Epoch: 3300 Train: 3.430113316 Test: 3.448332787
Epoch: 3350 Train: 3.434281111 Test: 3.453654766
Epoch: 3400 Train: 3.430034161 Test: 3.446829796
Epoch: 3450 Train: 3.427484512 Test: 3.445400476
Epoch: 3500 Train: 3.426031113 Test: 3.441634655
Epoch: 3550 Train: 3.424232483 Test: 3.442189217
Epoch: 3600 Train: 3.425775051 Test: 3.443310738
Epoch: 3650 Train: 3.428111076 Test: 3.444905996
Epoch: 3700 Train: 3.429415226 Test: 3.447938442
Epoch: 3750 Train: 3.432671309 Test: 3.450549603
Epoch: 3800 Train: 3.435020924 Test: 3.456063747
Epoch: 3850 Train: 3.438702583 Test: 3.459654808
Epoch: 3900 Train: 3.442861319 Test: 3.461179495
Epoch: 3950 Train: 3.445033789 Test: 3.464246273
Epoch: 4000 Train: 3.444807529 Test: 3.463130951
Epoch: 4050 Train: 3.445512772 Test: 3.464530468
Epoch: 4100 Train: 3.446419239 Test: 3.464122772
Epoch: 4150 Train: 3.449129105 Test: 3.467780352
Epoch: 4200 Train: 3.449445248 Test: 3.469348907
Epoch: 4250 Train: 3.451524258 Test: 3.471918821
Epoch: 4300 Train: 3.450465679 Test: 3.471847057
Epoch: 4350 Train: 3.451283455 Test: 3.471055269
Epoch: 4400 Train: 3.450507164 Test: 3.470992327
Epoch: 4450 Train: 3.449796200 Test: 3.469226360
Epoch: 4500 Train: 3.448223352 Test: 3.467877865
Epoch: 4550 Train: 3.449041128 Test: 3.468246937
Epoch: 4600 Train: 3.449282169 Test: 3.469700813
Epoch: 4650 Train: 3.448516846 Test: 3.470527887
Epoch: 4700 Train: 3.448230267 Test: 3.470996857
Epoch: 4750 Train: 3.448037624 Test: 3.471979380
Epoch: 4800 Train: 3.448415279 Test: 3.473843098
Epoch: 4850 Train: 3.450089455 Test: 3.476906300
Epoch: 4900 Train: 3.453763962 Test: 3.477756023
Epoch: 4950 Train: 3.456003189 Test: 3.480709314
Epoch: 4999 Train: 3.459656477 Test: 3.482896328
Training Loss: tensor(3.4597)
Test Loss: tensor(3.4829)
True Mean x: tensor(3.4447, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.5065, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0022)
Jacobian term Test Loss: tensor(0.0022)
Learned LE: [1.6132617  0.31525478]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
