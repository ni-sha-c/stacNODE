time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.44%, model saved.
Epoch: 0 Train: 60333.15625 Test: 4205.45947
Epoch: 100 Train: 16149.55176 Test: 1596.22888
Epoch: 200 Train: 15800.33105 Test: 1426.48120
Epoch: 300 Train: 13210.43750 Test: 1322.89636
Epoch 400: New minimal relative error: 88.59%, model saved.
Epoch: 400 Train: 14432.51172 Test: 1443.41919
Epoch 500: New minimal relative error: 68.21%, model saved.
Epoch: 500 Train: 15808.30664 Test: 1465.31030
Epoch 600: New minimal relative error: 61.90%, model saved.
Epoch: 600 Train: 13041.27148 Test: 1166.97656
Epoch: 700 Train: 11768.38086 Test: 920.68372
Epoch: 800 Train: 8715.72852 Test: 515.90552
Epoch: 900 Train: 7370.76953 Test: 390.23206
Epoch: 1000 Train: 4738.48047 Test: 154.55699
Epoch 1100: New minimal relative error: 34.70%, model saved.
Epoch: 1100 Train: 2064.24707 Test: 48.58473
Epoch 1200: New minimal relative error: 29.53%, model saved.
Epoch: 1200 Train: 1232.03345 Test: 18.38825
Epoch: 1300 Train: 964.85651 Test: 10.65330
Epoch 1400: New minimal relative error: 14.29%, model saved.
Epoch: 1400 Train: 800.86658 Test: 6.73872
Epoch: 1500 Train: 756.75439 Test: 44.33578
Epoch: 1600 Train: 613.35059 Test: 15.10565
Epoch: 1700 Train: 564.90869 Test: 7.45394
Epoch: 1800 Train: 597.65979 Test: 4.75230
Epoch 1900: New minimal relative error: 9.35%, model saved.
Epoch: 1900 Train: 527.75195 Test: 4.35224
Epoch: 2000 Train: 576.41663 Test: 10.56970
Epoch: 2100 Train: 467.12057 Test: 3.72220
Epoch: 2200 Train: 479.43539 Test: 3.96717
Epoch: 2300 Train: 409.46442 Test: 2.43704
Epoch: 2400 Train: 402.51880 Test: 3.47871
Epoch: 2500 Train: 385.98917 Test: 4.92514
Epoch: 2600 Train: 368.25320 Test: 2.40898
Epoch: 2700 Train: 388.47958 Test: 6.33693
Epoch: 2800 Train: 342.13065 Test: 3.57939
Epoch: 2900 Train: 343.69757 Test: 2.85052
Epoch 3000: New minimal relative error: 8.07%, model saved.
Epoch: 3000 Train: 296.69287 Test: 1.36182
Epoch: 3100 Train: 260.38651 Test: 1.27774
Epoch: 3200 Train: 263.60880 Test: 1.47919
Epoch: 3300 Train: 253.51457 Test: 1.20795
Epoch: 3400 Train: 264.93076 Test: 1.56777
Epoch: 3500 Train: 250.15854 Test: 1.58238
Epoch: 3600 Train: 231.72623 Test: 1.39419
Epoch: 3700 Train: 235.48558 Test: 2.79002
Epoch: 3800 Train: 260.95975 Test: 2.34956
Epoch: 3900 Train: 267.53284 Test: 1.82438
Epoch: 4000 Train: 282.55875 Test: 2.41501
Epoch: 4100 Train: 243.67351 Test: 2.40586
Epoch 4200: New minimal relative error: 7.35%, model saved.
Epoch: 4200 Train: 237.31685 Test: 1.52235
Epoch 4300: New minimal relative error: 6.57%, model saved.
Epoch: 4300 Train: 265.01022 Test: 1.76285
Epoch: 4400 Train: 223.88039 Test: 1.29719
Epoch: 4500 Train: 228.51740 Test: 1.60348
Epoch: 4600 Train: 229.16360 Test: 1.67028
Epoch: 4700 Train: 221.50591 Test: 1.59925
Epoch: 4800 Train: 222.11191 Test: 1.39025
Epoch 4900: New minimal relative error: 4.18%, model saved.
Epoch: 4900 Train: 227.68974 Test: 1.57433
Epoch: 5000 Train: 230.01375 Test: 1.30688
Epoch: 5100 Train: 249.81039 Test: 2.05431
Epoch: 5200 Train: 199.63142 Test: 1.19876
Epoch: 5300 Train: 205.15016 Test: 1.19023
Epoch: 5400 Train: 209.35443 Test: 1.17453
Epoch: 5500 Train: 213.31923 Test: 1.58884
Epoch: 5600 Train: 198.64258 Test: 1.24007
Epoch: 5700 Train: 164.64285 Test: 1.48987
Epoch: 5800 Train: 156.33846 Test: 0.48502
Epoch: 5900 Train: 160.20435 Test: 0.80273
Epoch: 6000 Train: 188.26349 Test: 1.42572
Epoch: 6100 Train: 170.63306 Test: 1.16628
Epoch: 6200 Train: 209.49146 Test: 2.49875
Epoch: 6300 Train: 176.13763 Test: 0.95597
Epoch: 6400 Train: 173.60475 Test: 1.54693
Epoch: 6500 Train: 167.18080 Test: 0.93482
Epoch: 6600 Train: 171.71072 Test: 1.40336
Epoch: 6700 Train: 148.34152 Test: 1.02262
Epoch: 6800 Train: 145.31317 Test: 0.96244
Epoch: 6900 Train: 177.19463 Test: 1.46209
Epoch: 7000 Train: 154.12621 Test: 1.61681
Epoch: 7100 Train: 149.95688 Test: 0.83541
Epoch: 7200 Train: 150.12546 Test: 0.99845
Epoch: 7300 Train: 124.92247 Test: 0.60469
Epoch: 7400 Train: 128.98141 Test: 0.66791
Epoch: 7500 Train: 124.41046 Test: 0.59182
Epoch: 7600 Train: 127.48794 Test: 0.59447
Epoch: 7700 Train: 124.18027 Test: 0.70367
Epoch: 7800 Train: 124.26354 Test: 1.05452
Epoch: 7900 Train: 114.30025 Test: 0.61384
Epoch: 8000 Train: 120.81038 Test: 0.51683
Epoch: 8100 Train: 104.98373 Test: 1.16099
Epoch: 8200 Train: 99.61546 Test: 0.27868
Epoch: 8300 Train: 105.91225 Test: 1.44137
Epoch: 8400 Train: 106.68141 Test: 1.14636
Epoch: 8500 Train: 105.75440 Test: 0.57089
Epoch: 8600 Train: 105.86253 Test: 0.32232
Epoch: 8700 Train: 95.73966 Test: 0.22895
Epoch: 8800 Train: 90.08892 Test: 0.32295
Epoch: 8900 Train: 101.55200 Test: 0.45448
Epoch: 9000 Train: 102.02469 Test: 0.31442
Epoch: 9100 Train: 92.05223 Test: 0.22493
Epoch: 9200 Train: 91.00783 Test: 0.30691
Epoch: 9300 Train: 90.51802 Test: 0.41028
Epoch 9400: New minimal relative error: 3.37%, model saved.
Epoch: 9400 Train: 89.46673 Test: 0.23888
Epoch: 9500 Train: 86.82601 Test: 0.24634
Epoch: 9600 Train: 88.76643 Test: 0.21544
Epoch: 9700 Train: 91.69192 Test: 0.30793
Epoch: 9800 Train: 94.59560 Test: 0.49890
Epoch: 9900 Train: 93.99468 Test: 0.44639
Epoch: 9999 Train: 94.59824 Test: 0.34487
Training Loss: tensor(94.5982)
Test Loss: tensor(0.3449)
Learned LE: [  0.8208525   0.0740684 -14.545364 ]
True LE: [ 8.8197899e-01  2.1636169e-03 -1.4556835e+01]
Relative Error: [7.90869    8.144772   8.481028   8.936196   9.476273   9.723655
 9.653158   9.251734   8.917745   8.638729   8.272898   7.8539395
 7.421565   6.9010324  6.4526396  5.9320116  5.3947377  4.8624077
 4.6644783  4.568822   4.3949018  3.9435322  3.6399577  3.4657042
 3.3623435  3.3325932  2.8461494  2.4993465  2.4533505  2.6239536
 2.849444   3.091162   3.3976157  3.9889429  4.6235304  5.133931
 5.6691995  6.3495703  6.5277414  6.5666027  6.7703137  7.0054054
 7.323579   7.4632406  7.548388   7.6268187  7.5897307  7.405343
 7.1505375  6.790933   6.489295   6.1030993  5.70158    5.260693
 4.8100367  4.6697803  4.7812896  5.110412   5.8087     6.2267413
 6.454946   6.949398   7.2210507  7.5038114  7.923846   8.531601
 9.063654   9.225996   9.1890335  8.92844    8.500695   8.30917
 8.031874   7.6363115  7.239444   6.890105   6.3580856  5.771168
 5.1441746  4.5166464  4.2117305  4.030382   4.084571   3.7924998
 3.3493035  3.0639803  2.8758693  2.7770786  2.4371893  2.1214266
 2.0707288  2.2428892  2.416753   2.5174768  2.5856497  3.2755628
 3.9575331  4.415921   4.908479   5.548995   5.8603697  5.9299684
 6.1298923  6.3383737  6.568162   6.7723765  6.933329   7.089354
 7.1207924  6.937881   6.58763    6.23349    5.9373784  5.5632443
 5.1672225  4.804955   4.3498096  4.173474   4.382442   4.8079643
 5.505536   5.9075     6.1243663  6.523022   6.6791     6.9154196
 7.342668   7.992089   8.397363   8.59701    8.559745   8.360144
 8.041803   7.7473536  7.6264777  7.3570013  7.111804   6.853831
 6.53564    5.872968   5.1446567  4.412036   3.9741545  3.5946507
 3.55668    3.6034324  3.2065935  2.7704837  2.4727967  2.3452058
 2.1481416  1.8104658  1.7325451  1.9556069  2.1354136  2.164658
 2.144805   2.7740037  3.3606687  3.7303588  4.1551156  4.6330237
 5.0932016  5.254453   5.4293103  5.5881257  5.7913055  6.077827
 6.3155165  6.5584645  6.6853657  6.523665   6.171685   5.7684226
 5.478439   5.1070857  4.7260094  4.3110027  3.976463   3.8653328
 4.0325527  4.49173    5.154051   5.5530987  5.7611165  6.199708
 6.2652736  6.520817   6.916446   7.3894978  7.4956136  7.702569
 7.891921   7.8913713  7.684473   7.252261   6.8434024  6.531277
 6.256186   6.10189    6.1894083  6.2242875  5.4381323  4.576173
 3.9741218  3.3951666  3.1449218  3.0846558  3.091659   2.6420302
 2.2095716  1.999007   1.8854668  1.5930433  1.420235   1.7196413
 1.9856856  2.034965   2.008073   2.5378668  2.9316587  3.1352248
 3.3920727  3.7521799  4.212023   4.483775   4.695051   4.802475
 5.003333   5.3882546  5.675447   6.008031   6.233923   6.1561136
 5.8592925  5.4300656  5.119977   4.749198   4.3718653  3.9097977
 3.6261168  3.628562   3.699325   4.056952   4.7700696  5.26033
 5.422455   5.836687   5.9469557  6.1322494  6.5455256  6.7160406
 6.7164807  6.8515596  7.1053185  7.367362   6.941215   6.399743
 5.891904   5.6166577  5.339467   5.1340146  5.075488   5.2838326
 5.454711   4.8915973  4.2673683  3.5390987  2.9940133  2.7045422
 2.6376653  2.5728989  2.1006289  1.7614903  1.5977846  1.3842396
 1.2190664  1.3163372  1.7999103  2.0574565  2.0753572  2.572984
 2.8026047  2.91369    2.7988243  2.9280624  3.2636423  3.6029372
 3.89396    3.9947515  4.2230783  4.613635   5.0052576  5.411185
 5.7499676  5.801338   5.614329   5.1729436  4.7429204  4.316227
 3.9109712  3.5246599  3.3453207  3.2622933  3.3779473  3.5765524
 4.2196164  4.9516773  5.081399   5.463838   5.622754   5.794107
 6.043103   6.0899777  6.077115   6.045308   6.14971    5.9873013
 5.7344394  5.494609   5.1445994  4.89338    4.652802   4.457534
 4.2889633  4.304022   4.515413   4.7482224  4.283497   3.9280248
 3.2527785  2.6357663  2.31041    2.1536226  2.0305557  1.6304609
 1.3368446  1.1371808  0.95694524 1.0791253  1.4639486  1.9387821
 2.1004748  2.7579598  3.0111458  2.960122   2.738574   2.402558
 2.3347518  2.5451677  2.9235816  3.1373236  3.3964238  3.817442
 4.289759   4.746553   5.1974545  5.4183354  5.311488   4.819128
 4.3867564  3.987982   3.4981756  3.102741   2.8614652  2.792833
 2.8818595  3.0853858  3.5519388  4.5375423  4.7315626  5.04082
 5.319866   5.461262   5.6042747  5.425856   5.2857413  5.3226523
 5.1289544  4.73526    4.566989   4.3543854  4.2551546  4.0090346
 3.8803346  3.8558304  3.8467717  3.7291732  3.7218413  3.8415372
 4.0834246  3.8336926  3.4211342  3.0776742  2.3808277  1.9203898
 1.6838843  1.5620941  1.2450204  0.91549194]
