time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.44%, model saved.
Epoch: 0 Train: 60333.15625 Test: 4205.46045
Epoch: 80 Train: 16263.73633 Test: 1638.10449
Epoch: 160 Train: 15016.53418 Test: 1427.81702
Epoch 240: New minimal relative error: 101.79%, model saved.
Epoch: 240 Train: 14468.19043 Test: 1478.30408
Epoch: 320 Train: 16340.78320 Test: 1420.32874
Epoch 400: New minimal relative error: 71.07%, model saved.
Epoch: 400 Train: 14662.34473 Test: 1434.55518
Epoch 480: New minimal relative error: 63.34%, model saved.
Epoch: 480 Train: 15864.36426 Test: 1479.83191
Epoch: 560 Train: 14075.24512 Test: 1183.15796
Epoch: 640 Train: 12909.06543 Test: 1110.12976
Epoch: 720 Train: 11445.18164 Test: 917.41791
Epoch 800: New minimal relative error: 61.91%, model saved.
Epoch: 800 Train: 10565.98633 Test: 701.04517
Epoch: 880 Train: 8490.18164 Test: 514.27319
Epoch: 960 Train: 6775.68799 Test: 326.03519
Epoch: 1040 Train: 5601.35303 Test: 218.35834
Epoch: 1120 Train: 3213.12158 Test: 79.79946
Epoch 1200: New minimal relative error: 23.36%, model saved.
Epoch: 1200 Train: 1447.40845 Test: 27.66098
Epoch 1280: New minimal relative error: 11.89%, model saved.
Epoch: 1280 Train: 989.55231 Test: 15.30192
Epoch: 1360 Train: 763.59851 Test: 8.72643
Epoch: 1440 Train: 658.99927 Test: 7.05009
Epoch: 1520 Train: 783.82037 Test: 10.10291
Epoch: 1600 Train: 666.11340 Test: 7.67810
Epoch: 1680 Train: 620.02875 Test: 7.85772
Epoch: 1760 Train: 583.55786 Test: 6.87026
Epoch: 1840 Train: 534.36462 Test: 4.25158
Epoch: 1920 Train: 449.03644 Test: 4.72695
Epoch: 2000 Train: 421.53976 Test: 3.27057
Epoch: 2080 Train: 395.28433 Test: 3.45963
Epoch: 2160 Train: 369.13950 Test: 2.41656
Epoch: 2240 Train: 379.63614 Test: 3.24572
Epoch: 2320 Train: 398.59119 Test: 3.47360
Epoch: 2400 Train: 374.48355 Test: 5.40322
Epoch: 2480 Train: 358.28699 Test: 11.23538
Epoch: 2560 Train: 313.14655 Test: 1.99908
Epoch 2640: New minimal relative error: 11.84%, model saved.
Epoch: 2640 Train: 328.76785 Test: 2.83031
Epoch: 2720 Train: 358.45105 Test: 3.61150
Epoch: 2800 Train: 360.89410 Test: 2.84370
Epoch: 2880 Train: 406.13455 Test: 6.83580
Epoch: 2960 Train: 311.50757 Test: 2.29170
Epoch: 3040 Train: 338.50507 Test: 2.89329
Epoch: 3120 Train: 363.08221 Test: 4.75295
Epoch: 3200 Train: 348.32092 Test: 3.40735
Epoch: 3280 Train: 353.01056 Test: 4.05765
Epoch: 3360 Train: 308.14206 Test: 2.36044
Epoch: 3440 Train: 277.15659 Test: 1.71992
Epoch 3520: New minimal relative error: 10.60%, model saved.
Epoch: 3520 Train: 283.39566 Test: 1.57036
Epoch: 3600 Train: 267.60547 Test: 3.27976
Epoch: 3680 Train: 308.66840 Test: 3.06446
Epoch 3760: New minimal relative error: 6.65%, model saved.
Epoch: 3760 Train: 258.39026 Test: 1.54368
Epoch: 3840 Train: 243.81883 Test: 1.19210
Epoch 3920: New minimal relative error: 5.86%, model saved.
Epoch: 3920 Train: 268.09802 Test: 1.81037
Epoch: 4000 Train: 290.44904 Test: 2.30205
Epoch: 4080 Train: 289.96970 Test: 4.69147
Epoch: 4160 Train: 273.53442 Test: 1.80895
Epoch: 4240 Train: 289.97058 Test: 2.62138
Epoch: 4320 Train: 250.11055 Test: 2.26480
Epoch: 4400 Train: 208.13713 Test: 1.14730
Epoch: 4480 Train: 288.73093 Test: 2.51668
Epoch: 4560 Train: 261.92532 Test: 3.14058
Epoch: 4640 Train: 254.07455 Test: 2.04383
Epoch: 4720 Train: 265.23148 Test: 1.90707
Epoch: 4800 Train: 248.08301 Test: 1.70678
Epoch: 4880 Train: 249.53281 Test: 1.78085
Epoch: 4960 Train: 225.16846 Test: 1.71811
Epoch: 5040 Train: 245.00529 Test: 2.41759
Epoch: 5120 Train: 228.42186 Test: 1.78328
Epoch: 5200 Train: 200.36234 Test: 1.36108
Epoch: 5280 Train: 210.30618 Test: 1.36972
Epoch: 5360 Train: 217.72964 Test: 1.91985
Epoch: 5440 Train: 221.07951 Test: 1.42661
Epoch: 5520 Train: 193.23239 Test: 1.10661
Epoch: 5600 Train: 184.21765 Test: 0.91194
Epoch: 5680 Train: 202.38025 Test: 1.59783
Epoch: 5760 Train: 244.67905 Test: 2.08642
Epoch: 5840 Train: 210.60562 Test: 1.38365
Epoch: 5920 Train: 187.20853 Test: 1.19976
Epoch 6000: New minimal relative error: 3.73%, model saved.
Epoch: 6000 Train: 174.99228 Test: 0.70315
Epoch: 6080 Train: 168.62479 Test: 0.76716
Epoch: 6160 Train: 181.53223 Test: 1.10158
Epoch: 6240 Train: 174.38960 Test: 0.89981
Epoch: 6320 Train: 171.70401 Test: 1.00706
Epoch: 6400 Train: 248.83969 Test: 2.58236
Epoch: 6480 Train: 179.71236 Test: 1.02531
Epoch: 6560 Train: 149.21207 Test: 0.59386
Epoch: 6640 Train: 145.66081 Test: 0.47822
Epoch: 6720 Train: 146.14026 Test: 0.81483
Epoch: 6800 Train: 141.08499 Test: 0.75331
Epoch: 6880 Train: 142.39432 Test: 0.72050
Epoch: 6960 Train: 139.68640 Test: 2.94858
Epoch: 7040 Train: 140.37617 Test: 0.63244
Epoch: 7120 Train: 126.78140 Test: 0.63363
Epoch: 7200 Train: 124.08353 Test: 0.45651
Epoch: 7280 Train: 112.31477 Test: 0.46320
Epoch: 7360 Train: 110.21106 Test: 0.33681
Epoch: 7440 Train: 110.20795 Test: 0.38646
Epoch: 7520 Train: 109.50550 Test: 0.41073
Epoch: 7600 Train: 101.00776 Test: 0.30568
Epoch: 7680 Train: 110.97615 Test: 0.47141
Epoch: 7760 Train: 105.54871 Test: 0.37024
Epoch: 7840 Train: 118.92631 Test: 0.57588
Epoch: 7920 Train: 90.29955 Test: 0.27238
Epoch: 7999 Train: 87.68735 Test: 0.20675
Training Loss: tensor(87.6873)
Test Loss: tensor(0.2067)
Learned LE: [  0.9120259   -0.03639746 -14.539771  ]
True LE: [ 8.8197857e-01  2.1635930e-03 -1.4556834e+01]
Relative Error: [ 4.569064    5.4602985   6.4786725   7.5550594   8.552696    9.4768915
 10.33525    11.16511    11.504293   11.585558   11.33079    11.364046
 11.475663   11.571364   10.889823    9.770669    8.340438    6.509324
  4.651645    3.0509229   1.6434003   0.8298634   1.6254375   2.5477958
  3.3505902   3.9712906   4.467226    4.829963    5.061036    5.1660037
  5.1683006   6.5769806   8.172569    9.680553   10.842363   11.852407
 12.682006   13.3198185  13.754559   14.066446   14.098586   13.946426
 13.612652   13.248274   12.778352   11.840557   10.437031    8.894214
  7.310915    5.6714697   3.8454835   1.9602302   0.9812413   2.6140227
  2.1747184   1.8741282   1.8385129   2.0403194   2.428556    2.97171
  3.4281633   3.8866441   4.610961    5.584389    6.6909924   7.94891
  9.203594   10.276496   10.2553005  10.261848   10.475585   10.639307
 10.496128   10.435462   10.505432   10.765261   10.989269   10.925017
  9.457153    7.5755916   5.6106887   3.9007003   2.2986066   0.9186197
  1.0963527   1.9603007   2.858996    3.4897952   4.051721    4.481816
  4.7586346   4.906958    4.944952    6.3576474   7.979157    9.473607
 10.80432    11.750099   12.544518   13.133051   13.538519   13.808535
 13.858639   13.70375    13.40149    13.030791   12.632868   11.767869
 10.296572    8.855261    7.4012203   5.8247824   4.108069    2.3123546
  0.8534839   2.2239103   2.237535    1.8937254   1.8707771   2.1174088
  2.5663872   3.1764553   3.5156493   4.0262475   4.727864    5.6743803
  6.870283    8.215901    9.407787    9.425482    9.370138    9.370594
  9.5844      9.807213    9.88342     9.652715    9.711131    9.919475
 10.173074   10.20474     9.975234    8.920095    6.863634    4.9935994
  3.23844     1.58212     0.7044788   1.3219349   2.2199738   2.9279318
  3.5960448   4.125383    4.484238    4.680638    4.7586203   6.0239716
  7.6475115   9.141207   10.448649   11.565575   12.305206   12.865983
 13.234727   13.463948   13.530195   13.403649   13.117666   12.731963
 12.416362   11.668795   10.311003    8.832197    7.4864917   6.024416
  4.4543824   2.7614691   1.1185683   1.470437    2.3439672   1.9676348
  1.915382    2.169301    2.660522    3.1998954   3.5825112   4.153985
  4.916152    5.844777    7.0082335   8.364612    8.266798    8.341588
  8.552748    8.636391    8.744407    8.85727     8.997397    8.813683
  8.86527     9.112439    9.473012    9.622006    9.424198    8.892201
  8.24692     6.4146886   4.483135    2.6839795   1.3713287   0.78354704
  1.4759114   2.268823    3.0664098   3.7203352   4.183525    4.4688
  4.6067243   5.5664654   7.1981573   8.694416    9.986485   11.1020355
 12.039973   12.603977   12.932851   13.082032   13.106484   13.027203
 12.757203   12.397758   12.103365   11.52492    10.354278    8.80899
  7.5712075   6.252987    4.847188    3.326704    1.7090871   0.77686244
  2.5814059   2.1225362   2.025668    2.2638578   2.7674525   3.2240624
  3.6061962   4.231672    5.0476575   6.0375276   7.183377    7.4460635
  7.2656937   7.339091    7.5778017   7.737107    7.6802754   7.8341246
  7.9298973   8.00864     7.834325    8.034383    8.435537    8.885538
  9.004928    8.618264    7.8729405   7.320563    6.096036    4.185175
  2.6558459   1.2861931   0.763387    1.4757282   2.3966885   3.2068188
  3.8105764   4.2139816   4.4505653   4.971914    6.5970564   8.090608
  9.415479   10.537467   11.462679   12.231888   12.581761   12.719963
 12.712084   12.592341   12.337614   12.0098095  11.675925   11.309159
 10.401844    8.942693    7.630498    6.478046    5.259154    3.9401195
  2.493152    0.96280175  1.633723    2.485707    2.2530994   2.3760862
  2.7966368   3.208759    3.6167848   4.2242017   5.011813    6.07383
  7.305854    6.813681    6.5571747   6.5245996   6.546493    6.56143
  6.7336826   6.856667    7.0127      7.2428436   7.190503    7.159159
  7.4188166   7.8882565   8.158241    8.107021    7.7993274   7.122938
  6.5052233   6.0881214   4.361908    2.715778    1.2258868   0.7219889
  1.5013597   2.5009804   3.283988    3.8423016   4.212283    4.447076
  5.770773    7.2842607   8.678406    9.860625   10.803514   11.539618
 12.139857   12.298787   12.282489   12.138501   11.908394   11.53099
 11.185308   10.934784   10.340355    9.10423     7.6626077   6.656608
  5.6397023   4.5474176   3.3375335   1.8362631   0.81078064  2.6917338
  2.5257716   2.4330251   2.7394612   3.1534243   3.5692294   4.205813
  5.0411687   6.0554996   6.9366975   6.3399024   5.8787465   5.7018194
  5.429921    5.3041553   5.4859567   5.837451    6.272062    6.418696
  6.5463805   6.4411416   6.5848393   7.012876    7.4868693   7.5336003
  7.3302455   6.9425726   6.5547748   5.899512    5.779066    4.705593
  2.907287    1.3210106   0.37886852  1.4894286 ]
