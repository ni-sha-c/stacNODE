time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.05%, model saved.
Epoch: 0 Train: 59591.64844 Test: 4024.12744
Epoch: 80 Train: 12784.11816 Test: 1499.69507
Epoch: 160 Train: 14023.39844 Test: 1531.08154
Epoch 240: New minimal relative error: 63.53%, model saved.
Epoch: 240 Train: 11487.53223 Test: 1252.03015
Epoch: 320 Train: 14226.10938 Test: 2003.78735
Epoch 400: New minimal relative error: 60.47%, model saved.
Epoch: 400 Train: 13099.03613 Test: 1292.16882
Epoch: 480 Train: 12313.96387 Test: 1179.33301
Epoch: 560 Train: 11667.84766 Test: 1075.53857
Epoch: 640 Train: 13656.45508 Test: 1174.18152
Epoch: 720 Train: 12631.58594 Test: 2025.46741
Epoch: 800 Train: 12139.71680 Test: 1215.48804
Epoch: 880 Train: 11278.07324 Test: 1023.73096
Epoch: 960 Train: 11206.09766 Test: 984.94513
Epoch: 1040 Train: 10192.72949 Test: 1024.33057
Epoch: 1120 Train: 10029.80078 Test: 829.24969
Epoch: 1200 Train: 10864.25879 Test: 924.37048
Epoch: 1280 Train: 9716.55859 Test: 830.17511
Epoch: 1360 Train: 8722.64062 Test: 629.91724
Epoch: 1440 Train: 9083.03809 Test: 592.84637
Epoch 1520: New minimal relative error: 57.73%, model saved.
Epoch: 1520 Train: 7884.21240 Test: 594.46912
Epoch: 1600 Train: 6961.09326 Test: 431.18939
Epoch: 1680 Train: 5030.60400 Test: 348.07141
Epoch: 1760 Train: 4101.38965 Test: 269.28696
Epoch: 1840 Train: 1788.54358 Test: 193.91988
Epoch 1920: New minimal relative error: 18.75%, model saved.
Epoch: 1920 Train: 1565.60107 Test: 35.78925
Epoch 2000: New minimal relative error: 14.16%, model saved.
Epoch: 2000 Train: 677.18353 Test: 10.53636
Epoch: 2080 Train: 478.13242 Test: 9.68677
Epoch 2160: New minimal relative error: 10.60%, model saved.
Epoch: 2160 Train: 363.28937 Test: 3.90449
Epoch: 2240 Train: 286.15210 Test: 4.76842
Epoch: 2320 Train: 315.32852 Test: 41.13430
Epoch: 2400 Train: 216.21628 Test: 1.74030
Epoch 2480: New minimal relative error: 4.31%, model saved.
Epoch: 2480 Train: 184.11620 Test: 1.46268
Epoch 2560: New minimal relative error: 3.88%, model saved.
Epoch: 2560 Train: 175.86462 Test: 2.12475
Epoch: 2640 Train: 168.94193 Test: 2.75802
Epoch: 2720 Train: 191.16754 Test: 12.91702
Epoch: 2800 Train: 170.26228 Test: 13.28929
Epoch: 2880 Train: 134.44225 Test: 0.93988
Epoch: 2960 Train: 135.35144 Test: 1.15540
Epoch: 3040 Train: 134.94476 Test: 3.58008
Epoch: 3120 Train: 123.94716 Test: 1.08684
Epoch: 3200 Train: 172.69011 Test: 14.06765
Epoch: 3280 Train: 134.06430 Test: 4.43624
Epoch: 3360 Train: 123.34446 Test: 9.92985
Epoch 3440: New minimal relative error: 3.45%, model saved.
Epoch: 3440 Train: 108.35362 Test: 0.61641
Epoch 3520: New minimal relative error: 1.86%, model saved.
Epoch: 3520 Train: 101.36893 Test: 0.51116
Epoch: 3600 Train: 118.76892 Test: 7.61326
Epoch: 3680 Train: 92.61603 Test: 0.56870
Epoch: 3760 Train: 110.86931 Test: 5.45256
Epoch: 3840 Train: 91.93163 Test: 0.46570
Epoch: 3920 Train: 93.02007 Test: 3.55629
Epoch: 4000 Train: 87.12655 Test: 0.56364
Epoch: 4080 Train: 81.07854 Test: 0.38722
Epoch: 4160 Train: 120.81230 Test: 9.38713
Epoch: 4240 Train: 78.86521 Test: 0.40911
Epoch: 4320 Train: 87.07552 Test: 7.87288
Epoch: 4400 Train: 72.55929 Test: 0.31926
Epoch: 4480 Train: 70.84511 Test: 0.48772
Epoch: 4560 Train: 72.80904 Test: 0.40956
Epoch: 4640 Train: 86.48766 Test: 2.81614
Epoch: 4720 Train: 79.72726 Test: 0.84619
Epoch: 4800 Train: 73.69043 Test: 0.49295
Epoch: 4880 Train: 69.83076 Test: 0.51351
Epoch: 4960 Train: 74.42876 Test: 0.54492
Epoch: 5040 Train: 79.57792 Test: 2.62653
Epoch: 5120 Train: 83.23528 Test: 2.13688
Epoch: 5200 Train: 76.92125 Test: 2.22448
Epoch: 5280 Train: 72.40797 Test: 0.81408
Epoch: 5360 Train: 73.36893 Test: 0.34857
Epoch: 5440 Train: 71.41225 Test: 0.42101
Epoch: 5520 Train: 74.09408 Test: 0.51826
Epoch: 5600 Train: 73.57571 Test: 0.33988
Epoch: 5680 Train: 92.69588 Test: 6.19807
Epoch: 5760 Train: 78.25357 Test: 0.40045
Epoch: 5840 Train: 75.85009 Test: 0.57941
Epoch: 5920 Train: 73.98729 Test: 0.59488
Epoch: 6000 Train: 86.41168 Test: 3.52989
Epoch: 6080 Train: 88.78175 Test: 2.42876
Epoch: 6160 Train: 72.11369 Test: 0.40813
Epoch 6240: New minimal relative error: 1.75%, model saved.
Epoch: 6240 Train: 67.69747 Test: 0.28178
Epoch: 6320 Train: 66.52459 Test: 0.27102
Epoch: 6400 Train: 65.07262 Test: 0.31549
Epoch: 6480 Train: 69.11869 Test: 1.21085
Epoch: 6560 Train: 61.55866 Test: 0.32422
Epoch: 6640 Train: 64.37478 Test: 0.61962
Epoch: 6720 Train: 63.49530 Test: 0.31719
Epoch: 6800 Train: 59.54657 Test: 0.75076
Epoch: 6880 Train: 60.86353 Test: 0.68764
Epoch: 6960 Train: 60.56172 Test: 0.49374
Epoch 7040: New minimal relative error: 1.60%, model saved.
Epoch: 7040 Train: 58.55302 Test: 0.23412
Epoch: 7120 Train: 59.67779 Test: 0.56042
Epoch: 7200 Train: 67.35503 Test: 1.21403
Epoch: 7280 Train: 60.29623 Test: 0.45350
Epoch: 7360 Train: 57.17826 Test: 0.24714
Epoch: 7440 Train: 58.00172 Test: 0.24416
Epoch: 7520 Train: 57.25306 Test: 0.25346
Epoch: 7600 Train: 57.10928 Test: 0.33605
Epoch: 7680 Train: 57.29581 Test: 0.27910
Epoch: 7760 Train: 57.49384 Test: 0.58208
Epoch: 7840 Train: 56.82338 Test: 0.24271
Epoch: 7920 Train: 55.34101 Test: 0.35982
Epoch: 7999 Train: 55.14224 Test: 0.24879
Training Loss: tensor(55.1422)
Test Loss: tensor(0.2488)
Learned LE: [ 8.1117249e-01  9.9787000e-04 -1.4484833e+01]
True LE: [ 8.1707698e-01 -3.3141158e-03 -1.4485752e+01]
Relative Error: [7.620197   7.2600684  6.568547   5.87247    5.285649   4.7784057
 4.4223857  4.2464294  3.8948052  2.8798127  1.9860241  1.1032735
 0.76708263 0.7508813  0.6684627  0.43285602 0.33595023 0.37692982
 0.451168   0.49271768 0.68992054 0.85618794 0.98072803 1.1376064
 1.3637482  1.5503871  1.6957915  1.8636051  2.1373777  2.4766486
 2.5988467  2.4726355  2.3064566  2.081125   2.131386   2.4467425
 2.7714145  3.0605202  3.184381   3.3766692  3.0311105  2.5675108
 2.2019157  2.1254935  2.1899018  2.1270287  1.8203903  1.4165833
 1.453529   2.121936   2.9193425  3.7268944  4.6368656  5.761517
 5.9916167  6.231106   6.3042307  6.8310156  7.315588   7.2666655
 7.285951   7.1234307  7.0493946  7.3246875  6.862917   6.198529
 5.436247   4.7596445  4.2458034  3.852323   3.6417863  3.149265
 2.262902   1.1597465  0.5525912  0.50826186 0.5347767  0.33372283
 0.16955908 0.2684985  0.42464966 0.45300755 0.59376764 0.82684934
 1.0051619  1.1556351  1.1223354  1.2885774  1.3304147  1.4124049
 1.6973575  1.9180188  2.2754896  2.2381554  2.1245825  1.8601424
 1.6814308  1.8894023  2.0596485  2.4109511  2.698415   2.865927
 2.9529672  2.5206895  2.0684268  1.7344183  1.759918   1.7582916
 1.5420644  1.2400957  1.0022553  1.4082582  2.1851463  3.0005543
 3.8506746  4.554566   5.0176463  5.353531   5.4811373  5.509482
 6.131403   6.0831     5.7297697  5.624656   5.7680035  6.2555685
 6.8116417  6.5321937  5.829173   5.02916    4.2076283  3.6459043
 3.2759163  3.140406   2.6227145  1.5585803  0.5972784  0.23918681
 0.40261662 0.3566917  0.12913373 0.1608718  0.43023708 0.4091888
 0.5957484  0.85802937 1.1489376  1.2745323  1.3134699  1.0483536
 1.1292224  1.1394188  1.2695498  1.5274774  1.7344055  2.1045105
 1.9508802  1.8306993  1.606861   1.5186222  1.5718486  1.6903847
 2.029204   2.3138368  2.3966095  2.3887749  1.9598911  1.5469928
 1.3992878  1.3465514  1.2787689  1.1291542  0.84029055 0.8673868
 1.3568534  1.9437286  2.8446422  3.6989186  4.3070226  4.371553
 4.7468486  4.6724887  4.4215655  4.611527   4.2661266  4.034843
 3.95874    4.241647   4.959175   5.657041   5.9980574  5.4930444
 4.6303754  3.761224   3.1106277  2.7433937  2.4935415  2.1921296
 1.1959192  0.34124687 0.3389588  0.41698846 0.24899083 0.11570682
 0.17872766 0.27879354 0.4221106  0.73104477 1.1744838  1.4404601
 1.3802841  1.1567909  0.9162048  0.8919645  0.93442315 1.0397211
 1.235331   1.4244365  1.7863379  1.6471399  1.5192081  1.4361145
 1.3458612  1.3235469  1.2919011  1.5701939  1.7774891  1.8911934
 1.7920872  1.4318296  1.2276708  1.0900337  0.85692906 0.92501986
 0.8338978  0.57678163 0.6038905  1.1768258  1.6327409  2.4081445
 3.391435   3.8662844  3.7210405  4.037587   3.6295686  2.8877878
 3.1625059  2.8775904  2.5631227  2.5587173  2.815208   3.648674
 4.340908   5.0321927  5.182499   4.3858113  3.4075122  2.7093232
 2.2419686  1.9559132  1.8524734  1.0783062  0.41927397 0.44233078
 0.3771444  0.3224725  0.36079022 0.3392781  0.1981946  0.37454727
 0.7117144  0.9444031  1.1254231  0.95224094 0.78025335 0.7491576
 0.75789165 0.7405071  0.6594071  0.87490195 0.9793263  1.278033
 1.2259216  1.1014678  1.094639   1.1063228  1.0316602  0.89034235
 0.99771273 1.2531608  1.2911773  1.25231    1.022594   0.97836125
 0.99164    0.84588027 0.78791875 0.7523721  0.4940971  0.4030969
 0.72570837 1.2342697  1.9314855  2.738129   3.4458547  3.4721422
 3.3229976  2.6930065  1.930417   1.9595951  1.8103067  1.410569
 1.3264034  1.4467008  2.292402   3.0437422  3.5805597  4.189541
 4.2360196  3.4148364  2.4775236  1.8192064  1.4510705  1.333857
 1.0525244  0.57214487 0.3376006  0.25073314 0.37042737 0.37772968
 0.30267566 0.3155611  0.49873945 0.6939929  0.6982502  0.45107472
 0.5028941  0.57754606 0.49839276 0.5136373  0.4282372  0.39307293
 0.46688393 0.5835403  0.7585879  0.90026695 0.684137   0.5988581
 0.7229588  0.7230365  0.5353341  0.4645987  0.59576577 0.76281697
 0.838167   0.7487296  0.7330292  0.8877534  0.82092816 0.65769416
 0.64137733 0.61736506 0.31382468 0.29890445 0.7063026  1.2047921
 2.092415   2.7621317  3.0606818  2.8030536  2.1220553  1.5523397
 1.1638639  1.1947504  0.778246   0.5700658  0.5013963  0.91292244
 1.7567297  2.2825263  2.6791852  3.2723505  3.5824113  2.6120636
 1.6142957  1.1100638  1.0242425  0.9161587  0.68036497 0.3729338
 0.1714484  0.18438424 0.35039377 0.21936086 0.31412145 0.4148521
 0.4812882  0.5046354  0.25068283 0.15631148]
