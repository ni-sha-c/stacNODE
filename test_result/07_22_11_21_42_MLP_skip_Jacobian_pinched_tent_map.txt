time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 3000
num_test: 3000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: pinched_tent_map
model_type: MLP_skip
s: 0.2
n_hidden: 512
n_layers: 2
reg_param: 200.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 9.213840485 Test: 6.444832802
Epoch 0: New minimal relative error: 6.44%, model saved.
Epoch: 80 Train: 0.427507132 Test: 0.362319231
Epoch 80: New minimal relative error: 0.36%, model saved.
Epoch: 160 Train: 0.142338410 Test: 0.094004706
Epoch 160: New minimal relative error: 0.09%, model saved.
Epoch: 240 Train: 0.137333393 Test: 0.066413693
Epoch 240: New minimal relative error: 0.07%, model saved.
Epoch: 320 Train: 0.115467839 Test: 0.050602492
Epoch 320: New minimal relative error: 0.05%, model saved.
Epoch: 400 Train: 0.136491910 Test: 0.066629976
Epoch: 480 Train: 0.135602280 Test: 0.066464290
Epoch: 560 Train: 0.126740977 Test: 0.064510725
Epoch: 640 Train: 0.137170732 Test: 0.065436386
Epoch: 720 Train: 0.134033680 Test: 0.061183609
Epoch: 800 Train: 0.134901986 Test: 0.050796442
Epoch: 880 Train: 0.138581827 Test: 0.064476155
Epoch: 960 Train: 0.138035744 Test: 0.044802215
Epoch 960: New minimal relative error: 0.04%, model saved.
Epoch: 1040 Train: 0.139071465 Test: 0.062516615
Epoch: 1120 Train: 0.139028683 Test: 0.064408399
Epoch: 1200 Train: 0.139011368 Test: 0.060776561
Epoch: 1280 Train: 0.138442457 Test: 0.061776530
Epoch: 1360 Train: 0.135736734 Test: 0.053311955
Epoch: 1440 Train: 0.139759943 Test: 0.061586995
Epoch: 1520 Train: 0.123454683 Test: 0.044115487
Epoch 1520: New minimal relative error: 0.04%, model saved.
Epoch: 1600 Train: 0.139234543 Test: 0.061368067
Epoch: 1680 Train: 0.137767911 Test: 0.057872698
Epoch: 1760 Train: 0.139710590 Test: 0.060070865
Epoch: 1840 Train: 0.139516324 Test: 0.060993169
Epoch: 1920 Train: 0.139751866 Test: 0.059388850
Epoch: 2000 Train: 0.139776498 Test: 0.061769646
Epoch: 2080 Train: 0.139667049 Test: 0.060933687
Epoch: 2160 Train: 0.139320806 Test: 0.061879247
Epoch: 2240 Train: 0.139499947 Test: 0.061009757
Epoch: 2320 Train: 0.139286786 Test: 0.061266772
Epoch: 2400 Train: 0.137950718 Test: 0.060768720
Epoch: 2480 Train: 0.139853224 Test: 0.061822008
Epoch: 2560 Train: 0.121260472 Test: 0.045715671
Epoch: 2640 Train: 0.139322594 Test: 0.060259201
Epoch: 2720 Train: 0.139761686 Test: 0.061970267
Epoch: 2800 Train: 0.139441267 Test: 0.061045945
Epoch: 2880 Train: 0.139818043 Test: 0.062276591
Epoch: 2960 Train: 0.139810607 Test: 0.063268565
Epoch: 3040 Train: 0.139787152 Test: 0.062109541
Epoch: 3120 Train: 0.139795572 Test: 0.060083497
Epoch: 3200 Train: 0.139798701 Test: 0.060392577
Epoch: 3280 Train: 0.139798149 Test: 0.062128972
Epoch: 3360 Train: 0.139803991 Test: 0.060612429
Epoch: 3440 Train: 0.139817387 Test: 0.061856039
Epoch: 3520 Train: 0.139797717 Test: 0.062339511
Epoch: 3600 Train: 0.139786258 Test: 0.057793155
Epoch: 3680 Train: 0.139809996 Test: 0.060268033
Epoch: 3760 Train: 0.139853507 Test: 0.060029950
Epoch: 3840 Train: 0.139883846 Test: 0.062550016
Epoch: 3920 Train: 0.139895365 Test: 0.060110193
Epoch: 4000 Train: 0.139515027 Test: 0.050837100
Epoch: 4080 Train: 0.139865071 Test: 0.059746113
Epoch: 4160 Train: 0.139852479 Test: 0.060646351
Epoch: 4240 Train: 0.139844239 Test: 0.058419008
Epoch: 4320 Train: 0.139840320 Test: 0.060807761
Epoch: 4400 Train: 0.139810354 Test: 0.060789075
Epoch: 4480 Train: 0.139793232 Test: 0.060447358
Epoch: 4560 Train: 0.139780104 Test: 0.060182579
Epoch: 4640 Train: 0.139810920 Test: 0.057824418
Epoch: 4720 Train: 0.139777198 Test: 0.061697956
Epoch: 4800 Train: 0.139837667 Test: 0.060983080
Epoch: 4880 Train: 0.139860630 Test: 0.061671428
Epoch: 4960 Train: 0.139863014 Test: 0.060791265
Epoch: 5040 Train: 0.139822111 Test: 0.062893100
Epoch: 5120 Train: 0.065473855 Test: 0.066294111
Epoch: 5200 Train: 0.139929205 Test: 0.064531900
Epoch: 5280 Train: 0.139833868 Test: 0.063186683
Epoch: 5360 Train: 0.139808655 Test: 0.063327827
Epoch: 5440 Train: 0.139820054 Test: 0.063828655
Epoch: 5520 Train: 0.139828309 Test: 0.064396694
Epoch: 5600 Train: 0.139852598 Test: 0.064699657
Epoch: 5680 Train: 0.139778912 Test: 0.062933519
Epoch: 5760 Train: 0.139787346 Test: 0.062723152
Epoch: 5840 Train: 0.139801472 Test: 0.063316032
Epoch: 5920 Train: 0.139823616 Test: 0.064645022
Epoch: 6000 Train: 0.139808357 Test: 0.062062033
Epoch: 6080 Train: 0.139806956 Test: 0.063568659
Epoch: 6160 Train: 0.139807731 Test: 0.062003773
Epoch: 6240 Train: 0.139757633 Test: 0.062118277
Epoch: 6320 Train: 0.139833406 Test: 0.065011278
Epoch: 6400 Train: 0.139863551 Test: 0.064895779
Epoch: 6480 Train: 0.139873996 Test: 0.065662697
Epoch: 6560 Train: 0.139960423 Test: 0.066376746
Epoch: 6640 Train: 0.140105516 Test: 0.067916155
Epoch: 6720 Train: 0.140206635 Test: 0.069323123
Epoch: 6800 Train: 0.140190959 Test: 0.067406349
Epoch: 6880 Train: 0.140044004 Test: 0.065937132
Epoch: 6960 Train: 0.139893964 Test: 0.063489653
Epoch: 7040 Train: 0.139869407 Test: 0.064274505
Epoch: 7120 Train: 0.139828712 Test: 0.064302839
Epoch: 7200 Train: 0.139873639 Test: 0.065081038
Epoch: 7280 Train: 0.139899015 Test: 0.065298177
Epoch: 7360 Train: 0.139915988 Test: 0.066211082
Epoch: 7440 Train: 0.139891773 Test: 0.061672460
Epoch: 7520 Train: 0.139906153 Test: 0.062945917
Epoch: 7600 Train: 0.139903545 Test: 0.062496118
Epoch: 7680 Train: 0.139930159 Test: 0.062679492
Epoch: 7760 Train: 0.139975145 Test: 0.059560928
Epoch: 7840 Train: 0.139915943 Test: 0.064214766
Epoch: 7920 Train: 0.139925510 Test: 0.063960798
Epoch: 7999 Train: 0.139915437 Test: 0.062880978
Training Loss: tensor(0.1399)
Test Loss: tensor(0.0629)
True Mean x: tensor(1.0009, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(0.9746, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(0.2930, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.3111, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0007)
Jacobian term Test Loss: tensor(0.0003)
Learned LE: [[0.68513966]]
True LE: [[0.6982655]]
Norm Diff:: tensor(0.0131)
