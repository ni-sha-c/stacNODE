time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.54%, model saved.
Epoch: 0 Train: 9842.40430 Test: 3847.04810
Epoch: 80 Train: 2818.96680 Test: 1092.40515
Epoch: 160 Train: 2724.92432 Test: 1035.10522
Epoch: 240 Train: 2432.57837 Test: 929.79553
Epoch: 320 Train: 1874.90063 Test: 659.89038
Epoch: 400 Train: 898.17297 Test: 248.55135
Epoch 480: New minimal relative error: 63.28%, model saved.
Epoch: 480 Train: 394.86359 Test: 73.60582
Epoch: 560 Train: 201.86925 Test: 29.40664
Epoch 640: New minimal relative error: 12.41%, model saved.
Epoch: 640 Train: 134.50803 Test: 17.37743
Epoch: 720 Train: 108.32650 Test: 15.24773
Epoch: 800 Train: 109.43037 Test: 35.43487
Epoch: 880 Train: 80.42233 Test: 7.31216
Epoch: 960 Train: 72.39204 Test: 7.33019
Epoch: 1040 Train: 65.11024 Test: 6.00520
Epoch 1120: New minimal relative error: 9.73%, model saved.
Epoch: 1120 Train: 56.30465 Test: 4.92743
Epoch: 1200 Train: 52.29522 Test: 4.30318
Epoch: 1280 Train: 47.10836 Test: 3.53586
Epoch: 1360 Train: 45.12805 Test: 3.61029
Epoch: 1440 Train: 41.53078 Test: 3.07073
Epoch: 1520 Train: 45.30725 Test: 5.68617
Epoch: 1600 Train: 37.85559 Test: 2.78747
Epoch: 1680 Train: 35.01685 Test: 2.49879
Epoch 1760: New minimal relative error: 5.53%, model saved.
Epoch: 1760 Train: 32.99435 Test: 2.25728
Epoch: 1840 Train: 31.85250 Test: 2.15691
Epoch: 1920 Train: 30.80786 Test: 2.12921
Epoch: 2000 Train: 30.51899 Test: 4.11705
Epoch: 2080 Train: 27.48240 Test: 1.81356
Epoch: 2160 Train: 26.91169 Test: 1.87092
Epoch: 2240 Train: 28.93782 Test: 2.06459
Epoch 2320: New minimal relative error: 5.40%, model saved.
Epoch: 2320 Train: 24.99417 Test: 1.57893
Epoch: 2400 Train: 25.72877 Test: 2.26156
Epoch: 2480 Train: 28.35098 Test: 3.74687
Epoch 2560: New minimal relative error: 4.25%, model saved.
Epoch: 2560 Train: 24.63387 Test: 1.59739
Epoch: 2640 Train: 24.83257 Test: 1.67315
Epoch: 2720 Train: 27.44743 Test: 5.02217
Epoch: 2800 Train: 23.74957 Test: 1.56762
Epoch: 2880 Train: 27.56166 Test: 6.81447
Epoch: 2960 Train: 22.49483 Test: 1.54113
Epoch: 3040 Train: 21.89429 Test: 1.51885
Epoch: 3120 Train: 23.01066 Test: 3.03938
Epoch: 3200 Train: 20.92358 Test: 1.43078
Epoch: 3280 Train: 20.73950 Test: 1.55678
Epoch: 3360 Train: 20.80990 Test: 1.99647
Epoch: 3440 Train: 19.53988 Test: 1.30496
Epoch: 3520 Train: 19.10559 Test: 1.30320
Epoch: 3600 Train: 18.64962 Test: 1.20793
Epoch: 3680 Train: 19.81016 Test: 2.20304
Epoch: 3760 Train: 17.81698 Test: 1.12292
Epoch: 3840 Train: 17.37366 Test: 1.04980
Epoch: 3920 Train: 16.99312 Test: 1.00897
Epoch: 4000 Train: 16.76982 Test: 0.99201
Epoch: 4080 Train: 16.52239 Test: 0.97699
Epoch 4160: New minimal relative error: 3.60%, model saved.
Epoch: 4160 Train: 16.06076 Test: 0.95072
Epoch: 4240 Train: 16.02870 Test: 1.38901
Epoch: 4320 Train: 15.59281 Test: 0.89299
Epoch: 4400 Train: 15.34718 Test: 0.86033
Epoch: 4480 Train: 15.00289 Test: 0.84299
Epoch: 4560 Train: 14.68589 Test: 0.83414
Epoch: 4640 Train: 14.53382 Test: 0.80454
Epoch: 4720 Train: 14.21646 Test: 0.77234
Epoch: 4800 Train: 14.11949 Test: 0.74832
Epoch: 4880 Train: 13.84982 Test: 0.72814
Epoch: 4960 Train: 13.70062 Test: 0.75711
Epoch: 5040 Train: 13.49982 Test: 0.70986
Epoch: 5120 Train: 15.21490 Test: 2.37966
Epoch: 5200 Train: 13.22340 Test: 0.69719
Epoch: 5280 Train: 13.12284 Test: 0.69807
Epoch: 5360 Train: 12.92624 Test: 0.70978
Epoch: 5440 Train: 12.76253 Test: 0.69000
Epoch: 5520 Train: 12.73580 Test: 0.71111
Epoch: 5600 Train: 12.68278 Test: 0.69130
Epoch: 5680 Train: 12.63040 Test: 0.69597
Epoch: 5760 Train: 13.26737 Test: 1.49067
Epoch: 5840 Train: 12.21454 Test: 0.70808
Epoch: 5920 Train: 11.96252 Test: 0.69145
Epoch: 6000 Train: 11.84322 Test: 0.67420
Epoch: 6080 Train: 11.67524 Test: 0.68461
Epoch: 6160 Train: 11.52528 Test: 0.67551
Epoch: 6240 Train: 11.55345 Test: 0.71035
Epoch: 6320 Train: 11.36971 Test: 0.68013
Epoch 6400: New minimal relative error: 3.35%, model saved.
Epoch: 6400 Train: 11.23873 Test: 0.68649
Epoch: 6480 Train: 11.13214 Test: 0.69269
Epoch: 6560 Train: 10.93808 Test: 0.63470
Epoch: 6640 Train: 11.20818 Test: 0.91442
Epoch: 6720 Train: 11.21906 Test: 0.91666
Epoch: 6800 Train: 10.96921 Test: 0.68416
Epoch: 6880 Train: 10.86025 Test: 0.67538
Epoch: 6960 Train: 10.91890 Test: 0.84837
Epoch: 7040 Train: 10.70529 Test: 0.68000
Epoch: 7120 Train: 10.73695 Test: 0.69043
Epoch: 7200 Train: 10.69984 Test: 0.68421
Epoch: 7280 Train: 10.72786 Test: 0.68458
Epoch: 7360 Train: 10.92616 Test: 1.02870
Epoch: 7440 Train: 10.53938 Test: 0.64638
Epoch 7520: New minimal relative error: 2.77%, model saved.
Epoch: 7520 Train: 10.48975 Test: 0.63925
Epoch: 7600 Train: 10.46769 Test: 0.62556
Epoch: 7680 Train: 10.52575 Test: 0.61101
Epoch: 7760 Train: 10.64811 Test: 0.57716
Epoch: 7840 Train: 11.62198 Test: 1.12049
Epoch: 7920 Train: 10.97342 Test: 0.57008
Epoch: 7999 Train: 11.01112 Test: 0.54879
Training Loss: tensor(11.0111)
Test Loss: tensor(0.5488)
Learned LE: [  0.86727357  -0.03891641 -14.53983   ]
True LE: [ 8.6252838e-01 -3.5233160e-03 -1.4536468e+01]
Relative Error: [ 4.6712575  3.3019266  1.9058604  1.3775098  2.3076317  3.2778752
  3.8275287  4.2286873  4.5703793  4.3996253  4.237737   3.9651685
  3.5511823  3.1319938  2.5825338  2.0127325  1.8605772  2.5399868
  3.7758424  4.160119   4.6399717  5.193335   5.794504   6.4167986
  6.92986    7.3642545  7.686024   7.8771033  8.300875   8.7081995
  9.144722   9.527448   9.707216   9.813281   9.779873   9.462659
  9.245188   8.536961   6.771104   5.167077   3.8382869  3.169865
  2.1308112  1.9267557  2.5145195  3.266159   3.9638796  4.585238
  5.1659794  5.7065177  6.2710047  6.9042063  7.6399827  8.474263
  9.362252  10.22871    9.914332   9.492144   8.854469   8.010267
  7.199277   5.9628363  4.6658697  3.3178158  1.918311   1.3647648
  2.3103106  3.2868652  3.7057095  4.1441     4.450324   4.302422
  4.184307   3.962106   3.5799298  3.077822   2.5142274  1.9224787
  1.739602   2.436787   3.3010504  3.8627791  4.363863   4.892577
  5.530469   6.210978   6.987777   7.386055   7.084367   6.8447137
  6.9869533  7.4675403  8.209427   9.111676   9.525119   9.534721
  9.595283   9.18665    8.906611   8.182199   6.7680755  5.0444674
  3.5482957  2.6907406  2.2206116  1.8089473  2.2992017  3.0123944
  3.6701872  4.238797   4.7576222  5.291878   5.825409   6.4419017
  7.1912484  8.065584   9.024936   9.985059   9.861569   9.525284
  9.054766   8.196824   7.3090663  6.16579    4.843147   3.530943
  2.1370509  1.422832   2.2659829  3.0829835  3.5276723  3.9474804
  4.229342   4.189433   4.1539845  4.0020714  3.6654837  3.1122434
  2.5039368  1.8881027  1.6746297  2.3795326  2.8578606  3.2998264
  3.888207   4.4708605  5.167756   5.9194145  5.8442984  5.648595
  5.54193    5.4746013  5.4245505  5.852762   6.491726   7.3003063
  8.13893    9.062126   9.384695   9.040551   8.634582   7.953619
  6.9751606  5.140246   3.468208   2.3491354  2.03161    1.720009
  2.043248   2.7119393  3.3356905  3.8535242  4.3051295  4.764762
  5.2973456  5.8943863  6.6472917  7.5597286  8.555434   9.573042
  9.661932   9.397299   9.027966   8.127784   7.2344527  6.1051297
  4.8484254  3.7458174  2.5733573  1.6142311  2.2097645  2.8122737
  3.2111778  3.6022787  3.9893672  4.057619   4.1434817  4.0681143
  3.794593   3.2426848  2.5082533  1.834421   1.5856689  2.0815692
  2.3428667  2.7316139  3.3517823  4.1053166  4.862705   4.564964
  4.161917   4.0935397  3.9997227  4.0181227  4.16009    4.543222
  5.0456486  5.719287   6.4914427  7.2560587  8.227848   9.060094
  8.500421   7.884333   7.0744553  5.4368043  3.614315   2.2386324
  1.5862893  1.7593404  1.7700487  2.3646245  2.965598   3.4455557
  3.8289819  4.1952567  4.657511   5.2578263  5.99467    6.9171286
  7.9384522  9.018005   9.419351   9.272677   8.860474   7.8501964
  6.656783   5.7888727  4.7194257  3.4715328  2.3938363  1.8082297
  2.2052422  2.5120935  2.7514203  3.187794   3.6465034  3.778265
  3.9311004  3.938574   3.7355309  3.2550783  2.5569696  1.8258035
  1.3102652  1.5848749  1.857849   2.2871435  2.851517   3.6464512
  3.7963536  3.1967666  2.7253237  2.6656072  2.80863    2.7878532
  2.8951385  3.311991   3.9731066  4.4526124  5.1228075  5.675269
  6.4469137  7.449142   8.236756   8.000508   7.0507026  6.0451794
  4.0991907  2.4659688  1.3894602  1.2458105  1.5536377  1.9660605
  2.5535357  3.0179486  3.347456   3.6188018  3.9528985  4.489413
  5.2311587  6.1384006  7.162664   8.280659   9.097374   9.037963
  8.611927   7.879567   6.3926287  5.512012   4.5934463  3.473834
  2.2934134  1.3692526  1.8852717  2.292631   2.2738376  2.6798055
  3.1918943  3.4603033  3.6753912  3.7501433  3.6203     3.2542627
  2.6406717  1.7838948  1.0393978  1.1533753  1.416421   1.8322763
  2.4347541  3.1852105  2.6563437  2.2028375  1.7617857  1.5978925
  1.8172747  2.079848   2.1073575  2.240703   2.8173263  3.603594
  4.004185   4.4269276  4.918824   5.6755857  6.642752   7.277763
  7.394821   6.5058804  5.036808   3.125871   1.6734878  0.893216
  1.1308334  1.5219377  2.0794566  2.5701575  2.883696   3.070006
  3.2573109  3.6200464  4.341891   5.21878    6.207863   7.326825
  8.546184   8.623724   8.402756   8.011076   6.530036   5.5606894
  4.6915107  3.6191134  2.4880252  1.362138   1.2102289  1.8842081
  1.915787   2.1274402  2.6508691  3.0915835  3.3346672  3.5000145
  3.481002   3.2219489  2.7059774  1.9033624  0.8491332  0.7861098
  1.0250455  1.4223008  2.0172691  2.4481995  1.8457938  1.557074
  1.3985538  1.1470188  1.4525727  1.7770002]
