time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.58%, model saved.
Epoch: 0 Train: 4061.46851 Test: 4005.08521
Epoch: 80 Train: 114.12467 Test: 122.15506
Epoch 160: New minimal relative error: 29.52%, model saved.
Epoch: 160 Train: 40.81099 Test: 38.70630
Epoch: 240 Train: 11.08223 Test: 19.42724
Epoch 320: New minimal relative error: 26.80%, model saved.
Epoch: 320 Train: 7.03858 Test: 10.33575
Epoch 400: New minimal relative error: 17.57%, model saved.
Epoch: 400 Train: 7.31292 Test: 11.51370
Epoch: 480 Train: 5.70265 Test: 8.44852
Epoch: 560 Train: 4.99168 Test: 8.86540
Epoch 640: New minimal relative error: 16.09%, model saved.
Epoch: 640 Train: 4.55256 Test: 6.70399
Epoch: 720 Train: 3.19108 Test: 5.79831
Epoch: 800 Train: 3.02021 Test: 5.51370
Epoch 880: New minimal relative error: 10.96%, model saved.
Epoch: 880 Train: 2.83688 Test: 5.23061
Epoch: 960 Train: 3.02960 Test: 5.45847
Epoch: 1040 Train: 5.48753 Test: 8.28546
Epoch: 1120 Train: 5.91721 Test: 6.74000
Epoch: 1200 Train: 2.55628 Test: 4.43643
Epoch: 1280 Train: 2.02375 Test: 4.31606
Epoch: 1360 Train: 1.84393 Test: 3.80924
Epoch: 1440 Train: 3.16367 Test: 5.00189
Epoch: 1520 Train: 1.50095 Test: 3.29953
Epoch: 1600 Train: 1.45176 Test: 3.25838
Epoch: 1680 Train: 1.92276 Test: 4.19488
Epoch: 1760 Train: 1.25392 Test: 2.94293
Epoch: 1840 Train: 1.69990 Test: 3.48429
Epoch: 1920 Train: 3.95412 Test: 4.44977
Epoch: 2000 Train: 2.07333 Test: 3.11275
Epoch: 2080 Train: 1.16260 Test: 2.34058
Epoch: 2160 Train: 1.17489 Test: 2.29443
Epoch: 2240 Train: 0.83785 Test: 1.87284
Epoch: 2320 Train: 1.70128 Test: 2.47448
Epoch: 2400 Train: 0.60249 Test: 2.19908
Epoch 2480: New minimal relative error: 10.81%, model saved.
Epoch: 2480 Train: 0.82643 Test: 1.95192
Epoch: 2560 Train: 0.36737 Test: 1.40655
Epoch: 2640 Train: 0.34974 Test: 1.34425
Epoch: 2720 Train: 0.32486 Test: 1.30012
Epoch 2800: New minimal relative error: 9.40%, model saved.
Epoch: 2800 Train: 0.59385 Test: 1.63294
Epoch: 2880 Train: 0.38930 Test: 1.26708
Epoch: 2960 Train: 0.45402 Test: 1.49511
Epoch: 3040 Train: 0.32763 Test: 1.20527
Epoch: 3120 Train: 0.26285 Test: 1.13976
Epoch 3200: New minimal relative error: 8.55%, model saved.
Epoch: 3200 Train: 0.26883 Test: 1.11426
Epoch: 3280 Train: 0.27025 Test: 1.09137
Epoch: 3360 Train: 0.35716 Test: 1.24020
Epoch: 3440 Train: 0.36926 Test: 1.13024
Epoch: 3520 Train: 0.44006 Test: 1.40931
Epoch: 3600 Train: 0.51425 Test: 1.28601
Epoch: 3680 Train: 0.32296 Test: 1.14559
Epoch: 3760 Train: 1.87090 Test: 2.73317
Epoch: 3840 Train: 0.62559 Test: 1.69031
Epoch: 3920 Train: 1.66101 Test: 2.28158
Epoch: 4000 Train: 0.27461 Test: 1.07722
Epoch: 4080 Train: 0.17556 Test: 0.89236
Epoch: 4160 Train: 1.36590 Test: 1.97424
Epoch: 4240 Train: 0.16223 Test: 0.86355
Epoch: 4320 Train: 0.16870 Test: 0.86203
Epoch: 4400 Train: 0.15588 Test: 0.85182
Epoch: 4480 Train: 2.05592 Test: 2.50554
Epoch: 4560 Train: 0.14885 Test: 0.81733
Epoch: 4640 Train: 0.19143 Test: 0.83878
Epoch: 4720 Train: 0.55768 Test: 1.20443
Epoch: 4800 Train: 0.15555 Test: 0.81770
Epoch: 4880 Train: 0.14458 Test: 0.79467
Epoch: 4960 Train: 0.17746 Test: 0.79118
Epoch: 5040 Train: 0.92491 Test: 1.50710
Epoch: 5120 Train: 0.13173 Test: 0.74269
Epoch: 5200 Train: 0.14874 Test: 0.74630
Epoch: 5280 Train: 0.12980 Test: 0.73708
Epoch: 5360 Train: 0.12916 Test: 0.73673
Epoch: 5440 Train: 0.83158 Test: 1.60185
Epoch: 5520 Train: 0.12164 Test: 0.70859
Epoch: 5600 Train: 0.17426 Test: 0.75185
Epoch 5680: New minimal relative error: 7.28%, model saved.
Epoch: 5680 Train: 0.11813 Test: 0.69151
Epoch: 5760 Train: 1.19225 Test: 2.05433
Epoch: 5840 Train: 0.11498 Test: 0.68218
Epoch: 5920 Train: 0.16456 Test: 0.77525
Epoch: 6000 Train: 0.11212 Test: 0.67253
Epoch: 6080 Train: 0.11548 Test: 0.65941
Epoch: 6160 Train: 0.10878 Test: 0.65640
Epoch: 6240 Train: 0.10721 Test: 0.64754
Epoch: 6320 Train: 0.16735 Test: 0.68853
Epoch: 6400 Train: 0.10469 Test: 0.63851
Epoch: 6480 Train: 1.21595 Test: 1.86853
Epoch: 6560 Train: 0.10240 Test: 0.63044
Epoch: 6640 Train: 0.10217 Test: 0.62739
Epoch 6720: New minimal relative error: 4.61%, model saved.
Epoch: 6720 Train: 0.10118 Test: 0.62590
Epoch: 6800 Train: 0.09938 Test: 0.61021
Epoch: 6880 Train: 0.09940 Test: 0.60728
Epoch: 6960 Train: 0.09655 Test: 0.60499
Epoch: 7040 Train: 0.10143 Test: 0.60077
Epoch: 7120 Train: 0.13304 Test: 0.59901
Epoch: 7200 Train: 0.09348 Test: 0.59168
Epoch: 7280 Train: 0.11514 Test: 0.59756
Epoch: 7360 Train: 0.09157 Test: 0.58273
Epoch: 7440 Train: 0.19180 Test: 0.65512
Epoch: 7520 Train: 0.09363 Test: 0.58404
Epoch: 7600 Train: 0.13245 Test: 0.64548
Epoch: 7680 Train: 0.08815 Test: 0.56551
Epoch: 7760 Train: 0.37379 Test: 0.96008
Epoch: 7840 Train: 0.08664 Test: 0.56097
Epoch: 7920 Train: 0.08805 Test: 0.56468
Epoch: 7999 Train: 0.08530 Test: 0.55537
Training Loss: tensor(0.0853)
Test Loss: tensor(0.5554)
Learned LE: [ 0.8306924   0.02935451 -3.5415916 ]
True LE: [ 8.6151522e-01  1.9877322e-03 -1.4539697e+01]
Relative Error: [ 5.5743947  5.2060285  4.794317   4.3333626  4.2953815  4.462203
  4.8173265  4.7752833  4.8250546  5.0645065  5.2832823  5.599333
  5.7209244  5.9397883  6.2180543  6.52119    6.831413   7.272418
  8.31884    9.401978   9.953579  10.068935   9.919335   9.884027
 10.260464  10.474078  10.667966  10.9531    10.879411  10.683344
 10.44251   10.247311  10.331792  10.494064  10.625765  10.805791
 11.193841  11.644798  10.883402   9.978385   9.252692   8.812715
  7.6896496  6.4793625  5.766932   5.1397233  5.12735    5.39862
  5.346178   5.4404597  5.9182825  6.0095253  6.0777793  5.8913765
  5.779674   5.7217073  5.7757316  5.682207   5.640923   5.671108
  5.6735883  5.5801053  5.2971134  5.0116816  4.6261272  4.2119837
  3.7493813  3.9123013  4.26865    4.42983    4.3435035  4.539268
  4.733444   5.1006203  5.365597   5.645293   6.0213194  6.1935043
  6.6028924  7.172485   8.237292   9.079571   9.459545   9.480131
  9.360215   9.26537    9.611502   9.792122  10.07326   10.381372
 10.614745  10.481635  10.153337   9.881563   9.839421   9.937813
 10.19899   10.305734  10.62239   11.130817  10.47604    9.564749
  8.753683   8.221048   7.00067    5.548278   4.9860077  4.5218725
  4.7144628  5.101961   5.0963697  5.369562   5.7330055  5.783305
  5.872184   5.7010717  5.5624833  5.473312   5.476558   5.3213186
  5.310326   5.4478755  5.6283855  5.348768   5.049727   4.8866453
  4.55964    4.1765323  3.722195   3.385449   3.7086408  3.873311
  3.8438056  3.898466   4.0637426  4.4267297  4.834629   5.212304
  5.777924   6.0626874  6.352681   7.027948   7.9371185  8.461456
  8.788803   8.896094   8.901971   8.742434   9.047764   9.215203
  9.5578785  9.814661   9.984076  10.092318   9.907779   9.61077
  9.408756   9.44366    9.612821   9.861223  10.0093565 10.43615
 10.130094   9.251809   8.40921    7.6666393  6.4432664  4.830225
  4.213334   3.9057949  4.2284026  4.643917   4.7611613  5.088253
  5.4716206  5.563386   5.6341233  5.555285   5.3545976  5.2229276
  5.067135   4.838492   4.8088098  4.968707   5.1582327  5.068574
  4.903524   4.829729   4.669168   4.345597   3.9112337  3.2365904
  3.011265   3.2621932  3.4200583  3.3700213  3.5138118  3.7315416
  4.234836   4.7226067  5.389647   5.864941   6.1463943  6.6522045
  7.182899   7.6932573  8.021318   8.217912   8.304433   8.245477
  8.502079   8.749923   9.116407   9.35268    9.429085   9.427367
  9.429761   9.429517   9.16579    8.935466   8.976734   9.190674
  9.429873   9.696193   9.762896   8.938603   8.192728   7.4047594
  6.06742    4.4003177  3.561645   3.3009942  3.6524408  4.1625447
  4.421533   4.638247   4.9089074  5.197596   5.341283   5.4097815
  5.135539   4.8398633  4.594058   4.369676   4.3551016  4.4997463
  4.6455646  4.668346   4.7123013  4.6290183  4.771692   4.6666927
  4.1893053  3.446673   2.712207   2.805627   3.02707    3.0732832
  3.078679   3.1470542  3.5347176  4.0986753  4.858981   5.5936785
  5.845866   6.02478    6.444756   6.9437222  7.3222375  7.419115
  7.313346   7.1805997  7.211301   7.664749   8.363983   8.854502
  8.9803505  8.870382   8.764152   8.945763   8.97943    8.562034
  8.329077   8.392332   8.693094   8.939773   9.336693   8.574311
  7.915363   7.311413   5.801006   4.2323785  2.9989572  2.683733
  2.9729855  3.6742666  4.0919757  4.205754   4.377682   4.641457
  4.931285   5.0696907  4.8168993  4.5645533  4.2581024  3.9487486
  3.935033   4.1347857  4.2590523  4.3130074  4.413862   4.50454
  4.5668235  4.6513925  4.4197006  3.8984344  3.0967526  2.7005517
  2.7617717  3.0061257  2.8422055  2.7863355  2.8499038  3.3444161
  4.158119   5.075339   5.547102   5.6067586  5.727573   6.146927
  6.369695   6.2351303  6.1095605  5.904965   5.650386   5.917388
  6.7413197  7.5528226  8.140678   8.364891   8.229353   8.289597
  8.444386   8.333185   7.8168445  7.596623   7.710389   8.106554
  8.412134   8.260077   7.5398192  7.042429   5.889702   4.120516
  2.7840047  2.1711996  2.3085432  3.1077092  3.7268639  3.8945253
  3.9224365  4.1120625  4.3901877  4.55025    4.61281    4.3175416
  3.987632   3.6823351  3.5500462  3.7048984  3.7433927  3.7883763
  3.9284132  4.2012944  4.4195867  4.480278   4.3352013  4.1103754
  3.7865427  3.27484    2.8975291  2.9754562  3.0302382  2.7514546
  2.4984722  2.6065924  3.2267456  4.1668496  4.9923625  5.3441305
  5.2264214  5.176383   5.2851667  5.218512   4.9618506  4.7732687
  4.542913   4.448125   5.0844874  5.779134 ]
