time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.51%, model saved.
Epoch: 0 Train: 4309.17041 Test: 3757.86987
Epoch 80: New minimal relative error: 85.50%, model saved.
Epoch: 80 Train: 123.06573 Test: 175.41187
Epoch 160: New minimal relative error: 80.56%, model saved.
Epoch: 160 Train: 70.19245 Test: 49.14185
Epoch 240: New minimal relative error: 30.70%, model saved.
Epoch: 240 Train: 22.56219 Test: 21.71314
Epoch: 320 Train: 14.98666 Test: 31.65775
Epoch 400: New minimal relative error: 27.91%, model saved.
Epoch: 400 Train: 5.78642 Test: 13.92652
Epoch 480: New minimal relative error: 26.49%, model saved.
Epoch: 480 Train: 46.64933 Test: 52.51337
Epoch: 560 Train: 6.94193 Test: 10.49121
Epoch: 640 Train: 2.79817 Test: 10.19674
Epoch 720: New minimal relative error: 19.40%, model saved.
Epoch: 720 Train: 2.27536 Test: 9.35764
Epoch: 800 Train: 4.20807 Test: 9.68997
Epoch 880: New minimal relative error: 18.90%, model saved.
Epoch: 880 Train: 8.88542 Test: 16.24145
Epoch: 960 Train: 4.87060 Test: 13.12182
Epoch 1040: New minimal relative error: 17.93%, model saved.
Epoch: 1040 Train: 6.57679 Test: 12.23124
Epoch: 1120 Train: 2.92298 Test: 7.70531
Epoch 1200: New minimal relative error: 12.70%, model saved.
Epoch: 1200 Train: 5.84348 Test: 10.08625
Epoch: 1280 Train: 3.39336 Test: 10.03611
Epoch: 1360 Train: 3.11306 Test: 6.95158
Epoch: 1440 Train: 0.89049 Test: 6.08860
Epoch: 1520 Train: 4.02533 Test: 11.48163
Epoch: 1600 Train: 5.93647 Test: 12.96862
Epoch: 1680 Train: 2.92159 Test: 8.21342
Epoch: 1760 Train: 0.97574 Test: 5.52884
Epoch: 1840 Train: 0.65585 Test: 5.71036
Epoch: 1920 Train: 0.60493 Test: 4.92613
Epoch: 2000 Train: 4.49857 Test: 10.39095
Epoch: 2080 Train: 1.52008 Test: 5.87423
Epoch: 2160 Train: 1.53043 Test: 5.82518
Epoch: 2240 Train: 1.05336 Test: 5.23655
Epoch: 2320 Train: 4.74287 Test: 7.99039
Epoch: 2400 Train: 0.75020 Test: 5.40480
Epoch 2480: New minimal relative error: 12.28%, model saved.
Epoch: 2480 Train: 0.54145 Test: 4.35992
Epoch: 2560 Train: 0.38123 Test: 4.92548
Epoch: 2640 Train: 1.27786 Test: 6.34725
Epoch: 2720 Train: 1.12188 Test: 5.21467
Epoch 2800: New minimal relative error: 10.93%, model saved.
Epoch: 2800 Train: 0.26957 Test: 4.28154
Epoch: 2880 Train: 0.69622 Test: 4.86067
Epoch: 2960 Train: 0.41221 Test: 4.25471
Epoch: 3040 Train: 1.88506 Test: 6.46689
Epoch: 3120 Train: 4.77306 Test: 11.19253
Epoch: 3200 Train: 1.34440 Test: 5.89109
Epoch: 3280 Train: 0.14628 Test: 3.67277
Epoch: 3360 Train: 0.13765 Test: 3.81719
Epoch 3440: New minimal relative error: 9.39%, model saved.
Epoch: 3440 Train: 0.45538 Test: 4.17732
Epoch: 3520 Train: 0.71503 Test: 3.92638
Epoch: 3600 Train: 2.47639 Test: 7.16750
Epoch: 3680 Train: 0.65350 Test: 3.62474
Epoch: 3760 Train: 1.68000 Test: 5.41949
Epoch: 3840 Train: 3.40203 Test: 7.88516
Epoch: 3920 Train: 0.21049 Test: 3.71184
Epoch: 4000 Train: 0.23426 Test: 3.53169
Epoch: 4080 Train: 0.70377 Test: 3.52290
Epoch 4160: New minimal relative error: 9.18%, model saved.
Epoch: 4160 Train: 1.68650 Test: 3.71719
Epoch: 4240 Train: 1.48211 Test: 3.80346
Epoch: 4320 Train: 1.38186 Test: 4.84499
Epoch: 4400 Train: 0.86706 Test: 3.51610
Epoch: 4480 Train: 0.10094 Test: 3.17417
Epoch: 4560 Train: 0.15025 Test: 3.09712
Epoch: 4640 Train: 0.08829 Test: 3.21048
Epoch: 4720 Train: 1.00461 Test: 4.92770
Epoch: 4800 Train: 0.07410 Test: 3.05855
Epoch: 4880 Train: 0.62719 Test: 4.06066
Epoch: 4960 Train: 0.23859 Test: 3.41078
Epoch: 5040 Train: 0.07419 Test: 3.02515
Epoch: 5120 Train: 0.14688 Test: 2.97289
Epoch: 5200 Train: 0.11759 Test: 2.87255
Epoch: 5280 Train: 0.14272 Test: 3.13801
Epoch: 5360 Train: 0.06672 Test: 2.88272
Epoch: 5440 Train: 0.61764 Test: 3.15170
Epoch: 5520 Train: 0.06092 Test: 2.85300
Epoch: 5600 Train: 0.17796 Test: 2.81498
Epoch: 5680 Train: 0.11364 Test: 2.97995
Epoch: 5760 Train: 0.07299 Test: 2.85860
Epoch: 5840 Train: 1.49734 Test: 4.61546
Epoch: 5920 Train: 0.05605 Test: 2.76043
Epoch 6000: New minimal relative error: 7.62%, model saved.
Epoch: 6000 Train: 0.06447 Test: 2.76590
Epoch: 6080 Train: 0.05770 Test: 2.79482
Epoch: 6160 Train: 0.05277 Test: 2.70889
Epoch: 6240 Train: 0.09489 Test: 3.58288
Epoch: 6320 Train: 0.05175 Test: 2.69665
Epoch: 6400 Train: 0.05018 Test: 2.65007
Epoch: 6480 Train: 0.07159 Test: 2.66140
Epoch: 6560 Train: 0.04883 Test: 2.62356
Epoch 6640: New minimal relative error: 6.11%, model saved.
Epoch: 6640 Train: 0.05521 Test: 2.73172
Epoch: 6720 Train: 0.04746 Test: 2.61906
Epoch: 6800 Train: 0.04654 Test: 2.58230
Epoch: 6880 Train: 0.07704 Test: 2.97561
Epoch: 6960 Train: 0.04571 Test: 2.59081
Epoch: 7040 Train: 0.04475 Test: 2.55089
Epoch: 7120 Train: 0.04437 Test: 2.54768
Epoch: 7200 Train: 0.40804 Test: 3.28738
Epoch: 7280 Train: 0.04346 Test: 2.53506
Epoch: 7360 Train: 0.04232 Test: 2.50440
Epoch: 7440 Train: 0.29270 Test: 2.84417
Epoch: 7520 Train: 0.04134 Test: 2.50458
Epoch: 7600 Train: 0.59583 Test: 3.41220
Epoch: 7680 Train: 0.04043 Test: 2.47626
Epoch: 7760 Train: 0.14202 Test: 2.40045
Epoch: 7840 Train: 0.03973 Test: 2.45998
Epoch: 7920 Train: 0.03984 Test: 2.42420
Epoch: 7999 Train: 0.04074 Test: 2.42361
Training Loss: tensor(0.0407)
Test Loss: tensor(2.4236)
Learned LE: [ 0.88513803 -0.04045327 -3.4744065 ]
True LE: [ 8.7956923e-01  1.3325802e-03 -1.4555155e+01]
Relative Error: [15.158507  16.197899  17.078306  17.981825  18.878914  19.601765
 20.155094  20.45134   20.723238  21.021423  21.219711  21.392513
 21.452168  21.40372   21.274576  21.252632  21.164646  20.97688
 20.842043  20.82775   20.78309   20.718065  20.6223    20.516235
 20.364717  20.180872  19.981958  20.03631   20.181189  20.205776
 20.158752  19.930962  19.497513  19.114086  18.697083  18.180779
 17.456892  16.727716  15.907947  14.895744  13.81197   12.743918
 11.708827  10.69986    9.770881   9.007401   8.531729   8.181134
  7.93823    7.753993   7.652055   7.6526694  7.7851973  8.016213
  8.286028   8.675458   9.17144    9.76019   10.458392  11.298001
 12.263909  13.362998  14.4288225 15.3871565 16.224699  17.131918
 18.02686   18.719484  19.183443  19.55053   19.906544  20.180153
 20.381937  20.56832   20.610445  20.541481  20.558342  20.497429
 20.34942   20.11      20.01755   20.024998  19.992968  19.89015
 19.776783  19.671608  19.505396  19.301516  19.080727  19.053083
 19.161219  19.163546  19.088585  18.794582  18.306871  17.907753
 17.522789  16.978443  16.28349   15.598093  14.687711  13.727111
 12.643126  11.652804  10.645054   9.653186   8.730083   8.0239725
  7.5924225  7.318072   7.092043   6.936842   6.8605795  6.8857536
  7.026723   7.2421985  7.5215592  7.935454   8.449877   9.072589
  9.776778  10.642302  11.604824  12.679884  13.732288  14.643515
 15.467052  16.389599  17.246061  17.853462  18.349766  18.750374
 19.114305  19.40624   19.63087   19.827724  19.849928  19.844398
 19.854172  19.717781  19.527008  19.296139  19.30483   19.28375
 19.216772  19.07913   18.916245  18.807554  18.637083  18.412891
 18.283527  18.176552  18.14584   18.141838  18.029278  17.713554
 17.162735  16.772083  16.411413  15.850015  15.212558  14.536344
 13.620225  12.623051  11.619546  10.659718   9.660491   8.596796
  7.647784   7.022133   6.663985   6.421262   6.2654314  6.17751
  6.1301017  6.1795793  6.3338413  6.5295978  6.818556   7.257195
  7.8249874  8.443743   9.152126  10.027221  10.959163  12.024347
 13.059781  13.936395  14.754873  15.712845  16.494808  17.056112
 17.586514  17.982254  18.402342  18.722282  18.975458  19.142416
 19.129948  19.214294  19.157324  18.985249  18.77298   18.64848
 18.615322  18.55513   18.45117   18.28175   18.086845  17.92583
 17.749205  17.558428  17.475676  17.304018  17.213211  17.222149
 17.102776  16.727478  16.1603    15.759807  15.355121  14.779155
 14.191482  13.403027  12.563276  11.6188755 10.695248   9.686726
  8.598833   7.538153   6.631149   6.1049066  5.8199067  5.6304684
  5.498391   5.443754   5.4766126  5.59273    5.748633   5.9485717
  6.2381973  6.703623   7.25523    7.86771    8.573271   9.430559
 10.357712  11.403919  12.392311  13.241652  14.071536  15.047707
 15.771519  16.378498  16.829588  17.293434  17.769306  18.117163
 18.368662  18.50337   18.57357   18.560022  18.446325  18.25884
 18.084324  18.012573  17.952139  17.843369  17.695261  17.489862
 17.265831  17.054638  16.862743  16.783092  16.642307  16.518787
 16.423836  16.334682  16.21443   15.77915   15.245025  14.827914
 14.398047  13.799514  13.232896  12.422319  11.589305  10.690562
  9.740166   8.698687   7.622792   6.5810075  5.716156   5.2987947
  5.085323   4.9263673  4.8507795  4.8350973  4.9085474  5.0767565
  5.244894   5.4481516  5.785106   6.2468     6.7881627  7.35807
  8.023823   8.845264   9.757432  10.798227  11.745046  12.574324
 13.424892  14.345683  15.104227  15.699798  16.154612  16.684273
 17.211372  17.543919  17.774557  17.810648  17.921192  17.871634
 17.706987  17.573166  17.389765  17.367577  17.27646   17.16968
 16.976845  16.763426  16.49206   16.240765  16.027632  16.035994
 15.959045  15.817492  15.662138  15.465464  15.306692  14.854261
 14.337394  13.943532  13.51655   12.94374   12.259826  11.494063
 10.726154   9.763252   8.8067465  7.7934084  6.735602   5.707362
  4.8882756  4.619274   4.4503164  4.348599   4.301377   4.3285027
  4.458282   4.652993   4.8358474  5.0808444  5.404236   5.878387
  6.4123077  6.925685   7.4656634  8.28397    9.192312  10.198214
 11.117781  11.931011  12.799582  13.694885  14.430708  15.08196
 15.607495  16.143885  16.663113  16.959007  17.093292  17.229752
 17.328726  17.248035  17.09194   16.913313  16.726885  16.755089
 16.709446  16.585943  16.398802  16.175882  15.8528805 15.497015
 15.271072  15.344888  15.272045  15.12439  ]
