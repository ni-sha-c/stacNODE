time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.75%, model saved.
Epoch: 0 Train: 3927.48242 Test: 4404.10107
Epoch: 80 Train: 362.19431 Test: 353.90378
Epoch 160: New minimal relative error: 22.96%, model saved.
Epoch: 160 Train: 46.42322 Test: 40.05213
Epoch: 240 Train: 15.99234 Test: 13.38571
Epoch: 320 Train: 9.91014 Test: 8.42620
Epoch 400: New minimal relative error: 13.32%, model saved.
Epoch: 400 Train: 7.93693 Test: 6.40704
Epoch: 480 Train: 6.04081 Test: 5.25095
Epoch: 560 Train: 5.16971 Test: 4.39033
Epoch: 640 Train: 6.87933 Test: 5.88637
Epoch: 720 Train: 4.14979 Test: 3.52081
Epoch: 800 Train: 3.75477 Test: 3.18044
Epoch: 880 Train: 3.46697 Test: 2.95559
Epoch 960: New minimal relative error: 12.94%, model saved.
Epoch: 960 Train: 3.16361 Test: 2.67762
Epoch: 1040 Train: 2.97474 Test: 2.55740
Epoch: 1120 Train: 3.31501 Test: 3.11109
Epoch: 1200 Train: 2.82642 Test: 2.45786
Epoch 1280: New minimal relative error: 9.36%, model saved.
Epoch: 1280 Train: 2.30316 Test: 1.98058
Epoch 1360: New minimal relative error: 9.14%, model saved.
Epoch: 1360 Train: 2.18032 Test: 1.88663
Epoch: 1440 Train: 4.02207 Test: 3.03257
Epoch: 1520 Train: 1.81247 Test: 1.61432
Epoch: 1600 Train: 1.62238 Test: 1.41109
Epoch: 1680 Train: 1.50426 Test: 1.33458
Epoch: 1760 Train: 1.42308 Test: 1.25372
Epoch: 1840 Train: 1.30615 Test: 1.14769
Epoch: 1920 Train: 1.27184 Test: 1.27394
Epoch: 2000 Train: 1.13270 Test: 0.99630
Epoch 2080: New minimal relative error: 6.42%, model saved.
Epoch: 2080 Train: 1.06877 Test: 0.93879
Epoch: 2160 Train: 1.06906 Test: 0.92554
Epoch: 2240 Train: 0.92953 Test: 0.82786
Epoch: 2320 Train: 0.88318 Test: 0.79708
Epoch: 2400 Train: 0.92923 Test: 0.91233
Epoch 2480: New minimal relative error: 4.81%, model saved.
Epoch: 2480 Train: 0.78543 Test: 0.70663
Epoch: 2560 Train: 0.74105 Test: 0.66575
Epoch: 2640 Train: 0.76168 Test: 0.66690
Epoch: 2720 Train: 0.67170 Test: 0.61138
Epoch: 2800 Train: 0.64123 Test: 0.58978
Epoch: 2880 Train: 0.62137 Test: 0.58941
Epoch: 2960 Train: 0.58116 Test: 0.53831
Epoch: 3040 Train: 0.55375 Test: 0.51582
Epoch: 3120 Train: 0.62102 Test: 0.59797
Epoch: 3200 Train: 0.50460 Test: 0.47645
Epoch: 3280 Train: 0.48692 Test: 0.46509
Epoch: 3360 Train: 0.46423 Test: 0.44442
Epoch: 3440 Train: 0.45213 Test: 0.43891
Epoch: 3520 Train: 0.43120 Test: 0.41547
Epoch: 3600 Train: 0.41718 Test: 0.40755
Epoch: 3680 Train: 0.39934 Test: 0.39282
Epoch: 3760 Train: 0.41038 Test: 0.41426
Epoch: 3840 Train: 0.37299 Test: 0.37242
Epoch: 3920 Train: 0.60238 Test: 0.70454
Epoch: 4000 Train: 0.35115 Test: 0.35693
Epoch: 4080 Train: 0.33974 Test: 0.34705
Epoch: 4160 Train: 0.33126 Test: 0.33763
Epoch: 4240 Train: 0.32992 Test: 0.35153
Epoch: 4320 Train: 0.33987 Test: 0.35479
Epoch: 4400 Train: 0.30516 Test: 0.31789
Epoch: 4480 Train: 0.29650 Test: 0.31277
Epoch: 4560 Train: 0.28969 Test: 0.30495
Epoch: 4640 Train: 0.28231 Test: 0.30039
Epoch: 4720 Train: 0.35787 Test: 0.36241
Epoch: 4800 Train: 0.26952 Test: 0.28969
Epoch: 4880 Train: 0.27326 Test: 0.28642
Epoch: 4960 Train: 0.25784 Test: 0.27910
Epoch: 5040 Train: 0.25239 Test: 0.27413
Epoch: 5120 Train: 0.25003 Test: 0.27326
Epoch: 5200 Train: 0.24179 Test: 0.26584
Epoch: 5280 Train: 0.42073 Test: 0.49911
Epoch: 5360 Train: 0.23238 Test: 0.25730
Epoch: 5440 Train: 0.26808 Test: 0.29257
Epoch: 5520 Train: 0.30719 Test: 0.34843
Epoch: 5600 Train: 0.21985 Test: 0.24622
Epoch: 5680 Train: 0.21630 Test: 0.24265
Epoch: 5760 Train: 0.21279 Test: 0.24055
Epoch: 5840 Train: 0.20837 Test: 0.23616
Epoch: 5920 Train: 0.45834 Test: 0.52895
Epoch: 6000 Train: 0.20126 Test: 0.23000
Epoch: 6080 Train: 0.20532 Test: 0.24048
Epoch: 6160 Train: 0.19412 Test: 0.22338
Epoch: 6240 Train: 0.19063 Test: 0.21944
Epoch: 6320 Train: 0.18795 Test: 0.21731
Epoch: 6400 Train: 0.18507 Test: 0.21465
Epoch: 6480 Train: 0.31664 Test: 0.36933
Epoch: 6560 Train: 0.17998 Test: 0.21048
Epoch: 6640 Train: 0.17978 Test: 0.22128
Epoch: 6720 Train: 0.17522 Test: 0.20566
Epoch: 6800 Train: 0.17304 Test: 0.20403
Epoch: 6880 Train: 0.17217 Test: 0.20205
Epoch: 6960 Train: 0.16871 Test: 0.19965
Epoch: 7040 Train: 0.22820 Test: 0.27315
Epoch: 7120 Train: 0.16482 Test: 0.19640
Epoch: 7200 Train: 0.16452 Test: 0.20009
Epoch: 7280 Train: 0.17776 Test: 0.21560
Epoch: 7360 Train: 0.16848 Test: 0.19387
Epoch: 7440 Train: 0.15764 Test: 0.19009
Epoch: 7520 Train: 0.15613 Test: 0.18760
Epoch: 7600 Train: 0.15398 Test: 0.18599
Epoch: 7680 Train: 0.16615 Test: 0.19626
Epoch: 7760 Train: 0.15072 Test: 0.18284
Epoch: 7840 Train: 0.15279 Test: 0.18334
Epoch: 7920 Train: 0.19195 Test: 0.21643
Epoch: 7999 Train: 0.14615 Test: 0.17869
Training Loss: tensor(0.1462)
Test Loss: tensor(0.1787)
Learned LE: [ 0.50652647  0.05277477 -2.8756244 ]
True LE: [ 8.5858452e-01  3.1048986e-03 -1.4540394e+01]
Relative Error: [0.33437598 0.17492108 0.17083806 0.40280068 0.7104089  0.7030284
 0.67408186 0.7292834  0.83134544 1.1034703  1.2903547  1.1970782
 0.9098972  0.8046462  0.6832061  1.0222013  1.1852338  1.438581
 1.2314398  1.213294   1.365197   1.5105969  0.863166   0.8190564
 1.575056   1.6885757  2.0144894  1.4435809  0.89229614 0.7212632
 0.9868309  1.5114492  1.6260855  1.470314   1.4001106  0.81103873
 0.7093382  1.4275763  1.923725   1.8621799  1.71       0.9750018
 0.7575863  1.1276653  1.0079558  0.79309547 0.70135987 1.3021508
 1.2443762  0.74819815 0.9251351  1.2110835  1.1046396  1.2570328
 1.6631589  1.858288   2.1987562  2.6810272  2.349393   1.845577
 1.3545448  0.8417886  0.5652444  0.40395552 0.36206746 0.43993884
 0.619176   0.9787881  0.91062915 0.82986045 0.7249729  0.9126147
 1.1171395  1.2393639  0.79929036 0.74019635 0.9937315  1.1556942
 1.2179731  1.3336096  1.4922442  1.4062198  1.2197667  1.3194251
 1.5858495  0.5462771  0.4751685  1.1589751  1.4963573  1.7573912
 1.5221713  1.0399121  0.90107256 0.82174724 0.9488222  1.4021415
 1.6986283  1.6354678  1.1790061  0.5378471  0.5868714  1.8754727
 2.1897733  2.025328   1.3588473  0.6605915  0.7455311  1.1126251
 1.3263748  0.86570793 0.8894016  1.0528728  0.47992784 0.520238
 0.69968724 0.82574433 0.9768499  1.465583   1.5413198  1.6910477
 2.0232327  2.5836906  2.1649528  1.5524935  1.028578   0.7718791
 0.6325344  0.6199833  0.5492393  0.68074393 1.0996189  1.062228
 1.098647   1.1270418  1.0449624  0.93052155 0.97471464 0.7562545
 0.6448382  0.94779116 1.5033863  1.4835577  1.6487384  1.4739033
 1.6768014  1.504415   1.736377   2.1407204  0.7860982  0.3614716
 0.84412795 1.3420728  1.7383907  1.7780505  1.766995   1.9642562
 1.9709833  1.5540942  1.1273841  1.1954969  1.4537894  1.6191192
 1.1347121  0.6670483  0.51496595 1.6095917  2.1948256  1.8830746
 1.2884725  0.38050678 0.58545697 1.0694246  1.0456191  0.6757994
 0.91503197 0.6791819  0.21188582 0.34630898 0.4311345  0.70908195
 1.1798631  1.5104718  1.3209993  1.4231522  1.9552903  2.3345916
 1.7516931  1.3979514  1.048522   0.93389964 0.83117306 0.59888035
 0.45481184 0.8718388  1.2146778  1.2877584  1.4007404  1.2811313
 1.0843803  1.0055804  0.69036293 0.71300197 0.81165636 1.1044987
 1.584518   1.6825678  1.8308109  1.5728285  1.4237934  1.6504036
 2.4627392  2.523982   1.0734173  0.6116989  1.1649036  1.9360207
 2.4015324  2.7492294  2.7109454  3.0661595  2.8843896  2.2205942
 1.6669174  1.1998988  0.91404086 1.1616278  1.1957971  0.5809364
 0.47690958 1.0975848  2.1709073  1.9079857  1.5955812  0.49214488
 0.5169031  0.88728106 1.0874895  0.6138164  0.50974625 0.8834842
 0.32020578 0.21337615 0.55247015 0.98277    1.2490396  1.2302574
 0.9826132  1.162215   1.5315009  1.905326   1.5852369  1.0843122
 0.9916324  0.89098066 0.9148581  0.621374   0.5184391  1.1557552
 1.4469409  1.5403472  1.1614558  1.1921097  1.3791277  1.1245157
 0.84454566 0.8966805  0.9742204  1.122973   1.3313226  1.571481
 1.739996   1.6794078  1.4036039  1.2335505  2.5426252  3.0558634
 2.2807631  1.461161   1.6821178  2.4594498  3.0011005  3.6300778
 3.418462   3.118477   2.9996264  2.458393   1.95552    1.4414685
 1.0754734  0.7346069  0.64653236 0.82204866 0.6472447  0.7556099
 1.0680976  1.6203853  1.4471722  1.3670332  0.4320826  0.5299712
 0.76974523 0.6498129  0.284612   0.33933023 0.31766638 0.2856363
 0.41303322 1.2786833  1.0145974  0.6479074  0.55614126 0.6601287
 0.9381653  1.376899   1.5475216  1.0755916  0.8991668  0.79360336
 0.9529721  0.88883424 0.5995351  0.8618272  1.2366908  1.35783
 1.0562674  1.0196753  1.2569423  1.2432259  0.8903118  0.7525854
 0.9572499  1.1149396  1.2591923  1.487451   1.5397713  1.6663703
 1.4938468  1.2499956  1.3925133  2.6701899  3.007645   3.0487175
 2.636661   2.7818015  3.0110447  3.3329277  3.1368752  2.8645964
 2.687251   2.6069295  2.3145397  2.1077104  1.7391596  1.4995791
 1.0834999  0.5153269  0.4741051  0.90677667 1.0298926  0.8718704
 0.88205105 1.2455057  0.7789771  0.5832661  0.28309774 0.56544954
 0.35839713 0.20001791 0.21843861 0.18331188 0.194569   0.5718229
 1.0081025  0.7550354  0.5234414  0.31441608 0.39500064 0.3537519
 0.5712519  0.7272539  1.0402368  0.48320633 0.779428   1.061794
 0.7911109  0.3738555  0.48143226 0.79644144 0.7986854  0.7561161
 0.87067336 1.1513371  1.0130922  0.741595   0.48645294 0.48850992
 0.9075694  0.9694797  1.2380824  1.4960036 ]
