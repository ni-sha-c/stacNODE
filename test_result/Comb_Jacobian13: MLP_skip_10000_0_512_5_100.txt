time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.12%, model saved.
Epoch: 0 Train: 9400.63672 Test: 4207.93164
Epoch: 100 Train: 2445.68213 Test: 1099.33105
Epoch: 200 Train: 1760.26099 Test: 566.21674
Epoch: 300 Train: 664.28052 Test: 121.28632
Epoch: 400 Train: 336.14984 Test: 167.07040
Epoch 500: New minimal relative error: 46.24%, model saved.
Epoch: 500 Train: 142.88354 Test: 20.06370
Epoch 600: New minimal relative error: 34.84%, model saved.
Epoch: 600 Train: 106.57315 Test: 16.21932
Epoch 700: New minimal relative error: 7.79%, model saved.
Epoch: 700 Train: 63.46403 Test: 5.50785
Epoch: 800 Train: 60.19086 Test: 13.29929
Epoch: 900 Train: 65.63316 Test: 12.40441
Epoch: 1000 Train: 38.75038 Test: 5.38461
Epoch: 1100 Train: 71.32336 Test: 35.29336
Epoch 1200: New minimal relative error: 5.01%, model saved.
Epoch: 1200 Train: 29.96155 Test: 1.91428
Epoch: 1300 Train: 33.60902 Test: 9.42688
Epoch: 1400 Train: 33.44368 Test: 7.97317
Epoch 1500: New minimal relative error: 2.16%, model saved.
Epoch: 1500 Train: 22.12610 Test: 0.68497
Epoch: 1600 Train: 21.08616 Test: 3.44172
Epoch: 1700 Train: 25.16388 Test: 8.82351
Epoch: 1800 Train: 22.87269 Test: 4.92895
Epoch: 1900 Train: 21.16225 Test: 4.46788
Epoch: 2000 Train: 15.11941 Test: 0.43774
Epoch: 2100 Train: 14.44634 Test: 0.47594
Epoch: 2200 Train: 17.73648 Test: 4.67279
Epoch: 2300 Train: 16.64161 Test: 0.87596
Epoch 2400: New minimal relative error: 1.69%, model saved.
Epoch: 2400 Train: 12.97813 Test: 0.52828
Epoch: 2500 Train: 14.35459 Test: 3.56303
Epoch: 2600 Train: 12.55994 Test: 0.89946
Epoch: 2700 Train: 12.39369 Test: 0.87903
Epoch: 2800 Train: 18.13541 Test: 6.11266
Epoch: 2900 Train: 12.43976 Test: 1.71784
Epoch: 3000 Train: 11.04869 Test: 1.28253
Epoch: 3100 Train: 10.26510 Test: 0.84620
Epoch: 3200 Train: 12.67286 Test: 2.55256
Epoch 3300: New minimal relative error: 1.67%, model saved.
Epoch: 3300 Train: 9.63845 Test: 0.33732
Epoch: 3400 Train: 20.07210 Test: 8.49152
Epoch: 3500 Train: 16.04484 Test: 4.87153
Epoch: 3600 Train: 11.16323 Test: 1.59431
Epoch: 3700 Train: 8.49926 Test: 0.66505
Epoch: 3800 Train: 9.82320 Test: 1.70344
Epoch: 3900 Train: 9.09338 Test: 1.39555
Epoch: 4000 Train: 10.13092 Test: 1.40295
Epoch: 4100 Train: 9.11952 Test: 1.76512
Epoch: 4200 Train: 7.88808 Test: 0.57130
Epoch: 4300 Train: 7.78584 Test: 0.80143
Epoch: 4400 Train: 8.29439 Test: 0.96284
Epoch 4500: New minimal relative error: 1.54%, model saved.
Epoch: 4500 Train: 7.07909 Test: 0.11085
Epoch: 4600 Train: 7.15148 Test: 0.67430
Epoch: 4700 Train: 6.68166 Test: 0.24086
Epoch: 4800 Train: 6.64657 Test: 0.11333
Epoch: 4900 Train: 13.20308 Test: 3.96804
Epoch: 5000 Train: 8.30124 Test: 1.38750
Epoch: 5100 Train: 15.14023 Test: 4.92519
Epoch 5200: New minimal relative error: 1.15%, model saved.
Epoch: 5200 Train: 6.19328 Test: 0.06154
Epoch: 5300 Train: 6.61996 Test: 0.43350
Epoch 5400: New minimal relative error: 1.03%, model saved.
Epoch: 5400 Train: 5.83014 Test: 0.12018
Epoch: 5500 Train: 5.76798 Test: 0.11263
Epoch: 5600 Train: 8.93631 Test: 4.19169
Epoch 5700: New minimal relative error: 0.83%, model saved.
Epoch: 5700 Train: 5.70327 Test: 0.06006
Epoch: 5800 Train: 5.67725 Test: 0.08130
Epoch: 5900 Train: 5.65797 Test: 0.32183
Epoch: 6000 Train: 6.52649 Test: 0.65426
Epoch: 6100 Train: 6.06644 Test: 0.65061
Epoch: 6200 Train: 5.75967 Test: 0.39895
Epoch: 6300 Train: 5.41593 Test: 0.15350
Epoch: 6400 Train: 7.15755 Test: 0.80149
Epoch: 6500 Train: 10.59653 Test: 3.02919
Epoch: 6600 Train: 9.69349 Test: 5.02822
Epoch: 6700 Train: 5.00087 Test: 0.05054
Epoch: 6800 Train: 5.01831 Test: 0.05699
Epoch: 6900 Train: 4.96884 Test: 0.05441
Epoch: 7000 Train: 5.16485 Test: 0.21944
Epoch: 7100 Train: 6.41856 Test: 1.40642
Epoch: 7200 Train: 5.26756 Test: 0.49916
Epoch: 7300 Train: 5.72080 Test: 0.19963
Epoch: 7400 Train: 5.34600 Test: 0.41305
Epoch: 7500 Train: 5.40162 Test: 0.26358
Epoch 7600: New minimal relative error: 0.62%, model saved.
Epoch: 7600 Train: 4.95761 Test: 0.09626
Epoch: 7700 Train: 4.90118 Test: 0.07589
Epoch: 7800 Train: 5.86528 Test: 0.67334
Epoch: 7900 Train: 4.69266 Test: 0.12459
Epoch: 8000 Train: 4.66588 Test: 0.21935
Epoch: 8100 Train: 5.69721 Test: 0.14867
Epoch: 8200 Train: 4.73082 Test: 0.33214
Epoch: 8300 Train: 4.33565 Test: 0.06749
Epoch: 8400 Train: 4.92048 Test: 0.41501
Epoch: 8500 Train: 4.70440 Test: 0.20008
Epoch: 8600 Train: 4.41238 Test: 0.07931
Epoch: 8700 Train: 4.92120 Test: 0.47128
Epoch: 8800 Train: 4.22600 Test: 0.07352
Epoch: 8900 Train: 4.35124 Test: 0.28416
Epoch: 9000 Train: 4.49262 Test: 0.22018
Epoch: 9100 Train: 4.36949 Test: 0.08833
Epoch: 9200 Train: 4.29517 Test: 0.11350
Epoch: 9300 Train: 4.61065 Test: 0.31510
Epoch: 9400 Train: 4.22725 Test: 0.09655
Epoch: 9500 Train: 4.31071 Test: 0.12841
Epoch: 9600 Train: 4.03063 Test: 0.04580
Epoch: 9700 Train: 4.00738 Test: 0.04952
Epoch: 9800 Train: 4.06070 Test: 0.18855
Epoch: 9900 Train: 3.99902 Test: 0.12250
Epoch: 9999 Train: 4.06557 Test: 0.13488
Training Loss: tensor(4.0656)
Test Loss: tensor(0.1349)
Learned LE: [  0.81061554   0.04897795 -14.53073   ]
True LE: [ 8.6082995e-01  3.3862288e-03 -1.4547920e+01]
Relative Error: [6.5178847  6.5212436  6.5314794  6.6171966  6.627027   6.1657963
 5.9913387  5.4041553  4.8825603  4.2668543  3.6383104  3.163517
 3.191968   3.1664543  2.881949   2.81132    2.8804476  2.9954932
 3.0575154  3.296277   3.519668   4.0470767  4.1389117  3.4811795
 3.1860857  2.7914605  2.2997782  1.6373804  0.8781555  1.0020552
 1.1402974  0.92708457 0.97970515 1.1318951  1.1018679  1.226258
 1.615173   1.8082479  1.9573437  2.0032868  1.7642405  1.3182576
 0.80399626 0.78761804 1.0345165  1.2843419  1.3578718  1.5275714
 1.8168539  2.373431   2.845434   2.9451537  3.2334433  3.386902
 3.7343342  4.2528358  5.011556   4.9511747  4.821374   4.908177
 5.227118   5.392418   5.554479   5.6164546  5.567814   5.579072
 5.636331   5.568774   5.4103427  5.133913   4.5847306  4.087511
 3.369728   2.5756202  2.4153311  2.4726253  2.434848   2.1705196
 2.1357083  2.258325   2.2849457  2.3823922  2.6990464  3.062706
 3.3224132  3.3126905  2.7824922  2.454284   2.0491004  1.702756
 1.0110763  0.7134577  1.1067501  1.2710347  0.97800547 1.0131671
 1.0454679  0.8834961  1.0236747  1.4274787  1.5789609  1.7091917
 1.7887688  1.4415363  0.90023595 0.59755504 0.7786075  1.0319082
 1.1659125  1.248305   1.5031576  1.8108209  2.1833484  2.1994689
 2.5070024  2.7149467  2.9151735  3.2957163  3.9267442  4.072113
 3.9376636  3.8197446  4.03453    4.205836   4.431177   4.6383233
 4.5531087  4.532062   4.6053715  4.7472095  4.681126   4.7853637
 4.433904   4.0650253  3.3364706  2.395055   1.9125149  1.7860082
 1.7946148  1.8022785  1.5988455  1.564549   1.5586959  1.5415417
 1.7654551  2.2136025  2.670141   2.7403095  2.6644695  2.1502807
 1.75882    1.4641321  1.1843824  0.6357255  0.73362046 1.0976794
 1.3120303  1.0110039  0.9740232  0.98032105 0.75739425 0.7928362
 1.1059674  1.2992601  1.5071082  1.5844641  1.3939434  0.9252462
 0.53847665 0.69470394 0.90159905 0.9898896  1.2313846  1.4695884
 1.8433863  1.6746819  1.7987807  2.0581455  2.1853354  2.3810918
 2.9193914  3.1728473  3.108074   3.0238023  2.935488   2.9978461
 2.9972715  3.1974993  3.4837914  3.5335844  3.474896   3.5077882
 3.6252313  3.782083   4.1293864  3.842201   3.5314157  2.6532364
 1.8118422  1.484872   1.203905   1.2615193  1.3593872  1.1442988
 1.0412323  0.94907135 0.98893523 1.3213936  1.828287   2.2493286
 2.1267707  2.1237967  1.7192928  1.2171621  0.92758834 0.80540943
 0.48643398 0.58314615 1.0002787  1.2204006  0.9968986  1.0328721
 1.0427724  0.8275651  0.64613914 0.6253073  0.90611374 1.4527639
 1.721058   1.4987537  1.0590806  0.5958782  0.57065564 0.71415144
 0.9017584  1.2201754  1.4355866  1.3217523  1.2828424  1.4316216
 1.6232893  1.7831523  2.1221917  2.454111   2.2652607  2.1615307
 2.037616   2.0050602  1.8594716  1.6720431  1.743078   2.083763
 2.500926   2.6592147  2.5789275  2.6265018  2.8004754  3.2056234
 3.3034482  2.8502603  2.2216249  1.611892   1.1850576  0.8066321
 0.8169267  0.9128097  0.89384115 0.8169307  0.56782925 0.54659027
 0.896102   1.3663839  1.7637352  1.6289798  1.8678377  1.4196279
 0.9190592  0.53895843 0.4975814  0.47203898 0.37302443 0.68216336
 1.098697   1.0116639  0.9923455  1.0764195  0.9944248  0.764081
 0.5436645  0.6232203  1.3828825  1.6337452  1.6064575  1.3500998
 0.7182654  0.4400616  0.5000195  0.6309075  1.0236069  1.3471342
 1.1100624  0.90529144 1.0791142  1.2777388  1.5440705  1.8182175
 2.0186768  1.6492301  1.2714195  1.089551   1.0278449  0.9752762
 0.58329254 0.5683138  0.6893042  1.1607696  1.7197142  2.0289562
 1.9946128  1.9705174  2.193608   2.4574726  2.4562678  1.9189205
 1.4933814  1.15947    0.7123504  0.49490103 0.65490717 0.82311577
 0.6829148  0.56238186 0.33210105 0.4325618  0.9433887  1.4432918
 1.1911216  1.3352122  1.381814   0.8857196  0.4090882  0.33623073
 0.48822403 0.42647925 0.292995   0.7514436  0.85554886 0.858723
 1.0151503  1.0565151  0.9226985  0.65885496 0.52728397 1.0778718
 1.2845795  1.5561881  1.5892746  0.9558454  0.40326616 0.37423974
 0.4609036  0.8785466  1.1563681  0.7872954  0.66037565 0.8616625
 1.0899475  1.4688547  1.6287763  1.7327139  1.347492   0.7904116
 0.3629277  0.22268799 0.2607608  0.15956521 0.21915568 0.12343364
 0.2837354  0.6100624  1.1543763  1.3957546  1.3421384  1.37865
 1.3924278  1.6043351  1.6510739  1.4012642  1.1860867  0.90902066
 0.55663395 0.44259945 0.55487263 0.8170135  0.779866   0.55057746
 0.35618508 0.45058584 0.93071294 0.98642755]
