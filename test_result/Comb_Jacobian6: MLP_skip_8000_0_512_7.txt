time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.37%, model saved.
Epoch: 0 Train: 171223.23438 Test: 4311.61035
Epoch: 80 Train: 51663.74219 Test: 2553.28296
Epoch: 160 Train: 39632.15625 Test: 1678.39038
Epoch: 240 Train: 37027.57031 Test: 1755.48364
Epoch: 320 Train: 38560.62500 Test: 2888.00098
Epoch 400: New minimal relative error: 91.26%, model saved.
Epoch: 400 Train: 39304.94141 Test: 1272.09290
Epoch: 480 Train: 38000.02734 Test: 1367.76013
Epoch 560: New minimal relative error: 60.78%, model saved.
Epoch: 560 Train: 37246.39062 Test: 1337.54041
Epoch: 640 Train: 40037.96484 Test: 1984.05322
Epoch: 720 Train: 40568.03125 Test: 1956.84863
Epoch: 800 Train: 37091.08203 Test: 1426.59961
Epoch 880: New minimal relative error: 59.60%, model saved.
Epoch: 880 Train: 38754.32422 Test: 1455.02966
Epoch: 960 Train: 36246.72656 Test: 1363.92578
Epoch: 1040 Train: 36642.33984 Test: 1309.23669
Epoch: 1120 Train: 37489.82031 Test: 1344.25342
Epoch: 1200 Train: 36970.82812 Test: 1419.42468
Epoch: 1280 Train: 37559.70703 Test: 1354.71094
Epoch 1360: New minimal relative error: 52.23%, model saved.
Epoch: 1360 Train: 37319.66406 Test: 1320.22058
Epoch: 1440 Train: 35131.96094 Test: 1280.62561
Epoch: 1520 Train: 36827.03906 Test: 1310.66333
Epoch: 1600 Train: 35500.31641 Test: 1359.72632
Epoch: 1680 Train: 36372.33594 Test: 1432.99866
Epoch: 1760 Train: 33353.14453 Test: 1291.35400
Epoch: 1840 Train: 34108.00781 Test: 1178.13098
Epoch: 1920 Train: 35848.06250 Test: 1340.95984
Epoch: 2000 Train: 34670.88672 Test: 1411.22046
Epoch: 2080 Train: 35601.91797 Test: 1253.72534
Epoch: 2160 Train: 33895.74609 Test: 1247.17639
Epoch: 2240 Train: 34143.58594 Test: 1255.02502
Epoch: 2320 Train: 33970.64453 Test: 1328.96033
Epoch: 2400 Train: 37119.49609 Test: 1256.30481
Epoch: 2480 Train: 31634.06445 Test: 1189.41528
Epoch: 2560 Train: 32873.19141 Test: 1196.70752
Epoch: 2640 Train: 30981.88281 Test: 1150.44373
Epoch: 2720 Train: 31349.96289 Test: 1060.85999
Epoch: 2800 Train: 32657.22070 Test: 1178.88000
Epoch: 2880 Train: 33048.62109 Test: 1196.03430
Epoch: 2960 Train: 32788.25000 Test: 1146.15393
Epoch: 3040 Train: 30374.97461 Test: 975.15948
Epoch 3120: New minimal relative error: 51.62%, model saved.
Epoch: 3120 Train: 29730.21484 Test: 914.09064
Epoch: 3200 Train: 29295.02734 Test: 998.75421
Epoch: 3280 Train: 29874.37109 Test: 937.48749
Epoch: 3360 Train: 30348.84375 Test: 1010.50073
Epoch: 3440 Train: 28762.01758 Test: 911.80206
Epoch: 3520 Train: 26675.11523 Test: 764.86627
Epoch: 3600 Train: 27976.87695 Test: 790.50610
Epoch: 3680 Train: 28895.71875 Test: 905.09058
Epoch: 3760 Train: 27111.37305 Test: 825.28345
Epoch: 3840 Train: 26687.44727 Test: 731.01440
Epoch: 3920 Train: 25449.83984 Test: 734.78656
Epoch: 4000 Train: 25328.78125 Test: 724.23755
Epoch 4080: New minimal relative error: 47.26%, model saved.
Epoch: 4080 Train: 24897.02148 Test: 697.31812
Epoch: 4160 Train: 24381.88086 Test: 619.69647
Epoch: 4240 Train: 20935.49805 Test: 474.34088
Epoch: 4320 Train: 18839.99219 Test: 393.83270
Epoch: 4400 Train: 18496.26562 Test: 373.25806
Epoch: 4480 Train: 19404.28516 Test: 404.31989
Epoch: 4560 Train: 19690.68945 Test: 405.41492
Epoch: 4640 Train: 15005.94141 Test: 280.71259
Epoch: 4720 Train: 12276.96094 Test: 180.06319
Epoch: 4800 Train: 6357.48779 Test: 76.40194
Epoch 4880: New minimal relative error: 36.34%, model saved.
Epoch: 4880 Train: 4432.14453 Test: 30.92027
Epoch: 4960 Train: 4433.60400 Test: 35.87628
Epoch 5040: New minimal relative error: 27.77%, model saved.
Epoch: 5040 Train: 3642.93994 Test: 23.67950
Epoch 5120: New minimal relative error: 19.15%, model saved.
Epoch: 5120 Train: 2994.52563 Test: 18.21450
Epoch 5200: New minimal relative error: 16.13%, model saved.
Epoch: 5200 Train: 2788.08862 Test: 18.67646
Epoch 5280: New minimal relative error: 13.68%, model saved.
Epoch: 5280 Train: 2662.44434 Test: 15.68860
Epoch: 5360 Train: 2137.87964 Test: 12.95937
Epoch: 5440 Train: 1901.90906 Test: 9.00401
Epoch 5520: New minimal relative error: 11.77%, model saved.
Epoch: 5520 Train: 1584.45557 Test: 7.27865
Epoch: 5600 Train: 1470.70544 Test: 9.12550
Epoch: 5680 Train: 1449.55835 Test: 13.47020
Epoch: 5760 Train: 1342.77063 Test: 11.98231
Epoch 5840: New minimal relative error: 8.53%, model saved.
Epoch: 5840 Train: 1235.40210 Test: 10.02046
Epoch 5920: New minimal relative error: 8.22%, model saved.
Epoch: 5920 Train: 1120.56885 Test: 3.35066
Epoch: 6000 Train: 1200.20642 Test: 9.84932
Epoch: 6080 Train: 1229.13525 Test: 8.09066
Epoch: 6160 Train: 1331.72644 Test: 14.49683
Epoch: 6240 Train: 1165.21045 Test: 5.94949
Epoch: 6320 Train: 1002.06982 Test: 3.35183
Epoch: 6400 Train: 954.46692 Test: 4.14701
Epoch: 6480 Train: 962.32977 Test: 4.34393
Epoch: 6560 Train: 1029.13599 Test: 4.52939
Epoch: 6640 Train: 1072.94080 Test: 11.36425
Epoch: 6720 Train: 945.63611 Test: 5.98424
Epoch 6800: New minimal relative error: 8.09%, model saved.
Epoch: 6800 Train: 830.21564 Test: 2.26173
Epoch: 6880 Train: 789.42853 Test: 2.49752
Epoch: 6960 Train: 821.95776 Test: 10.41326
Epoch: 7040 Train: 759.11768 Test: 3.56611
Epoch 7120: New minimal relative error: 7.17%, model saved.
Epoch: 7120 Train: 735.00690 Test: 3.61536
Epoch: 7200 Train: 729.97229 Test: 5.04523
Epoch: 7280 Train: 686.13275 Test: 2.00250
Epoch: 7360 Train: 706.69397 Test: 3.30890
Epoch: 7440 Train: 683.27356 Test: 3.04621
Epoch 7520: New minimal relative error: 4.72%, model saved.
Epoch: 7520 Train: 605.20673 Test: 1.67202
Epoch: 7600 Train: 633.07349 Test: 2.67932
Epoch: 7680 Train: 615.82770 Test: 1.26588
Epoch: 7760 Train: 638.43091 Test: 3.71427
Epoch: 7840 Train: 604.96637 Test: 2.58045
Epoch: 7920 Train: 547.82642 Test: 1.13885
Epoch: 7999 Train: 528.77307 Test: 1.14641
Training Loss: tensor(528.7731)
Test Loss: tensor(1.1464)
Learned LE: [  0.94006157  -0.10258601 -14.534372  ]
True LE: [ 8.6642021e-01  4.2731673e-03 -1.4540831e+01]
Relative Error: [5.661584   6.143602   6.6791577  7.190948   7.692811   8.108386
 8.14683    7.8938427  7.4834504  7.017104   6.2334895  5.432571
 4.379561   3.6516864  2.9662027  2.329063   1.8776426  1.6989423
 1.7302608  2.001553   2.6835992  3.0955     3.7780173  4.614156
 5.8436675  6.1651325  5.795432   4.9650965  4.352419   4.1688848
 4.2783303  3.9814095  3.4218643  3.031088   3.2762275  4.0801654
 5.318673   5.932524   6.3143244  7.02168    7.7546983  8.170037
 8.45805    8.944881   9.332352   9.138822   9.033847   8.156468
 6.7127357  5.709986   5.240246   5.002726   5.066193   5.2629766
 4.749883   4.5280004  4.421691   4.410215   4.4667006  4.4001245
 4.504751   4.663908   4.88418    5.3767233  5.819087   6.391157
 7.04642    7.496001   7.910833   7.997187   7.692365   7.494088
 6.9908233  6.0285583  5.1015706  4.167563   3.2502203  2.3842645
 1.6255606  1.3781601  1.2920642  1.3283868  1.5052774  2.0437858
 2.6722317  3.2495708  4.0899816  5.288119   5.5556717  5.1254606
 4.146086   3.612047   3.4825895  3.5473402  3.0624943  2.504332
 2.3721495  2.874767   3.854792   4.8541627  5.2301135  5.672545
 6.4024014  6.721478   6.7121916  6.891781   7.7651596  8.143446
 7.9234004  7.9527116  7.7019477  6.3161426  5.3809233  4.795732
 4.4793715  4.4997396  4.3379006  3.9225464  3.7226768  3.6344376
 3.827576   4.0647087  4.1045437  4.215608   4.3925633  4.6471643
 5.074501   5.6089325  6.090838   6.773384   7.219398   7.4981794
 7.5591216  7.3637843  7.106801   6.857688   5.7884088  4.8549137
 4.1442814  3.115216   2.0314102  1.384567   1.0914588  0.91917485
 0.9153242  0.9520211  1.3193965  2.0932188  2.657382   3.4186826
 4.536907   5.0872765  4.5965953  3.5730999  3.0462456  2.9560287
 2.7118542  2.2444623  1.7843943  1.8502312  2.5265198  3.3653603
 4.3743987  4.695411   5.2082024  5.6302776  5.5035443  5.348121
 5.4507866  6.336545   6.9809275  6.9507966  6.995931   6.781026
 6.1103373  4.962377   4.2106347  3.9274151  3.813596   3.5516188
 3.1267576  2.8543808  2.9514403  3.2224855  3.5755231  3.7091765
 3.8374782  4.160941   4.464289   4.665483   5.2131243  5.6619477
 6.0717044  6.43242    6.554078   6.622567   6.5989776  6.4676366
 6.240066   5.4526687  4.4282045  3.702449   3.1469536  2.0691333
 1.3620064  1.0555923  0.8143236  0.59195185 0.52023965 0.7197854
 1.2039593  1.9903988  2.539309   3.4800048  4.4504805  4.3108997
 3.455779   2.7408574  2.4727466  2.238603   1.5308214  1.2932932
 1.324379   1.959711   2.710825   3.768211   4.3064585  4.4997616
 4.7409787  4.489628   4.211119   4.1209707  4.6452413  5.5402517
 5.9548903  5.996226   5.9272194  5.7564545  4.8423634  3.865163
 3.3606303  3.227469   2.9533617  2.380308   2.1734977  2.340829
 2.6991448  3.1202765  3.4669235  3.5468392  3.726738   3.912102
 4.197954   4.304384   4.616176   4.797721   4.987953   5.3381367
 5.522218   5.612773   5.5080967  5.436406   5.383222   4.393476
 3.3516798  2.7030263  2.3129354  1.706528   1.1984037  0.92274445
 0.69705045 0.59535384 0.5840956  0.47921395 0.958013   1.6997583
 2.405085   3.4228709  4.1359606  3.5226629  2.8149252  2.449322
 2.114056   1.2690343  0.79288256 0.8430418  1.2620207  1.87688
 2.7841024  3.6800768  3.874039   3.9483361  3.9189343  3.5135581
 3.1694658  3.207393   3.6557357  4.53781    4.9574904  5.1013765
 4.9802966  4.8119516  4.127792   3.22126    2.7568197  2.6298501
 1.9175608  1.4187305  1.6000754  2.0518277  2.633886   3.2786236
 3.5931702  3.592366   3.3730094  3.2665644  3.3880453  3.6603715
 3.7486916  3.8104575  3.969949   4.1582413  4.548207   4.749598
 4.711582   4.572701   4.465162   3.677941   2.848706   2.0755262
 1.8006874  1.5538851  1.2851661  1.002032   0.630923   0.55378103
 0.6404854  0.54176056 0.40410668 1.0488737  1.9752781  2.9769886
 3.7259226  3.246753   2.4602888  2.2073987  2.0310044  1.0215745
 0.54611945 0.45421636 0.96004224 1.6810619  2.4161448  3.206227
 3.470344   3.378292   3.297343   2.839569   2.4322293  2.2936041
 2.493641   3.223563   4.043803   4.119946   4.235426   4.240895
 4.130304   3.0753896  2.38282    2.234832   1.3492484  0.75561064
 0.88471335 1.4490508  2.1361697  2.974083   3.4262943  3.3205283
 3.2772613  3.048946   2.7760763  2.753464   2.920832   3.1266992
 3.2211747  3.308363   3.625968   4.1484895  4.1677036  3.9048965
 3.7578874  3.413819   2.6115012  2.2157946  1.8322405  1.3708404
 1.3411816  1.148717   0.8110526  0.406204  ]
