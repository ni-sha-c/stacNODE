time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 109.08%, model saved.
Epoch: 0 Train: 4118.86133 Test: 4101.46875
Epoch 100: New minimal relative error: 41.30%, model saved.
Epoch: 100 Train: 116.32513 Test: 89.71783
Epoch: 200 Train: 40.40170 Test: 41.35457
Epoch 300: New minimal relative error: 24.03%, model saved.
Epoch: 300 Train: 12.85343 Test: 10.33097
Epoch 400: New minimal relative error: 18.33%, model saved.
Epoch: 400 Train: 9.05877 Test: 14.21095
Epoch: 500 Train: 63.33188 Test: 62.96713
Epoch 600: New minimal relative error: 14.15%, model saved.
Epoch: 600 Train: 5.92117 Test: 5.70868
Epoch: 700 Train: 4.13338 Test: 4.00992
Epoch: 800 Train: 15.57739 Test: 13.57150
Epoch: 900 Train: 8.53202 Test: 12.15740
Epoch: 1000 Train: 3.95383 Test: 6.57300
Epoch: 1100 Train: 7.76678 Test: 9.65199
Epoch: 1200 Train: 3.59985 Test: 2.47327
Epoch: 1300 Train: 3.87544 Test: 13.31028
Epoch: 1400 Train: 1.04968 Test: 1.14769
Epoch 1500: New minimal relative error: 12.82%, model saved.
Epoch: 1500 Train: 0.96494 Test: 0.89533
Epoch: 1600 Train: 1.47256 Test: 1.15335
Epoch: 1700 Train: 3.86134 Test: 3.76023
Epoch 1800: New minimal relative error: 10.89%, model saved.
Epoch: 1800 Train: 1.30669 Test: 1.60946
Epoch: 1900 Train: 2.30016 Test: 6.95219
Epoch: 2000 Train: 0.50307 Test: 0.61045
Epoch: 2100 Train: 3.32424 Test: 3.62872
Epoch: 2200 Train: 1.04006 Test: 1.25093
Epoch: 2300 Train: 4.49640 Test: 4.74990
Epoch: 2400 Train: 0.68360 Test: 1.05619
Epoch: 2500 Train: 6.13801 Test: 7.83552
Epoch: 2600 Train: 0.41859 Test: 0.92595
Epoch: 2700 Train: 6.47293 Test: 6.00925
Epoch 2800: New minimal relative error: 7.82%, model saved.
Epoch: 2800 Train: 0.31522 Test: 0.46095
Epoch: 2900 Train: 1.69109 Test: 1.38995
Epoch: 3000 Train: 0.63824 Test: 0.86019
Epoch: 3100 Train: 0.42070 Test: 0.59047
Epoch: 3200 Train: 4.10367 Test: 2.90021
Epoch: 3300 Train: 1.43450 Test: 1.35649
Epoch: 3400 Train: 0.61790 Test: 0.49084
Epoch: 3500 Train: 0.23682 Test: 0.32046
Epoch: 3600 Train: 0.69497 Test: 0.72881
Epoch: 3700 Train: 0.46883 Test: 0.61972
Epoch 3800: New minimal relative error: 7.17%, model saved.
Epoch: 3800 Train: 0.88158 Test: 1.24658
Epoch: 3900 Train: 1.51087 Test: 1.81721
Epoch: 4000 Train: 0.95096 Test: 0.73651
Epoch: 4100 Train: 0.15222 Test: 0.22162
Epoch: 4200 Train: 0.27760 Test: 0.41225
Epoch: 4300 Train: 3.13508 Test: 4.44540
Epoch: 4400 Train: 1.22458 Test: 1.79193
Epoch: 4500 Train: 0.19819 Test: 0.29981
Epoch: 4600 Train: 4.12038 Test: 1.67896
Epoch: 4700 Train: 0.24949 Test: 0.33768
Epoch: 4800 Train: 1.91826 Test: 2.15616
Epoch: 4900 Train: 0.29378 Test: 0.34820
Epoch: 5000 Train: 0.49666 Test: 0.62490
Epoch: 5100 Train: 0.09660 Test: 0.14737
Epoch: 5200 Train: 0.08986 Test: 0.14099
Epoch: 5300 Train: 0.12307 Test: 0.17298
Epoch: 5400 Train: 0.53016 Test: 0.81609
Epoch: 5500 Train: 0.47227 Test: 0.66120
Epoch: 5600 Train: 0.10064 Test: 0.37795
Epoch: 5700 Train: 0.31467 Test: 0.38149
Epoch: 5800 Train: 0.32001 Test: 0.29952
Epoch: 5900 Train: 0.75721 Test: 0.87948
Epoch: 6000 Train: 0.13836 Test: 0.15419
Epoch 6100: New minimal relative error: 4.62%, model saved.
Epoch: 6100 Train: 0.16781 Test: 0.24513
Epoch: 6200 Train: 0.09463 Test: 0.12903
Epoch: 6300 Train: 0.14088 Test: 0.18722
Epoch: 6400 Train: 0.27424 Test: 0.41165
Epoch: 6500 Train: 0.07169 Test: 0.13121
Epoch: 6600 Train: 0.69187 Test: 0.53525
Epoch: 6700 Train: 0.08803 Test: 0.13222
Epoch: 6800 Train: 0.10993 Test: 0.16010
Epoch: 6900 Train: 1.13093 Test: 1.64445
Epoch: 7000 Train: 0.20866 Test: 0.22271
Epoch: 7100 Train: 0.14224 Test: 0.17658
Epoch: 7200 Train: 0.26956 Test: 0.16059
Epoch: 7300 Train: 0.22882 Test: 0.29610
Epoch: 7400 Train: 0.04896 Test: 0.08798
Epoch: 7500 Train: 0.05497 Test: 0.08835
Epoch: 7600 Train: 0.22895 Test: 0.18011
Epoch: 7700 Train: 0.05188 Test: 0.08471
Epoch: 7800 Train: 0.08544 Test: 0.14035
Epoch: 7900 Train: 0.04907 Test: 0.09016
Epoch: 8000 Train: 0.07183 Test: 0.10226
Epoch: 8100 Train: 0.04758 Test: 0.08781
Epoch: 8200 Train: 0.06880 Test: 0.09733
Epoch: 8300 Train: 0.11075 Test: 0.14445
Epoch: 8400 Train: 0.22352 Test: 0.29313
Epoch: 8500 Train: 0.35807 Test: 0.44309
Epoch: 8600 Train: 1.03148 Test: 1.16421
Epoch: 8700 Train: 0.03664 Test: 0.06771
Epoch: 8800 Train: 0.03653 Test: 0.06791
Epoch: 8900 Train: 0.03870 Test: 0.06836
Epoch: 9000 Train: 0.12404 Test: 0.16875
Epoch: 9100 Train: 0.04717 Test: 0.08256
Epoch: 9200 Train: 0.19223 Test: 0.19641
Epoch: 9300 Train: 0.04358 Test: 0.09301
Epoch: 9400 Train: 0.13284 Test: 0.18973
Epoch: 9500 Train: 0.04407 Test: 0.08255
Epoch: 9600 Train: 0.03490 Test: 0.06281
Epoch: 9700 Train: 0.06481 Test: 0.09989
Epoch: 9800 Train: 0.05281 Test: 0.08059
Epoch: 9900 Train: 0.05530 Test: 0.08673
Epoch: 9999 Train: 0.10460 Test: 0.15375
Training Loss: tensor(0.1046)
Test Loss: tensor(0.1538)
Learned LE: [ 0.77716744  0.06925005 -3.048664  ]
True LE: [ 8.6952084e-01  7.3477537e-03 -1.4548327e+01]
Relative Error: [6.6680655  7.1933994  7.757318   8.20641    8.16771    7.8847113
 7.4695916  6.611882   6.0227947  4.987485   4.701622   4.3772187
 3.5834684  2.7354903  2.0341666  1.6098927  0.86275774 0.8797979
 1.7257926  2.8926632  3.2555442  3.3077424  3.4671009  3.6657486
 3.9349372  3.77285    3.573182   3.042473   2.605367   2.0891638
 1.0475127  1.0307406  0.9790083  0.7189641  0.9792421  1.014925
 1.0556108  1.0783554  1.3257216  1.5924826  1.830711   1.976191
 2.198207   2.1099908  1.7528626  1.4982985  1.2286764  1.0876273
 1.1757722  1.2866713  1.617861   1.9201107  1.7020086  1.4196912
 1.2876185  1.3406593  1.718239   2.4925451  3.3068738  4.020917
 4.803963   5.30642    5.557551   5.8270297  6.195444   6.6442304
 7.048513   7.032789   7.016299   6.174722   5.130288   4.174537
 3.5361805  3.4210258  3.0388803  2.422214   1.437568   1.1010836
 0.80460924 0.67950904 1.3816506  2.2779698  3.0827131  3.686449
 3.5728033  3.4730885  3.5977814  3.7489176  3.496782   3.336845
 2.9764767  2.6361632  1.8279406  1.0871102  1.1752989  0.75986564
 0.6926716  0.95398116 1.0762919  1.0959611  1.0732715  1.2491041
 1.4853331  1.66386    1.8126767  1.6749697  1.4452509  1.2631168
 1.0195624  0.86324066 0.8727377  0.996729   1.1742     1.5674587
 1.8824197  1.7773254  1.2978324  0.8597389  0.7546632  1.2580453
 1.8778145  2.63903    3.3766735  4.0346427  4.436722   4.6798472
 4.7300324  4.9138756  5.2104263  5.672838   6.0397005  5.923406
 4.5599713  3.7180007  2.775678   2.2934654  2.0399969  1.8900454
 1.1570613  0.54158545 0.64136976 0.8313643  1.2762392  1.8675966
 2.3948948  2.9618318  3.4344814  3.7047207  3.4703023  3.3953383
 3.4538608  3.1505744  2.9999447  2.768047   2.4657018  1.7194035
 1.0728159  1.1428729  0.7614731  0.7651212  0.9772576  1.0410669
 0.9561794  1.0117943  1.1104728  1.3098332  1.4156288  1.2299689
 1.0995448  0.9707639  0.8621772  0.7174986  0.6304046  0.70746726
 0.79877174 1.012278   1.3368652  1.5879241  1.7970262  1.2969793
 0.7004265  0.47458178 0.7762244  1.2185507  1.8896837  2.5853102
 3.0744126  3.4603462  3.6111784  3.6113718  3.6301703  3.813143
 4.206178   4.742729   4.401903   3.280382   2.338002   1.5562044
 0.98402715 0.56700164 0.62139755 0.63794315 1.0344038  1.1574119
 1.5365181  1.9183662  2.1182714  2.3623204  2.6560066  2.933445
 3.450752   3.4931092  3.166637   3.0616484  2.8662796  2.625015
 2.502377   2.2029884  1.7301753  1.185182   1.1193422  0.8079017
 0.76177883 1.1076736  0.8605199  0.83765805 0.80894613 0.86465925
 1.051347   0.92330956 0.7490789  0.655484   0.6460124  0.54007065
 0.4734761  0.44494992 0.5049336  0.56593853 0.6843904  0.92051893
 1.1389277  1.354543   1.4382277  0.96177423 0.5754267  0.38457507
 0.71198374 1.2525952  1.7779689  2.0999515  2.4978447  2.5860984
 2.6212656  2.498305   2.483679   2.821419   3.3083394  2.793974
 2.2685761  1.238479   0.41207546 0.36641166 0.84050715 1.014113
 1.4689039  1.725154   1.8843329  2.0253003  2.2219594  2.1182399
 2.1533928  2.3426173  2.404224   2.6342397  3.1781158  2.8935921
 2.674329   2.6088574  2.230189   2.036626   2.1058898  1.6868999
 1.3742703  1.0320462  0.80696774 0.9119218  0.7965401  0.67075807
 0.72868997 0.6808563  0.5863078  0.7440111  0.47178626 0.37784508
 0.36361438 0.36136013 0.3669769  0.3314889  0.33967236 0.31452695
 0.35834184 0.46077776 0.5642187  0.6696085  0.8872706  1.1682061
 1.2614658  0.86519647 0.35620233 0.17836396 0.6176024  1.1446482
 1.344933   1.6360486  1.7731891  1.7747164  1.7388357  1.634612
 1.4429244  1.859257   1.2647908  1.5040864  1.2786156  1.5853863
 1.8590974  2.0485938  2.039978   2.0809946  1.9750648  2.180404
 2.1351683  2.2797585  2.045926   1.8247313  1.9163874  1.9368466
 1.8860406  2.0156965  2.4104073  2.2973075  2.0365396  1.9028273
 1.5805295  1.5731348  1.5457485  1.3118886  1.1749194  0.68564785
 0.92485845 0.8855949  0.48518595 0.6240365  0.7339818  0.50817466
 0.32232147 0.28642923 0.24750869 0.28054133 0.3876156  0.43152767
 0.46009436 0.3674364  0.27964798 0.29032242 0.30482638 0.41033575
 0.46051633 0.6272973  0.83645976 1.0153803  1.1826043  0.8053055
 0.339017   0.2120668  0.6377628  1.0140917  0.972095   1.1827396
 1.214989   1.2310573  1.534977   1.5801436  1.1571223  0.9533917
 0.91139513 1.3159517  1.3068714  1.824056   2.0120747  1.9128442
 2.3494341  2.1281202  1.8141472  1.8658636  1.7760068  1.946928
 1.7616801  1.5368003  1.5873625  1.4544309 ]
