time_step: 0.25
lr: 0.001
weight_decay: 0.0005
num_epoch: 3000
num_train: 3000
num_test: 3000
num_val: 0
num_trans: 0
loss_type: MSE
dyn_sys: KS
model_type: MLP_skip
s: 0.2
n_hidden: 512
n_layers: 3
reg_param: 0.5
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 0.238153997 Test: 0.160656209
Epoch 0: New minimal relative error: 0.16%, model saved.
Epoch: 30 Train: 0.042032799 Test: 0.114577906
Epoch 30: New minimal relative error: 0.11%, model saved.
Epoch: 60 Train: 0.012326366 Test: 0.120751790
Epoch: 90 Train: 0.006390746 Test: 0.125230488
Epoch: 120 Train: 0.003779694 Test: 0.126545615
Epoch: 150 Train: 0.003141692 Test: 0.129328566
Epoch: 180 Train: 0.001877446 Test: 0.129944194
Epoch: 210 Train: 0.001536032 Test: 0.130824591
Epoch: 240 Train: 0.001801431 Test: 0.131468005
Epoch: 270 Train: 0.000997151 Test: 0.132710022
Epoch: 300 Train: 0.000906269 Test: 0.133125328
Epoch: 330 Train: 0.000782604 Test: 0.133742846
Epoch: 360 Train: 0.000841393 Test: 0.134043169
Epoch: 390 Train: 0.000573957 Test: 0.134686464
Epoch: 420 Train: 0.000623449 Test: 0.135095388
Epoch: 450 Train: 0.000461628 Test: 0.135573730
Epoch: 480 Train: 0.000458853 Test: 0.135566774
Epoch: 510 Train: 0.000381690 Test: 0.136161314
Epoch: 540 Train: 0.000349194 Test: 0.136426288
Epoch: 570 Train: 0.000399221 Test: 0.136872886
Epoch: 600 Train: 0.000302753 Test: 0.137052298
Epoch: 630 Train: 0.000286402 Test: 0.137306568
Epoch: 660 Train: 0.000269147 Test: 0.137514997
Epoch: 690 Train: 0.000305831 Test: 0.137597439
Epoch: 720 Train: 0.000240222 Test: 0.138002152
Epoch: 750 Train: 0.000218246 Test: 0.138114977
Epoch: 780 Train: 0.000220893 Test: 0.138268075
Epoch: 810 Train: 0.000195475 Test: 0.138430684
Epoch: 840 Train: 0.000195048 Test: 0.138598715
Epoch: 870 Train: 0.000176526 Test: 0.138703648
Epoch: 900 Train: 0.000203378 Test: 0.138776543
Epoch: 930 Train: 0.000161087 Test: 0.139022287
Epoch: 960 Train: 0.000177222 Test: 0.139206623
Epoch: 990 Train: 0.000147450 Test: 0.139264591
Epoch: 1020 Train: 0.000157388 Test: 0.139372558
Epoch: 1050 Train: 0.000136063 Test: 0.139438795
Epoch: 1080 Train: 0.000131593 Test: 0.139543800
Epoch: 1110 Train: 0.000133471 Test: 0.139636527
Epoch: 1140 Train: 0.000125357 Test: 0.139742726
Epoch: 1170 Train: 0.000120537 Test: 0.139805068
Epoch: 1200 Train: 0.000114498 Test: 0.139878562
Epoch: 1230 Train: 0.000111142 Test: 0.139961230
Epoch: 1260 Train: 0.000108807 Test: 0.140036310
Epoch: 1290 Train: 0.000104173 Test: 0.140097031
Epoch: 1320 Train: 0.000101466 Test: 0.140155101
Epoch: 1350 Train: 0.000098607 Test: 0.140224238
Epoch: 1380 Train: 0.000095736 Test: 0.140286390
Epoch: 1410 Train: 0.000093273 Test: 0.140346068
Epoch: 1440 Train: 0.000090922 Test: 0.140396915
Epoch: 1470 Train: 0.000088612 Test: 0.140449760
Epoch: 1500 Train: 0.000086483 Test: 0.140500416
Epoch: 1530 Train: 0.000084461 Test: 0.140551632
Epoch: 1560 Train: 0.000082521 Test: 0.140596385
Epoch: 1590 Train: 0.000080676 Test: 0.140642103
Epoch: 1620 Train: 0.000078920 Test: 0.140683930
Epoch: 1650 Train: 0.000077244 Test: 0.140728010
Epoch: 1680 Train: 0.000075642 Test: 0.140768746
Epoch: 1710 Train: 0.000074115 Test: 0.140810102
Epoch: 1740 Train: 0.000072651 Test: 0.140850309
Epoch: 1770 Train: 0.000071250 Test: 0.140887967
Epoch: 1800 Train: 0.000069908 Test: 0.140925014
Epoch: 1830 Train: 0.000068621 Test: 0.140962101
Epoch: 1860 Train: 0.000067386 Test: 0.140997904
Epoch: 1890 Train: 0.000066199 Test: 0.141032738
Epoch: 1920 Train: 0.000065059 Test: 0.141066614
Epoch: 1950 Train: 0.000063964 Test: 0.141099284
Epoch: 1980 Train: 0.000062911 Test: 0.141131553
Epoch: 2010 Train: 0.000061899 Test: 0.141162493
Epoch: 2040 Train: 0.000060923 Test: 0.141192547
Epoch: 2070 Train: 0.000059982 Test: 0.141221806
Epoch: 2100 Train: 0.000059074 Test: 0.141250762
Epoch: 2130 Train: 0.000058199 Test: 0.141279061
Epoch: 2160 Train: 0.000057353 Test: 0.141306882
Epoch: 2190 Train: 0.000056536 Test: 0.141333648
Epoch: 2220 Train: 0.000055746 Test: 0.141360614
Epoch: 2250 Train: 0.000054983 Test: 0.141387013
Epoch: 2280 Train: 0.000054246 Test: 0.141412854
Epoch: 2310 Train: 0.000053532 Test: 0.141437644
Epoch: 2340 Train: 0.000052841 Test: 0.141462423
Epoch: 2370 Train: 0.000052171 Test: 0.141486520
Epoch: 2400 Train: 0.000051521 Test: 0.141510226
Epoch: 2430 Train: 0.000050893 Test: 0.141533274
Epoch: 2460 Train: 0.000050284 Test: 0.141555483
Epoch: 2490 Train: 0.000049694 Test: 0.141577954
Epoch: 2520 Train: 0.000049120 Test: 0.141599826
Epoch: 2550 Train: 0.000048564 Test: 0.141621268
Epoch: 2580 Train: 0.000048024 Test: 0.141641965
Epoch: 2610 Train: 0.000047501 Test: 0.141662240
Epoch: 2640 Train: 0.000046993 Test: 0.141681859
Epoch: 2670 Train: 0.000046499 Test: 0.141701171
Epoch: 2700 Train: 0.000046019 Test: 0.141719954
Epoch: 2730 Train: 0.000045551 Test: 0.141738239
Epoch: 2760 Train: 0.000045096 Test: 0.141756456
Epoch: 2790 Train: 0.000044655 Test: 0.141774167
Epoch: 2820 Train: 0.000044225 Test: 0.141791966
Epoch: 2850 Train: 0.000043807 Test: 0.141809451
Epoch: 2880 Train: 0.000043400 Test: 0.141826422
Epoch: 2910 Train: 0.000043003 Test: 0.141843028
Epoch: 2940 Train: 0.000042618 Test: 0.141859250
Epoch: 2970 Train: 0.000042242 Test: 0.141874998
Epoch: 2999 Train: 0.000041887 Test: 0.141889913
Training Loss: tensor(4.1887e-05)
Test Loss: tensor(0.1419)
True Mean x: tensor(-0.7255, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)
Learned Mean x: tensor(2.8881e+42, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
True Var x: tensor(0.2436, device='cuda:0', dtype=torch.float64, grad_fn=<VarBackward0>)
Learned Var x: tensor(5.2928e+86, device='cuda:0', dtype=torch.float64,
       grad_fn=<VarBackward0>)
True Mean z: tensor(-1.5290, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)
Learned Mean z: tensor(-9.6824e+41, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
True Var z: tensor(0.6490, device='cuda:0', dtype=torch.float64, grad_fn=<VarBackward0>)
Learned Var z: tensor(9.2187e+85, device='cuda:0', dtype=torch.float64,
       grad_fn=<VarBackward0>)
Learned LE: [0.15646686 0.151236   0.14327458 0.13866721 0.14039522 0.13692333
 0.13082786 0.13331771 0.12819725 0.11997113 0.11711146 0.11721824
 0.11257835 0.1057974  0.10548273]
True LE: [0.30574609 0.28004271 0.26752761 0.23201314 0.20837498 0.19183291
 0.16715513 0.15416377 0.12779687 0.11077707 0.09813166 0.07510605
 0.05770563 0.04353978 0.02605524]
Norm Diff:: tensor(0.2964, dtype=torch.float64)
