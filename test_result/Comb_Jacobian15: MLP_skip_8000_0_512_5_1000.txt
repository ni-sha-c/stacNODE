time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.47%, model saved.
Epoch: 0 Train: 60785.43750 Test: 4027.20654
Epoch: 80 Train: 16229.13477 Test: 1870.94116
Epoch 160: New minimal relative error: 90.53%, model saved.
Epoch: 160 Train: 14621.72461 Test: 1213.07715
Epoch: 240 Train: 12590.98340 Test: 1211.85352
Epoch: 320 Train: 13714.48730 Test: 1155.13293
Epoch 400: New minimal relative error: 58.19%, model saved.
Epoch: 400 Train: 14038.11621 Test: 1088.09839
Epoch: 480 Train: 11829.76953 Test: 1800.39929
Epoch: 560 Train: 11017.99121 Test: 984.99323
Epoch: 640 Train: 10061.37891 Test: 821.88873
Epoch: 720 Train: 9549.29590 Test: 612.74512
Epoch: 800 Train: 8459.73047 Test: 546.04626
Epoch 880: New minimal relative error: 58.16%, model saved.
Epoch: 880 Train: 6975.96680 Test: 401.56570
Epoch: 960 Train: 5674.98486 Test: 250.33417
Epoch: 1040 Train: 4979.20508 Test: 203.39566
Epoch: 1120 Train: 2759.64478 Test: 82.91147
Epoch 1200: New minimal relative error: 54.34%, model saved.
Epoch: 1200 Train: 1761.70288 Test: 60.22104
Epoch 1280: New minimal relative error: 53.86%, model saved.
Epoch: 1280 Train: 2063.75366 Test: 133.08582
Epoch 1360: New minimal relative error: 12.27%, model saved.
Epoch: 1360 Train: 1150.38550 Test: 18.97604
Epoch: 1440 Train: 869.47430 Test: 18.37691
Epoch: 1520 Train: 746.52979 Test: 19.45781
Epoch 1600: New minimal relative error: 10.12%, model saved.
Epoch: 1600 Train: 642.23950 Test: 7.95389
Epoch: 1680 Train: 567.89215 Test: 10.67627
Epoch: 1760 Train: 525.99451 Test: 10.08616
Epoch 1840: New minimal relative error: 9.72%, model saved.
Epoch: 1840 Train: 461.72543 Test: 4.85979
Epoch 1920: New minimal relative error: 5.87%, model saved.
Epoch: 1920 Train: 426.09213 Test: 5.36518
Epoch: 2000 Train: 396.51260 Test: 10.39362
Epoch 2080: New minimal relative error: 4.53%, model saved.
Epoch: 2080 Train: 370.83621 Test: 2.72775
Epoch 2160: New minimal relative error: 3.60%, model saved.
Epoch: 2160 Train: 350.76483 Test: 3.02938
Epoch: 2240 Train: 326.06787 Test: 2.15878
Epoch: 2320 Train: 312.38809 Test: 2.18833
Epoch: 2400 Train: 281.45264 Test: 2.05099
Epoch: 2480 Train: 268.46500 Test: 2.02329
Epoch: 2560 Train: 245.12347 Test: 2.13499
Epoch: 2640 Train: 236.82607 Test: 1.72190
Epoch: 2720 Train: 223.25833 Test: 1.53502
Epoch: 2800 Train: 225.53944 Test: 1.81632
Epoch 2880: New minimal relative error: 3.29%, model saved.
Epoch: 2880 Train: 214.67009 Test: 1.87472
Epoch: 2960 Train: 210.70859 Test: 2.63892
Epoch: 3040 Train: 213.68246 Test: 3.29946
Epoch: 3120 Train: 193.24017 Test: 11.85352
Epoch: 3200 Train: 167.27411 Test: 1.39551
Epoch: 3280 Train: 165.80325 Test: 0.85090
Epoch 3360: New minimal relative error: 2.85%, model saved.
Epoch: 3360 Train: 160.70856 Test: 0.78844
Epoch: 3440 Train: 150.99374 Test: 2.11407
Epoch: 3520 Train: 156.64740 Test: 2.36704
Epoch: 3600 Train: 140.47775 Test: 0.58172
Epoch: 3680 Train: 147.68230 Test: 5.03380
Epoch: 3760 Train: 130.91313 Test: 0.50225
Epoch 3840: New minimal relative error: 1.92%, model saved.
Epoch: 3840 Train: 130.66759 Test: 0.51625
Epoch: 3920 Train: 128.69310 Test: 0.62737
Epoch: 4000 Train: 129.69624 Test: 1.70975
Epoch: 4080 Train: 122.90334 Test: 0.78541
Epoch: 4160 Train: 119.28108 Test: 0.46634
Epoch: 4240 Train: 115.77829 Test: 2.33350
Epoch: 4320 Train: 115.09045 Test: 0.99361
Epoch: 4400 Train: 116.75098 Test: 1.99721
Epoch: 4480 Train: 111.39020 Test: 2.18593
Epoch: 4560 Train: 111.31941 Test: 0.70946
Epoch: 4640 Train: 111.29366 Test: 0.98876
Epoch: 4720 Train: 109.64228 Test: 3.90291
Epoch: 4800 Train: 104.74802 Test: 1.55632
Epoch: 4880 Train: 106.04379 Test: 0.44400
Epoch: 4960 Train: 113.92300 Test: 0.56945
Epoch: 5040 Train: 112.19697 Test: 0.51924
Epoch: 5120 Train: 104.07080 Test: 0.52762
Epoch 5200: New minimal relative error: 1.87%, model saved.
Epoch: 5200 Train: 98.26254 Test: 0.38106
Epoch: 5280 Train: 98.52853 Test: 0.51916
Epoch: 5360 Train: 99.62946 Test: 0.44727
Epoch: 5440 Train: 97.35944 Test: 0.45057
Epoch 5520: New minimal relative error: 1.57%, model saved.
Epoch: 5520 Train: 100.88355 Test: 0.54686
Epoch: 5600 Train: 104.90016 Test: 0.53782
Epoch: 5680 Train: 102.13162 Test: 0.57871
Epoch: 5760 Train: 102.17874 Test: 0.51498
Epoch: 5840 Train: 101.75528 Test: 0.53158
Epoch: 5920 Train: 105.15307 Test: 0.52356
Epoch: 6000 Train: 107.35617 Test: 0.58675
Epoch 6080: New minimal relative error: 1.28%, model saved.
Epoch: 6080 Train: 108.56872 Test: 0.52170
Epoch: 6160 Train: 103.85257 Test: 0.54132
Epoch: 6240 Train: 101.99206 Test: 1.30269
Epoch: 6320 Train: 93.43826 Test: 0.35092
Epoch: 6400 Train: 93.03243 Test: 1.45925
Epoch: 6480 Train: 96.15919 Test: 0.39881
Epoch: 6560 Train: 95.83465 Test: 0.58770
Epoch: 6640 Train: 94.95220 Test: 0.41322
Epoch: 6720 Train: 92.16033 Test: 0.49260
Epoch: 6800 Train: 88.06824 Test: 0.49191
Epoch: 6880 Train: 93.04265 Test: 0.42903
Epoch: 6960 Train: 89.09980 Test: 0.37985
Epoch: 7040 Train: 100.17766 Test: 0.47018
Epoch: 7120 Train: 103.38738 Test: 1.27856
Epoch: 7200 Train: 107.20891 Test: 0.71240
Epoch: 7280 Train: 96.18925 Test: 1.64608
Epoch: 7360 Train: 87.77338 Test: 0.47706
Epoch: 7440 Train: 85.32674 Test: 0.42585
Epoch: 7520 Train: 85.63789 Test: 0.33658
Epoch: 7600 Train: 89.58897 Test: 0.51070
Epoch: 7680 Train: 84.44833 Test: 0.32050
Epoch: 7760 Train: 85.19169 Test: 0.37357
Epoch: 7840 Train: 74.80458 Test: 0.37466
Epoch: 7920 Train: 70.40112 Test: 0.39745
Epoch: 7999 Train: 73.46095 Test: 0.21206
Training Loss: tensor(73.4609)
Test Loss: tensor(0.2121)
Learned LE: [  0.8788813   -0.01733306 -14.554451  ]
True LE: [ 8.7599558e-01 -1.8137411e-04 -1.4552431e+01]
Relative Error: [ 3.8049746  4.7298822  5.309404   6.0265875  6.5725393  7.180117
  7.9792337  9.019071   9.374463   9.561199   9.976654  10.53014
 10.668228  10.156944   9.658148   8.345487   6.9615693  5.713111
  4.3410826  3.4620266  3.1901672  3.27215    3.262075   3.1127508
  3.9927428  4.8817677  5.612359   6.1271863  6.430084   6.554598
  6.59289    6.56536    6.4221725  6.262406   6.174303   6.2280183
  6.4639597  7.0425606  7.393551   7.57414    7.6223664  7.4053802
  7.034388   6.7871614  7.0008426  7.2690063  7.2614865  7.1657934
  7.097703   6.958089   6.7807937  6.6074924  6.391199   5.9975386
  5.3994303  4.6394277  3.661181   3.5342116  3.348134   2.9822605
  2.79352    2.934463   3.4239993  4.1612806  4.667659   5.448966
  6.114286   6.730125   7.4285955  8.009682   8.410499   8.567485
  8.873987   9.392829   9.819586   9.637527   9.342706   8.835794
  7.5468936  6.063031   4.390766   3.2432046  2.887572   3.0952137
  3.2260761  2.9309018  3.2946646  4.2751646  5.0702863  5.6313314
  5.9255886  5.984721   5.9027267  5.70492    5.425755   5.1176276
  4.999823   5.0732603  5.169678   5.3107285  5.817781   6.009361
  6.072926   6.0528293  5.850366   5.5380287  5.47604    5.7075777
  6.033376   6.1509175  6.1765175  6.1826234  6.272577   6.38225
  6.3595853  6.142271   5.766395   5.160729   4.4638867  3.8075783
  3.7967212  3.418851   2.889597   2.6248302  2.9748929  3.682629
  4.100209   4.667713   5.5594163  6.3231497  6.9169393  7.0323954
  7.4897823  7.7386856  7.9119735  8.355654   8.809941   8.745291
  8.44044    8.444188   8.310746   6.818358   4.835926   3.2681227
  2.5817738  2.759539   2.9883661  2.8043635  2.4865844  3.5458393
  4.457466   5.1039724  5.4275055  5.506717   5.3703675  5.0217214
  4.627839   4.1303644  3.9547052  4.0043726  3.9640248  3.966305
  3.9577649  4.348019   4.4143314  4.4821725  4.4505887  4.2060776
  4.1359997  4.27604    4.5314717  4.8108826  5.112027   5.263725
  5.5384417  5.9322724  6.1092753  6.106835   5.981213   5.635263
  5.1218987  4.236448   3.990443   3.7507944  3.2526703  2.6823456
  2.5773988  3.2050502  3.5523534  4.0049047  4.6607776  5.415286
  5.8746295  5.7738185  6.1114416  6.8437724  7.1993327  7.4098244
  7.9234014  8.144181   7.7203884  7.416178   7.522194   8.01702
  5.8532977  3.8272817  2.459146   2.3024325  2.6167974  2.631009
  2.4581845  2.7047033  3.696443   4.467423   4.903536   5.048502
  4.9414377  4.5607085  4.0063562  3.4987257  3.2109895  3.0117955
  2.935164   2.893678   2.7862093  2.5852702  2.8028767  2.9704723
  3.0155978  2.9979608  2.7861807  2.798442   3.0347562  3.5530963
  3.8078747  4.148142   4.497502   4.9982862  5.5133424  5.785205
  5.9135623  5.929388   5.5753736  4.99022    4.0399475  3.8950727
  3.5059624  3.0791113  2.5968237  2.5235963  3.0341842  3.5301824
  3.5854242  3.9326928  4.823057   4.7511     4.7114725  5.1006093
  5.7990255  6.3944488  7.107812   7.4416213  7.438492   6.8554425
  6.508014   6.869148   7.1092567  5.2353954  3.2159534  2.12747
  2.1980462  2.4622495  2.5460153  2.2994404  2.8732297  3.7539237
  4.2983003  4.5602684  4.5297074  4.205912   3.6800377  3.0597613
  2.9030812  2.7272396  2.4633756  2.2547505  2.1119263  2.0639172
  1.9747761  1.9248714  2.08818    1.9886129  1.9904073  1.7487056
  1.6320994  1.9413886  2.5759037  3.1719186  3.394013   3.7787886
  4.307487   4.90334    5.34552    5.6898093  5.7608895  5.377764
  4.611452   3.4800284  3.2199461  2.9462934  2.5367725  2.2676983
  2.398995   2.8018384  2.9535391  2.9047825  3.2983377  3.6761527
  3.7983615  3.8913875  4.2267857  4.874085   5.4805303  6.3224173
  7.12831    6.9829993  6.2416406  5.813695   6.151388   6.0123954
  5.215295   3.136178   2.0983367  2.1509523  2.4237907  2.5208304
  2.1330955  2.7815669  3.6538444  4.1559687  4.319658   4.078885
  3.5383587  2.8454597  2.643156   2.516191   2.2937574  2.026269
  1.6374624  1.5216793  1.7115647  1.7559288  1.5832171  1.6370105
  1.6125379  1.5217007  1.2673991  0.8302942  1.2069263  1.855339
  2.4911335  3.0137124  3.1409476  3.4729533  4.000964   4.690249
  5.206162   5.2352014  4.7299123  4.145108   3.216682   2.646622
  2.4605463  2.1506226  1.8537323  2.0897377  2.3008795  2.2477913
  2.2514908  2.6697621  2.7276604  2.9988582  3.294187   3.5254974
  4.1638126  4.8008385  5.462181   6.3375463  6.907309   6.0237956
  5.3604383  5.454878   5.237724   5.0699873  3.5794024  2.2161891
  2.0264363  2.2489314  2.2913673  1.9233098]
