time_step: 0.25
lr: 0.001
weight_decay: 0.0001
num_epoch: 3000
num_train: 3000
num_test: 3000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: KS
model_type: MLP
s: 0.5
n_hidden: 128
n_layers: 4
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 2.371427431 Test: 0.000000012
Epoch 0: New minimal relative error: 0.00%, model saved.
Epoch: 30 Train: 2.095257334 Test: 0.000000057
Epoch: 60 Train: 1.564431707 Test: 0.000000053
Epoch: 90 Train: 1.104015781 Test: 0.000000050
Epoch: 120 Train: 0.800602947 Test: 0.000000055
Epoch: 150 Train: 0.604981615 Test: 0.000000059
Epoch: 180 Train: 0.473953002 Test: 0.000000067
Epoch: 210 Train: 0.402020591 Test: 0.000000073
Epoch: 240 Train: 0.360206818 Test: 0.000000080
Epoch: 270 Train: 0.337987457 Test: 0.000000084
Epoch: 300 Train: 0.323079274 Test: 0.000000086
Epoch: 330 Train: 0.311682062 Test: 0.000000091
Epoch: 360 Train: 0.307975544 Test: 0.000000092
Epoch: 390 Train: 0.304509650 Test: 0.000000093
Epoch: 420 Train: 0.300523397 Test: 0.000000096
Epoch: 450 Train: 0.334221422 Test: 0.000000313
Epoch: 480 Train: 0.301115010 Test: 0.000000104
Epoch: 510 Train: 0.297106164 Test: 0.000000103
Epoch: 540 Train: 0.295807098 Test: 0.000000105
Epoch: 570 Train: 0.294848610 Test: 0.000000105
Epoch: 600 Train: 0.293951043 Test: 0.000000105
Epoch: 630 Train: 0.293102369 Test: 0.000000105
Epoch: 660 Train: 0.292360824 Test: 0.000000105
Epoch: 690 Train: 0.291763156 Test: 0.000000105
Epoch: 720 Train: 0.291306283 Test: 0.000000105
Epoch: 750 Train: 0.290948756 Test: 0.000000105
Epoch: 780 Train: 0.290651572 Test: 0.000000105
Epoch: 810 Train: 0.290387958 Test: 0.000000105
Epoch: 840 Train: 0.290136555 Test: 0.000000105
Epoch: 870 Train: 0.289880763 Test: 0.000000105
Epoch: 900 Train: 0.289610715 Test: 0.000000105
Epoch: 930 Train: 0.289327584 Test: 0.000000105
Epoch: 960 Train: 0.289046060 Test: 0.000000105
Epoch: 990 Train: 0.288785354 Test: 0.000000105
Epoch: 1020 Train: 0.288552531 Test: 0.000000105
Epoch: 1050 Train: 0.288340373 Test: 0.000000105
Epoch: 1080 Train: 0.288138287 Test: 0.000000105
Epoch: 1110 Train: 0.287937455 Test: 0.000000105
Epoch: 1140 Train: 0.287730743 Test: 0.000000105
Epoch: 1170 Train: 0.287514647 Test: 0.000000105
Epoch: 1200 Train: 0.287289586 Test: 0.000000105
Epoch: 1230 Train: 0.287060372 Test: 0.000000105
Epoch: 1260 Train: 0.286837012 Test: 0.000000105
Epoch: 1290 Train: 0.286629215 Test: 0.000000105
Epoch: 1320 Train: 0.286440268 Test: 0.000000106
Epoch: 1350 Train: 0.286267424 Test: 0.000000106
Epoch: 1380 Train: 0.286105989 Test: 0.000000106
Epoch: 1410 Train: 0.285951635 Test: 0.000000106
Epoch: 1440 Train: 0.285801021 Test: 0.000000106
Epoch: 1470 Train: 0.285652268 Test: 0.000000106
Epoch: 1500 Train: 0.285505480 Test: 0.000000106
Epoch: 1530 Train: 0.285362645 Test: 0.000000107
Epoch: 1560 Train: 0.285226411 Test: 0.000000107
Epoch: 1590 Train: 0.285098199 Test: 0.000000107
Epoch: 1620 Train: 0.284977207 Test: 0.000000107
Epoch: 1650 Train: 0.284861110 Test: 0.000000107
Epoch: 1680 Train: 0.284747372 Test: 0.000000107
Epoch: 1710 Train: 0.284633956 Test: 0.000000108
Epoch: 1740 Train: 0.284519395 Test: 0.000000108
Epoch: 1770 Train: 0.284402782 Test: 0.000000108
Epoch: 1800 Train: 0.284283907 Test: 0.000000108
Epoch: 1830 Train: 0.284163524 Test: 0.000000108
Epoch: 1860 Train: 0.284043477 Test: 0.000000108
Epoch: 1890 Train: 0.283926359 Test: 0.000000108
Epoch: 1920 Train: 0.283814562 Test: 0.000000109
Epoch: 1950 Train: 0.283709203 Test: 0.000000109
Epoch: 1980 Train: 0.283609748 Test: 0.000000109
Epoch: 2010 Train: 0.283514609 Test: 0.000000109
Epoch: 2040 Train: 0.283422071 Test: 0.000000109
Epoch: 2070 Train: 0.283330872 Test: 0.000000110
Epoch: 2100 Train: 0.283240335 Test: 0.000000110
Epoch: 2130 Train: 0.283150310 Test: 0.000000110
Epoch: 2160 Train: 0.283061074 Test: 0.000000110
Epoch: 2190 Train: 0.282973224 Test: 0.000000110
Epoch: 2220 Train: 0.282887516 Test: 0.000000111
Epoch: 2250 Train: 0.282804669 Test: 0.000000111
Epoch: 2280 Train: 0.282725184 Test: 0.000000111
Epoch: 2310 Train: 0.282649244 Test: 0.000000111
Epoch: 2340 Train: 0.282576739 Test: 0.000000112
Epoch: 2370 Train: 0.282507383 Test: 0.000000112
Epoch: 2400 Train: 0.282440841 Test: 0.000000112
Epoch: 2430 Train: 0.282376808 Test: 0.000000112
Epoch: 2460 Train: 0.282315041 Test: 0.000000112
Epoch: 2490 Train: 0.282255351 Test: 0.000000113
Epoch: 2520 Train: 0.282197580 Test: 0.000000113
Epoch: 2550 Train: 0.282141582 Test: 0.000000113
Epoch: 2580 Train: 0.282087211 Test: 0.000000113
Epoch: 2610 Train: 0.282034316 Test: 0.000000113
Epoch: 2640 Train: 0.281982742 Test: 0.000000114
Epoch: 2670 Train: 0.281932336 Test: 0.000000114
Epoch: 2700 Train: 0.281882951 Test: 0.000000114
Epoch: 2730 Train: 0.281834448 Test: 0.000000114
Epoch: 2760 Train: 0.281786697 Test: 0.000000115
Epoch: 2790 Train: 0.281739580 Test: 0.000000115
Epoch: 2820 Train: 0.281692992 Test: 0.000000115
Epoch: 2850 Train: 0.281646834 Test: 0.000000115
Epoch: 2880 Train: 0.281601016 Test: 0.000000115
Epoch: 2910 Train: 0.281555455 Test: 0.000000115
Epoch: 2940 Train: 0.281510075 Test: 0.000000116
Epoch: 2970 Train: 0.281464804 Test: 0.000000116
Epoch: 2999 Train: 0.281421086 Test: 0.000000116
Training Loss: tensor(0.2814)
Test Loss: tensor(1.1605e-07)
Jacobian term Training Loss: tensor(0.0006)
Jacobian term Test Loss: tensor(0.0006)
Learned LE: [0.01001998 0.01008421 0.00794502 0.00570875 0.00655274 0.00512615
 0.00538351 0.00581482 0.00398543 0.00437757 0.00331723 0.0030067
 0.00221252 0.00252975 0.00256641]
True LE: [0.30574609 0.28004271 0.26752761 0.23201314 0.20837498 0.19183291
 0.16715513 0.15416377 0.12779687 0.11077707 0.09813166 0.07510605
 0.05770563 0.04353978 0.02605524]
Norm Diff:: tensor(0.6700, dtype=torch.float64)
