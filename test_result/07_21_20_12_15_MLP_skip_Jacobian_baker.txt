time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 5000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 5.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 9.126430511 Test: 6.966927528
Epoch 0: New minimal relative error: 6.97%, model saved.
Epoch: 50 Train: 1.291462183 Test: 1.351412535
Epoch 50: New minimal relative error: 1.35%, model saved.
Epoch: 100 Train: 0.839300513 Test: 0.885365486
Epoch 100: New minimal relative error: 0.89%, model saved.
Epoch: 150 Train: 0.323601663 Test: 0.300124496
Epoch 150: New minimal relative error: 0.30%, model saved.
Epoch: 200 Train: 0.246742278 Test: 0.236630693
Epoch 200: New minimal relative error: 0.24%, model saved.
Epoch: 250 Train: 0.224685401 Test: 0.207309276
Epoch 250: New minimal relative error: 0.21%, model saved.
Epoch: 300 Train: 0.214527011 Test: 0.196706444
Epoch 300: New minimal relative error: 0.20%, model saved.
Epoch: 350 Train: 0.212142557 Test: 0.196028605
Epoch 350: New minimal relative error: 0.20%, model saved.
Epoch: 400 Train: 0.202653289 Test: 0.184419245
Epoch 400: New minimal relative error: 0.18%, model saved.
Epoch: 450 Train: 0.199580252 Test: 0.183157533
Epoch 450: New minimal relative error: 0.18%, model saved.
Epoch: 500 Train: 0.200743705 Test: 0.188654438
Epoch: 550 Train: 0.196138471 Test: 0.178427294
Epoch 550: New minimal relative error: 0.18%, model saved.
Epoch: 600 Train: 0.200321615 Test: 0.180123225
Epoch: 650 Train: 0.196355343 Test: 0.177828670
Epoch 650: New minimal relative error: 0.18%, model saved.
Epoch: 700 Train: 0.197593927 Test: 0.177378029
Epoch 700: New minimal relative error: 0.18%, model saved.
Epoch: 750 Train: 0.196859807 Test: 0.179209515
Epoch: 800 Train: 0.198517591 Test: 0.177535176
Epoch: 850 Train: 0.195543498 Test: 0.177184761
Epoch 850: New minimal relative error: 0.18%, model saved.
Epoch: 900 Train: 0.196730942 Test: 0.177267313
Epoch: 950 Train: 0.195794851 Test: 0.177283168
Epoch: 1000 Train: 0.195975289 Test: 0.177187532
Epoch: 1050 Train: 0.196245447 Test: 0.177240044
Epoch: 1100 Train: 0.196975738 Test: 0.178130075
Epoch: 1150 Train: 0.197455794 Test: 0.179397196
Epoch: 1200 Train: 0.197845995 Test: 0.178142309
Epoch: 1250 Train: 0.199634999 Test: 0.179010823
Epoch: 1300 Train: 0.197589576 Test: 0.180610090
Epoch: 1350 Train: 0.197397947 Test: 0.178752035
Epoch: 1400 Train: 0.198022798 Test: 0.179795742
Epoch: 1450 Train: 0.196964204 Test: 0.179807618
Epoch: 1500 Train: 0.198789418 Test: 0.181790903
Epoch: 1550 Train: 0.197354943 Test: 0.180477262
Epoch: 1600 Train: 0.196123987 Test: 0.179845244
Epoch: 1650 Train: 0.196355283 Test: 0.180323124
Epoch: 1700 Train: 0.197425276 Test: 0.181080520
Epoch: 1750 Train: 0.196412623 Test: 0.181832626
Epoch: 1800 Train: 0.196276963 Test: 0.180116147
Epoch: 1850 Train: 0.199133009 Test: 0.185695112
Epoch: 1900 Train: 0.195875078 Test: 0.182163566
Epoch: 1950 Train: 0.195717603 Test: 0.178800061
Epoch: 2000 Train: 0.198706239 Test: 0.181717098
Epoch: 2050 Train: 0.196950212 Test: 0.179267704
Epoch: 2100 Train: 0.196912318 Test: 0.179948360
Epoch: 2150 Train: 0.196027458 Test: 0.179096177
Epoch: 2200 Train: 0.195383251 Test: 0.178788841
Epoch: 2250 Train: 0.196803316 Test: 0.179213494
Epoch: 2300 Train: 0.196350530 Test: 0.179040819
Epoch: 2350 Train: 0.199165732 Test: 0.181209579
Epoch: 2400 Train: 0.203010261 Test: 0.185952514
Epoch: 2450 Train: 0.199346304 Test: 0.183395997
Epoch: 2500 Train: 0.196566463 Test: 0.181673020
Epoch: 2550 Train: 0.197705835 Test: 0.181319252
Epoch: 2600 Train: 0.196935266 Test: 0.180682033
Epoch: 2650 Train: 0.195919573 Test: 0.180534244
Epoch: 2700 Train: 0.195797250 Test: 0.179220766
Epoch: 2750 Train: 0.195701927 Test: 0.179366484
Epoch: 2800 Train: 0.196506590 Test: 0.179462463
Epoch: 2850 Train: 0.197562382 Test: 0.180123836
Epoch: 2900 Train: 0.196798652 Test: 0.180177629
Epoch: 2950 Train: 0.195401728 Test: 0.179782495
Epoch: 3000 Train: 0.195637107 Test: 0.180570155
Epoch: 3050 Train: 0.194477499 Test: 0.180742741
Epoch: 3100 Train: 0.195176452 Test: 0.181009442
Epoch: 3150 Train: 0.194860816 Test: 0.180573955
Epoch: 3200 Train: 0.196294948 Test: 0.181177169
Epoch: 3250 Train: 0.197541147 Test: 0.183100849
Epoch: 3300 Train: 0.197011352 Test: 0.181003094
Epoch: 3350 Train: 0.196489230 Test: 0.180557176
Epoch: 3400 Train: 0.195187867 Test: 0.180046201
Epoch: 3450 Train: 0.195396632 Test: 0.179791272
Epoch: 3500 Train: 0.195993006 Test: 0.179864824
Epoch: 3550 Train: 0.195651114 Test: 0.179834038
Epoch: 3600 Train: 0.195740074 Test: 0.180075601
Epoch: 3650 Train: 0.196586609 Test: 0.180354729
Epoch: 3700 Train: 0.196499825 Test: 0.180979878
Epoch: 3750 Train: 0.196312398 Test: 0.180632830
Epoch: 3800 Train: 0.197742581 Test: 0.181913108
Epoch: 3850 Train: 0.198311388 Test: 0.183219507
Epoch: 3900 Train: 0.197278619 Test: 0.182137236
Epoch: 3950 Train: 0.197812766 Test: 0.181963056
Epoch: 4000 Train: 0.196246490 Test: 0.181016117
Epoch: 4050 Train: 0.195292443 Test: 0.181649029
Epoch: 4100 Train: 0.195731074 Test: 0.181302696
Epoch: 4150 Train: 0.196286738 Test: 0.181531683
Epoch: 4200 Train: 0.195617497 Test: 0.181695491
Epoch: 4250 Train: 0.195485815 Test: 0.181860879
Epoch: 4300 Train: 0.194996238 Test: 0.182052672
Epoch: 4350 Train: 0.195959941 Test: 0.182163879
Epoch: 4400 Train: 0.195493817 Test: 0.182586908
Epoch: 4450 Train: 0.196079791 Test: 0.183315888
Epoch: 4500 Train: 0.195191324 Test: 0.182583630
Epoch: 4550 Train: 0.196766451 Test: 0.182605326
Epoch: 4600 Train: 0.198163182 Test: 0.183478609
Epoch: 4650 Train: 0.198709399 Test: 0.184179172
Epoch: 4700 Train: 0.198471099 Test: 0.183288068
Epoch: 4750 Train: 0.197874665 Test: 0.182409912
Epoch: 4800 Train: 0.197818413 Test: 0.182087749
Epoch: 4850 Train: 0.197319791 Test: 0.182301477
Epoch: 4900 Train: 0.197731450 Test: 0.182110727
Epoch: 4950 Train: 0.197838664 Test: 0.182464913
Epoch: 4999 Train: 0.197886184 Test: 0.182604313
Training Loss: tensor(0.1979)
Test Loss: tensor(0.1826)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(2.9918, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(2.6111, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0188)
Jacobian term Test Loss: tensor(0.0160)
Learned LE: [1.6489826  0.43192092]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
