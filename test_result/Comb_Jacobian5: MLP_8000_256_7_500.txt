time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 32159.86328 Test: 3933.62500
Epoch 80: New minimal relative error: 63.47%, model saved.
Epoch: 80 Train: 6966.42920 Test: 1422.47510
Epoch: 160 Train: 3113.06738 Test: 1589.02991
Epoch 240: New minimal relative error: 27.73%, model saved.
Epoch: 240 Train: 291.10516 Test: 16.56968
Epoch 320: New minimal relative error: 7.56%, model saved.
Epoch: 320 Train: 64.55776 Test: 23.40137
Epoch: 400 Train: 39.14057 Test: 6.35103
Epoch: 480 Train: 25.01737 Test: 8.81171
Epoch: 560 Train: 20.20641 Test: 8.15220
Epoch: 640 Train: 40.92322 Test: 23.94704
Epoch: 720 Train: 11.09704 Test: 5.14862
Epoch: 800 Train: 12.91657 Test: 8.02990
Epoch: 880 Train: 15.07690 Test: 7.12665
Epoch: 960 Train: 20.31310 Test: 7.05845
Epoch: 1040 Train: 4.27244 Test: 0.70116
Epoch: 1120 Train: 37.10843 Test: 17.54570
Epoch 1200: New minimal relative error: 1.30%, model saved.
Epoch: 1200 Train: 2.81696 Test: 0.07545
Epoch 1280: New minimal relative error: 0.99%, model saved.
Epoch: 1280 Train: 2.29431 Test: 0.03346
Epoch: 1360 Train: 8.74046 Test: 1.81766
Epoch: 1440 Train: 15.73262 Test: 6.73398
Epoch: 1520 Train: 14.01906 Test: 5.49530
Epoch: 1600 Train: 18.93183 Test: 13.12667
Epoch: 1680 Train: 1.77643 Test: 1.04512
Epoch: 1760 Train: 17.26747 Test: 5.21586
Epoch: 1840 Train: 6.70476 Test: 2.89671
Epoch: 1920 Train: 3.38133 Test: 1.71846
Epoch: 2000 Train: 10.07988 Test: 3.57766
Epoch: 2080 Train: 1.10587 Test: 0.05209
Epoch: 2160 Train: 9.94651 Test: 2.83994
Epoch: 2240 Train: 9.19381 Test: 2.99915
Epoch: 2320 Train: 7.96273 Test: 2.50640
Epoch: 2400 Train: 6.03250 Test: 1.69173
Epoch: 2480 Train: 2.60436 Test: 1.87120
Epoch: 2560 Train: 1.74724 Test: 0.35667
Epoch: 2640 Train: 3.18934 Test: 1.22943
Epoch: 2720 Train: 5.83591 Test: 1.38371
Epoch: 2800 Train: 2.94884 Test: 0.84633
Epoch: 2880 Train: 0.94300 Test: 0.11688
Epoch: 2960 Train: 4.49064 Test: 1.53335
Epoch: 3040 Train: 7.08572 Test: 2.50934
Epoch: 3120 Train: 2.39029 Test: 0.70437
Epoch: 3200 Train: 5.17048 Test: 1.57721
Epoch: 3280 Train: 4.09182 Test: 2.06828
Epoch: 3360 Train: 0.58519 Test: 0.02824
Epoch: 3440 Train: 1.03019 Test: 0.22909
Epoch: 3520 Train: 2.01176 Test: 0.54034
Epoch: 3600 Train: 3.75186 Test: 1.42151
Epoch: 3680 Train: 17.43138 Test: 4.95442
Epoch: 3760 Train: 0.47179 Test: 0.01388
Epoch: 3840 Train: 2.36131 Test: 0.64605
Epoch: 3920 Train: 5.32360 Test: 1.48151
Epoch: 4000 Train: 0.50962 Test: 0.09956
Epoch: 4080 Train: 0.40051 Test: 0.00380
Epoch: 4160 Train: 0.48162 Test: 0.13388
Epoch 4240: New minimal relative error: 0.38%, model saved.
Epoch: 4240 Train: 0.38064 Test: 0.00316
Epoch: 4320 Train: 3.51241 Test: 0.79848
Epoch: 4400 Train: 0.37642 Test: 0.01050
Epoch: 4480 Train: 0.50126 Test: 0.04939
Epoch: 4560 Train: 3.42158 Test: 0.76846
Epoch: 4640 Train: 20.11488 Test: 9.38169
Epoch: 4720 Train: 0.33058 Test: 0.00401
Epoch: 4800 Train: 0.36223 Test: 0.05651
Epoch: 4880 Train: 0.43996 Test: 0.02482
Epoch: 4960 Train: 0.30079 Test: 0.00323
Epoch: 5040 Train: 0.45319 Test: 0.01961
Epoch: 5120 Train: 0.73992 Test: 0.19777
Epoch: 5200 Train: 1.12548 Test: 0.44208
Epoch: 5280 Train: 3.82522 Test: 2.03022
Epoch: 5360 Train: 0.27195 Test: 0.00692
Epoch: 5440 Train: 0.28030 Test: 0.02151
Epoch: 5520 Train: 1.62393 Test: 0.34590
Epoch: 5600 Train: 0.60750 Test: 0.12929
Epoch: 5680 Train: 0.92016 Test: 0.34392
Epoch: 5760 Train: 0.41738 Test: 0.09836
Epoch: 5840 Train: 0.36209 Test: 0.05550
Epoch: 5920 Train: 1.21807 Test: 0.24396
Epoch: 6000 Train: 0.47871 Test: 0.38231
Epoch: 6080 Train: 3.63965 Test: 1.95448
Epoch: 6160 Train: 0.21593 Test: 0.00407
Epoch: 6240 Train: 0.27733 Test: 0.03107
Epoch: 6320 Train: 3.12169 Test: 1.10623
Epoch: 6400 Train: 0.25944 Test: 0.03808
Epoch 6480: New minimal relative error: 0.20%, model saved.
Epoch: 6480 Train: 0.19582 Test: 0.00259
Epoch: 6560 Train: 1.42672 Test: 0.38646
Epoch: 6640 Train: 0.93192 Test: 0.54417
Epoch: 6720 Train: 0.19001 Test: 0.00377
Epoch: 6800 Train: 2.44917 Test: 0.43736
Epoch: 6880 Train: 0.17813 Test: 0.00228
Epoch: 6960 Train: 0.54313 Test: 0.07921
Epoch: 7040 Train: 0.17182 Test: 0.00237
Epoch: 7120 Train: 0.18248 Test: 0.00644
Epoch: 7200 Train: 0.21243 Test: 0.01021
Epoch: 7280 Train: 0.16445 Test: 0.00315
Epoch: 7360 Train: 0.31712 Test: 0.06367
Epoch: 7440 Train: 0.16066 Test: 0.00481
Epoch: 7520 Train: 0.16540 Test: 0.00338
Epoch: 7600 Train: 0.19593 Test: 0.00479
Epoch: 7680 Train: 0.22869 Test: 0.11419
Epoch: 7760 Train: 0.82572 Test: 0.40979
Epoch: 7840 Train: 0.14473 Test: 0.00241
Epoch: 7920 Train: 0.40847 Test: 0.16806
Epoch: 7999 Train: 0.19494 Test: 0.02967
Training Loss: tensor(0.1949)
Test Loss: tensor(0.0297)
Learned LE: [ 8.6824924e-01 -7.7377437e-03 -1.4542935e+01]
True LE: [ 8.5954267e-01  1.1089287e-02 -1.4554151e+01]
Relative Error: [0.17254442 0.17589304 0.18290097 0.1912796  0.20107034 0.2126554
 0.22622971 0.24101827 0.2554364  0.2677121  0.27626684 0.28026646
 0.28001446 0.27796605 0.27766293 0.2827175  0.2952174  0.31327298
 0.3288141  0.33251566 0.3218166  0.303791   0.28842762 0.2931157
 0.3183207  0.3238966  0.28117964 0.20833148 0.14249593 0.10237613
 0.08182991 0.07143597 0.06758306 0.06838575 0.07128149 0.07489396
 0.07859259 0.08206227 0.08512168 0.08749454 0.08894105 0.0888162
 0.08589699 0.07908343 0.07109114 0.07304659 0.09179    0.11385182
 0.1294335  0.1536514  0.21025687 0.2861601  0.3241467  0.29289988
 0.23076749 0.20297754 0.22047125 0.2387563  0.22650428 0.19481167
 0.16838445 0.1582908  0.16012475 0.16629007 0.17276712 0.17957547
 0.18770319 0.198323   0.21188436 0.22733974 0.24316017 0.2574979
 0.26818871 0.27386874 0.27396122 0.27022076 0.26597363 0.2655419
 0.2725146  0.2873468  0.3036828  0.31026235 0.30064973 0.28104126
 0.26373598 0.26600736 0.29289103 0.30289942 0.26271924 0.19042702
 0.12555347 0.08621608 0.06596285 0.05641056 0.05451642 0.05697419
 0.06119338 0.06599981 0.07097606 0.07591411 0.08051412 0.08449553
 0.08755609 0.08954939 0.08962551 0.08646514 0.07996384 0.07839332
 0.09383204 0.11770646 0.13191473 0.14426875 0.18601078 0.25779226
 0.30257165 0.27800027 0.21842109 0.19325441 0.2110201  0.22162592
 0.1996353  0.16618197 0.14703812 0.14545026 0.15147075 0.15712778
 0.16114289 0.16533194 0.17192967 0.18214926 0.19558117 0.21138394
 0.22808136 0.2439412  0.25685918 0.26495463 0.26688215 0.263296
 0.25675023 0.25151056 0.25202414 0.26143146 0.2766391  0.2866208
 0.2803705  0.2602025  0.23998988 0.23782784 0.26511872 0.28224805
 0.24830823 0.1780595  0.11277065 0.07259517 0.0515536  0.04253872
 0.04199343 0.04590415 0.05134509 0.05740869 0.0638826  0.070606
 0.07709399 0.08256304 0.08675358 0.08990003 0.09192472 0.09171422
 0.08805188 0.08423627 0.09348945 0.11777218 0.13516039 0.1391674
 0.16270863 0.22584419 0.2786921  0.2646475  0.20782927 0.18175925
 0.19774458 0.20191534 0.17338458 0.14259389 0.1323633  0.13714835
 0.14357634 0.14624724 0.1472596  0.14994043 0.15638573 0.16693547
 0.18060008 0.19647625 0.21310283 0.22921133 0.24310248 0.253083
 0.25746292 0.25572345 0.24897619 0.24068555 0.23542863 0.23750207
 0.24842839 0.2607459  0.26004344 0.24140593 0.21799651 0.20933165
 0.23367015 0.2602315  0.23743871 0.1715457  0.10520397 0.06260671
 0.03988821 0.03022369 0.03060425 0.03572245 0.04218498 0.04933095
 0.05730229 0.06594941 0.0747411  0.0824564  0.08803617 0.09139195
 0.09345333 0.09492876 0.09404366 0.0906797  0.09304143 0.11299433
 0.13598049 0.139132   0.14365795 0.19072121 0.25032595 0.25209844
 0.19948867 0.16872124 0.18152957 0.18201208 0.1508002  0.12493672
 0.12260831 0.13038987 0.13405226 0.13289689 0.1322435  0.13574684
 0.14405876 0.15604429 0.17061454 0.18625927 0.20189972 0.21670659
 0.22976121 0.23991218 0.24585024 0.24618316 0.24105622 0.23212148
 0.22302817 0.21793312 0.22134747 0.23184878 0.23745592 0.22449179
 0.19888668 0.18234286 0.19807562 0.23347437 0.2282799  0.1714003
 0.10434807 0.05837648 0.03345704 0.0223298  0.02266176 0.02794409
 0.0342847  0.04154801 0.05026959 0.06066813 0.07208513 0.08296041
 0.09126275 0.09559723 0.09693337 0.09733821 0.09767165 0.09627528
 0.0948194  0.10512603 0.13024624 0.14181179 0.13358712 0.15569995
 0.21506694 0.23710679 0.19446228 0.15535341 0.1634995  0.16402316
 0.13325869 0.11257201 0.11588145 0.12297877 0.12203927 0.11811742
 0.11826505 0.12485958 0.13638525 0.15109567 0.16696975 0.182904
 0.19770421 0.21067296 0.22111598 0.22904624 0.2340352  0.23522459
 0.2316696  0.22383125 0.21377417 0.2043467  0.1993133  0.20287775
 0.21069303 0.20689452 0.18407348 0.15958929 0.16064751 0.19803809
 0.21608737 0.17663883 0.11141009 0.06176678 0.03516009 0.02385276
 0.02261803 0.02520191 0.02862014 0.0338832  0.0419053  0.05303451
 0.06653263 0.08101651 0.09372554 0.10170733 0.1037601  0.10196365
 0.10017905 0.09973147 0.09833322 0.10008053 0.11734673 0.1395791
 0.13465028 0.12878504 0.17264016 0.21489891 0.1921317  0.14482556
 0.14426564 0.149309   0.12175874 0.10433652 0.11020452 0.11432163
 0.10870145 0.10365856 0.10643268 0.11683527 0.13173063 0.14879793
 0.16682199 0.18453535 0.20042302 0.21306607 0.22142462 0.22571492
 0.22692916 0.22591858 0.22233488 0.21513692 0.20516454 0.19471116
 0.18507886 0.17932841 0.18128447 0.1844528  0.17182404 0.1440304
 0.1279846  0.15217055 0.19214721 0.1826077 ]
