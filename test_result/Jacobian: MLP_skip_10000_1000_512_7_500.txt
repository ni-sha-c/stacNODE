time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.03%, model saved.
Epoch: 0 Train: 31705.52734 Test: 4114.97168
Epoch 100: New minimal relative error: 79.79%, model saved.
Epoch: 100 Train: 8814.34473 Test: 1446.28259
Epoch 200: New minimal relative error: 72.16%, model saved.
Epoch: 200 Train: 7580.35107 Test: 1106.97913
Epoch: 300 Train: 7021.27197 Test: 1284.70789
Epoch: 400 Train: 5961.42725 Test: 835.79987
Epoch: 500 Train: 5389.82520 Test: 617.08716
Epoch: 600 Train: 3859.23120 Test: 387.32474
Epoch 700: New minimal relative error: 39.74%, model saved.
Epoch: 700 Train: 3200.42920 Test: 329.92764
Epoch: 800 Train: 1356.04712 Test: 152.69916
Epoch 900: New minimal relative error: 37.18%, model saved.
Epoch: 900 Train: 1131.76074 Test: 75.30382
Epoch 1000: New minimal relative error: 11.11%, model saved.
Epoch: 1000 Train: 352.28613 Test: 5.24698
Epoch 1100: New minimal relative error: 8.15%, model saved.
Epoch: 1100 Train: 283.52371 Test: 3.27308
Epoch 1200: New minimal relative error: 5.80%, model saved.
Epoch: 1200 Train: 220.43742 Test: 2.40518
Epoch: 1300 Train: 193.04980 Test: 3.08180
Epoch 1400: New minimal relative error: 4.55%, model saved.
Epoch: 1400 Train: 172.97038 Test: 1.63260
Epoch: 1500 Train: 151.64799 Test: 3.91743
Epoch: 1600 Train: 135.13683 Test: 1.14630
Epoch: 1700 Train: 164.82834 Test: 26.87075
Epoch: 1800 Train: 153.26688 Test: 5.62517
Epoch: 1900 Train: 119.58782 Test: 1.45257
Epoch: 2000 Train: 149.42000 Test: 9.20256
Epoch: 2100 Train: 127.72839 Test: 3.86167
Epoch: 2200 Train: 118.42532 Test: 2.35847
Epoch: 2300 Train: 107.18507 Test: 0.95217
Epoch: 2400 Train: 104.86030 Test: 11.57292
Epoch: 2500 Train: 113.71191 Test: 8.16891
Epoch: 2600 Train: 89.24006 Test: 1.65296
Epoch: 2700 Train: 106.74844 Test: 11.25491
Epoch: 2800 Train: 83.30534 Test: 2.72940
Epoch: 2900 Train: 96.28955 Test: 10.19771
Epoch: 3000 Train: 70.43217 Test: 0.60114
Epoch: 3100 Train: 75.74049 Test: 2.92417
Epoch: 3200 Train: 60.50845 Test: 0.40044
Epoch: 3300 Train: 58.33632 Test: 0.46787
Epoch: 3400 Train: 57.80878 Test: 0.40663
Epoch 3500: New minimal relative error: 4.49%, model saved.
Epoch: 3500 Train: 58.55958 Test: 0.46599
Epoch: 3600 Train: 55.77442 Test: 0.73991
Epoch: 3700 Train: 86.19508 Test: 14.36639
Epoch 3800: New minimal relative error: 3.95%, model saved.
Epoch: 3800 Train: 45.33605 Test: 0.24093
Epoch: 3900 Train: 54.13402 Test: 1.48930
Epoch 4000: New minimal relative error: 1.99%, model saved.
Epoch: 4000 Train: 45.96772 Test: 0.43625
Epoch: 4100 Train: 48.52211 Test: 4.86556
Epoch: 4200 Train: 63.12330 Test: 5.48779
Epoch: 4300 Train: 44.71560 Test: 0.60493
Epoch: 4400 Train: 40.42487 Test: 0.22684
Epoch: 4500 Train: 41.74480 Test: 2.36194
Epoch: 4600 Train: 41.97455 Test: 2.69014
Epoch: 4700 Train: 35.72600 Test: 0.63127
Epoch: 4800 Train: 37.39596 Test: 0.42137
Epoch: 4900 Train: 35.38988 Test: 0.58382
Epoch: 5000 Train: 43.90363 Test: 2.84391
Epoch: 5100 Train: 39.74414 Test: 1.57460
Epoch: 5200 Train: 30.66543 Test: 0.17963
Epoch: 5300 Train: 29.84897 Test: 0.07934
Epoch: 5400 Train: 30.74937 Test: 1.00685
Epoch 5500: New minimal relative error: 1.70%, model saved.
Epoch: 5500 Train: 30.72195 Test: 0.36452
Epoch: 5600 Train: 29.30067 Test: 0.45006
Epoch: 5700 Train: 33.79625 Test: 0.32941
Epoch: 5800 Train: 32.77900 Test: 0.73107
Epoch: 5900 Train: 41.96368 Test: 3.53720
Epoch: 6000 Train: 38.67650 Test: 0.13894
Epoch: 6100 Train: 35.72289 Test: 0.34461
Epoch: 6200 Train: 36.63201 Test: 1.02005
Epoch: 6300 Train: 31.06383 Test: 0.31034
Epoch: 6400 Train: 35.14989 Test: 0.72573
Epoch: 6500 Train: 30.95177 Test: 0.17966
Epoch: 6600 Train: 29.19258 Test: 0.28803
Epoch: 6700 Train: 32.81176 Test: 0.29101
Epoch: 6800 Train: 27.02906 Test: 0.42810
Epoch: 6900 Train: 23.64908 Test: 0.20642
Epoch: 7000 Train: 24.05001 Test: 1.08254
Epoch: 7100 Train: 22.08136 Test: 0.22729
Epoch: 7200 Train: 33.32611 Test: 4.29319
Epoch 7300: New minimal relative error: 1.48%, model saved.
Epoch: 7300 Train: 22.82347 Test: 0.08852
Epoch: 7400 Train: 21.08080 Test: 0.25648
Epoch: 7500 Train: 26.54555 Test: 0.86229
Epoch: 7600 Train: 19.77316 Test: 0.04436
Epoch: 7700 Train: 19.24863 Test: 0.24429
Epoch: 7800 Train: 20.37700 Test: 0.10038
Epoch: 7900 Train: 22.70726 Test: 1.46045
Epoch: 8000 Train: 20.65034 Test: 0.31800
Epoch: 8100 Train: 18.72757 Test: 0.46745
Epoch: 8200 Train: 19.40182 Test: 0.04064
Epoch: 8300 Train: 23.00693 Test: 0.12401
Epoch: 8400 Train: 18.46503 Test: 0.05205
Epoch: 8500 Train: 17.66379 Test: 0.03019
Epoch: 8600 Train: 17.65012 Test: 0.17503
Epoch: 8700 Train: 16.44626 Test: 0.04428
Epoch: 8800 Train: 17.68751 Test: 0.14099
Epoch: 8900 Train: 17.13827 Test: 0.03313
Epoch: 9000 Train: 17.34604 Test: 0.29054
Epoch: 9100 Train: 16.57721 Test: 0.19241
Epoch: 9200 Train: 16.09536 Test: 0.03064
Epoch: 9300 Train: 16.16487 Test: 0.09162
Epoch: 9400 Train: 17.24503 Test: 0.09328
Epoch: 9500 Train: 18.46624 Test: 0.40956
Epoch: 9600 Train: 18.99840 Test: 0.65666
Epoch: 9700 Train: 16.93856 Test: 0.05101
Epoch 9800: New minimal relative error: 1.34%, model saved.
Epoch: 9800 Train: 16.97448 Test: 0.03930
Epoch: 9900 Train: 17.56397 Test: 0.17184
Epoch: 9999 Train: 19.69567 Test: 0.13877
Training Loss: tensor(19.6957)
Test Loss: tensor(0.1388)
Learned LE: [  0.8927173   -0.02507586 -14.536132  ]
True LE: [ 8.7800753e-01 -1.4378878e-03 -1.4547396e+01]
Relative Error: [3.768675   3.7746344  3.7854643  3.8327687  3.7904632  3.777216
 3.670862   3.741224   3.8219285  3.7867985  3.5117764  2.91627
 2.251406   1.5982051  0.91691625 0.36287647 0.30677974 0.70321506
 1.2385108  1.7861938  2.3155677  2.4868138  2.007936   1.7555118
 1.6817126  1.7709471  1.6005576  1.7601752  2.0909054  2.5305574
 3.0239117  3.359416   3.3807952  3.1669903  3.0343287  2.8686862
 2.5761154  2.2423084  1.9739425  1.7047638  1.4400634  1.4469076
 1.3761024  1.1454703  0.82850957 0.6703503  0.91103745 1.3962944
 1.8336852  2.2326124  2.8763435  3.5259345  4.0328274  4.328829
 3.8851712  3.4426067  3.1822755  2.9792113  3.078679   3.2206388
 3.3348572  3.3018835  3.2924466  3.2278478  3.238007   3.3635263
 3.4374363  3.3998244  3.3515267  3.2790277  3.5038126  3.5919688
 3.5752695  3.246594   2.6143785  2.0828784  1.4679863  0.81759644
 0.248432   0.21800292 0.52798367 0.9818206  1.503249   2.0032954
 2.0802238  1.5773678  1.3020886  1.2064276  1.0715358  1.0678904
 1.3955755  1.9438131  2.669131   3.2128866  3.3132048  3.1426477
 2.8717616  2.7040594  2.5028787  2.359765   2.0862277  1.9566132
 1.7817812  1.6944321  1.5103725  1.2066861  0.9583415  0.5982173
 0.45034173 0.8968152  1.3336929  1.7161449  2.1763883  2.8521404
 3.5650487  4.0240116  4.0304885  3.4760635  3.1827521  2.9587142
 2.8463197  2.9614835  2.9869816  2.9003656  2.8008442  2.6791456
 2.6085246  2.7490447  3.0397508  3.161672   3.189187   3.2018669
 3.0841684  3.2814302  3.365283   3.2298431  2.8745003  2.4365826
 1.9674016  1.4563167  0.89588946 0.4664178  0.45578298 0.4279904
 0.65450233 1.0706824  1.6340712  1.6197888  1.0253193  0.7977183
 0.72659177 0.5752441  0.71051705 1.1013728  1.7898545  2.6499786
 3.1844254  3.2089996  2.782947   2.336458   2.0551114  1.7204312
 1.5491965  1.6139792  1.5041456  1.3403549  1.5499316  1.4898064
 1.1299891  0.820001   0.39663878 0.4278217  0.9272655  1.2745261
 1.56351    2.056994   2.7740006  3.462074   3.8800075  3.5842764
 3.1273232  2.8636053  2.592943   2.5841424  2.6209812  2.5198746
 2.3905892  2.2212186  2.1031215  2.2743647  2.8107898  3.1890974
 3.2669327  3.3055377  3.3526242  3.2350273  3.3437808  3.245791
 2.9322586  2.5960262  2.2717073  1.8999388  1.4556459  1.0569063
 0.6580596  0.64868754 0.5447881  0.46554244 0.6133051  1.0751789
 1.0664097  0.5954222  0.4284583  0.3635547  0.5153486  0.6298814
 0.9636833  1.5123711  2.4246542  3.0110414  2.7234006  2.3000908
 1.7199106  1.347573   1.084292   0.83996224 0.8150379  0.92852527
 0.9825392  1.1954484  1.3462825  1.2073054  0.79177904 0.3520378
 0.46787277 0.9565194  1.3107324  1.5818387  1.9637942  2.5480378
 3.1788752  3.7210062  3.184855   2.6889095  2.4238174  2.1116936
 2.1634624  2.1487162  2.0265005  1.8192102  1.75874    1.7753769
 2.1921363  2.7649941  3.2131348  3.3307974  3.3000422  3.2230973
 3.0289102  3.1274457  3.123455   2.8231573  2.3212726  1.9921961
 1.7431937  1.37114    1.1263409  0.9486046  0.8029943  0.8077616
 0.629975   0.41973352 0.41376188 0.5289935  0.40659857 0.39071375
 0.41483733 0.6689995  0.7194878  0.738433   1.1377233  2.0779724
 2.317308   2.0638163  1.7407898  1.2664438  0.943164   0.6114047
 0.32372245 0.25371078 0.38475084 0.5427995  0.9877468  1.0195951
 1.0648835  0.91903347 0.37227523 0.36755317 0.9719628  1.3867425
 1.6500744  1.7727815  2.1652067  2.6521456  3.2394226  2.8306394
 2.2325633  1.8553276  1.6152955  1.6530017  1.6215321  1.4238867
 1.3681602  1.3452314  1.621905   2.0595226  2.5068848  2.9085839
 3.0379982  2.9898899  2.8304024  2.7001946  2.5263932  2.7304103
 2.5732908  2.2480159  1.8679587  1.4904411  1.3005004  0.96709067
 0.9337574  0.8531412  0.92312366 0.98172796 0.8480973  0.56475496
 0.2752412  0.3926696  0.3012489  0.38556504 0.47360408 0.5215431
 0.48348406 0.75624675 1.4017447  1.6058054  1.4751943  1.1262883
 0.9464153  0.7388263  0.564714   0.365279   0.26504284 0.3597591
 0.5122958  0.7796944  0.8578302  0.69220734 0.77711415 0.68637675
 0.06870613 0.70643455 1.2587318  1.6123412  1.692213   1.7674456
 2.0860882  2.4403017  2.6585221  1.9365788  1.4666983  1.1165937
 1.1354529  1.0543021  0.7480416  0.69475967 0.8928716  1.3117316
 1.7889867  2.1578429  2.3681366  2.4273434  2.4314492  2.3042617
 2.0689526  2.0875597  2.0795357  2.1738737  2.0551455  1.8908117
 1.7366953  1.4418503  1.0051855  0.8545736  0.74674445 0.7042024
 0.88163745 1.1561178  1.0941628  0.68519044]
