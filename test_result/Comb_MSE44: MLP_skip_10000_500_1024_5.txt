time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.93%, model saved.
Epoch: 0 Train: 3908.23584 Test: 4342.50635
Epoch 100: New minimal relative error: 53.69%, model saved.
Epoch: 100 Train: 52.70328 Test: 55.50031
Epoch 200: New minimal relative error: 14.95%, model saved.
Epoch: 200 Train: 22.15388 Test: 28.98211
Epoch: 300 Train: 8.36612 Test: 12.42052
Epoch 400: New minimal relative error: 13.67%, model saved.
Epoch: 400 Train: 4.05847 Test: 5.51944
Epoch: 500 Train: 3.33981 Test: 5.67320
Epoch 600: New minimal relative error: 12.11%, model saved.
Epoch: 600 Train: 4.63282 Test: 6.41300
Epoch: 700 Train: 4.04049 Test: 1.89268
Epoch: 800 Train: 5.45068 Test: 7.53672
Epoch: 900 Train: 2.47196 Test: 3.46701
Epoch: 1000 Train: 7.22396 Test: 8.70332
Epoch: 1100 Train: 2.65300 Test: 2.41852
Epoch: 1200 Train: 10.87384 Test: 11.00479
Epoch: 1300 Train: 6.62947 Test: 6.23240
Epoch: 1400 Train: 0.99327 Test: 1.23888
Epoch: 1500 Train: 2.37605 Test: 2.27326
Epoch 1600: New minimal relative error: 11.09%, model saved.
Epoch: 1600 Train: 0.59280 Test: 0.51866
Epoch: 1700 Train: 0.59082 Test: 0.62641
Epoch: 1800 Train: 1.12878 Test: 1.19451
Epoch: 1900 Train: 1.88117 Test: 2.17935
Epoch: 2000 Train: 1.41971 Test: 1.92000
Epoch 2100: New minimal relative error: 10.52%, model saved.
Epoch: 2100 Train: 0.58153 Test: 0.56583
Epoch 2200: New minimal relative error: 7.76%, model saved.
Epoch: 2200 Train: 1.04586 Test: 0.58281
Epoch: 2300 Train: 0.63785 Test: 0.39972
Epoch: 2400 Train: 0.42181 Test: 0.65318
Epoch: 2500 Train: 3.34171 Test: 4.29204
Epoch: 2600 Train: 2.20154 Test: 3.33514
Epoch: 2700 Train: 0.77437 Test: 1.26289
Epoch: 2800 Train: 0.99809 Test: 0.91801
Epoch: 2900 Train: 1.55545 Test: 1.33128
Epoch: 3000 Train: 0.70675 Test: 0.71703
Epoch: 3100 Train: 0.86586 Test: 1.05814
Epoch: 3200 Train: 0.56047 Test: 0.57706
Epoch: 3300 Train: 1.52686 Test: 1.13645
Epoch: 3400 Train: 5.14975 Test: 5.50923
Epoch: 3500 Train: 1.11121 Test: 1.07117
Epoch 3600: New minimal relative error: 3.82%, model saved.
Epoch: 3600 Train: 0.27415 Test: 0.37127
Epoch: 3700 Train: 0.74384 Test: 0.88740
Epoch: 3800 Train: 0.61694 Test: 0.54057
Epoch: 3900 Train: 0.79560 Test: 0.53236
Epoch: 4000 Train: 0.55833 Test: 0.71000
Epoch: 4100 Train: 0.44988 Test: 0.19268
Epoch: 4200 Train: 0.72313 Test: 0.68461
Epoch: 4300 Train: 0.55469 Test: 0.62952
Epoch: 4400 Train: 1.68258 Test: 1.85619
Epoch: 4500 Train: 1.06769 Test: 1.24577
Epoch: 4600 Train: 1.42297 Test: 1.50708
Epoch: 4700 Train: 1.05208 Test: 1.44464
Epoch: 4800 Train: 1.21586 Test: 1.19465
Epoch: 4900 Train: 0.73305 Test: 1.02642
Epoch: 5000 Train: 0.30499 Test: 0.13920
Epoch: 5100 Train: 0.19224 Test: 0.17917
Epoch: 5200 Train: 1.03161 Test: 0.76572
Epoch: 5300 Train: 0.14783 Test: 0.10763
Epoch: 5400 Train: 0.19892 Test: 0.23279
Epoch: 5500 Train: 0.18966 Test: 0.20880
Epoch: 5600 Train: 0.41672 Test: 0.28413
Epoch: 5700 Train: 0.70155 Test: 0.85665
Epoch: 5800 Train: 0.62096 Test: 0.79265
Epoch: 5900 Train: 0.14803 Test: 0.13364
Epoch: 6000 Train: 0.10393 Test: 0.12610
Epoch: 6100 Train: 0.35558 Test: 0.29353
Epoch: 6200 Train: 0.29081 Test: 0.37151
Epoch: 6300 Train: 0.12713 Test: 0.10113
Epoch: 6400 Train: 0.07156 Test: 0.06900
Epoch: 6500 Train: 0.16185 Test: 0.19463
Epoch: 6600 Train: 0.28696 Test: 0.35831
Epoch: 6700 Train: 0.07872 Test: 0.09253
Epoch: 6800 Train: 0.07216 Test: 0.11173
Epoch: 6900 Train: 0.14711 Test: 0.17933
Epoch: 7000 Train: 1.12324 Test: 1.13954
Epoch: 7100 Train: 0.20358 Test: 0.22691
Epoch: 7200 Train: 0.06123 Test: 0.08067
Epoch: 7300 Train: 0.56028 Test: 0.59264
Epoch: 7400 Train: 0.08992 Test: 0.10859
Epoch: 7500 Train: 0.04328 Test: 0.06254
Epoch: 7600 Train: 0.03694 Test: 0.05428
Epoch: 7700 Train: 0.19066 Test: 0.12794
Epoch: 7800 Train: 0.26169 Test: 0.23602
Epoch: 7900 Train: 0.04848 Test: 0.05756
Epoch: 8000 Train: 0.14263 Test: 0.12903
Epoch: 8100 Train: 0.15695 Test: 0.10195
Epoch: 8200 Train: 0.16043 Test: 0.16827
Epoch: 8300 Train: 0.05637 Test: 0.06303
Epoch: 8400 Train: 0.03978 Test: 0.05189
Epoch: 8500 Train: 0.15769 Test: 0.20216
Epoch: 8600 Train: 0.03655 Test: 0.04560
Epoch: 8700 Train: 0.21832 Test: 0.17279
Epoch: 8800 Train: 0.22647 Test: 0.25995
Epoch 8900: New minimal relative error: 3.72%, model saved.
Epoch: 8900 Train: 0.03884 Test: 0.04626
Epoch: 9000 Train: 0.40755 Test: 0.43333
Epoch: 9100 Train: 0.08185 Test: 0.11051
Epoch: 9200 Train: 0.09381 Test: 0.08742
Epoch: 9300 Train: 0.04862 Test: 0.06181
Epoch: 9400 Train: 0.04197 Test: 0.05511
Epoch: 9500 Train: 0.02032 Test: 0.03029
Epoch: 9600 Train: 0.21623 Test: 0.21897
Epoch: 9700 Train: 0.02122 Test: 0.02851
Epoch: 9800 Train: 0.04775 Test: 0.05776
Epoch: 9900 Train: 0.03258 Test: 0.04746
Epoch: 9999 Train: 0.02857 Test: 0.03609
Training Loss: tensor(0.0286)
Test Loss: tensor(0.0361)
Learned LE: [ 0.8853579   0.02149072 -4.1559787 ]
True LE: [ 8.7649196e-01 -5.7233003e-04 -1.4548853e+01]
Relative Error: [0.7413041  0.712347   1.1136483  1.2189296  1.2253623  0.86198926
 0.89416236 1.4472181  2.1007543  2.3279932  2.025465   1.6727922
 1.4341738  1.4845543  3.053798   4.943297   6.680069   7.789197
 8.382182   8.427976   7.816288   7.0189767  6.8637214  6.6061606
 6.1518903  5.4975586  4.966644   4.7125955  4.8415804  4.834532
 4.591022   4.548464   3.9403949  3.4212058  3.3632393  3.1636622
 2.9620492  3.1018858  2.877725   2.4164853  2.3585327  2.3943717
 2.3338566  2.1670847  2.0551312  1.9166735  1.757206   1.568941
 1.1972789  0.9861529  0.6561695  0.52921534 0.39480934 0.32510343
 0.35592538 0.3928476  0.4654293  0.6860615  0.96187925 1.0310113
 1.0374182  0.95382166 0.82433206 0.7920159  0.5611253  0.8515739
 0.8555944  0.8571994  0.4576434  0.82486784 1.3428748  1.8066896
 1.633862   1.5260679  1.3699712  1.1100763  1.2924316  2.3818452
 4.27081    5.681812   6.669554   6.993579   6.861597   6.440348
 5.694187   5.195631   4.904144   4.5140324  4.0861287  3.6761475
 3.53437    3.6174867  3.4901247  3.3537772  3.2141156  2.8625216
 2.2994976  2.4738367  2.0820162  2.1347604  2.2995706  2.1920028
 1.668081   1.48997    1.6633683  1.7061435  1.4428438  1.4088053
 1.2625821  1.1601413  1.1412165  0.951787   0.91589636 0.6808075
 0.61235    0.5234357  0.3933304  0.3037928  0.39271843 0.54058605
 0.74617916 0.9216572  0.80036426 0.79384035 0.88323456 0.75630194
 0.7501516  0.54121304 0.48769447 0.6232711  0.57769173 0.27635294
 0.6424289  0.7031756  0.8814123  1.0082107  1.0682822  0.8424461
 0.8446066  1.344191   2.118897   3.5452862  4.5307684  5.2028127
 5.269628   5.075478   4.8076925  4.180794   3.8443272  3.4219134
 2.9048166  2.651067   2.3361974  2.1938925  2.3227043  2.1304977
 2.1262932  1.958921   1.7368476  1.3462597  1.4283587  1.2786962
 1.4203844  1.4781982  1.5031265  0.9887988  0.8287712  0.99376774
 1.0957493  0.8950021  0.8399522  0.72628033 0.6717135  0.7463149
 0.74024796 0.7180933  0.5315907  0.5507583  0.3962555  0.30415925
 0.285503   0.37598395 0.5080113  0.63170993 0.88922364 0.7481781
 0.57909554 0.5801208  0.706663   0.68146497 0.60255706 0.44083646
 0.49007615 0.44770786 0.32987168 0.12665519 0.18958624 0.2865938
 0.42381057 0.48285764 0.74339753 1.7505659  2.3627098  2.7569048
 3.2038774  3.4203582  3.2027667  3.1144822  3.0913284  3.0095196
 2.7537448  2.2060876  1.9665904  1.7802701  1.3254108  1.2976952
 1.2717986  1.2531066  1.392355   1.1516837  1.244994   1.1807244
 0.8967855  0.82879496 0.71665776 0.695571   1.0044355  0.96296483
 0.7439358  0.42903227 0.49796212 0.6860291  0.6924245  0.62365395
 0.5725306  0.45505637 0.38930753 0.3567249  0.4515711  0.41027433
 0.49042186 0.4207307  0.2322939  0.27033693 0.3653774  0.36063245
 0.44350162 0.4217085  0.5721874  0.3437316  0.2646288  0.51830304
 0.5289945  0.37208217 0.55020064 0.44470397 0.49032    0.35370702
 0.32420143 0.28275785 0.33820468 0.8165404  1.1357739  1.3126417
 2.2654226  2.5190096  2.4215915  2.204556   2.288435   2.5072563
 2.2713683  2.220608   1.8837432  1.6769025  1.6553687  1.2588785
 1.1961855  1.3275826  0.98490363 0.87733185 0.8901857  0.7360945
 0.90401584 0.75743604 0.7704968  0.8887809  0.7961651  0.6894462
 0.5286403  0.6938761  0.5790185  0.6622035  0.48632565 0.4096408
 0.55870813 0.54497397 0.44162464 0.5416521  0.4758819  0.34943098
 0.27251452 0.4809075  0.34749472 0.3730631  0.38539198 0.3902356
 0.29207534 0.29710892 0.32606277 0.2893981  0.36069652 0.29405668
 0.38457295 0.12037032 0.11463502 0.4026109  0.49672028 0.22820202
 0.40908983 0.404674   0.4310674  0.5297233  0.49742633 0.42456254
 0.75423443 1.1667845  1.5511664  1.4239151  1.8107722  1.9251821
 2.0123887  1.9875069  2.2826455  1.9807918  1.7677188  1.2097274
 1.3589177  1.0974307  0.86280096 0.7957208  0.7036307  0.6573002
 0.79382944 0.65061647 0.6697942  0.58194345 0.6067758  0.46994597
 0.5652656  0.40614316 0.6518166  0.5406526  0.5697893  0.52854943
 0.43668938 0.4523065  0.5371555  0.5867267  0.48994738 0.41664243
 0.35916078 0.44205853 0.3985574  0.46544367 0.38264403 0.44177926
 0.2894073  0.2701607  0.3531613  0.3237236  0.30160818 0.25280696
 0.2794771  0.3338931  0.27527225 0.25196666 0.402501   0.27202117
 0.13940012 0.1712525  0.3659372  0.3359164  0.3626843  0.3447389
 0.348373   0.5616197  0.64552414 0.31314573 0.6563865  1.0015017
 1.2093278  1.0628016  1.2491381  1.3035376  1.5517474  1.4844942
 1.3169533  0.832869   0.5603399  0.4511211 ]
