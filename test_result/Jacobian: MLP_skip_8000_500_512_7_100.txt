time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.31%, model saved.
Epoch: 0 Train: 9100.99805 Test: 4224.30713
Epoch: 80 Train: 2649.59326 Test: 1231.90967
Epoch: 160 Train: 2561.76562 Test: 1161.25513
Epoch: 240 Train: 2043.16345 Test: 750.79523
Epoch: 320 Train: 1492.71143 Test: 497.71390
Epoch: 400 Train: 868.85352 Test: 229.71747
Epoch 480: New minimal relative error: 97.54%, model saved.
Epoch: 480 Train: 443.07819 Test: 90.35864
Epoch 560: New minimal relative error: 32.28%, model saved.
Epoch: 560 Train: 233.10735 Test: 47.07773
Epoch 640: New minimal relative error: 12.15%, model saved.
Epoch: 640 Train: 144.43716 Test: 44.60548
Epoch: 720 Train: 123.40245 Test: 20.27930
Epoch: 800 Train: 93.10303 Test: 16.28325
Epoch: 880 Train: 63.96065 Test: 5.39032
Epoch: 960 Train: 65.69844 Test: 22.81131
Epoch: 1040 Train: 74.83932 Test: 16.45812
Epoch: 1120 Train: 62.78859 Test: 17.73842
Epoch: 1200 Train: 44.18753 Test: 4.42053
Epoch: 1280 Train: 51.20603 Test: 10.58962
Epoch: 1360 Train: 40.00174 Test: 4.79877
Epoch: 1440 Train: 45.37312 Test: 9.43573
Epoch: 1520 Train: 37.26218 Test: 6.19484
Epoch 1600: New minimal relative error: 11.36%, model saved.
Epoch: 1600 Train: 39.51681 Test: 6.54987
Epoch: 1680 Train: 65.37678 Test: 30.22188
Epoch 1760: New minimal relative error: 4.51%, model saved.
Epoch: 1760 Train: 27.20492 Test: 0.62765
Epoch: 1840 Train: 24.20291 Test: 0.67059
Epoch: 1920 Train: 35.78009 Test: 12.64975
Epoch: 2000 Train: 24.42903 Test: 0.56945
Epoch: 2080 Train: 21.80620 Test: 1.18494
Epoch 2160: New minimal relative error: 3.94%, model saved.
Epoch: 2160 Train: 19.69324 Test: 0.34061
Epoch: 2240 Train: 18.88468 Test: 2.33866
Epoch: 2320 Train: 28.12845 Test: 7.78157
Epoch: 2400 Train: 23.50383 Test: 8.86847
Epoch: 2480 Train: 16.91521 Test: 0.78561
Epoch: 2560 Train: 20.69052 Test: 6.20212
Epoch: 2640 Train: 23.00038 Test: 4.83840
Epoch: 2720 Train: 16.37357 Test: 3.10631
Epoch: 2800 Train: 18.36827 Test: 2.54553
Epoch: 2880 Train: 14.15248 Test: 0.95664
Epoch: 2960 Train: 15.22214 Test: 1.70011
Epoch: 3040 Train: 15.21396 Test: 2.28876
Epoch: 3120 Train: 17.76344 Test: 6.64245
Epoch: 3200 Train: 17.59514 Test: 2.16765
Epoch: 3280 Train: 11.53411 Test: 1.04594
Epoch: 3360 Train: 11.55870 Test: 1.05992
Epoch: 3440 Train: 14.41660 Test: 1.93496
Epoch: 3520 Train: 11.20686 Test: 0.34616
Epoch: 3600 Train: 10.45287 Test: 0.29630
Epoch: 3680 Train: 10.21057 Test: 0.24522
Epoch 3760: New minimal relative error: 2.84%, model saved.
Epoch: 3760 Train: 9.51328 Test: 0.29212
Epoch: 3840 Train: 8.92958 Test: 0.45322
Epoch: 3920 Train: 9.87004 Test: 0.62402
Epoch: 4000 Train: 12.08724 Test: 2.21193
Epoch: 4080 Train: 10.49664 Test: 2.01608
Epoch: 4160 Train: 8.30717 Test: 0.56939
Epoch: 4240 Train: 8.89332 Test: 1.97079
Epoch: 4320 Train: 8.16631 Test: 1.35001
Epoch: 4400 Train: 8.16209 Test: 0.21659
Epoch: 4480 Train: 7.66856 Test: 0.53657
Epoch: 4560 Train: 10.41380 Test: 1.70954
Epoch: 4640 Train: 7.59512 Test: 0.23309
Epoch: 4720 Train: 8.07733 Test: 0.13165
Epoch: 4800 Train: 7.49215 Test: 0.13597
Epoch: 4880 Train: 8.55711 Test: 1.82766
Epoch: 4960 Train: 6.95889 Test: 0.04739
Epoch: 5040 Train: 7.10188 Test: 0.16782
Epoch: 5120 Train: 9.39289 Test: 1.51030
Epoch: 5200 Train: 6.58935 Test: 0.10243
Epoch: 5280 Train: 7.65022 Test: 1.08961
Epoch: 5360 Train: 6.33923 Test: 0.12368
Epoch: 5440 Train: 6.53774 Test: 0.46258
Epoch: 5520 Train: 9.45219 Test: 1.48878
Epoch: 5600 Train: 6.69318 Test: 0.64387
Epoch: 5680 Train: 7.49030 Test: 1.06348
Epoch: 5760 Train: 12.46380 Test: 3.51114
Epoch: 5840 Train: 6.01748 Test: 0.10497
Epoch: 5920 Train: 6.11276 Test: 0.07267
Epoch: 6000 Train: 5.55769 Test: 0.33257
Epoch 6080: New minimal relative error: 2.66%, model saved.
Epoch: 6080 Train: 5.38348 Test: 0.03022
Epoch: 6160 Train: 5.34114 Test: 0.14788
Epoch: 6240 Train: 5.41283 Test: 0.38376
Epoch 6320: New minimal relative error: 2.27%, model saved.
Epoch: 6320 Train: 5.99555 Test: 1.26977
Epoch: 6400 Train: 5.89191 Test: 0.09212
Epoch 6480: New minimal relative error: 1.79%, model saved.
Epoch: 6480 Train: 5.36710 Test: 0.16867
Epoch: 6560 Train: 5.41280 Test: 0.06758
Epoch: 6640 Train: 6.93803 Test: 0.81962
Epoch: 6720 Train: 5.16065 Test: 0.17649
Epoch: 6800 Train: 5.98348 Test: 0.31208
Epoch: 6880 Train: 5.01093 Test: 0.06297
Epoch 6960: New minimal relative error: 1.39%, model saved.
Epoch: 6960 Train: 5.07403 Test: 0.06583
Epoch: 7040 Train: 6.43869 Test: 0.71999
Epoch: 7120 Train: 5.65049 Test: 0.15398
Epoch: 7200 Train: 9.13433 Test: 2.23543
Epoch: 7280 Train: 5.08753 Test: 0.02632
Epoch: 7360 Train: 5.05373 Test: 0.02976
Epoch: 7440 Train: 5.27144 Test: 0.03813
Epoch: 7520 Train: 5.18868 Test: 0.21515
Epoch: 7600 Train: 5.10422 Test: 0.06116
Epoch: 7680 Train: 4.88815 Test: 0.17189
Epoch: 7760 Train: 4.94873 Test: 0.23816
Epoch: 7840 Train: 4.76133 Test: 0.02254
Epoch: 7920 Train: 5.07238 Test: 0.10882
Epoch: 7999 Train: 4.83756 Test: 0.11698
Training Loss: tensor(4.8376)
Test Loss: tensor(0.1170)
Learned LE: [  0.82351863   0.05188931 -14.561582  ]
True LE: [ 8.6544806e-01  3.0280377e-03 -1.4538586e+01]
Relative Error: [2.2349708  1.8061172  1.3598903  1.5320128  1.6262379  1.6806569
 1.6906848  1.6288437  1.262415   1.4808819  1.209841   0.96082675
 0.905688   0.81832874 0.7846163  0.8128166  1.1486459  1.3457992
 1.4423087  1.5390338  1.6975979  1.7513882  1.7877347  1.7284002
 1.5153469  1.8167015  2.227856   2.3411899  2.520686   2.502174
 2.4486358  2.33365    2.1132147  2.0058603  1.7606223  1.8483241
 1.9029915  1.4896166  1.2488332  0.8799554  0.62531674 0.50816184
 0.6402398  0.46003553 0.22369802 0.78886354 1.3234541  1.5353558
 1.7092593  1.8402948  2.0507126  2.2829473  2.2640314  2.040242
 1.9155903  1.9995351  2.2492082  2.67861    3.1905375  3.3089447
 3.1548355  2.884723   2.3698368  1.8224902  1.3959169  1.2631857
 1.4804689  1.4531388  1.4493779  1.2978771  0.9437808  1.175255
 1.1901929  0.90116847 0.9465443  0.78691256 0.7285324  0.78039
 1.0631496  0.9733434  0.7908066  0.9085559  1.1362505  1.5754471
 1.7544204  1.6202384  1.4169699  1.6395705  2.0157344  2.0117526
 2.0615153  2.0268104  1.9311789  1.8759195  1.8116368  1.8222454
 1.567127   1.5624834  1.8296556  1.4323709  1.1821676  0.9599876
 0.67599213 0.4674577  0.45799175 0.24031657 0.361349   0.8829182
 1.2613319  1.423934   1.5341276  1.6628706  1.7834868  2.013582
 2.1403053  1.8922521  1.7610947  1.7919639  2.0750766  2.7457447
 3.2296765  3.2227154  2.970807   2.7211583  2.5411892  1.9371811
 1.633153   1.3070568  1.5982608  1.5183135  1.2936918  1.0313579
 0.69725585 0.84578013 1.030189   0.891441   0.9241025  0.8963433
 0.7668135  0.78711927 0.97067106 0.57410944 0.33526605 0.5343136
 0.9348968  1.4888718  1.7027546  1.4484782  1.2192158  1.5166366
 2.0003605  1.9124684  1.9184235  1.5294943  1.4493191  1.5007571
 1.5576674  1.5441922  1.4968631  1.3015393  1.578371   1.4348094
 1.1450392  1.0375903  0.7992852  0.47943717 0.34479037 0.22037011
 0.6690396  0.8307507  1.1230285  1.3378668  1.37741    1.4574312
 1.5207505  1.5840745  1.6021364  1.3586004  1.2623026  1.3797903
 1.8609325  2.4559853  2.8852668  2.6998005  2.4101195  2.1670725
 2.0697987  1.9559631  1.6867555  1.5315837  1.4285094  1.6269388
 1.4438477  0.9549455  0.5228695  0.5697849  0.7181693  0.84182936
 0.88047993 0.993225   0.90005344 0.7831445  0.88883424 0.35552973
 0.33744302 0.44026035 0.7216624  1.1019465  1.445754   1.2585328
 0.9307403  1.0489775  1.4873627  1.7069753  1.9965996  1.4382864
 1.1107601  1.2037369  1.2847044  1.4557841  1.4636797  1.1533054
 1.2243035  1.4204844  1.1729232  1.0522877  1.0159483  0.6115651
 0.21450129 0.28941286 0.9208497  0.7633092  0.97545224 1.1769629
 1.2095433  1.1428056  0.8834455  0.80200255 1.049638   1.0148382
 0.86428934 0.88658315 1.2234741  1.7868624  2.4243255  2.151085
 1.8750502  1.7335954  1.6555572  1.6629788  1.5554603  1.415214
 1.3186598  1.6382012  1.5259792  1.1788849  0.73552555 0.38979355
 0.54099095 0.8144654  0.842126   0.9686636  1.1864913  0.9850125
 0.8140005  0.33463556 0.40661004 0.34155872 0.55209553 0.7460443
 1.1301279  1.2347146  0.9481129  0.7998389  1.1206259  1.2417418
 1.5721673  1.3906459  1.08266    0.9583913  1.1222204  1.1330063
 1.2344815  1.1752803  0.95432913 1.2742602  1.108899   1.079748
 1.1631684  0.8916052  0.19976185 0.40241787 0.9966738  0.8129743
 0.8789016  1.0336055  0.9330902  0.62798303 0.52751553 0.6563383
 0.9101648  1.0210756  0.8493104  0.7803245  0.990114   1.1727343
 1.5076256  1.4909425  1.4826487  1.3573041  1.2780441  1.2108476
 1.2218261  1.1158743  1.0804932  1.1836025  1.5903863  1.4197836
 0.96233076 0.5368542  0.51971954 0.7698648  0.8422139  0.8709098
 1.1575031  1.320672   0.9983041  0.28394708 0.2934928  0.32178596
 0.4266641  0.5671229  0.821453   1.050452   0.8454018  0.6802726
 0.77870625 1.0208558  1.2836357  1.0109924  0.9228998  0.93744975
 0.85793334 0.90999484 1.0361652  1.1506631  0.93954074 0.9385972
 1.0919853  1.1378849  1.143781   1.2124966  0.44902423 0.49753004
 0.90937114 0.95957744 0.81272507 0.8724229  0.49413517 0.43671232
 0.4616543  0.5102052  0.76939857 0.965082   0.9258247  0.70032454
 0.8659481  0.976652   1.1169628  0.91265124 0.7810199  0.85860676
 0.9116595  0.8816661  0.8945493  0.98702085 0.9504765  0.89066356
 1.0347956  1.2786862  1.1157585  0.7881717  0.61804545 0.6524474
 0.6317085  0.7951067  0.9364729  1.1816993  1.2785552  0.38543296
 0.2835277  0.23464234 0.35841545 0.40959093 0.59269726 0.7811875
 0.80560577 0.6788817  0.5318107  0.7625351 ]
