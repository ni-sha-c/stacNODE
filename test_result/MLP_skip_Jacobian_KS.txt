05_19_03_29_14_

time_step: 0.25
lr: 0.001
weight_decay: 0.0005
num_epoch: 3000
num_train: 3000
num_test: 3000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: KS
model_type: MLP_skip
s: 0.2
n_hidden: 512
n_layers: 3
reg_param: 0.5
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 0.240536848 Test: 0.160657654
Epoch 0: New minimal relative error: 0.16%, model saved.
Epoch: 30 Train: 0.044394101 Test: 0.114563257
Epoch 30: New minimal relative error: 0.11%, model saved.
Epoch: 60 Train: 0.014627458 Test: 0.120816304
Epoch: 90 Train: 0.008316620 Test: 0.124541144
Epoch: 120 Train: 0.005974944 Test: 0.126723462
Epoch: 150 Train: 0.004972547 Test: 0.128118678
Epoch: 180 Train: 0.004247942 Test: 0.129991985
Epoch: 210 Train: 0.003847066 Test: 0.130746598
Epoch: 240 Train: 0.003616927 Test: 0.131576766
Epoch: 270 Train: 0.003387924 Test: 0.132456793
Epoch: 300 Train: 0.003205478 Test: 0.132983891
Epoch: 330 Train: 0.003095477 Test: 0.133640125
Epoch: 360 Train: 0.003114639 Test: 0.133759791
Epoch: 390 Train: 0.002914664 Test: 0.134496383
Epoch: 420 Train: 0.002834450 Test: 0.134846540
Epoch: 450 Train: 0.002863804 Test: 0.135544157
Epoch: 480 Train: 0.002734540 Test: 0.135640370
Epoch: 510 Train: 0.002741949 Test: 0.135754939
Epoch: 540 Train: 0.002680003 Test: 0.136107294
Epoch: 570 Train: 0.002658703 Test: 0.136324827
Epoch: 600 Train: 0.002631077 Test: 0.136492129
Epoch: 630 Train: 0.002592139 Test: 0.136868443
Epoch: 660 Train: 0.002560405 Test: 0.137005615
Epoch: 690 Train: 0.002585422 Test: 0.137094910
Epoch: 720 Train: 0.002583914 Test: 0.137381413
Epoch: 750 Train: 0.002515651 Test: 0.137506752
Epoch: 780 Train: 0.002496766 Test: 0.137649857
Epoch: 810 Train: 0.002484926 Test: 0.137785561
Epoch: 840 Train: 0.002468863 Test: 0.137920999
Epoch: 870 Train: 0.002492144 Test: 0.138134813
Epoch: 900 Train: 0.002466387 Test: 0.138163241
Epoch: 930 Train: 0.002438683 Test: 0.138330774
Epoch: 960 Train: 0.002433188 Test: 0.138371591
Epoch: 990 Train: 0.002433013 Test: 0.138451575
Epoch: 1020 Train: 0.002427460 Test: 0.138667092
Epoch: 1050 Train: 0.002404155 Test: 0.138766905
Epoch: 1080 Train: 0.002399345 Test: 0.138882025
Epoch: 1110 Train: 0.002390219 Test: 0.138933446
Epoch: 1140 Train: 0.002382817 Test: 0.139037578
Epoch: 1170 Train: 0.002383690 Test: 0.139144161
Epoch: 1200 Train: 0.002370936 Test: 0.139205219
Epoch: 1230 Train: 0.002365119 Test: 0.139308414
Epoch: 1260 Train: 0.002360268 Test: 0.139386162
Epoch: 1290 Train: 0.002354699 Test: 0.139442384
Epoch: 1320 Train: 0.002349213 Test: 0.139525202
Epoch: 1350 Train: 0.002343826 Test: 0.139608993
Epoch: 1380 Train: 0.002339047 Test: 0.139676031
Epoch: 1410 Train: 0.002334416 Test: 0.139746458
Epoch: 1440 Train: 0.002329916 Test: 0.139811211
Epoch: 1470 Train: 0.002325614 Test: 0.139873784
Epoch: 1500 Train: 0.002321367 Test: 0.139936108
Epoch: 1530 Train: 0.002317255 Test: 0.139996199
Epoch: 1560 Train: 0.002313267 Test: 0.140056382
Epoch: 1590 Train: 0.002309369 Test: 0.140115935
Epoch: 1620 Train: 0.002305569 Test: 0.140173610
Epoch: 1650 Train: 0.002301863 Test: 0.140230114
Epoch: 1680 Train: 0.002298250 Test: 0.140285397
Epoch: 1710 Train: 0.002294715 Test: 0.140339540
Epoch: 1740 Train: 0.002291255 Test: 0.140394689
Epoch: 1770 Train: 0.002287861 Test: 0.140449205
Epoch: 1800 Train: 0.002284539 Test: 0.140503216
Epoch: 1830 Train: 0.002281279 Test: 0.140557276
Epoch: 1860 Train: 0.002278082 Test: 0.140609249
Epoch: 1890 Train: 0.002274947 Test: 0.140660260
Epoch: 1920 Train: 0.002271873 Test: 0.140710151
Epoch: 1950 Train: 0.002268851 Test: 0.140759828
Epoch: 1980 Train: 0.002265882 Test: 0.140809515
Epoch: 2010 Train: 0.002262962 Test: 0.140859270
Epoch: 2040 Train: 0.002260092 Test: 0.140908646
Epoch: 2070 Train: 0.002257266 Test: 0.140958030
Epoch: 2100 Train: 0.002254484 Test: 0.141006845
Epoch: 2130 Train: 0.002251750 Test: 0.141054976
Epoch: 2160 Train: 0.002249053 Test: 0.141103126
Epoch: 2190 Train: 0.002246400 Test: 0.141150580
Epoch: 2220 Train: 0.002243782 Test: 0.141197437
Epoch: 2250 Train: 0.002241210 Test: 0.141243798
Epoch: 2280 Train: 0.002238669 Test: 0.141289111
Epoch: 2310 Train: 0.002236170 Test: 0.141334819
Epoch: 2340 Train: 0.002233698 Test: 0.141380900
Epoch: 2370 Train: 0.002231265 Test: 0.141427192
Epoch: 2400 Train: 0.002228861 Test: 0.141473429
Epoch: 2430 Train: 0.002226491 Test: 0.141519244
Epoch: 2460 Train: 0.002224152 Test: 0.141565651
Epoch: 2490 Train: 0.002221842 Test: 0.141611376
Epoch: 2520 Train: 0.002219562 Test: 0.141656655
Epoch: 2550 Train: 0.002217314 Test: 0.141701597
Epoch: 2580 Train: 0.002215090 Test: 0.141746378
Epoch: 2610 Train: 0.002212894 Test: 0.141791058
Epoch: 2640 Train: 0.002210722 Test: 0.141835176
Epoch: 2670 Train: 0.002208572 Test: 0.141879278
Epoch: 2700 Train: 0.002206448 Test: 0.141923293
Epoch: 2730 Train: 0.002204346 Test: 0.141968011
Epoch: 2760 Train: 0.002202269 Test: 0.142012929
Epoch: 2790 Train: 0.002200215 Test: 0.142057246
Epoch: 2820 Train: 0.002198188 Test: 0.142101529
Epoch: 2850 Train: 0.002196185 Test: 0.142145823
Epoch: 2880 Train: 0.002194208 Test: 0.142190267
Epoch: 2910 Train: 0.002192243 Test: 0.142234454
Epoch: 2940 Train: 0.002190301 Test: 0.142278503
Epoch: 2970 Train: 0.002188380 Test: 0.142322139
Epoch: 2999 Train: 0.002186541 Test: 0.142363812
Training Loss: tensor(0.0022)
Test Loss: tensor(0.1424)
True Mean x: tensor(-0.7255, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)
Learned Mean x: tensor(1.0820e+45, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
True Var x: tensor(0.2436, device='cuda:0', dtype=torch.float64, grad_fn=<VarBackward0>)
Learned Var x: tensor(6.8182e+91, device='cuda:0', dtype=torch.float64,
       grad_fn=<VarBackward0>)
True Mean z: tensor(-1.5290, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)
Learned Mean z: tensor(-7.2877e+44, device='cuda:0', dtype=torch.float64,
       grad_fn=<MeanBackward0>)
True Var z: tensor(0.6490, device='cuda:0', dtype=torch.float64, grad_fn=<VarBackward0>)
Learned Var z: tensor(3.1129e+91, device='cuda:0', dtype=torch.float64,
       grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0043)
Jacobian term Test Loss: tensor(0.0043)
Learned LE: [0.1520014  0.15083541 0.14872693 0.1414078  0.14718279 0.1402593
 0.13204373 0.13335788 0.1295203  0.11994456 0.11938628 0.11770424
 0.11257799 0.11059079 0.10447412]
True LE: [0.30574609 0.28004271 0.26752761 0.23201314 0.20837498 0.19183291
 0.16715513 0.15416377 0.12779687 0.11077707 0.09813166 0.07510605
 0.05770563 0.04353978 0.02605524]
Norm Diff:: tensor(0.2945, dtype=torch.float64)
