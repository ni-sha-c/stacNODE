time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 5000
num_train: 5000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 512
n_layers: 3
reg_param: 400.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 15.554532051 Test: 10.138384819
Epoch 0: New minimal relative error: 10.14%, model saved.
Epoch: 50 Train: 2.447200060 Test: 3.424002171
Epoch 50: New minimal relative error: 3.42%, model saved.
Epoch: 100 Train: 2.314439535 Test: 3.280395508
Epoch 100: New minimal relative error: 3.28%, model saved.
Epoch: 150 Train: 2.251342773 Test: 3.171279669
Epoch 150: New minimal relative error: 3.17%, model saved.
Epoch: 200 Train: 2.085430861 Test: 3.025795460
Epoch 200: New minimal relative error: 3.03%, model saved.
Epoch: 250 Train: 2.083387375 Test: 3.005876303
Epoch 250: New minimal relative error: 3.01%, model saved.
Epoch: 300 Train: 1.984619141 Test: 2.876645565
Epoch 300: New minimal relative error: 2.88%, model saved.
Epoch: 350 Train: 1.990129471 Test: 2.907838345
Epoch: 400 Train: 1.982231140 Test: 2.865043163
Epoch 400: New minimal relative error: 2.87%, model saved.
Epoch: 450 Train: 1.977585316 Test: 2.860762119
Epoch 450: New minimal relative error: 2.86%, model saved.
Epoch: 500 Train: 1.956558108 Test: 2.858215332
Epoch 500: New minimal relative error: 2.86%, model saved.
Epoch: 550 Train: 1.955957174 Test: 2.895231247
Epoch: 600 Train: 1.959006071 Test: 2.830348730
Epoch 600: New minimal relative error: 2.83%, model saved.
Epoch: 650 Train: 1.928810358 Test: 2.838396072
Epoch: 700 Train: 1.939842224 Test: 2.869967461
Epoch: 750 Train: 1.987573147 Test: 2.861576796
Epoch: 800 Train: 1.927847385 Test: 2.864233494
Epoch: 850 Train: 1.952788591 Test: 2.842474937
Epoch: 900 Train: 1.954341769 Test: 2.854910851
Epoch: 950 Train: 1.968454123 Test: 2.864774704
Epoch: 1000 Train: 1.981607556 Test: 2.871959448
Epoch: 1050 Train: 2.020906687 Test: 2.889060974
Epoch: 1100 Train: 1.977818251 Test: 2.870620012
Epoch: 1150 Train: 2.011703491 Test: 2.961749554
Epoch: 1200 Train: 1.980844736 Test: 2.904537201
Epoch: 1250 Train: 1.972275496 Test: 2.857789278
Epoch: 1300 Train: 1.963761926 Test: 2.872335434
Epoch: 1350 Train: 1.997395992 Test: 2.902609587
Epoch: 1400 Train: 1.965432286 Test: 2.868604422
Epoch: 1450 Train: 2.000279427 Test: 2.908508778
Epoch: 1500 Train: 1.975048780 Test: 2.889545918
Epoch: 1550 Train: 2.003314972 Test: 2.891028881
Epoch: 1600 Train: 1.990293980 Test: 2.890964031
Epoch: 1650 Train: 1.982897758 Test: 2.880859852
Epoch: 1700 Train: 1.995948911 Test: 2.899152756
Epoch: 1750 Train: 2.018167496 Test: 2.917763472
Epoch: 1800 Train: 2.014746428 Test: 2.901137829
Epoch: 1850 Train: 2.001888990 Test: 2.861990929
Epoch: 1900 Train: 2.022596121 Test: 2.883368015
Epoch: 1950 Train: 2.017604589 Test: 2.890563011
Epoch: 2000 Train: 2.032676458 Test: 2.915931702
Epoch: 2050 Train: 2.009156227 Test: 2.877963781
Epoch: 2100 Train: 2.012654066 Test: 2.881678104
Epoch: 2150 Train: 2.024116516 Test: 2.885772705
Epoch: 2200 Train: 2.020614386 Test: 2.898583889
Epoch: 2250 Train: 2.028951645 Test: 2.905563831
Epoch: 2300 Train: 2.014931679 Test: 2.909632206
Epoch: 2350 Train: 2.020698786 Test: 2.898589134
Epoch: 2400 Train: 2.023017406 Test: 2.907436371
Epoch: 2450 Train: 2.019791126 Test: 2.899010658
Epoch: 2500 Train: 2.019093752 Test: 2.903500080
Epoch: 2550 Train: 2.023060560 Test: 2.912437677
Epoch: 2600 Train: 2.027001858 Test: 2.907621384
Epoch: 2650 Train: 2.031154871 Test: 2.894510984
Epoch: 2700 Train: 2.029828548 Test: 2.896294117
Epoch: 2750 Train: 2.009383440 Test: 2.879627466
Epoch: 2800 Train: 2.024156570 Test: 2.902051687
Epoch: 2850 Train: 2.023469210 Test: 2.905352116
Epoch: 2900 Train: 2.018757582 Test: 2.893393517
Epoch: 2950 Train: 2.012804031 Test: 2.891656876
Epoch: 3000 Train: 2.003956079 Test: 2.879858017
Epoch: 3050 Train: 2.003596783 Test: 2.878203392
Epoch: 3100 Train: 1.999088049 Test: 2.880026102
Epoch: 3150 Train: 1.997910023 Test: 2.876286030
Epoch: 3200 Train: 1.997053385 Test: 2.877368927
Epoch: 3250 Train: 1.997308969 Test: 2.875946522
Epoch: 3300 Train: 1.997487307 Test: 2.879589558
Epoch: 3350 Train: 1.999036551 Test: 2.875058174
Epoch: 3400 Train: 2.001162529 Test: 2.875929356
Epoch: 3450 Train: 2.001438141 Test: 2.888130188
Epoch: 3500 Train: 2.004437208 Test: 2.889871597
Epoch: 3550 Train: 2.007844210 Test: 2.892637253
Epoch: 3600 Train: 2.010964870 Test: 2.895058155
Epoch: 3650 Train: 2.015860081 Test: 2.899136066
Epoch: 3700 Train: 2.021598816 Test: 2.908004761
Epoch: 3750 Train: 2.023530483 Test: 2.907869816
Epoch: 3800 Train: 2.021454096 Test: 2.901410103
Epoch: 3850 Train: 2.026591301 Test: 2.904807806
Epoch: 3900 Train: 2.028037548 Test: 2.902156830
Epoch: 3950 Train: 2.029408216 Test: 2.902326345
Epoch: 4000 Train: 2.030556202 Test: 2.911830902
Epoch: 4050 Train: 2.034775496 Test: 2.914320707
Epoch: 4100 Train: 2.041510105 Test: 2.906263828
Epoch: 4150 Train: 2.042784691 Test: 2.906117201
Epoch: 4200 Train: 2.041642189 Test: 2.909921169
Epoch: 4250 Train: 2.039890289 Test: 2.907850742
Epoch: 4300 Train: 2.040969133 Test: 2.907235384
Epoch: 4350 Train: 2.040964842 Test: 2.905961275
Epoch: 4400 Train: 2.041653395 Test: 2.903122663
Epoch: 4450 Train: 2.040862799 Test: 2.907987118
Epoch: 4500 Train: 2.041432381 Test: 2.926554918
Epoch: 4550 Train: 2.037522554 Test: 2.924786329
Epoch: 4600 Train: 2.033569574 Test: 2.924381256
Epoch: 4650 Train: 2.027239084 Test: 2.912217379
Epoch: 4700 Train: 2.020954370 Test: 2.914851904
Epoch: 4750 Train: 2.016936064 Test: 2.903716087
Epoch: 4800 Train: 2.010257244 Test: 2.892352104
Epoch: 4850 Train: 2.003701687 Test: 2.872371197
Epoch: 4900 Train: 1.987875819 Test: 2.867426395
Epoch: 4950 Train: 1.988687515 Test: 2.841178894
Epoch: 4999 Train: 1.985810637 Test: 2.838501692
Training Loss: tensor(1.9858)
Test Loss: tensor(2.8385)
True Mean x: tensor(3.0323, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.3541, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.0377, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0572, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0018)
Jacobian term Test Loss: tensor(0.0039)
Learned LE: [1.3488793 0.6085714]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
