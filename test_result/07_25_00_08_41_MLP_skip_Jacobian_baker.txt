time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 3000
num_train: 2000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 6
reg_param: 40.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 52.664268494 Test: 10.446305275
Epoch 0: New minimal relative error: 10.45%, model saved.
Epoch: 30 Train: 8.742444992 Test: 3.331490993
Epoch 30: New minimal relative error: 3.33%, model saved.
Epoch: 60 Train: 6.979125500 Test: 3.931073427
Epoch: 90 Train: 6.984149933 Test: 3.974507809
Epoch: 120 Train: 6.625594139 Test: 3.981577635
Epoch: 150 Train: 6.895179749 Test: 3.671696186
Epoch: 180 Train: 7.209873199 Test: 3.504596949
Epoch: 210 Train: 6.491670609 Test: 3.763230085
Epoch: 240 Train: 6.554870605 Test: 3.471338034
Epoch: 270 Train: 6.216648579 Test: 3.940044641
Epoch: 300 Train: 6.051936150 Test: 3.901299477
Epoch: 330 Train: 5.876793385 Test: 3.878406525
Epoch: 360 Train: 5.844835281 Test: 3.913727283
Epoch: 390 Train: 5.912092686 Test: 3.845892668
Epoch: 420 Train: 5.935988903 Test: 3.898378134
Epoch: 450 Train: 5.720304489 Test: 3.977837086
Epoch: 480 Train: 5.628499985 Test: 4.166762352
Epoch: 510 Train: 5.605358124 Test: 4.164278030
Epoch: 540 Train: 5.551640034 Test: 4.106913090
Epoch: 570 Train: 5.498490334 Test: 4.224150181
Epoch: 600 Train: 5.566806793 Test: 4.224848747
Epoch: 630 Train: 5.678026676 Test: 4.022994995
Epoch: 660 Train: 5.562066078 Test: 4.024158478
Epoch: 690 Train: 5.530066013 Test: 4.125330925
Epoch: 720 Train: 5.548351288 Test: 4.282626152
Epoch: 750 Train: 5.493446350 Test: 4.252815723
Epoch: 780 Train: 5.568715096 Test: 4.219739914
Epoch: 810 Train: 5.489762306 Test: 4.322519302
Epoch: 840 Train: 5.725294113 Test: 3.996052742
Epoch: 870 Train: 5.653484344 Test: 4.086920738
Epoch: 900 Train: 5.562143803 Test: 4.236949921
Epoch: 930 Train: 5.535825253 Test: 4.241707802
Epoch: 960 Train: 5.470151424 Test: 4.324419022
Epoch: 990 Train: 5.698006153 Test: 4.012351990
Epoch: 1020 Train: 5.578710556 Test: 4.035044193
Epoch: 1050 Train: 5.552738667 Test: 4.140663624
Epoch: 1080 Train: 5.595896244 Test: 4.093267918
Epoch: 1110 Train: 5.578419209 Test: 4.233213425
Epoch: 1140 Train: 5.550184250 Test: 4.155477524
Epoch: 1170 Train: 5.482373238 Test: 4.308229923
Epoch: 1200 Train: 5.446371078 Test: 4.314048767
Epoch: 1230 Train: 5.580831528 Test: 4.368850231
Epoch: 1260 Train: 5.525065422 Test: 4.221894264
Epoch: 1290 Train: 5.511773109 Test: 4.173612118
Epoch: 1320 Train: 5.496716499 Test: 4.184382439
Epoch: 1350 Train: 5.458121300 Test: 4.222253799
Epoch: 1380 Train: 5.598421574 Test: 3.985513926
Epoch: 1410 Train: 5.537554741 Test: 4.069936275
Epoch: 1440 Train: 5.497086525 Test: 4.148499489
Epoch: 1470 Train: 5.469884872 Test: 4.186968803
Epoch: 1500 Train: 5.447980881 Test: 4.226232052
Epoch: 1530 Train: 5.434794426 Test: 4.251637936
Epoch: 1560 Train: 5.441793442 Test: 4.246085167
Epoch: 1590 Train: 5.442003250 Test: 4.275925636
Epoch: 1620 Train: 5.444990158 Test: 4.297142982
Epoch: 1650 Train: 5.446118832 Test: 4.314865589
Epoch: 1680 Train: 5.457432747 Test: 4.325067997
Epoch: 1710 Train: 5.470114231 Test: 4.305751324
Epoch: 1740 Train: 5.472802639 Test: 4.323694706
Epoch: 1770 Train: 5.472062111 Test: 4.337034225
Epoch: 1800 Train: 5.465701103 Test: 4.341763496
Epoch: 1830 Train: 5.453306198 Test: 4.342830658
Epoch: 1860 Train: 5.433693409 Test: 4.339461803
Epoch: 1890 Train: 5.410815716 Test: 4.339317322
Epoch: 1920 Train: 5.411201477 Test: 4.335895538
Epoch: 1950 Train: 5.426802635 Test: 4.330013752
Epoch: 1980 Train: 5.411562443 Test: 4.321814537
Epoch: 2010 Train: 5.417350292 Test: 4.327855587
Epoch: 2040 Train: 5.413049698 Test: 4.342998028
Epoch: 2070 Train: 5.406937599 Test: 4.340680122
Epoch: 2100 Train: 5.410737514 Test: 4.332221985
Epoch: 2130 Train: 5.407604218 Test: 4.327270031
Epoch: 2160 Train: 5.398709297 Test: 4.325164318
Epoch: 2190 Train: 5.395499229 Test: 4.330460548
Epoch: 2220 Train: 5.407102585 Test: 4.351229191
Epoch: 2250 Train: 5.406100750 Test: 4.361336231
Epoch: 2280 Train: 5.417349339 Test: 4.366534710
Epoch: 2310 Train: 5.434792519 Test: 4.380645275
Epoch: 2340 Train: 5.435394287 Test: 4.384035587
Epoch: 2370 Train: 5.432235241 Test: 4.378168583
Epoch: 2400 Train: 5.427847385 Test: 4.375062466
Epoch: 2430 Train: 5.425917625 Test: 4.373865128
Epoch: 2460 Train: 5.420309067 Test: 4.381062031
Epoch: 2490 Train: 5.427656174 Test: 4.385307789
Epoch: 2520 Train: 5.433896065 Test: 4.401127338
Epoch: 2550 Train: 5.465242386 Test: 4.310244083
Epoch: 2580 Train: 5.452188492 Test: 4.353510857
Epoch: 2610 Train: 5.439543247 Test: 4.369695187
Epoch: 2640 Train: 5.437933922 Test: 4.380968094
Epoch: 2670 Train: 5.433444023 Test: 4.379631519
Epoch: 2700 Train: 5.432552338 Test: 4.378324986
Epoch: 2730 Train: 5.427604675 Test: 4.382820129
Epoch: 2760 Train: 5.424471855 Test: 4.387039185
Epoch: 2790 Train: 5.426161289 Test: 4.391730309
Epoch: 2820 Train: 5.424355984 Test: 4.394173622
Epoch: 2850 Train: 5.426026344 Test: 4.395829678
Epoch: 2880 Train: 5.429105759 Test: 4.400720596
Epoch: 2910 Train: 5.433886051 Test: 4.403134346
Epoch: 2940 Train: 5.434527874 Test: 4.403771877
Epoch: 2970 Train: 5.434824944 Test: 4.401477337
Epoch: 2999 Train: 5.427487850 Test: 4.404349327
Training Loss: tensor(5.4275)
Test Loss: tensor(4.4043)
True Mean x: tensor(2.9991, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(9.0270e+08, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3920, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(8.5628e+18, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0258)
Jacobian term Test Loss: tensor(0.0004)
Learned LE: [1.5426037 0.4587168]
True LE: tensor([ 0.6932, -0.7437], dtype=torch.float64)
