time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.99%, model saved.
Epoch: 0 Train: 3513.70972 Test: 3633.24878
Epoch 80: New minimal relative error: 68.92%, model saved.
Epoch: 80 Train: 62.93700 Test: 88.20094
Epoch 160: New minimal relative error: 32.73%, model saved.
Epoch: 160 Train: 6.23327 Test: 12.95763
Epoch: 240 Train: 4.29016 Test: 6.91597
Epoch 320: New minimal relative error: 26.17%, model saved.
Epoch: 320 Train: 6.31048 Test: 9.53356
Epoch: 400 Train: 13.69827 Test: 9.47026
Epoch: 480 Train: 11.52090 Test: 9.22323
Epoch: 560 Train: 4.79587 Test: 4.47612
Epoch: 640 Train: 6.01499 Test: 5.26787
Epoch: 720 Train: 2.01602 Test: 1.97650
Epoch: 800 Train: 1.19543 Test: 1.28881
Epoch 880: New minimal relative error: 18.56%, model saved.
Epoch: 880 Train: 0.31590 Test: 0.98014
Epoch: 960 Train: 2.34210 Test: 1.93686
Epoch: 1040 Train: 2.50460 Test: 3.06028
Epoch: 1120 Train: 1.62952 Test: 1.22436
Epoch 1200: New minimal relative error: 15.91%, model saved.
Epoch: 1200 Train: 0.46469 Test: 1.25450
Epoch: 1280 Train: 1.44314 Test: 0.95977
Epoch: 1360 Train: 0.21338 Test: 0.37812
Epoch: 1440 Train: 1.51064 Test: 1.79998
Epoch: 1520 Train: 0.44108 Test: 0.58311
Epoch: 1600 Train: 1.14448 Test: 1.27261
Epoch: 1680 Train: 0.46773 Test: 0.94321
Epoch: 1760 Train: 0.20399 Test: 0.40264
Epoch: 1840 Train: 0.27408 Test: 0.47400
Epoch: 1920 Train: 0.44550 Test: 0.54752
Epoch: 2000 Train: 1.34888 Test: 1.68143
Epoch: 2080 Train: 0.38884 Test: 0.43341
Epoch 2160: New minimal relative error: 15.75%, model saved.
Epoch: 2160 Train: 0.99720 Test: 1.22893
Epoch: 2240 Train: 0.42241 Test: 0.46452
Epoch: 2320 Train: 0.57597 Test: 0.61896
Epoch: 2400 Train: 2.75149 Test: 3.43056
Epoch: 2480 Train: 2.00067 Test: 1.80477
Epoch: 2560 Train: 2.75380 Test: 4.06437
Epoch: 2640 Train: 0.42963 Test: 0.67735
Epoch: 2720 Train: 1.06185 Test: 1.33983
Epoch 2800: New minimal relative error: 10.23%, model saved.
Epoch: 2800 Train: 0.79390 Test: 0.98780
Epoch 2880: New minimal relative error: 7.78%, model saved.
Epoch: 2880 Train: 0.72132 Test: 0.93979
Epoch: 2960 Train: 0.23002 Test: 0.35534
Epoch: 3040 Train: 0.60507 Test: 0.85542
Epoch: 3120 Train: 0.36780 Test: 0.60655
Epoch: 3200 Train: 1.41706 Test: 1.32103
Epoch: 3280 Train: 0.89226 Test: 1.12301
Epoch: 3360 Train: 0.98360 Test: 1.08187
Epoch: 3440 Train: 0.15183 Test: 0.33227
Epoch: 3520 Train: 0.15367 Test: 0.41077
Epoch: 3600 Train: 0.05383 Test: 0.21964
Epoch 3680: New minimal relative error: 4.18%, model saved.
Epoch: 3680 Train: 0.11915 Test: 0.26432
Epoch: 3760 Train: 0.03833 Test: 0.19423
Epoch: 3840 Train: 0.94996 Test: 1.05238
Epoch: 3920 Train: 0.09274 Test: 0.25191
Epoch: 4000 Train: 1.69158 Test: 1.73082
Epoch: 4080 Train: 0.42411 Test: 0.48408
Epoch: 4160 Train: 0.38994 Test: 0.60048
Epoch: 4240 Train: 0.11226 Test: 0.27697
Epoch: 4320 Train: 0.55223 Test: 0.66661
Epoch: 4400 Train: 0.48526 Test: 0.70028
Epoch: 4480 Train: 0.05878 Test: 0.21687
Epoch: 4560 Train: 0.32788 Test: 0.57427
Epoch: 4640 Train: 0.21518 Test: 0.40947
Epoch: 4720 Train: 0.24926 Test: 0.46070
Epoch: 4800 Train: 0.13332 Test: 0.31076
Epoch: 4880 Train: 1.27817 Test: 1.31689
Epoch: 4960 Train: 0.35495 Test: 0.51942
Epoch: 5040 Train: 0.04920 Test: 0.19719
Epoch: 5120 Train: 0.62936 Test: 0.80851
Epoch: 5200 Train: 0.72392 Test: 0.74888
Epoch: 5280 Train: 0.07909 Test: 0.23800
Epoch: 5360 Train: 0.17676 Test: 0.34271
Epoch: 5440 Train: 0.03484 Test: 0.16232
Epoch: 5520 Train: 0.10811 Test: 0.21087
Epoch: 5600 Train: 0.06401 Test: 0.20157
Epoch: 5680 Train: 0.08536 Test: 0.22705
Epoch: 5760 Train: 0.44110 Test: 0.62786
Epoch: 5840 Train: 0.19188 Test: 0.35753
Epoch: 5920 Train: 0.09617 Test: 0.25342
Epoch: 6000 Train: 0.05488 Test: 0.18876
Epoch: 6080 Train: 0.19060 Test: 0.33989
Epoch: 6160 Train: 0.01683 Test: 0.15939
Epoch: 6240 Train: 0.04141 Test: 0.17893
Epoch: 6320 Train: 0.11957 Test: 0.28064
Epoch: 6400 Train: 0.01369 Test: 0.15103
Epoch: 6480 Train: 0.01484 Test: 0.15330
Epoch: 6560 Train: 0.01923 Test: 0.15044
Epoch: 6640 Train: 0.07152 Test: 0.21143
Epoch: 6720 Train: 0.03069 Test: 0.16847
Epoch: 6800 Train: 0.01175 Test: 0.14552
Epoch: 6880 Train: 0.00819 Test: 0.14239
Epoch: 6960 Train: 0.03102 Test: 0.15951
Epoch: 7040 Train: 0.01308 Test: 0.14830
Epoch: 7120 Train: 0.33953 Test: 0.51791
Epoch: 7200 Train: 0.00737 Test: 0.13765
Epoch: 7280 Train: 0.00968 Test: 0.14010
Epoch: 7360 Train: 0.50085 Test: 0.69514
Epoch: 7440 Train: 0.00901 Test: 0.13823
Epoch: 7520 Train: 0.06980 Test: 0.21990
Epoch: 7600 Train: 0.00604 Test: 0.13458
Epoch: 7680 Train: 0.01576 Test: 0.16184
Epoch: 7760 Train: 0.01089 Test: 0.13751
Epoch: 7840 Train: 0.01554 Test: 0.14367
Epoch: 7920 Train: 0.16374 Test: 0.23261
Epoch: 7999 Train: 0.02120 Test: 0.14973
Training Loss: tensor(0.0212)
Test Loss: tensor(0.1497)
Learned LE: [ 0.9102528   0.01879423 -5.1733155 ]
True LE: [ 8.3755505e-01 -1.9926773e-03 -1.4506880e+01]
Relative Error: [0.713148   0.50356746 0.2855798  0.1897942  0.37163875 0.6467969
 0.9323407  1.1718003  1.3365457  1.4422406  1.517459   1.5712857
 1.620038   1.699449   1.821868   1.9624614  2.0883555  2.183364
 2.2535288  2.3191044  2.3997533  2.5018616  2.6222491  2.766164
 2.945189   3.150247   3.3500009  3.518512   3.6517818  3.7605
 3.8497593  3.9014227  3.8884373  3.7994583  3.634445   3.4062438
 3.1464272  2.8861735  2.6450596  2.451743   2.3155622  2.198138
 2.0669954  1.9263436  1.8032064  1.7188443  1.6694454  1.630339
 1.576772   1.4990132  1.4026589  1.3099711  1.2482791  1.2246096
 1.2236556  1.2227604  1.2040462  1.1587455  1.0885328  1.0037807
 0.912309   0.8042746  0.66128373 0.48133796 0.27856123 0.12567806
 0.27226827 0.520697   0.7789504  0.9959721  1.1323805  1.1977457
 1.2325058  1.2550278  1.2762008  1.3323162  1.4403528  1.571979
 1.6873201  1.7642494  1.8066996  1.8387797  1.8893614  1.9742086
 2.0897353  2.2306702  2.4048069  2.6067197  2.8014092  2.955259
 3.0635788  3.146667   3.2269735  3.2996564  3.3309448  3.2935598
 3.1767983  2.980804   2.7323985  2.4750235  2.2378297  2.0456357
 1.9197252  1.8263601  1.7219882  1.603751   1.503037   1.4523859
 1.4516573  1.4678538  1.4631948  1.4159615  1.3229485  1.2141509
 1.1325102  1.0942775  1.0877734  1.0902478  1.0812577  1.0484152
 0.9896045  0.91382104 0.83204955 0.73966587 0.6203992  0.47592917
 0.3170725  0.14472915 0.13920216 0.3572921  0.59227103 0.79717225
 0.9233516  0.9634382  0.96361625 0.962747   0.9666305  1.0048175
 1.100555   1.2258077  1.3352194  1.4004108  1.4199169  1.4170295
 1.4272786  1.4796603  1.5801122  1.7161403  1.8850532  2.085932
 2.2830706  2.429906   2.513938   2.5621023  2.6153705  2.6924703
 2.7678256  2.796418   2.751972   2.6181328  2.4045007  2.1565018
 1.9224349  1.7261721  1.5974339  1.521859   1.4440918  1.3437743
 1.2496125  1.2111709  1.2425237  1.308638   1.3595339  1.3568236
 1.278185   1.1563894  1.0491216  0.9852105  0.96336275 0.96405464
 0.96424454 0.94641113 0.9027102  0.8380072  0.7655197  0.6877773
 0.5890642  0.46978077 0.35299516 0.2346327  0.09199114 0.1611102
 0.3743722  0.57339704 0.707047   0.7433347  0.71403223 0.68815655
 0.6849933  0.71516174 0.8036011  0.9255249  1.0337325  1.0959533
 1.103572   1.0735224  1.0415295  1.046424   1.1117799  1.2326541
 1.393353   1.5916871  1.8003312  1.9573771  2.0295835  2.0411506
 2.0497863  2.1020095  2.197723   2.2905197  2.3323336  2.2919803
 2.1505952  1.9358337  1.706896   1.5077287  1.3611764  1.2888033
 1.2377652  1.160512   1.063811   1.0103955  1.0440269  1.139868
 1.241787   1.2982836  1.2577397  1.1389956  1.0087256  0.9093771
 0.8581483  0.84681994 0.85198116 0.85023046 0.8257132  0.77558595
 0.7116457  0.6460276  0.5689421  0.4695697  0.37134382 0.29388985
 0.21231481 0.07715331 0.13099088 0.32939416 0.48367447 0.54579604
 0.50639826 0.43682653 0.42085367 0.45759246 0.54944617 0.6727931
 0.7820734  0.84806246 0.8581092  0.8185055  0.7570894  0.713559
 0.7235614  0.804024   0.9442581  1.130643   1.3499582  1.5395099
 1.6301383  1.6222742  1.5793068  1.5768992  1.6481053  1.7702913
 1.8875222  1.9520924  1.9243035  1.7893798  1.590357   1.3919636
 1.2281581  1.1299978  1.0956062  1.0536989  0.967723   0.8810914
 0.879478   0.9685401  1.0956153  1.2084645  1.2350892  1.1515881
 1.0193819  0.8868638  0.7913984  0.7495917  0.747461   0.7578023
 0.7553823  0.72576594 0.6723583  0.6128491  0.5538788  0.4783616
 0.39253807 0.32307068 0.27598044 0.22630058 0.11444609 0.08919533
 0.26348764 0.3773812  0.38391018 0.2845752  0.1906418  0.23413809
 0.3454868  0.47384053 0.5826675  0.6506571  0.67073137 0.64216757
 0.57729685 0.507216   0.46839476 0.48533836 0.57184327 0.72430605
 0.9317412  1.1573837  1.308255   1.3266761  1.2550614  1.1841403
 1.1891892  1.2793591  1.4179882  1.556068   1.6479796  1.6455961
 1.5358437  1.3656142  1.2000155  1.06536    1.0059142  0.99599415
 0.9562079  0.85721546 0.78980625 0.82887524 0.93929005 1.0693028
 1.166965   1.1556889  1.0640185  0.93026936 0.79288256 0.69791853
 0.66289467 0.6690751  0.6850042  0.68250227 0.648673   0.5935615
 0.53967935 0.4875919  0.41892114 0.35298938 0.30583936 0.27829367
 0.2614283  0.20146796 0.15133691 0.2540979  0.34191772 0.33590984
 0.24021438 0.18440916 0.2574101  0.36049682 0.45097777 0.5085406
 0.52737015 0.51317245 0.47037625 0.40832296 0.3546421  0.33459485
 0.3511787  0.4238873  0.5759811  0.7946097 ]
