time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.73%, model saved.
Epoch: 0 Train: 3938.57324 Test: 3975.93115
Epoch 100: New minimal relative error: 22.78%, model saved.
Epoch: 100 Train: 39.13476 Test: 36.03832
Epoch: 200 Train: 8.68447 Test: 8.22380
Epoch 300: New minimal relative error: 20.34%, model saved.
Epoch: 300 Train: 5.56524 Test: 5.37313
Epoch 400: New minimal relative error: 15.22%, model saved.
Epoch: 400 Train: 4.24567 Test: 4.20968
Epoch: 500 Train: 3.54267 Test: 3.43945
Epoch: 600 Train: 3.90898 Test: 4.06271
Epoch 700: New minimal relative error: 10.28%, model saved.
Epoch: 700 Train: 2.83447 Test: 2.48319
Epoch: 800 Train: 2.07104 Test: 2.12153
Epoch: 900 Train: 2.02286 Test: 1.92260
Epoch: 1000 Train: 1.57307 Test: 1.47167
Epoch: 1100 Train: 6.85356 Test: 8.20616
Epoch: 1200 Train: 1.95307 Test: 2.30277
Epoch: 1300 Train: 1.22252 Test: 1.15147
Epoch: 1400 Train: 10.42763 Test: 11.15678
Epoch: 1500 Train: 4.19310 Test: 4.17331
Epoch: 1600 Train: 1.66161 Test: 1.34557
Epoch: 1700 Train: 1.03615 Test: 0.80489
Epoch: 1800 Train: 1.02837 Test: 1.08104
Epoch: 1900 Train: 1.09442 Test: 0.86473
Epoch: 2000 Train: 2.53656 Test: 1.68259
Epoch: 2100 Train: 5.27035 Test: 5.89024
Epoch: 2200 Train: 2.78918 Test: 2.93245
Epoch: 2300 Train: 0.79802 Test: 0.77685
Epoch 2400: New minimal relative error: 9.18%, model saved.
Epoch: 2400 Train: 0.45080 Test: 0.42410
Epoch: 2500 Train: 2.51213 Test: 2.99592
Epoch: 2600 Train: 0.75559 Test: 0.71988
Epoch: 2700 Train: 0.79382 Test: 0.48469
Epoch: 2800 Train: 0.73199 Test: 0.79158
Epoch 2900: New minimal relative error: 6.88%, model saved.
Epoch: 2900 Train: 0.58113 Test: 0.57368
Epoch: 3000 Train: 3.56838 Test: 2.71825
Epoch 3100: New minimal relative error: 5.16%, model saved.
Epoch: 3100 Train: 0.43760 Test: 0.26240
Epoch: 3200 Train: 2.27203 Test: 2.34796
Epoch: 3300 Train: 0.27386 Test: 0.28913
Epoch: 3400 Train: 1.63037 Test: 1.97749
Epoch: 3500 Train: 0.73679 Test: 0.76226
Epoch: 3600 Train: 0.25116 Test: 0.23920
Epoch: 3700 Train: 1.50122 Test: 1.70750
Epoch: 3800 Train: 0.59963 Test: 0.63141
Epoch: 3900 Train: 0.47705 Test: 0.44651
Epoch: 4000 Train: 0.30621 Test: 0.31355
Epoch: 4100 Train: 3.43612 Test: 2.96038
Epoch: 4200 Train: 1.18472 Test: 0.89335
Epoch: 4300 Train: 0.18751 Test: 0.19255
Epoch: 4400 Train: 0.17038 Test: 0.17712
Epoch: 4500 Train: 0.24684 Test: 0.26973
Epoch: 4600 Train: 0.29782 Test: 0.38610
Epoch: 4700 Train: 2.30294 Test: 2.58385
Epoch: 4800 Train: 0.15555 Test: 0.16848
Epoch: 4900 Train: 0.15760 Test: 0.16623
Epoch: 5000 Train: 0.84597 Test: 0.61922
Epoch: 5100 Train: 0.29381 Test: 0.33229
Epoch: 5200 Train: 0.13266 Test: 0.13436
Epoch: 5300 Train: 0.12508 Test: 0.12957
Epoch: 5400 Train: 0.13456 Test: 0.14287
Epoch: 5500 Train: 0.74454 Test: 0.96375
Epoch: 5600 Train: 0.38160 Test: 0.37151
Epoch: 5700 Train: 0.11257 Test: 0.11675
Epoch: 5800 Train: 0.11233 Test: 0.12028
Epoch: 5900 Train: 0.26142 Test: 0.41015
Epoch: 6000 Train: 0.41846 Test: 0.49016
Epoch: 6100 Train: 0.23757 Test: 0.23378
Epoch: 6200 Train: 0.13801 Test: 0.15021
Epoch 6300: New minimal relative error: 5.11%, model saved.
Epoch: 6300 Train: 0.32648 Test: 0.27951
Epoch: 6400 Train: 0.09848 Test: 0.10360
Epoch: 6500 Train: 0.10500 Test: 0.11333
Epoch: 6600 Train: 0.09216 Test: 0.10063
Epoch: 6700 Train: 0.17865 Test: 0.20682
Epoch: 6800 Train: 0.09845 Test: 0.11138
Epoch: 6900 Train: 0.13780 Test: 0.16497
Epoch: 7000 Train: 0.08758 Test: 0.09885
Epoch: 7100 Train: 0.09057 Test: 0.10084
Epoch: 7200 Train: 0.09855 Test: 0.11676
Epoch: 7300 Train: 0.76218 Test: 0.62312
Epoch: 7400 Train: 0.86749 Test: 0.93174
Epoch: 7500 Train: 0.28585 Test: 0.25095
Epoch: 7600 Train: 0.10367 Test: 0.11346
Epoch: 7700 Train: 0.07564 Test: 0.08777
Epoch: 7800 Train: 0.07590 Test: 0.08627
Epoch: 7900 Train: 0.07205 Test: 0.08268
Epoch: 8000 Train: 0.07469 Test: 0.08566
Epoch: 8100 Train: 0.06965 Test: 0.08073
Epoch: 8200 Train: 0.06915 Test: 0.07994
Epoch: 8300 Train: 0.06758 Test: 0.08053
Epoch: 8400 Train: 0.06880 Test: 0.07897
Epoch: 8500 Train: 0.30631 Test: 0.25762
Epoch: 8600 Train: 0.19917 Test: 0.17068
Epoch: 8700 Train: 0.12131 Test: 0.15966
Epoch: 8800 Train: 0.06365 Test: 0.07571
Epoch: 8900 Train: 0.06154 Test: 0.07354
Epoch 9000: New minimal relative error: 4.16%, model saved.
Epoch: 9000 Train: 0.06110 Test: 0.07287
Epoch: 9100 Train: 0.06370 Test: 0.07856
Epoch: 9200 Train: 0.06964 Test: 0.07936
Epoch: 9300 Train: 0.06151 Test: 0.07582
Epoch: 9400 Train: 0.05861 Test: 0.07190
Epoch: 9500 Train: 0.05757 Test: 0.07004
Epoch: 9600 Train: 0.06057 Test: 0.07217
Epoch: 9700 Train: 0.05739 Test: 0.07050
Epoch: 9800 Train: 0.16838 Test: 0.14864
Epoch: 9900 Train: 0.34061 Test: 0.34788
Epoch: 9999 Train: 0.05883 Test: 0.07457
Training Loss: tensor(0.0588)
Test Loss: tensor(0.0746)
Learned LE: [ 8.8022661e-01  1.7900427e-03 -4.6782074e+00]
True LE: [ 8.8446367e-01  1.8212291e-04 -1.4563602e+01]
Relative Error: [6.538526   7.2657704  7.6572614  7.632791   7.014677   6.30541
 5.678495   5.04174    4.5172243  4.01909    3.6331291  3.2462552
 2.9080534  2.764069   2.603447   2.4896064  2.4243853  2.4007013
 2.3997881  2.4240463  2.2849696  2.1751947  2.1876707  1.9399872
 1.6279234  1.4210008  1.3623914  1.670219   2.0463846  2.2749858
 2.9466455  2.9327884  3.0422626  3.1489997  3.029977   3.0580475
 3.0787284  3.1182773  3.1593404  3.2165105  3.2765117  3.32583
 3.3491101  3.4145718  3.746781   3.70638    3.457141   2.6047256
 1.6888105  1.0255035  0.8496656  1.2304833  1.7797472  2.171331
 2.329524   2.2972527  2.0563436  1.8761556  2.247225   3.0295858
 4.152571   5.309102   6.2829914  7.0766606  7.4881873  7.434809
 6.605628   5.9337206  5.2197638  4.5429597  4.039435   3.624605
 3.301147   2.9086492  2.5614064  2.5621796  2.4657912  2.3634179
 2.281267   2.239353   2.2409441  2.231988   2.0386457  1.9446479
 1.9871175  1.6616553  1.3826334  1.215281   1.1564921  1.478462
 1.8923838  2.2658231  2.754285   2.561363   2.7456431  2.8027697
 2.7665124  2.8819132  2.9756281  2.9583082  2.9956567  3.0653071
 3.0042844  3.0078049  2.950264   3.142392   3.4643135  3.3434072
 3.2122107  2.4693253  1.7580609  1.040786   0.8260728  1.2299215
 1.6402321  2.043417   2.1955826  1.9922247  1.7263921  1.4211321
 1.7984837  2.4967453  3.715568   4.932217   6.012668   6.8525066
 7.2161345  6.7537346  6.14269    5.5838323  4.76092    4.096436
 3.6061268  3.282166   2.967439   2.6237297  2.2340198  2.326189
 2.3730254  2.2962797  2.120471   2.0235348  2.0710938  1.9656354
 1.8011229  1.7139703  1.7879477  1.409788   1.1746709  1.0261565
 0.96366525 1.2607833  1.7190764  2.2638054  2.3347895  2.2421212
 2.4330974  2.4730654  2.5589375  2.7636113  2.832346   2.8223655
 2.9589005  2.9408758  2.7908995  2.7407577  2.7103195  2.9147835
 3.1541831  2.9549565  2.7274969  2.5284054  1.7261376  1.0895727
 0.82116127 1.0966603  1.4997543  1.9119332  2.0102944  1.8027123
 1.514748   1.0107821  1.1375808  1.985601   3.1540167  4.3789635
 5.5889416  6.4975276  6.6922374  6.058443   5.5816555  5.111882
 4.3140388  3.6821604  3.2368972  3.0368552  2.7999501  2.4752166
 2.0965862  2.1532311  2.144479   2.1008828  1.9558845  1.8298529
 1.8385863  1.7292302  1.5935073  1.5191731  1.5768454  1.2607558
 1.0462677  0.9250873  0.80329144 0.97136    1.4775708  1.7706711
 1.9237466  1.9726338  2.1631722  2.2111268  2.3784359  2.5579047
 2.5488522  2.5433867  2.653755   2.6314204  2.5774474  2.6026645
 2.561618   2.6850169  2.834628   2.5651398  2.3826153  2.505203
 1.9032439  1.2547355  0.8704368  0.9813166  1.3260645  1.6876347
 1.7804655  1.6709759  1.2290823  0.58130294 0.73560405 1.5672157
 2.621898   3.858918   5.006079   5.906237   5.980186   5.414363
 4.9055157  4.505525   3.7984161  3.3664603  3.0725331  2.9704506
 2.826039   2.4583094  2.0655806  1.9947447  1.9844339  1.8798537
 1.8241843  1.737541   1.5806327  1.4975988  1.3361639  1.3410269
 1.5648468  1.1363783  0.93665606 0.89487994 0.7194499  0.67329943
 1.2042563  1.2648427  1.5290315  1.6676512  1.935992   2.0060487
 2.0043094  2.0937152  2.1973896  2.2950585  2.2903173  2.2479172
 2.22439    2.2155745  2.3868957  2.4768069  2.5258698  2.3279893
 2.2101305  2.2401583  2.0878804  1.4139807  1.0514935  1.0002075
 1.1660705  1.401918   1.4941502  1.4478309  1.0022384  0.4632895
 0.39677173 1.2588673  2.1363134  3.2969582  4.351639   5.106109
 5.201109   4.806239   4.251066   3.9263976  3.406367   3.1210248
 2.9033616  2.7639334  2.6504495  2.3573115  1.9308265  1.7411947
 1.7312921  1.7079681  1.7049575  1.5808198  1.3058399  1.2405102
 1.1063459  1.2023637  1.5290685  1.2206569  0.81388324 0.77956796
 0.6670875  0.49772435 0.7664147  0.7278123  1.1007452  1.3289608
 1.6966496  1.570182   1.5449735  1.7171803  1.9217422  2.00859
 1.9531032  1.9131157  1.8651032  1.8122047  1.9210277  2.1342623
 2.3545995  2.1787467  2.0414808  1.9440213  1.9656159  1.6195219
 1.2732766  1.0847976  1.0934042  1.232302   1.1683518  1.0322067
 0.7878365  0.54843026 0.2026858  1.066002   1.7321337  2.740727
 3.6795576  4.3364677  4.4443536  4.1239543  3.665585   3.389032
 3.109498   2.7938523  2.5624628  2.446103   2.2507074  1.923224
 1.4079837  1.2569528  1.3951445  1.5459886  1.5736561  1.4783175
 1.0860724  0.98692876 0.9173617  1.0137087  1.4497472  1.3999485
 0.7618939  0.69588053 0.5908965  0.4094779 ]
