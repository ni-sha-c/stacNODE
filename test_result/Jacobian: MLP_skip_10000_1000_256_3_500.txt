time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.10%, model saved.
Epoch: 0 Train: 33030.03516 Test: 4221.52686
Epoch 100: New minimal relative error: 78.09%, model saved.
Epoch: 100 Train: 9360.04688 Test: 1564.72107
Epoch: 200 Train: 8741.17773 Test: 1374.27527
Epoch: 300 Train: 8269.31250 Test: 1284.01221
Epoch: 400 Train: 7905.46680 Test: 1110.52026
Epoch: 500 Train: 8151.50342 Test: 1126.45447
Epoch: 600 Train: 5353.82275 Test: 565.30194
Epoch: 700 Train: 4210.63184 Test: 328.19058
Epoch 800: New minimal relative error: 55.45%, model saved.
Epoch: 800 Train: 1953.76587 Test: 115.22866
Epoch: 900 Train: 764.32245 Test: 27.49389
Epoch: 1000 Train: 473.89389 Test: 13.69509
Epoch 1100: New minimal relative error: 13.19%, model saved.
Epoch: 1100 Train: 301.63525 Test: 3.93895
Epoch: 1200 Train: 252.32579 Test: 3.09997
Epoch: 1300 Train: 234.01172 Test: 4.47152
Epoch: 1400 Train: 206.22623 Test: 3.15712
Epoch: 1500 Train: 188.50200 Test: 1.99102
Epoch: 1600 Train: 178.44800 Test: 1.79581
Epoch: 1700 Train: 190.04909 Test: 9.06183
Epoch 1800: New minimal relative error: 6.56%, model saved.
Epoch: 1800 Train: 161.30600 Test: 1.39956
Epoch: 1900 Train: 156.98116 Test: 1.51759
Epoch: 2000 Train: 146.77667 Test: 1.29747
Epoch: 2100 Train: 143.66270 Test: 1.30469
Epoch: 2200 Train: 137.39090 Test: 1.45081
Epoch: 2300 Train: 130.74374 Test: 1.08906
Epoch: 2400 Train: 129.68326 Test: 1.22466
Epoch: 2500 Train: 124.27441 Test: 1.08115
Epoch: 2600 Train: 113.62794 Test: 1.70536
Epoch: 2700 Train: 110.00826 Test: 0.94447
Epoch: 2800 Train: 104.46447 Test: 0.76281
Epoch: 2900 Train: 108.55737 Test: 4.81515
Epoch: 3000 Train: 97.55029 Test: 0.70263
Epoch: 3100 Train: 92.03597 Test: 2.24314
Epoch: 3200 Train: 86.06440 Test: 0.62895
Epoch: 3300 Train: 82.59025 Test: 0.69773
Epoch: 3400 Train: 83.90889 Test: 0.48619
Epoch: 3500 Train: 79.98121 Test: 0.83942
Epoch: 3600 Train: 78.70591 Test: 0.40198
Epoch: 3700 Train: 82.75824 Test: 0.44157
Epoch: 3800 Train: 83.75992 Test: 0.55672
Epoch: 3900 Train: 77.53694 Test: 0.48396
Epoch 4000: New minimal relative error: 5.98%, model saved.
Epoch: 4000 Train: 79.82206 Test: 0.49695
Epoch: 4100 Train: 74.00271 Test: 0.72443
Epoch: 4200 Train: 75.99950 Test: 0.58901
Epoch 4300: New minimal relative error: 5.44%, model saved.
Epoch: 4300 Train: 69.38478 Test: 0.39771
Epoch: 4400 Train: 71.45415 Test: 0.48101
Epoch: 4500 Train: 71.18478 Test: 0.69489
Epoch: 4600 Train: 63.46816 Test: 0.31707
Epoch: 4700 Train: 61.48114 Test: 0.31216
Epoch: 4800 Train: 59.49432 Test: 0.31122
Epoch: 4900 Train: 56.09023 Test: 0.30615
Epoch: 5000 Train: 56.20984 Test: 0.32124
Epoch: 5100 Train: 57.31257 Test: 0.29122
Epoch: 5200 Train: 61.86527 Test: 0.38673
Epoch: 5300 Train: 63.35466 Test: 0.35965
Epoch 5400: New minimal relative error: 4.02%, model saved.
Epoch: 5400 Train: 56.37725 Test: 0.27950
Epoch: 5500 Train: 55.95137 Test: 0.41240
Epoch: 5600 Train: 55.92609 Test: 0.28744
Epoch: 5700 Train: 52.08082 Test: 0.27662
Epoch: 5800 Train: 51.69254 Test: 0.25752
Epoch: 5900 Train: 54.71848 Test: 0.29959
Epoch: 6000 Train: 55.78355 Test: 0.31911
Epoch: 6100 Train: 60.85596 Test: 0.45253
Epoch: 6200 Train: 57.62466 Test: 0.35399
Epoch: 6300 Train: 59.32125 Test: 0.44450
Epoch: 6400 Train: 52.71638 Test: 0.29740
Epoch: 6500 Train: 49.88280 Test: 0.26782
Epoch: 6600 Train: 48.05331 Test: 0.22498
Epoch: 6700 Train: 47.04271 Test: 0.40309
Epoch 6800: New minimal relative error: 3.16%, model saved.
Epoch: 6800 Train: 47.52970 Test: 0.26092
Epoch: 6900 Train: 49.89922 Test: 0.26122
Epoch: 7000 Train: 47.50709 Test: 0.24474
Epoch: 7100 Train: 46.30527 Test: 0.28527
Epoch: 7200 Train: 48.46704 Test: 0.27120
Epoch: 7300 Train: 48.12094 Test: 0.25309
Epoch: 7400 Train: 58.57212 Test: 0.44905
Epoch: 7500 Train: 46.80482 Test: 0.25130
Epoch: 7600 Train: 45.41634 Test: 0.22825
Epoch: 7700 Train: 48.31153 Test: 0.32380
Epoch: 7800 Train: 43.82506 Test: 0.21062
Epoch: 7900 Train: 43.88632 Test: 0.27022
Epoch: 8000 Train: 43.80603 Test: 0.23841
Epoch: 8100 Train: 44.06752 Test: 0.20959
Epoch: 8200 Train: 45.97626 Test: 0.24699
Epoch: 8300 Train: 46.47036 Test: 0.24613
Epoch: 8400 Train: 44.38029 Test: 0.33657
Epoch: 8500 Train: 46.58616 Test: 0.24609
Epoch: 8600 Train: 44.97459 Test: 0.23804
Epoch: 8700 Train: 44.66235 Test: 0.22786
Epoch: 8800 Train: 43.25492 Test: 0.19371
Epoch: 8900 Train: 42.46957 Test: 0.21853
Epoch: 9000 Train: 40.81247 Test: 0.16603
Epoch: 9100 Train: 50.55584 Test: 0.28393
Epoch: 9200 Train: 40.98135 Test: 0.28031
Epoch: 9300 Train: 41.25222 Test: 0.16724
Epoch: 9400 Train: 39.25332 Test: 0.15179
Epoch: 9500 Train: 39.92429 Test: 0.15983
Epoch: 9600 Train: 42.69299 Test: 0.17278
Epoch: 9700 Train: 41.46126 Test: 0.21302
Epoch: 9800 Train: 44.09040 Test: 0.24328
Epoch: 9900 Train: 41.91565 Test: 0.18634
Epoch: 9999 Train: 43.59220 Test: 0.22022
Training Loss: tensor(43.5922)
Test Loss: tensor(0.2202)
Learned LE: [ 8.2797009e-01 -1.5320700e-04 -1.4499392e+01]
True LE: [ 8.4026521e-01  5.7190098e-03 -1.4515159e+01]
Relative Error: [2.7896833  2.893513   3.0481923  3.376931   3.8721318  4.6442285
 5.50142    6.277562   6.759433   6.530791   6.415779   6.2363734
 5.877545   5.715351   5.4832687  4.8408074  4.3475184  3.7192304
 2.9331412  2.5412748  1.7937396  1.5288728  1.6032119  1.732998
 1.763422   1.5134484  1.2063271  1.1583524  1.1689694  1.2105391
 1.4153546  1.5832281  1.559581   1.4894372  1.36592    1.3636653
 1.7247382  1.8172083  1.7837446  1.5441251  1.0078696  1.0429147
 1.2204587  1.442053   1.8166313  2.1251662  2.8068306  3.3175528
 2.742739   2.55538    2.6283906  2.2326899  2.1155875  2.1122706
 2.2280347  2.4901419  2.837798   2.7930086  2.8635957  2.7776322
 2.695945   2.6734667  2.6531854  2.671084   2.6942399  2.6215374
 2.8561888  3.4155688  4.21661    5.1549177  5.9335923  6.3408027
 5.968171   5.922681   5.6242757  5.276891   5.099652   4.8210883
 4.2098174  3.6551535  2.8054442  2.2394576  1.7179327  1.4363954
 1.5943298  1.8334875  2.011742   2.0002902  1.7265627  1.3357799
 1.1613481  1.0771295  1.1076691  1.2562319  1.1803944  1.1289501
 1.1070874  1.1612226  1.5673265  1.8190341  1.8526822  1.6595192
 1.1366893  0.9434618  1.2498939  1.5450189  1.8192225  1.9572847
 2.5391324  3.0354905  2.4670796  2.2343392  2.3190026  2.0385559
 1.8466088  1.7727643  1.8706031  2.105934   2.3781781  2.3984218
 2.5165417  2.5623822  2.5947704  2.6132317  2.635941   2.3304117
 2.2362423  2.1216655  2.001482   2.2570112  2.886316   3.662353
 4.601365   5.3253503  5.692174   5.4916224  5.2583876  4.973831
 4.7396674  4.570396   4.1905627  3.5103009  2.8283834  2.1555471
 1.8020885  1.3761417  1.5001587  1.8195156  2.133999   2.3719783
 2.3028471  1.8984193  1.5996745  1.3402752  1.1381801  1.0456437
 0.9822427  0.9073741  0.73553526 0.89649427 1.2627041  1.6944283
 1.9024842  1.8521593  1.2455956  0.8384726  1.1843325  1.5576248
 1.7930336  1.84412    2.2565823  2.7178898  2.4044394  2.0762565
 2.070792   1.8392063  1.4537681  1.3553349  1.4922905  1.7942369
 1.9853396  2.0464475  2.246536   2.52191    2.555439   2.59101
 2.3345013  2.0841706  1.91711    1.8487396  1.6295238  1.486991
 1.76734    2.3567357  3.0797358  4.007662   4.574167   5.033705
 4.7986283  4.5657053  4.4379697  4.2429485  4.1227765  3.6011412
 2.987478   2.236231   1.7147352  1.3172861  1.223801   1.59564
 1.9761147  2.3095238  2.5879698  2.6232297  2.2163417  2.0240831
 1.6983813  1.3659788  1.0349394  0.95878786 0.6676724  0.43958673
 0.86009365 1.3404238  1.7639651  1.9733356  1.5454477  0.95368326
 1.0442272  1.4852004  1.6979207  1.7832959  2.0657449  2.5469382
 2.5244853  2.154664   1.8902801  1.71678    1.2949312  1.1173197
 1.1466006  1.4583462  1.672828   1.7293204  1.9275825  2.2867923
 2.6508381  2.5898829  2.217524   1.9865963  1.7189986  1.6964581
 1.5803351  1.3132919  1.1182165  1.3542771  1.812118   2.4652185
 3.2482543  3.891164   4.4429584  3.907226   3.8897004  3.9885373
 3.7682998  3.6362693  3.0908267  2.5530677  1.7648853  1.3845621
 1.1418059  1.3169873  1.789946   2.1959558  2.5114188  2.7670524
 2.9138858  2.7079995  2.6108217  2.2152977  1.7604686  1.3450834
 1.2298974  0.73975307 0.2736262  0.8023514  1.3760864  1.8051801
 1.7476165  1.3425436  0.9345195  1.3475336  1.5303408  1.7284276
 1.9433429  2.3789701  2.3411403  2.197054   1.7414529  1.5042118
 1.3154761  1.0645449  1.0197427  1.2024738  1.466956   1.5255129
 1.6998297  1.97412    2.4383626  2.636617   2.4048696  2.090812
 1.8111011  1.6023692  1.488622   1.4124542  1.1823549  0.9080948
 1.0211843  1.2591845  1.7484531  2.4069457  3.1654065  3.6197002
 3.1536102  3.0688398  3.334536   3.3552666  3.1161387  2.7238011
 2.1080267  1.4170647  1.1710292  1.1256434  1.3993359  1.9494269
 2.4077682  2.730667   3.0398154  3.3326416  3.2591329  3.2993894
 2.9057493  2.4064887  1.9472672  1.7212001  1.1026666  0.39924923
 0.6758168  1.2412071  1.6107334  1.6037441  1.2915571  1.0683862
 1.3106327  1.5778174  1.6786966  1.8713521  2.0880542  2.1017215
 1.6662174  1.2759027  1.0841091  0.86854386 0.74785554 0.87849426
 1.2351292  1.3713026  1.5025511  1.8067671  2.2218041  2.4731445
 2.4028044  2.1523428  1.8878626  1.6001059  1.400607   1.2336308
 1.1943055  1.0087274  0.62279886 0.589542   0.75779486 1.0176194
 1.5462191  2.3982987  2.6233933  2.4343047  2.1688852  2.36458
 2.927418   2.7123387  2.4280977  1.8777665  1.1939608  1.0292192
 1.1616106  1.4239404  1.8741397  2.3207712 ]
