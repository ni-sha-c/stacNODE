time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.31%, model saved.
Epoch: 0 Train: 9334.37598 Test: 4299.63721
Epoch: 100 Train: 2668.90576 Test: 1145.20288
Epoch: 200 Train: 2354.04712 Test: 956.30945
Epoch: 300 Train: 1425.15991 Test: 481.83185
Epoch: 400 Train: 862.72668 Test: 201.31970
Epoch: 500 Train: 526.29608 Test: 99.70284
Epoch 600: New minimal relative error: 58.94%, model saved.
Epoch: 600 Train: 219.75243 Test: 41.41117
Epoch 700: New minimal relative error: 32.75%, model saved.
Epoch: 700 Train: 169.09383 Test: 47.08900
Epoch 800: New minimal relative error: 14.37%, model saved.
Epoch: 800 Train: 81.69398 Test: 5.98533
Epoch: 900 Train: 69.09976 Test: 13.19363
Epoch 1000: New minimal relative error: 12.07%, model saved.
Epoch: 1000 Train: 67.30700 Test: 16.55572
Epoch 1100: New minimal relative error: 9.36%, model saved.
Epoch: 1100 Train: 57.70533 Test: 13.37281
Epoch: 1200 Train: 64.66351 Test: 24.72475
Epoch 1300: New minimal relative error: 5.02%, model saved.
Epoch: 1300 Train: 38.55624 Test: 1.78418
Epoch: 1400 Train: 33.96754 Test: 5.61601
Epoch: 1500 Train: 30.11465 Test: 7.40818
Epoch: 1600 Train: 35.08477 Test: 14.11489
Epoch: 1700 Train: 35.67546 Test: 10.14013
Epoch: 1800 Train: 28.49504 Test: 1.43911
Epoch: 1900 Train: 35.82972 Test: 9.44458
Epoch 2000: New minimal relative error: 3.34%, model saved.
Epoch: 2000 Train: 22.99582 Test: 1.59201
Epoch: 2100 Train: 29.20834 Test: 6.55744
Epoch: 2200 Train: 21.20590 Test: 6.13912
Epoch: 2300 Train: 19.08139 Test: 2.43045
Epoch: 2400 Train: 19.69796 Test: 1.25950
Epoch: 2500 Train: 18.62362 Test: 2.66275
Epoch: 2600 Train: 17.20808 Test: 2.58465
Epoch: 2700 Train: 18.84765 Test: 4.34717
Epoch: 2800 Train: 22.27144 Test: 6.25425
Epoch: 2900 Train: 17.14945 Test: 1.87325
Epoch: 3000 Train: 14.01670 Test: 1.44451
Epoch: 3100 Train: 12.74180 Test: 0.92965
Epoch: 3200 Train: 19.84657 Test: 3.72071
Epoch: 3300 Train: 14.85083 Test: 2.37345
Epoch: 3400 Train: 12.79254 Test: 0.67477
Epoch: 3500 Train: 12.02242 Test: 1.02439
Epoch: 3600 Train: 11.77105 Test: 0.85732
Epoch: 3700 Train: 13.78293 Test: 1.97538
Epoch: 3800 Train: 11.73393 Test: 1.58463
Epoch: 3900 Train: 18.42184 Test: 4.33752
Epoch: 4000 Train: 11.72506 Test: 1.68216
Epoch: 4100 Train: 12.31823 Test: 2.19908
Epoch: 4200 Train: 9.91235 Test: 0.40222
Epoch: 4300 Train: 9.32985 Test: 0.52832
Epoch: 4400 Train: 16.81808 Test: 4.33146
Epoch: 4500 Train: 10.72519 Test: 2.83458
Epoch: 4600 Train: 10.85775 Test: 1.75962
Epoch: 4700 Train: 10.01220 Test: 1.51749
Epoch: 4800 Train: 11.31969 Test: 1.73115
Epoch: 4900 Train: 7.93320 Test: 1.03637
Epoch: 5000 Train: 7.73914 Test: 0.99349
Epoch: 5100 Train: 7.30378 Test: 0.52069
Epoch: 5200 Train: 11.08527 Test: 3.22744
Epoch: 5300 Train: 8.82838 Test: 0.48742
Epoch: 5400 Train: 8.19731 Test: 2.42696
Epoch: 5500 Train: 9.52316 Test: 1.04202
Epoch: 5600 Train: 6.89069 Test: 0.49122
Epoch 5700: New minimal relative error: 2.36%, model saved.
Epoch: 5700 Train: 6.38103 Test: 0.04591
Epoch: 5800 Train: 6.96646 Test: 1.34723
Epoch: 5900 Train: 7.23502 Test: 1.13187
Epoch: 6000 Train: 6.08961 Test: 0.28867
Epoch 6100: New minimal relative error: 1.98%, model saved.
Epoch: 6100 Train: 5.52485 Test: 0.03694
Epoch: 6200 Train: 6.21369 Test: 0.75449
Epoch: 6300 Train: 6.30987 Test: 0.69098
Epoch: 6400 Train: 13.43819 Test: 7.08654
Epoch: 6500 Train: 6.56772 Test: 0.60434
Epoch: 6600 Train: 6.13012 Test: 0.38533
Epoch: 6700 Train: 5.38748 Test: 0.12363
Epoch: 6800 Train: 5.29863 Test: 0.07005
Epoch: 6900 Train: 5.50239 Test: 0.53034
Epoch: 7000 Train: 5.64777 Test: 0.52472
Epoch: 7100 Train: 5.66852 Test: 0.49536
Epoch: 7200 Train: 5.19987 Test: 0.05562
Epoch: 7300 Train: 5.45139 Test: 0.04383
Epoch: 7400 Train: 7.17389 Test: 1.68544
Epoch: 7500 Train: 5.29658 Test: 0.43359
Epoch: 7600 Train: 5.27878 Test: 0.32043
Epoch: 7700 Train: 5.26757 Test: 0.11992
Epoch: 7800 Train: 5.76730 Test: 0.61518
Epoch: 7900 Train: 5.65469 Test: 0.38052
Epoch: 8000 Train: 4.71917 Test: 0.03764
Epoch: 8100 Train: 5.12246 Test: 0.31999
Epoch: 8200 Train: 4.49390 Test: 0.06182
Epoch: 8300 Train: 4.48988 Test: 0.19244
Epoch: 8400 Train: 4.90602 Test: 0.39137
Epoch: 8500 Train: 5.16741 Test: 0.36136
Epoch: 8600 Train: 5.95523 Test: 0.28315
Epoch 8700: New minimal relative error: 1.28%, model saved.
Epoch: 8700 Train: 4.35026 Test: 0.09624
Epoch: 8800 Train: 4.26389 Test: 0.03226
Epoch: 8900 Train: 6.87923 Test: 2.85391
Epoch: 9000 Train: 4.19384 Test: 0.02050
Epoch: 9100 Train: 4.43745 Test: 0.02620
Epoch: 9200 Train: 7.70683 Test: 2.38232
Epoch: 9300 Train: 4.09106 Test: 0.02043
Epoch: 9400 Train: 4.13568 Test: 0.02138
Epoch: 9500 Train: 4.19579 Test: 0.03482
Epoch: 9600 Train: 4.20383 Test: 0.02304
Epoch: 9700 Train: 4.16544 Test: 0.06114
Epoch: 9800 Train: 4.06807 Test: 0.07762
Epoch: 9900 Train: 4.01756 Test: 0.04072
Epoch: 9999 Train: 4.05632 Test: 0.08406
Training Loss: tensor(4.0563)
Test Loss: tensor(0.0841)
Learned LE: [  0.9298795   -0.06662425 -14.537864  ]
True LE: [ 8.6544806e-01  3.0280377e-03 -1.4538586e+01]
Relative Error: [2.0412765  2.1092482  2.0413122  1.8955473  1.6307544  1.3490963
 0.9755049  0.6972082  1.1001889  1.3095958  1.377089   1.4811697
 1.6577914  1.915324   2.014059   2.201491   2.782145   3.2862556
 3.2274358  3.0927277  3.1462     2.9322708  2.526049   2.1175952
 1.7862412  1.5483413  1.4795587  1.4374774  1.5115236  1.4807359
 1.3843848  1.3023443  1.1482157  1.2045646  1.4330684  1.9267037
 2.4765184  2.453402   2.3044376  2.5819345  2.7606964  2.6266894
 2.182409   1.9409336  1.5995982  1.4026319  1.448646   1.8791758
 2.4807885  3.2115319  3.724045   3.7053123  3.527062   3.3151095
 3.06744    2.792554   2.6249826  2.5515852  2.5175602  2.3776937
 2.2447097  1.9170073  1.9107548  1.9755119  1.8332789  1.6084648
 1.2891126  1.11364    0.81274825 0.51148015 0.7604549  1.0299792
 1.1693591  1.2052492  1.3552464  1.5711483  1.5978359  1.742202
 2.293611   2.6107419  2.5488162  2.4627826  2.496512   2.3210633
 2.1367517  1.7791914  1.344202   1.1874192  1.2146096  1.3127015
 1.3192532  1.2389649  1.0716078  1.0630776  0.8125323  0.86596787
 1.1337998  1.5688714  2.1255496  1.9656639  1.898047   2.104755
 2.3134098  2.245427   2.0709963  1.8348815  1.4320623  1.1266297
 1.2050487  1.6112007  2.1181052  2.664501   3.10228    3.0932643
 3.036822   2.8901424  2.6132963  2.3004446  2.0980701  2.0227818
 2.0722249  2.0312858  2.149957   1.9903861  1.8813757  1.8010646
 1.4987186  1.2645398  1.1047406  0.924893   0.70369446 0.36275414
 0.50784    0.7380098  0.85936004 0.94672936 0.8667015  1.0529569
 1.1317319  1.3723149  1.7865502  1.9522727  1.9570198  1.9472367
 1.9363083  1.8291318  1.8081466  1.5917867  1.1340547  0.9606484
 0.9304606  0.99786204 1.1610467  1.1025854  0.9377302  0.8173896
 0.5940725  0.58597267 0.8719065  1.1927426  1.5608373  1.5520185
 1.5891231  1.7692208  1.9972669  2.0024364  1.8447214  1.9051667
 1.4069694  0.9893408  0.9591338  1.3124404  1.7079443  2.0701506
 2.4273405  2.5164165  2.4793732  2.4310415  2.2142954  1.9307914
 1.6817513  1.6021272  1.6149083  1.6619589  1.8530706  2.1106977
 2.0142484  1.6195947  1.3368628  1.0825679  0.9160578  0.83418024
 0.63757825 0.31099945 0.26012942 0.38370058 0.6225613  0.567906
 0.52079654 0.539944   0.69072366 0.9472347  1.247628   1.3770702
 1.3722634  1.4449266  1.5663399  1.5434557  1.4773532  1.2997252
 0.9645629  0.8224643  0.8457615  0.8168641  0.8514869  0.95496804
 0.9649884  0.7247907  0.5306679  0.4502398  0.57740897 0.8666236
 1.1055951  1.1297159  1.1989179  1.3931559  1.6625526  1.6895173
 1.6239617  1.8637462  1.5147263  1.0173581  0.8138254  0.9725605
 1.2065238  1.5277846  1.8600526  2.112833   2.0932198  1.9967493
 1.9038726  1.6728789  1.5090365  1.345111   1.3496208  1.3297487
 1.5581551  1.9267275  2.1718495  1.5626425  1.2381295  0.93537664
 0.71737057 0.7485079  0.68957615 0.31385306 0.13767347 0.16793479
 0.473836   0.3653264  0.3582655  0.3296743  0.40027216 0.67170733
 0.79212517 0.83478755 0.8844571  1.0655358  1.3068811  1.3753077
 1.2402028  0.9866264  0.7687327  0.6380186  0.6657077  0.73634183
 0.74322104 0.73281026 0.81635815 0.85032916 0.63002014 0.4032043
 0.40038788 0.588373   0.72845954 0.78796613 0.9646134  1.2112972
 1.4487231  1.4425707  1.388118   1.4301779  1.4164207  1.087997
 0.81301695 0.68135726 0.7486551  1.041781   1.4751675  1.7742406
 1.7632488  1.6091924  1.5824448  1.4233152  1.3565482  1.3417821
 1.2753944  1.1983693  1.2132812  1.648074   1.8165771  1.5607423
 1.0977956  0.8408651  0.5983681  0.52973497 0.49379823 0.36924356
 0.119454   0.11963536 0.23220249 0.35050628 0.27317393 0.19071981
 0.19926585 0.3567948  0.5206318  0.5131318  0.510681   0.77694523
 1.0529293  1.3171781  1.2185515  0.8301881  0.59341025 0.6086895
 0.6220309  0.6759214  0.6742089  0.67171794 0.6230087  0.6607282
 0.83222455 0.5409668  0.33161616 0.36383373 0.5552714  0.6735048
 0.8622771  1.2292063  1.1829699  1.2815205  1.2372211  1.1602387
 1.095785   0.88996476 0.69436246 0.4825096  0.29195088 0.7336353
 1.143387   1.3346484  1.400952   1.304158   1.276304   1.1811498
 1.1825674  1.1880127  1.1981161  1.1222442  1.0619639  1.1188685
 1.4166799  1.2992699  1.1979042  0.8633759  0.61896855 0.51983607
 0.39946163 0.2738504  0.15053129 0.2456815  0.19318438 0.26035318
 0.2446899  0.13923533 0.06439877 0.17742428 0.35801807 0.3819783
 0.35762557 0.49179062 0.7258812  1.0231986  1.2309941  0.89948934
 0.48684272 0.5390081  0.6223038  0.65736246]
