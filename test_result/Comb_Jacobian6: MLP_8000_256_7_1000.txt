time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 58748.03906 Test: 3975.97412
Epoch 80: New minimal relative error: 72.70%, model saved.
Epoch: 80 Train: 11076.67578 Test: 1190.85034
Epoch: 160 Train: 1368.99780 Test: 524.57996
Epoch 240: New minimal relative error: 34.09%, model saved.
Epoch: 240 Train: 374.03125 Test: 27.16978
Epoch: 320 Train: 114.33054 Test: 30.28473
Epoch: 400 Train: 84.14886 Test: 5.43586
Epoch 480: New minimal relative error: 32.74%, model saved.
Epoch: 480 Train: 43.47414 Test: 18.68505
Epoch: 560 Train: 52.58404 Test: 17.97470
Epoch 640: New minimal relative error: 18.78%, model saved.
Epoch: 640 Train: 109.97159 Test: 35.60624
Epoch 720: New minimal relative error: 10.17%, model saved.
Epoch: 720 Train: 17.59222 Test: 0.91404
Epoch: 800 Train: 13.41318 Test: 1.02010
Epoch 880: New minimal relative error: 7.39%, model saved.
Epoch: 880 Train: 15.97368 Test: 3.33189
Epoch 960: New minimal relative error: 2.04%, model saved.
Epoch: 960 Train: 7.97853 Test: 0.35830
Epoch: 1040 Train: 36.35352 Test: 9.02020
Epoch: 1120 Train: 63.07983 Test: 20.12737
Epoch: 1200 Train: 14.56629 Test: 5.43454
Epoch: 1280 Train: 9.72059 Test: 1.95629
Epoch: 1360 Train: 7.28856 Test: 1.81951
Epoch: 1440 Train: 7.81234 Test: 2.34825
Epoch: 1520 Train: 4.74455 Test: 0.53736
Epoch: 1600 Train: 6.29628 Test: 2.19842
Epoch: 1680 Train: 3.26665 Test: 0.09926
Epoch: 1760 Train: 16.94032 Test: 3.34401
Epoch: 1840 Train: 24.36882 Test: 9.62602
Epoch: 1920 Train: 15.57417 Test: 8.46219
Epoch: 2000 Train: 2.41734 Test: 0.18391
Epoch: 2080 Train: 9.39688 Test: 1.92582
Epoch: 2160 Train: 16.56980 Test: 0.89463
Epoch: 2240 Train: 29.37602 Test: 8.30386
Epoch: 2320 Train: 33.20847 Test: 17.32988
Epoch: 2400 Train: 1.71805 Test: 0.01610
Epoch: 2480 Train: 2.06010 Test: 0.01067
Epoch: 2560 Train: 11.22527 Test: 1.92106
Epoch: 2640 Train: 7.15572 Test: 2.04923
Epoch: 2720 Train: 8.95718 Test: 4.45605
Epoch: 2800 Train: 1.39667 Test: 0.00663
Epoch 2880: New minimal relative error: 0.78%, model saved.
Epoch: 2880 Train: 1.16278 Test: 0.00382
Epoch: 2960 Train: 2.42191 Test: 0.41303
Epoch: 3040 Train: 5.62265 Test: 1.30214
Epoch: 3120 Train: 1.64982 Test: 0.27341
Epoch: 3200 Train: 1.34786 Test: 0.05431
Epoch: 3280 Train: 2.07426 Test: 0.28967
Epoch: 3360 Train: 7.21452 Test: 1.25471
Epoch: 3440 Train: 3.46105 Test: 0.79564
Epoch: 3520 Train: 5.73676 Test: 2.33983
Epoch 3600: New minimal relative error: 0.48%, model saved.
Epoch: 3600 Train: 0.85168 Test: 0.00451
Epoch: 3680 Train: 1.10009 Test: 0.08197
Epoch: 3760 Train: 2.09869 Test: 0.55425
Epoch: 3840 Train: 0.86642 Test: 0.07278
Epoch: 3920 Train: 2.83079 Test: 0.74866
Epoch: 4000 Train: 0.73857 Test: 0.00369
Epoch: 4080 Train: 1.17900 Test: 0.21230
Epoch: 4160 Train: 1.29536 Test: 0.29336
Epoch: 4240 Train: 1.51690 Test: 0.31543
Epoch: 4320 Train: 1.81029 Test: 0.17700
Epoch: 4400 Train: 2.41472 Test: 0.80263
Epoch: 4480 Train: 8.02817 Test: 4.22134
Epoch: 4560 Train: 5.16427 Test: 1.54610
Epoch: 4640 Train: 1.49377 Test: 0.32270
Epoch: 4720 Train: 2.37729 Test: 0.55077
Epoch: 4800 Train: 0.91432 Test: 0.06437
Epoch: 4880 Train: 3.70854 Test: 0.38582
Epoch: 4960 Train: 29.43804 Test: 13.22744
Epoch: 5040 Train: 0.51723 Test: 0.00390
Epoch: 5120 Train: 0.61773 Test: 0.06818
Epoch: 5200 Train: 1.00996 Test: 0.38273
Epoch: 5280 Train: 0.49370 Test: 0.00993
Epoch: 5360 Train: 0.47384 Test: 0.00920
Epoch: 5440 Train: 0.47278 Test: 0.00848
Epoch: 5520 Train: 0.89413 Test: 0.09770
Epoch: 5600 Train: 0.42390 Test: 0.00249
Epoch: 5680 Train: 7.14418 Test: 1.80913
Epoch: 5760 Train: 17.73508 Test: 8.92572
Epoch: 5840 Train: 0.39557 Test: 0.00381
Epoch: 5920 Train: 1.25385 Test: 0.21638
Epoch: 6000 Train: 1.46664 Test: 0.63399
Epoch: 6080 Train: 0.37492 Test: 0.00546
Epoch: 6160 Train: 0.42656 Test: 0.01559
Epoch: 6240 Train: 0.44766 Test: 0.04837
Epoch: 6320 Train: 0.34450 Test: 0.00277
Epoch: 6400 Train: 0.37925 Test: 0.01586
Epoch: 6480 Train: 1.13609 Test: 0.22491
Epoch: 6560 Train: 0.32156 Test: 0.00275
Epoch: 6640 Train: 0.42074 Test: 0.01533
Epoch: 6720 Train: 0.75499 Test: 0.09020
Epoch: 6800 Train: 0.30128 Test: 0.00243
Epoch: 6880 Train: 2.76550 Test: 0.98928
Epoch: 6960 Train: 0.48097 Test: 0.10454
Epoch: 7040 Train: 0.28998 Test: 0.00586
Epoch: 7120 Train: 1.54715 Test: 0.31821
Epoch: 7200 Train: 1.46221 Test: 0.40123
Epoch: 7280 Train: 0.26874 Test: 0.00245
Epoch: 7360 Train: 0.43727 Test: 0.05339
Epoch: 7440 Train: 0.49190 Test: 0.05465
Epoch: 7520 Train: 0.25597 Test: 0.00393
Epoch: 7600 Train: 0.27521 Test: 0.00829
Epoch: 7680 Train: 0.94690 Test: 0.48059
Epoch: 7760 Train: 0.24861 Test: 0.00549
Epoch: 7840 Train: 2.62723 Test: 0.48160
Epoch: 7920 Train: 0.23130 Test: 0.00228
Epoch: 7999 Train: 4.62393 Test: 2.80384
Training Loss: tensor(4.6239)
Test Loss: tensor(2.8038)
Learned LE: [ 8.8104379e-01 -3.5840028e-03 -1.4531766e+01]
True LE: [ 8.8209689e-01 -8.2020089e-04 -1.4556761e+01]
Relative Error: [0.9324092  0.8738102  0.79283345 0.70330024 0.63930136 0.6542507
 0.7814679  0.9992318  1.265407   1.5441499  1.8079482  2.0356405
 2.2129614  2.3319743  2.3924477  2.3994324  2.3637276  2.2983627
 2.2179463  2.1367698  2.0659542  2.0134766  1.984639   1.9799392
 1.9978747  2.0360622  2.0909147  2.1601746  2.2431595  2.3398373
 2.4425502  2.5242856  2.545386   2.4855878  2.3612573  2.2077987
 2.0567458  1.9267172  1.8240309  1.7441962  1.6728451  1.5898066
 1.4765606  1.3226764  1.1289374  0.90821373 0.6808838  0.47365806
 0.32312596 0.27677464 0.3235857  0.39620772 0.46587744 0.5365766
 0.616496   0.70343745 0.78759456 0.86075616 0.91846126 0.9584319
 0.9786843  0.9761567  0.94696146 0.889075   0.8033363  0.6987242
 0.6024333  0.5713451  0.6616204  0.8613567  1.1195018  1.3932176
 1.6518593  1.873507   2.0429204  2.1525643  2.2015786  2.1965327
 2.148461   2.071362   1.9811994  1.8923595  1.817029   1.7634275
 1.7355716  1.7340243  1.7565128  1.7991052  1.8587408  1.9323024
 2.0198631  2.1225903  2.2324967  2.3166502  2.329497   2.2554936
 2.1210167  1.9653058  1.8174376  1.6934359  1.6005304  1.5339552
 1.4785341  1.4122919  1.313773   1.1706159  0.98366505 0.7673954
 0.54583025 0.35255468 0.2457988  0.27502045 0.36540812 0.45036083
 0.5164914  0.57326704 0.6358304  0.70786357 0.7819893  0.84954727
 0.9052671  0.94701976 0.9713295  0.97467786 0.952583   0.90065414
 0.8170357  0.7062322  0.5865926  0.50874853 0.54991513 0.7239324
 0.9711199  1.2388196  1.4929088  1.7092088  1.8722906  1.9737078
 2.013268   1.997387   1.9382069  1.8501496  1.7504098  1.6541343
 1.5739878  1.5183902  1.4910606  1.492034   1.5181139  1.564967
 1.6281145  1.7046258  1.7951396  1.902589   2.0193806  2.1071882
 2.1156297  2.0337024  1.8968519  1.7446995  1.6014148  1.4821671
 1.3958572  1.3404224  1.3001848  1.2508049  1.1689099  1.0389519
 0.8610694  0.6506242  0.43610647 0.26218557 0.2168943  0.30269107
 0.40841734 0.4937762  0.55236953 0.5940015  0.63682866 0.6916294
 0.7543066  0.8156876  0.8701086  0.91399497 0.94432366 0.95658517
 0.94483626 0.90378356 0.8294053  0.72152007 0.5904463  0.47293437
 0.45306566 0.58822477 0.819801   1.0802422  1.329594   1.5419995
 1.7002052  1.7958449  1.8282406  1.8039651  1.7351098  1.6379045
 1.5290637  1.4250655  1.3392528  1.2805169  1.2527558  1.2551533
 1.2838756  1.3336169  1.3992434  1.4777423  1.5693072  1.6788936
 1.8013431  1.8956621  1.904591   1.822057   1.6907245  1.5468798
 1.4092404  1.2925451  1.2094918  1.1616515  1.1343086  1.1026654
 1.0394559  0.9260534  0.7598228  0.55741537 0.35105273 0.20281425
 0.22223869 0.33485818 0.4412388  0.5216497  0.5727455  0.60018384
 0.6220631  0.6564382  0.70483035 0.758758   0.81095946 0.85755485
 0.89458066 0.9172552  0.91931224 0.89365786 0.8345698  0.73851997
 0.6093316  0.46828553 0.38290253 0.4582579  0.6656841  0.9166363
 1.1615249  1.3710431  1.5266994  1.6193973  1.6473647  1.6179459
 1.542778   1.438068   1.3212609  1.209246   1.1168662  1.0531802
 1.0228492  1.0248566  1.0545675  1.106157   1.173349   1.2520899
 1.342677   1.45115    1.5774693  1.6809449  1.6971118  1.6212076
 1.5023478  1.3719445  1.2412069  1.125179   1.0417035  0.9966253
 0.97894764 0.96432847 0.9223747  0.82948923 0.67946196 0.48749858
 0.2895674  0.1692867  0.23633684 0.35765594 0.4590317  0.5324975
 0.5775012  0.5946774  0.5966402  0.60679996 0.63642997 0.67967254
 0.7278653  0.77592635 0.81967926 0.85360855 0.8711643  0.8644947
 0.8262979  0.75030535 0.6354684  0.49028057 0.3553028  0.34432155
 0.51012003 0.747276   0.98709697 1.195408   1.3515052  1.4445504
 1.4725473  1.4418178  1.3642391  1.2550638  1.1317528  1.0121143
 0.9116421  0.84089565 0.80514485 0.8039969  0.8323892  0.88383687
 0.95107096 1.0291642  1.1172671  1.2209085  1.346893   1.4621841
 1.4924984  1.4292768  1.3283957  1.216695   1.0969678  0.98178285
 0.89387196 0.84614205 0.8326713  0.83193994 0.81279826 0.7458992
 0.6178791  0.44013196 0.2503081  0.14930668 0.24196129 0.3648123
 0.46000165 0.52580565 0.56657875 0.5796732  0.567481   0.55155545
 0.55572337 0.5823406  0.62279713 0.66964364 0.718181   0.76241565
 0.79583347 0.8100723  0.79670846 0.747923   0.6579773  0.5280898
 0.37506473 0.27206674 0.35926542 0.5727043  0.8057487  1.0142721
 1.173596   1.2708585  1.3034484  1.276697   1.2017176  1.0927645
 0.96650386 0.84042996 0.7308665  0.6504171  0.6057149  0.59737295
 0.6208459  0.66906583 0.7341637  0.8104217 ]
