time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 59004.44922 Test: 3888.23438
Epoch: 80 Train: 8658.89648 Test: 904.17169
Epoch 160: New minimal relative error: 17.21%, model saved.
Epoch: 160 Train: 1233.91052 Test: 15.58779
Epoch 240: New minimal relative error: 15.63%, model saved.
Epoch: 240 Train: 115.69844 Test: 3.66152
Epoch: 320 Train: 155.59041 Test: 38.15965
Epoch 400: New minimal relative error: 11.48%, model saved.
Epoch: 400 Train: 66.69504 Test: 11.70112
Epoch: 480 Train: 140.52071 Test: 17.59099
Epoch: 560 Train: 149.11975 Test: 26.90684
Epoch: 640 Train: 134.25436 Test: 49.20374
Epoch 720: New minimal relative error: 9.30%, model saved.
Epoch: 720 Train: 26.37813 Test: 8.04288
Epoch: 800 Train: 4.32564 Test: 0.99353
Epoch: 880 Train: 46.07562 Test: 7.91948
Epoch: 960 Train: 15.22486 Test: 5.29914
Epoch: 1040 Train: 58.12146 Test: 10.93333
Epoch 1120: New minimal relative error: 8.31%, model saved.
Epoch: 1120 Train: 31.09454 Test: 3.58440
Epoch: 1200 Train: 21.41669 Test: 2.00224
Epoch: 1280 Train: 39.09873 Test: 6.01860
Epoch: 1360 Train: 41.04378 Test: 10.13220
Epoch: 1440 Train: 18.47563 Test: 5.59940
Epoch: 1520 Train: 32.49155 Test: 10.03628
Epoch: 1600 Train: 28.94261 Test: 9.70653
Epoch 1680: New minimal relative error: 6.93%, model saved.
Epoch: 1680 Train: 7.07819 Test: 1.43228
Epoch: 1760 Train: 49.36347 Test: 5.68992
Epoch 1840: New minimal relative error: 2.65%, model saved.
Epoch: 1840 Train: 2.08240 Test: 0.33800
Epoch: 1920 Train: 32.57886 Test: 12.93278
Epoch: 2000 Train: 1.06430 Test: 0.10842
Epoch: 2080 Train: 41.21051 Test: 7.53274
Epoch: 2160 Train: 1.19764 Test: 0.18314
Epoch: 2240 Train: 12.03853 Test: 2.92819
Epoch: 2320 Train: 46.00898 Test: 10.70226
Epoch: 2400 Train: 2.80539 Test: 0.34337
Epoch: 2480 Train: 28.58374 Test: 10.51334
Epoch 2560: New minimal relative error: 1.47%, model saved.
Epoch: 2560 Train: 0.54480 Test: 0.02504
Epoch: 2640 Train: 16.30927 Test: 3.70846
Epoch: 2720 Train: 39.56036 Test: 17.77085
Epoch: 2800 Train: 0.47294 Test: 0.01894
Epoch: 2880 Train: 23.69493 Test: 6.55723
Epoch: 2960 Train: 0.81811 Test: 0.13331
Epoch: 3040 Train: 67.39754 Test: 15.64580
Epoch: 3120 Train: 0.37054 Test: 0.00638
Epoch: 3200 Train: 1.39170 Test: 0.11047
Epoch: 3280 Train: 0.41234 Test: 0.03392
Epoch: 3360 Train: 5.31791 Test: 1.19023
Epoch: 3440 Train: 3.09793 Test: 1.30649
Epoch: 3520 Train: 0.39545 Test: 0.03248
Epoch: 3600 Train: 9.29832 Test: 1.82966
Epoch 3680: New minimal relative error: 0.45%, model saved.
Epoch: 3680 Train: 0.31745 Test: 0.01510
Epoch: 3760 Train: 1.77299 Test: 0.40305
Epoch: 3840 Train: 10.84126 Test: 2.20038
Epoch: 3920 Train: 15.31853 Test: 5.97299
Epoch: 4000 Train: 0.26913 Test: 0.01218
Epoch: 4080 Train: 0.39374 Test: 0.03662
Epoch: 4160 Train: 17.37555 Test: 4.81893
Epoch: 4240 Train: 0.21634 Test: 0.00215
Epoch: 4320 Train: 0.19963 Test: 0.00192
Epoch: 4400 Train: 1.99100 Test: 0.75651
Epoch: 4480 Train: 0.25316 Test: 0.01844
Epoch: 4560 Train: 0.46849 Test: 0.09116
Epoch: 4640 Train: 1.36521 Test: 0.05772
Epoch: 4720 Train: 9.90284 Test: 2.85968
Epoch: 4800 Train: 0.60582 Test: 0.11085
Epoch: 4880 Train: 0.28579 Test: 0.01827
Epoch: 4960 Train: 17.12522 Test: 4.66470
Epoch: 5040 Train: 1.35967 Test: 0.16948
Epoch: 5120 Train: 0.23640 Test: 0.03947
Epoch: 5200 Train: 13.43625 Test: 1.82730
Epoch: 5280 Train: 1.56498 Test: 0.12073
Epoch: 5360 Train: 1.42197 Test: 0.19739
Epoch: 5440 Train: 8.64324 Test: 2.29051
Epoch: 5520 Train: 0.82283 Test: 0.24029
Epoch: 5600 Train: 0.19973 Test: 0.02751
Epoch: 5680 Train: 0.32164 Test: 0.10058
Epoch: 5760 Train: 0.24240 Test: 0.03323
Epoch: 5840 Train: 17.87882 Test: 3.68893
Epoch: 5920 Train: 0.13848 Test: 0.00393
Epoch: 6000 Train: 0.43541 Test: 0.02540
Epoch: 6080 Train: 10.04057 Test: 1.69191
Epoch: 6160 Train: 0.12181 Test: 0.00352
Epoch: 6240 Train: 1.04305 Test: 0.57423
Epoch: 6320 Train: 0.11106 Test: 0.00179
Epoch: 6400 Train: 1.44111 Test: 0.23517
Epoch: 6480 Train: 0.10929 Test: 0.00189
Epoch: 6560 Train: 0.24245 Test: 0.04720
Epoch: 6640 Train: 1.59183 Test: 0.20501
Epoch: 6720 Train: 0.10353 Test: 0.00232
Epoch: 6800 Train: 0.24519 Test: 0.02759
Epoch: 6880 Train: 3.04913 Test: 0.67669
Epoch: 6960 Train: 0.09459 Test: 0.00179
Epoch: 7040 Train: 3.24362 Test: 1.08795
Epoch: 7120 Train: 0.09299 Test: 0.00185
Epoch: 7200 Train: 3.01085 Test: 0.75304
Epoch: 7280 Train: 0.09094 Test: 0.00285
Epoch: 7360 Train: 0.38468 Test: 0.22765
Epoch: 7440 Train: 0.08680 Test: 0.00212
Epoch: 7520 Train: 0.64971 Test: 0.15446
Epoch: 7600 Train: 2.81032 Test: 0.89178
Epoch: 7680 Train: 0.08934 Test: 0.00632
Epoch: 7760 Train: 4.83671 Test: 1.15292
Epoch: 7840 Train: 0.15804 Test: 0.02737
Epoch: 7920 Train: 0.10561 Test: 0.00987
Epoch: 7999 Train: 0.11561 Test: 0.01055
Training Loss: tensor(0.1156)
Test Loss: tensor(0.0106)
Learned LE: [  0.88710326  -0.02102367 -14.539679  ]
True LE: [ 8.6514986e-01 -7.5291619e-03 -1.4534584e+01]
Relative Error: [0.44499138 0.44686395 0.45400062 0.46569002 0.48095807 0.49845228
 0.51739657 0.5377283  0.5592859  0.5805024  0.5984781  0.6090286
 0.6102936  0.6033508  0.5915866  0.5785777  0.56692344 0.55785316
 0.5510544  0.5458088  0.5413419  0.5372913  0.5339564  0.5317855
 0.53167385 0.53435725 0.540766   0.55078596 0.5641779  0.57973
 0.59615153 0.6121856  0.6285471  0.6466529  0.6698496  0.70121956
 0.7417764  0.7864278  0.82565945 0.8548776  0.8769138  0.8930129
 0.89711314 0.8869527  0.8686263  0.84993774 0.8325492  0.8138592
 0.7908863  0.76304173 0.73015165 0.69352144 0.6548558  0.61595154
 0.57834285 0.54321617 0.5114664  0.48369378 0.4600999  0.44132593
 0.42724866 0.41811672 0.4142113  0.4156212  0.42229477 0.43373024
 0.4489791  0.46670103 0.48557502 0.50524867 0.52526915 0.54425603
 0.5590629  0.5659626  0.5636791  0.5542281  0.5415599  0.5294181
 0.5200262  0.51389027 0.5103789  0.5079533  0.50571287 0.5030814
 0.5004908  0.49858043 0.4985855  0.5015455  0.5083183  0.5192109
 0.5340053  0.5514049  0.5699134  0.5878677  0.6046689  0.62204355
 0.6426587  0.6701887  0.7069137  0.74917305 0.7868573  0.8137976
 0.83485514 0.8520145  0.8574309  0.8481199  0.83191323 0.8166342
 0.8024038  0.78558934 0.76341885 0.73522    0.701497   0.6641751
 0.62498546 0.58593166 0.548616   0.51400346 0.4829902  0.4558704
 0.43284222 0.41429394 0.4000441  0.39050284 0.3857462  0.38618234
 0.3918829  0.40264848 0.4175554  0.43528834 0.45424163 0.47342893
 0.49223405 0.5092649  0.5212861  0.52497184 0.5198216  0.50846076
 0.495504   0.48446405 0.47719106 0.4737402  0.47292885 0.47301674
 0.47256625 0.47113326 0.46893153 0.4670784  0.46709728 0.47008163
 0.47706807 0.48851132 0.5041196  0.52304405 0.54352266 0.5634377
 0.5818296  0.59919435 0.61797225 0.6416816  0.6736736  0.71262664
 0.74869907 0.7733888  0.79255646 0.810623   0.8180416  0.81028163
 0.79649675 0.78464735 0.77339745 0.7579916  0.7361059  0.7074362
 0.67327857 0.6353849  0.59613055 0.5575019  0.5210258  0.48741838
 0.4574033  0.4312041  0.408895   0.39040664 0.37589753 0.36550948
 0.35957098 0.35852987 0.3626883  0.37206003 0.38625407 0.40373802
 0.42273366 0.44176382 0.45972717 0.47514912 0.48498031 0.48630008
 0.4790475  0.466687   0.45380682 0.44390145 0.43834752 0.43687126
 0.43816382 0.44020027 0.44120672 0.44049653 0.43853143 0.4368513
 0.4368984  0.44017047 0.4474842  0.45911977 0.47486857 0.4941666
 0.51584435 0.53770477 0.5580417  0.5765247  0.59472847 0.6154324
 0.6423451  0.67695904 0.71126044 0.73381615 0.75000465 0.7684285
 0.77837056 0.7728781  0.76176554 0.7532063  0.7445826  0.73033667
 0.708575   0.6797016  0.64523214 0.6073417  0.56865066 0.5311126
 0.49593058 0.46386966 0.43510157 0.40990013 0.38819435 0.3698103
 0.35483897 0.34343913 0.33585647 0.33281648 0.33478674 0.34211805
 0.3546817  0.3714     0.3903863  0.4095999  0.42731002 0.4416922
 0.4501468  0.44998628 0.4417138  0.4290124  0.41671115 0.4078357
 0.40335813 0.4029894  0.40534526 0.40859193 0.41068992 0.41063628
 0.4089387  0.40727127 0.4077022  0.4116959  0.41972902 0.4316274
 0.4469729  0.46536466 0.48645568 0.5090892  0.53118944 0.55147225
 0.5705909  0.59027535 0.61298215 0.64236194 0.6745411  0.69600016
 0.70784914 0.7244336  0.7376276  0.7353135  0.72699374 0.7216198
 0.71506    0.7018331  0.68035007 0.6516612  0.61755574 0.58049476
 0.5430713  0.5070947  0.47367945 0.44331184 0.41610482 0.3920065
 0.37087333 0.35242295 0.33673167 0.32408604 0.3146943  0.3092203
 0.30840743 0.31282437 0.32277986 0.33791286 0.35635132 0.3758572
 0.3939421  0.40829274 0.41640905 0.415993   0.4075812  0.3953327
 0.38405427 0.37624362 0.37246096 0.37200856 0.3742789  0.37777194
 0.38053954 0.38103867 0.37959918 0.37809145 0.37892044 0.38384137
 0.39325655 0.40604454 0.42104283 0.43756273 0.45588762 0.4766351
 0.49888465 0.52080035 0.5419979  0.5629451  0.58434486 0.6089116
 0.6380743  0.6603708  0.6678854  0.67921066 0.694975   0.696936
 0.6913878  0.68870884 0.6843947  0.6722419  0.6514752  0.62355465
 0.5905321  0.5550193  0.51956165 0.4859079  0.4547901  0.42640516
 0.4006526  0.37748325 0.3565271  0.33793092 0.3213741  0.30733085
 0.29614505 0.28823137 0.28429997 0.28509703 0.29133618 0.30333683
 0.32016116 0.33959058 0.35850707 0.37385812 0.38295302 0.38358882
 0.3762088  0.36514652 0.35524875 0.34889865 0.3457369  0.3446392
 0.34538218 0.34774375 0.35053316 0.35173604 0.35071698 0.34921926
 0.35006025 0.35560435 0.366355   0.38083494]
