time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 50.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 10.124257088 Test: 7.880028725
Epoch 0: New minimal relative error: 7.88%, model saved.
Epoch: 100 Train: 1.536270142 Test: 1.596251011
Epoch 100: New minimal relative error: 1.60%, model saved.
Epoch: 200 Train: 0.945979714 Test: 0.914732933
Epoch 200: New minimal relative error: 0.91%, model saved.
Epoch: 300 Train: 0.949310780 Test: 0.924371541
Epoch: 400 Train: 0.942649364 Test: 0.912027657
Epoch 400: New minimal relative error: 0.91%, model saved.
Epoch: 500 Train: 0.930930376 Test: 0.912555039
Epoch: 600 Train: 0.942849040 Test: 0.933518410
Epoch: 700 Train: 0.931665182 Test: 0.906572938
Epoch 700: New minimal relative error: 0.91%, model saved.
Epoch: 800 Train: 0.931743503 Test: 0.908497334
Epoch: 900 Train: 0.936264217 Test: 0.915193081
Epoch: 1000 Train: 0.929523766 Test: 0.907490969
Epoch: 1100 Train: 0.931359529 Test: 0.902612865
Epoch 1100: New minimal relative error: 0.90%, model saved.
Epoch: 1200 Train: 0.926301539 Test: 0.912842989
Epoch: 1300 Train: 1.014475822 Test: 0.925711513
Epoch: 1400 Train: 0.927793741 Test: 0.899022460
Epoch 1400: New minimal relative error: 0.90%, model saved.
Epoch: 1500 Train: 0.942974210 Test: 0.930648446
Epoch: 1600 Train: 0.938785672 Test: 0.917396069
Epoch: 1700 Train: 0.946446419 Test: 0.932145476
Epoch: 1800 Train: 0.943021178 Test: 0.925431013
Epoch: 1900 Train: 0.928384066 Test: 0.923707008
Epoch: 2000 Train: 0.935927510 Test: 0.930434465
Epoch: 2100 Train: 0.939874053 Test: 0.923891008
Epoch: 2200 Train: 0.927012086 Test: 0.916190922
Epoch: 2300 Train: 0.916270494 Test: 0.899889350
Epoch: 2400 Train: 0.920831919 Test: 0.912727177
Epoch: 2500 Train: 0.918089151 Test: 0.894917130
Epoch 2500: New minimal relative error: 0.89%, model saved.
Epoch: 2600 Train: 0.949598789 Test: 0.919081569
Epoch: 2700 Train: 0.921570718 Test: 0.900266647
Epoch: 2800 Train: 0.930739045 Test: 0.924267530
Epoch: 2900 Train: 0.954839110 Test: 0.949883938
Epoch: 3000 Train: 0.918971181 Test: 0.920614839
Epoch: 3100 Train: 0.930925131 Test: 0.922160983
Epoch: 3200 Train: 0.940333009 Test: 0.932904065
Epoch: 3300 Train: 0.942838907 Test: 0.940528512
Epoch: 3400 Train: 0.948870361 Test: 0.933836818
Epoch: 3500 Train: 0.958619416 Test: 0.960694730
Epoch: 3600 Train: 0.940580606 Test: 0.924457550
Epoch: 3700 Train: 0.937641025 Test: 0.921618342
Epoch: 3800 Train: 0.941459060 Test: 0.920355678
Epoch: 3900 Train: 0.929982543 Test: 0.902448297
Epoch: 4000 Train: 0.930962682 Test: 0.915046692
Epoch: 4100 Train: 0.939262927 Test: 0.917290926
Epoch: 4200 Train: 0.947070956 Test: 0.920835435
Epoch: 4300 Train: 0.940165877 Test: 0.920195043
Epoch: 4400 Train: 0.946624517 Test: 0.927418590
Epoch: 4500 Train: 0.955978572 Test: 0.917404413
Epoch: 4600 Train: 0.942862034 Test: 0.918996453
Epoch: 4700 Train: 0.937674820 Test: 0.919584990
Epoch: 4800 Train: 0.951372623 Test: 0.924887896
Epoch: 4900 Train: 0.942318559 Test: 0.910210252
Epoch: 5000 Train: 0.950681686 Test: 0.930189967
Epoch: 5100 Train: 0.949222505 Test: 0.915008187
Epoch: 5200 Train: 0.944735944 Test: 0.927159607
Epoch: 5300 Train: 0.948063374 Test: 0.917532146
Epoch: 5400 Train: 0.938581347 Test: 0.919304907
Epoch: 5500 Train: 0.938419521 Test: 0.920939565
Epoch: 5600 Train: 0.942168951 Test: 0.925827503
Epoch: 5700 Train: 0.940106452 Test: 0.916495860
Epoch: 5800 Train: 0.933751464 Test: 0.915697813
Epoch: 5900 Train: 0.935108423 Test: 0.920390725
Epoch: 6000 Train: 0.937631011 Test: 0.921529353
Epoch: 6100 Train: 0.920902610 Test: 0.905100942
Epoch: 6200 Train: 0.917885065 Test: 0.900100350
Epoch: 6300 Train: 0.920255899 Test: 0.906064034
Epoch: 6400 Train: 0.925568223 Test: 0.910717726
Epoch: 6500 Train: 0.930637956 Test: 0.910284758
Epoch: 6600 Train: 0.930812001 Test: 0.909461021
Epoch: 6700 Train: 0.934497237 Test: 0.922904253
Epoch: 6800 Train: 0.929456711 Test: 0.910850167
Epoch: 6900 Train: 0.930088878 Test: 0.900758028
Epoch: 7000 Train: 0.936241627 Test: 0.905069709
Epoch: 7100 Train: 0.937466264 Test: 0.912581801
Epoch: 7200 Train: 0.946699381 Test: 0.928836465
Epoch: 7300 Train: 0.948709846 Test: 0.922233820
Epoch: 7400 Train: 0.955008805 Test: 0.936290622
Epoch: 7500 Train: 0.954707503 Test: 0.931738377
Epoch: 7600 Train: 0.943943620 Test: 0.937366009
Epoch: 7700 Train: 0.958558679 Test: 0.946354985
Epoch: 7800 Train: 0.963308811 Test: 0.944881678
Epoch: 7900 Train: 0.967264891 Test: 0.946858644
Epoch: 8000 Train: 0.964281201 Test: 0.940982699
Epoch: 8100 Train: 0.962455630 Test: 0.950327039
Epoch: 8200 Train: 0.958722830 Test: 0.958946705
Epoch: 8300 Train: 0.960138083 Test: 0.951860845
Epoch: 8400 Train: 0.954276383 Test: 0.952085555
Epoch: 8500 Train: 0.943850875 Test: 0.944212079
Epoch: 8600 Train: 0.948558927 Test: 0.951118112
Epoch: 8700 Train: 0.949018657 Test: 0.948561847
Epoch: 8800 Train: 0.956735611 Test: 0.951301575
Epoch: 8900 Train: 0.956803679 Test: 0.953531981
Epoch: 9000 Train: 0.957639873 Test: 0.951224685
Epoch: 9100 Train: 0.958728313 Test: 0.953449607
Epoch: 9200 Train: 0.957406640 Test: 0.950033545
Epoch: 9300 Train: 0.948964536 Test: 0.942395031
Epoch: 9400 Train: 0.962560058 Test: 0.950812459
Epoch: 9500 Train: 0.963777900 Test: 0.946826696
Epoch: 9600 Train: 0.962417006 Test: 0.958337188
Epoch: 9700 Train: 0.964805126 Test: 0.944521546
Epoch: 9800 Train: 0.962406278 Test: 0.943241656
Epoch: 9900 Train: 0.964365065 Test: 0.939458251
Epoch: 9999 Train: 0.957579017 Test: 0.924760222
Training Loss: tensor(0.9576)
Test Loss: tensor(0.9248)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.1480, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0139, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0091)
Jacobian term Test Loss: tensor(0.0091)
Learned LE: [1.564449   0.10964173]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
