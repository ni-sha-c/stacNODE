time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.68%, model saved.
Epoch: 0 Train: 4231.68945 Test: 3721.52051
Epoch: 100 Train: 80.73852 Test: 89.64082
Epoch 200: New minimal relative error: 68.96%, model saved.
Epoch: 200 Train: 74.84444 Test: 75.50211
Epoch 300: New minimal relative error: 17.73%, model saved.
Epoch: 300 Train: 9.87091 Test: 13.87237
Epoch 400: New minimal relative error: 15.47%, model saved.
Epoch: 400 Train: 5.85358 Test: 10.91970
Epoch: 500 Train: 5.69147 Test: 9.25657
Epoch: 600 Train: 5.45509 Test: 8.11460
Epoch: 700 Train: 8.78577 Test: 10.79779
Epoch: 800 Train: 3.87797 Test: 6.43447
Epoch: 900 Train: 11.61447 Test: 15.60897
Epoch: 1000 Train: 2.83359 Test: 5.12697
Epoch: 1100 Train: 12.62171 Test: 13.33462
Epoch: 1200 Train: 2.62317 Test: 4.66741
Epoch: 1300 Train: 5.86476 Test: 8.01451
Epoch: 1400 Train: 2.61527 Test: 4.34642
Epoch 1500: New minimal relative error: 10.83%, model saved.
Epoch: 1500 Train: 5.69303 Test: 5.84348
Epoch: 1600 Train: 2.94305 Test: 4.83825
Epoch: 1700 Train: 2.25057 Test: 3.89086
Epoch: 1800 Train: 2.26803 Test: 3.88137
Epoch: 1900 Train: 2.45838 Test: 3.91147
Epoch: 2000 Train: 1.13840 Test: 2.23255
Epoch: 2100 Train: 4.11111 Test: 5.98578
Epoch: 2200 Train: 2.17892 Test: 3.15199
Epoch: 2300 Train: 0.88999 Test: 1.73647
Epoch: 2400 Train: 0.75617 Test: 1.59313
Epoch: 2500 Train: 0.81385 Test: 1.58301
Epoch: 2600 Train: 0.97726 Test: 1.79182
Epoch: 2700 Train: 6.09723 Test: 4.93923
Epoch: 2800 Train: 1.76442 Test: 2.21248
Epoch: 2900 Train: 0.73444 Test: 1.18773
Epoch: 3000 Train: 0.55511 Test: 1.10105
Epoch: 3100 Train: 0.47214 Test: 1.01849
Epoch: 3200 Train: 0.31948 Test: 0.80305
Epoch 3300: New minimal relative error: 9.93%, model saved.
Epoch: 3300 Train: 0.33773 Test: 0.80513
Epoch: 3400 Train: 0.44882 Test: 0.86481
Epoch: 3500 Train: 2.29504 Test: 2.43898
Epoch: 3600 Train: 0.46458 Test: 0.71719
Epoch: 3700 Train: 0.25970 Test: 0.67218
Epoch: 3800 Train: 0.25575 Test: 0.60798
Epoch: 3900 Train: 0.24049 Test: 0.59762
Epoch 4000: New minimal relative error: 9.12%, model saved.
Epoch: 4000 Train: 0.27280 Test: 0.62432
Epoch: 4100 Train: 1.38167 Test: 1.77468
Epoch 4200: New minimal relative error: 8.69%, model saved.
Epoch: 4200 Train: 0.35709 Test: 0.66033
Epoch: 4300 Train: 0.25875 Test: 0.52627
Epoch: 4400 Train: 0.24250 Test: 0.57949
Epoch: 4500 Train: 0.39367 Test: 0.65586
Epoch: 4600 Train: 0.19327 Test: 0.51254
Epoch: 4700 Train: 0.26952 Test: 0.49606
Epoch: 4800 Train: 0.15873 Test: 0.43552
Epoch: 4900 Train: 0.17759 Test: 0.42905
Epoch: 5000 Train: 0.18871 Test: 0.42170
Epoch: 5100 Train: 0.75193 Test: 0.86909
Epoch: 5200 Train: 0.18215 Test: 0.42143
Epoch: 5300 Train: 0.14904 Test: 0.37050
Epoch: 5400 Train: 0.22442 Test: 0.48358
Epoch: 5500 Train: 0.12952 Test: 0.34979
Epoch: 5600 Train: 0.13144 Test: 0.36047
Epoch: 5700 Train: 0.88628 Test: 0.96067
Epoch: 5800 Train: 0.17088 Test: 0.37177
Epoch 5900: New minimal relative error: 7.79%, model saved.
Epoch: 5900 Train: 0.12352 Test: 0.32319
Epoch: 6000 Train: 0.11745 Test: 0.31886
Epoch: 6100 Train: 0.12447 Test: 0.32437
Epoch: 6200 Train: 0.77715 Test: 0.89614
Epoch: 6300 Train: 0.15576 Test: 0.34283
Epoch: 6400 Train: 0.14683 Test: 0.32607
Epoch: 6500 Train: 0.37499 Test: 0.56406
Epoch: 6600 Train: 0.15805 Test: 0.36142
Epoch: 6700 Train: 0.10917 Test: 0.29575
Epoch: 6800 Train: 0.10442 Test: 0.28188
Epoch 6900: New minimal relative error: 7.68%, model saved.
Epoch: 6900 Train: 0.12616 Test: 0.29302
Epoch: 7000 Train: 0.10090 Test: 0.27907
Epoch: 7100 Train: 0.09950 Test: 0.27641
Epoch: 7200 Train: 0.14545 Test: 0.28034
Epoch: 7300 Train: 0.09559 Test: 0.26532
Epoch: 7400 Train: 0.09403 Test: 0.26121
Epoch: 7500 Train: 0.14875 Test: 0.30627
Epoch: 7600 Train: 0.11651 Test: 0.27907
Epoch: 7700 Train: 0.08955 Test: 0.25202
Epoch: 7800 Train: 0.08999 Test: 0.26003
Epoch: 7900 Train: 0.08951 Test: 0.25160
Epoch: 8000 Train: 0.08611 Test: 0.24358
Epoch: 8100 Train: 0.08866 Test: 0.24655
Epoch: 8200 Train: 0.09251 Test: 0.23968
Epoch: 8300 Train: 0.10539 Test: 0.27008
Epoch: 8400 Train: 0.38400 Test: 0.57225
Epoch: 8500 Train: 0.12050 Test: 0.25201
Epoch: 8600 Train: 0.10505 Test: 0.24487
Epoch: 8700 Train: 0.50033 Test: 0.66434
Epoch: 8800 Train: 0.47574 Test: 0.64090
Epoch: 8900 Train: 0.07777 Test: 0.22051
Epoch: 9000 Train: 0.07838 Test: 0.21858
Epoch: 9100 Train: 0.07662 Test: 0.21724
Epoch: 9200 Train: 0.07924 Test: 0.21929
Epoch: 9300 Train: 0.07489 Test: 0.21170
Epoch: 9400 Train: 0.07389 Test: 0.20968
Epoch: 9500 Train: 0.07875 Test: 0.21810
Epoch: 9600 Train: 0.33740 Test: 0.33152
Epoch: 9700 Train: 0.07157 Test: 0.20380
Epoch: 9800 Train: 0.07092 Test: 0.20248
Epoch: 9900 Train: 0.07085 Test: 0.19910
Epoch: 9999 Train: 0.07623 Test: 0.21171
Training Loss: tensor(0.0762)
Test Loss: tensor(0.2117)
Learned LE: [ 0.7767159   0.00339234 -3.1282961 ]
True LE: [ 8.9256883e-01  2.1593659e-03 -1.4564608e+01]
Relative Error: [1.4665431  1.485986   2.1878326  3.2270446  4.1138725  4.848713
 5.239762   4.8830547  4.178251   3.408322   2.648314   1.9409348
 1.4693407  1.2658607  1.3878601  1.6894443  1.9585062  2.12804
 2.2271762  2.107114   2.122085   2.312173   2.7704973  3.5039966
 4.4631433  4.6080327  5.0744886  5.427399   5.417069   5.663551
 6.06115    6.416917   6.398437   6.114071   5.9056373  5.4712205
 4.935026   4.657544   4.8610234  5.405475   6.063574   6.4767656
 6.817231   7.449318   7.992384   7.993591   7.626784   7.306081
 7.0606337  7.054477   7.3222713  7.65889    7.8396864  8.0430565
 7.868511   7.428687   6.9582386  5.940217   4.9164166  3.907033
 2.8838398  2.0218346  1.5769045  1.3821032  2.0427763  2.956765
 3.8419275  4.4727726  5.146199   5.015655   4.073451   3.1328626
 2.2273881  1.4566735  0.9691156  0.9057936  1.3254678  1.6288595
 1.7659936  1.8624356  1.8605258  1.877406   1.94721    1.96457
 2.2102618  2.7606277  3.7171185  4.2784877  4.5791116  4.682581
 4.7526445  5.0390363  5.5134745  5.975794   5.6851635  5.4129
 5.2159915  4.8778167  4.334075   3.9383466  4.103324   4.70073
 5.3307376  5.635016   6.0010896  6.5715     7.0352397  7.3870993
 6.9819794  6.553462   6.4080544  6.6020384  6.907728   7.192764
 7.3999906  7.5840893  7.7576375  7.4205914  7.0456305  6.247014
 5.2367096  4.1886954  3.072334   2.2154033  1.7019448  1.3805344
 1.7703546  2.714038   3.5794816  4.0201035  4.5704255  4.9251413
 4.071776   2.8956375  2.0267518  1.1902951  0.7551818  1.0101058
 1.5704149  1.8673517  1.892023   1.7215561  1.6031537  1.6010191
 1.6416093  1.6312966  1.6887436  2.0632102  2.8498976  3.953288
 4.0981503  3.9649396  4.0254903  4.3351545  4.898525   5.564486
 5.126784   4.792921   4.580289   4.3540225  3.889275   3.3985224
 3.372799   4.0393376  4.579404   4.8465753  5.201928   5.7178426
 6.1062703  6.595115   6.3907557  5.910791   5.8484597  6.049785
 6.313727   6.622521   6.8871827  7.0115147  7.456346   7.332798
 6.944076   6.554061   5.6014733  4.6240907  3.543169   2.5604143
 1.9236816  1.4610565  1.644367   2.315409   3.0793655  3.663953
 4.064133   4.557158   3.9406147  2.9930012  1.9684266  1.1132332
 1.1835479  1.6600058  2.2342813  2.3943572  2.2526236  1.9753708
 1.5077534  1.3532351  1.3657007  1.2338849  1.2041975  1.3935832
 2.0603971  3.15951    3.6338017  3.39025    3.214552   3.5123863
 4.2274933  5.083323   4.6463675  4.220966   4.005135   3.871547
 3.5274684  3.0001433  2.7549734  3.3890274  3.8705251  4.1362443
 4.4240127  4.9010606  5.2846746  5.6928387  5.876014   5.378915
 5.2494497  5.3144584  5.4413857  5.6586394  5.9668803  6.3170447
 6.793641   7.166072   6.8768635  6.4680805  5.9321356  5.031256
 4.1107016  3.0737936  2.255398   1.6317319  1.6639875  2.1111655
 2.513512   3.3589535  3.700202   3.948933   3.7408602  2.882559
 1.9711778  1.4681344  1.7698554  2.1032457  2.7841938  3.253091
 3.1734889  2.6013327  1.8540618  1.3002951  1.0810966  0.93473595
 0.7175997  0.8686985  1.3991143  2.213454   3.1693358  2.8253736
 2.5924711  2.6399708  3.378882   4.4181156  4.362228   3.7934356
 3.5551076  3.3918898  3.2215147  2.7928145  2.384535   2.7300847
 3.2027836  3.4918413  3.636675   4.050283   4.44183    4.760126
 4.9181094  4.8221273  4.663982   4.5902457  4.5649843  4.618774
 4.8354926  5.496273   5.9925556  6.5591254  6.8001413  6.5474415
 6.155685   5.5086875  4.5927067  3.7066464  2.7330952  2.0201705
 1.5945497  2.1040184  2.3104744  2.757119   3.4494524  3.6118734
 3.2486417  2.6043673  2.2037585  2.297756   2.1815603  2.2849207
 2.5815542  2.9457924  3.118993   3.1191983  2.8747509  1.7943956
 1.080929   0.6345547  0.4727473  0.54505503 0.87787116 1.4650027
 2.3253531  2.4573467  2.2266767  1.8001401  2.2952921  3.3829935
 3.9818325  3.4218366  3.119125   2.9570196  2.9241211  2.6705487
 2.3182225  2.2323468  2.5302358  2.7142656  2.7177463  3.0958514
 3.4910347  3.6796603  3.806625   4.2499375  4.0203733  3.8648617
 3.7294853  3.667977   3.799794   4.3321414  5.0201235  5.508476
 5.984132   6.1641793  6.1118383  5.725108   5.1643076  4.3582625
 3.470009   2.5087454  1.7596617  1.8856242  2.3627293  2.5531585
 3.039584   3.4722056  2.7233212  2.3967059  2.8749285  2.9423437
 3.0423648  2.858744   2.7720592  2.9605172  3.041301   2.9201417
 2.6404037  2.2768762  1.9627422  0.88653713 0.42454088 0.4005961
 0.7019717  1.0443615  1.611649   2.0669923 ]
