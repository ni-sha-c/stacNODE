time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.01%, model saved.
Epoch: 0 Train: 3901.92651 Test: 4036.98560
Epoch 100: New minimal relative error: 72.94%, model saved.
Epoch: 100 Train: 96.70904 Test: 105.89731
Epoch 200: New minimal relative error: 30.01%, model saved.
Epoch: 200 Train: 21.89344 Test: 24.24685
Epoch: 300 Train: 16.52458 Test: 17.97999
Epoch: 400 Train: 22.55665 Test: 15.04724
Epoch: 500 Train: 76.37778 Test: 99.71748
Epoch: 600 Train: 4.50599 Test: 5.86795
Epoch: 700 Train: 4.12476 Test: 6.48849
Epoch: 800 Train: 1.00219 Test: 1.24746
Epoch: 900 Train: 3.58771 Test: 4.52384
Epoch: 1000 Train: 7.31816 Test: 9.75364
Epoch: 1100 Train: 0.47788 Test: 1.66684
Epoch 1200: New minimal relative error: 21.47%, model saved.
Epoch: 1200 Train: 1.86850 Test: 3.48299
Epoch: 1300 Train: 1.10353 Test: 1.82091
Epoch: 1400 Train: 13.22014 Test: 16.66709
Epoch 1500: New minimal relative error: 16.37%, model saved.
Epoch: 1500 Train: 0.33694 Test: 0.86488
Epoch: 1600 Train: 2.20101 Test: 3.39491
Epoch: 1700 Train: 8.37474 Test: 10.10023
Epoch: 1800 Train: 1.29782 Test: 1.71253
Epoch: 1900 Train: 0.41077 Test: 0.76271
Epoch: 2000 Train: 10.63138 Test: 10.76727
Epoch: 2100 Train: 0.43079 Test: 0.76095
Epoch: 2200 Train: 5.25682 Test: 7.29968
Epoch: 2300 Train: 1.08115 Test: 1.86200
Epoch: 2400 Train: 4.57497 Test: 3.75674
Epoch: 2500 Train: 0.62085 Test: 1.00422
Epoch: 2600 Train: 4.77941 Test: 4.35318
Epoch: 2700 Train: 0.94778 Test: 1.26943
Epoch: 2800 Train: 5.70266 Test: 6.60704
Epoch 2900: New minimal relative error: 15.48%, model saved.
Epoch: 2900 Train: 0.75214 Test: 0.78631
Epoch: 3000 Train: 1.30904 Test: 2.45061
Epoch: 3100 Train: 0.30312 Test: 0.72926
Epoch 3200: New minimal relative error: 13.87%, model saved.
Epoch: 3200 Train: 0.37566 Test: 0.71210
Epoch: 3300 Train: 0.17531 Test: 0.58939
Epoch: 3400 Train: 1.25511 Test: 1.50970
Epoch: 3500 Train: 0.31208 Test: 0.54854
Epoch: 3600 Train: 0.25088 Test: 0.67140
Epoch: 3700 Train: 1.75855 Test: 2.26787
Epoch: 3800 Train: 0.08699 Test: 0.46045
Epoch: 3900 Train: 0.18814 Test: 0.64898
Epoch: 4000 Train: 0.60825 Test: 1.22595
Epoch: 4100 Train: 0.32678 Test: 0.59287
Epoch: 4200 Train: 0.40581 Test: 0.98313
Epoch: 4300 Train: 0.12972 Test: 0.50698
Epoch: 4400 Train: 2.86860 Test: 3.27085
Epoch: 4500 Train: 0.55468 Test: 0.82851
Epoch: 4600 Train: 0.48368 Test: 0.97225
Epoch: 4700 Train: 0.36796 Test: 0.71285
Epoch: 4800 Train: 0.18391 Test: 0.55132
Epoch: 4900 Train: 0.06831 Test: 0.43828
Epoch: 5000 Train: 0.60204 Test: 1.06478
Epoch: 5100 Train: 1.15582 Test: 1.44866
Epoch: 5200 Train: 0.06259 Test: 0.43400
Epoch: 5300 Train: 0.42733 Test: 0.90568
Epoch: 5400 Train: 0.46445 Test: 1.11716
Epoch: 5500 Train: 2.24456 Test: 2.14018
Epoch 5600: New minimal relative error: 11.75%, model saved.
Epoch: 5600 Train: 0.09041 Test: 0.40106
Epoch: 5700 Train: 0.03042 Test: 0.38897
Epoch: 5800 Train: 0.03169 Test: 0.40366
Epoch: 5900 Train: 0.16957 Test: 0.46422
Epoch: 6000 Train: 0.07437 Test: 0.42753
Epoch: 6100 Train: 1.17796 Test: 1.79253
Epoch: 6200 Train: 0.19233 Test: 0.62396
Epoch 6300: New minimal relative error: 10.22%, model saved.
Epoch: 6300 Train: 0.03313 Test: 0.38377
Epoch: 6400 Train: 0.02476 Test: 0.38506
Epoch: 6500 Train: 0.04782 Test: 0.40572
Epoch: 6600 Train: 0.04525 Test: 0.41596
Epoch: 6700 Train: 0.04573 Test: 0.40856
Epoch: 6800 Train: 0.37427 Test: 0.66015
Epoch: 6900 Train: 0.25422 Test: 0.84140
Epoch: 7000 Train: 0.21871 Test: 0.62867
Epoch 7100: New minimal relative error: 4.62%, model saved.
Epoch: 7100 Train: 0.03901 Test: 0.38913
Epoch: 7200 Train: 0.03320 Test: 0.38347
Epoch: 7300 Train: 0.61281 Test: 1.18939
Epoch: 7400 Train: 0.15209 Test: 0.54636
Epoch: 7500 Train: 1.13408 Test: 0.93141
Epoch: 7600 Train: 0.01859 Test: 0.36588
Epoch: 7700 Train: 0.01973 Test: 0.37924
Epoch: 7800 Train: 0.06260 Test: 0.43606
Epoch: 7900 Train: 1.47490 Test: 1.67204
Epoch: 8000 Train: 0.01797 Test: 0.37084
Epoch: 8100 Train: 0.01740 Test: 0.36307
Epoch: 8200 Train: 0.01849 Test: 0.37183
Epoch: 8300 Train: 0.01888 Test: 0.36345
Epoch: 8400 Train: 0.03662 Test: 0.38811
Epoch: 8500 Train: 0.06078 Test: 0.42279
Epoch: 8600 Train: 0.71250 Test: 1.18933
Epoch: 8700 Train: 0.06039 Test: 0.42008
Epoch: 8800 Train: 0.01496 Test: 0.36657
Epoch: 8900 Train: 0.01964 Test: 0.36549
Epoch: 9000 Train: 0.05155 Test: 0.40928
Epoch: 9100 Train: 0.01397 Test: 0.36102
Epoch: 9200 Train: 0.01596 Test: 0.36397
Epoch: 9300 Train: 0.04843 Test: 0.40410
Epoch: 9400 Train: 0.02895 Test: 0.38906
Epoch: 9500 Train: 0.01346 Test: 0.35816
Epoch: 9600 Train: 0.02276 Test: 0.37001
Epoch: 9700 Train: 0.01696 Test: 0.36192
Epoch: 9800 Train: 0.08482 Test: 0.52191
Epoch: 9900 Train: 0.01309 Test: 0.35956
Epoch: 9999 Train: 0.01248 Test: 0.35637
Training Loss: tensor(0.0125)
Test Loss: tensor(0.3564)
Learned LE: [ 0.9013093  -0.04877951 -5.128669  ]
True LE: [ 8.7159425e-01 -3.9346502e-03 -1.4549732e+01]
Relative Error: [1.2784283  1.1179569  0.84642905 0.4571537  0.23674436 0.79577714
 1.4908042  2.2087681  2.8940277  3.5087552  4.033219   4.4503007
 4.7334795  4.8555603  4.807344   4.6064234  4.2876906  3.890133
 3.4579663  3.0400255  2.673199   2.3896785  2.2535193  2.3210998
 2.5188737  2.6690383  2.661747   2.5376444  2.389436   2.3074207
 2.3021998  2.240703   2.032846   1.7613614  1.5848902  1.5761323
 1.6688691  1.7723534  1.8489517  1.8996149  1.9295255  1.9304879
 1.8873714  1.8000398  1.6998218  1.637219   1.644801   1.7165724
 1.8257627  1.9393437  2.007558   1.9695718  1.8048158  1.5719633
 1.3552598  1.1989684  1.1110785  1.0770755  1.0644377  1.0493313
 1.0307105  1.0117614  0.97220504 0.8664501  0.64947426 0.31080487
 0.23790738 0.782558   1.4137297  2.0762908  2.7295074  3.3343773
 3.864103   4.2992806  4.6089373  4.754928   4.719132   4.5231037
 4.2172594  3.848749   3.4473035  3.042411   2.6615186  2.3198817
 2.0704165  2.0238464  2.1960864  2.4054582  2.4685328  2.3728085
 2.210184   2.12822    2.1670067  2.1383178  1.9140203  1.5932761
 1.3689655  1.337917   1.4239929  1.5205679  1.5903229  1.6410228
 1.680196   1.6928891  1.6523486  1.5509529  1.4255478  1.3457587
 1.3558136  1.4372463  1.54875    1.6633189  1.7510667  1.7596222
 1.6459786  1.4406836  1.227513   1.0545591  0.9273094  0.84151196
 0.7829492  0.7380292  0.7106964  0.7070312  0.70494056 0.65058625
 0.49055666 0.22527687 0.30672336 0.79023147 1.3429792  1.92592
 2.5211859  3.1014972  3.6340003  4.0923605  4.440627   4.626226
 4.6132765  4.4181457  4.1098194  3.7660024  3.4193676  3.0665207
 2.711804   2.3562658  2.0168283  1.8016763  1.8457035  2.062646
 2.2099092  2.187942   2.0413697  1.9565861  2.0374413  2.0531976
 1.8289138  1.4740992  1.2018049  1.1299092  1.188392   1.2630295
 1.3169686  1.3644824  1.4157327  1.4494685  1.4257604  1.3242959
 1.1802546  1.0813707  1.0944685  1.1904615  1.3049375  1.4109716
 1.499133   1.5407915  1.4871324  1.3298903  1.1408215  0.9804515
 0.8468098  0.7279646  0.6226438  0.5356265  0.4810469  0.46988064
 0.48688293 0.4793937  0.3910924  0.25876698 0.40893456 0.8175295
 1.2920977  1.7813314  2.2845848  2.8061018  3.3242333  3.802936
 4.197923   4.4436593  4.4796343  4.299493   3.9739053  3.6202366
 3.3181946  3.0494199  2.7640743  2.446378   2.0888069  1.742568
 1.5701035  1.6728488  1.8600749  1.9449279  1.8750681  1.794493
 1.9009205  1.9726154  1.773777   1.4119668  1.1077219  0.9809075
 0.9836872  1.015461   1.0402993  1.0747627  1.1328933  1.1894946
 1.193652   1.1105936  0.96303904 0.8448802  0.8526423  0.9618224
 1.0845683  1.1840937  1.2614554  1.3117092  1.305904   1.2076031
 1.0531626  0.92145914 0.81759673 0.7078508  0.58500904 0.4684352
 0.38510233 0.34408483 0.34335467 0.36077362 0.34840313 0.33849844
 0.49984735 0.8425353  1.2537825  1.6626556  2.0577104  2.4710524
 2.9263835  3.4013703  3.837867   4.158283   4.279749   4.158929
 3.8360064  3.4387202  3.116218   2.9163725  2.7421296  2.507834
 2.200698   1.8314768  1.4903194  1.3582143  1.4540854  1.5949512
 1.662101   1.635829   1.7454664  1.8789798  1.7357982  1.3924447
 1.0828847  0.91363406 0.8430113  0.8105738  0.79227144 0.79957414
 0.8497225  0.9178594  0.9482609  0.89660424 0.76621354 0.6371054
 0.6277295  0.7366677  0.8647347  0.96564543 1.0383183  1.0839912
 1.097016   1.0522281  0.9404905  0.8361609  0.77982664 0.71927947
 0.6191231  0.49924353 0.40719324 0.35281557 0.31497535 0.31068674
 0.33450195 0.3871089  0.54557276 0.8356087  1.1970521  1.5575079
 1.874382   2.1628683  2.482029   2.8799794  3.322239   3.7096593
 3.9425395  3.943138   3.694304   3.2809548  2.8756878  2.636362
 2.557618   2.46193    2.253643   1.948899   1.5838388  1.2664762
 1.150692   1.184877   1.322782   1.427262   1.5537336  1.744175
 1.695835   1.3911474  1.085891   0.90968364 0.7874855  0.68128
 0.61264986 0.58551973 0.61293316 0.6721325  0.70824105 0.68121874
 0.58006924 0.45130566 0.41780317 0.51473784 0.63449675 0.7295206
 0.8049028  0.85827327 0.8791264  0.8636488  0.8024519  0.72462034
 0.70820224 0.7186214  0.67889297 0.5806501  0.47992644 0.42375344
 0.37893042 0.33768633 0.33929294 0.38951874 0.5238055  0.7708427
 1.0904939  1.4222485  1.7104499  1.9308196  2.1085846  2.3228948
 2.6569695  3.0559454  3.3850133  3.5439696  3.4652047  3.1453397
 2.7001092  2.3286407  2.1785824  2.2061708  2.1720765  1.9817045
 1.6934588  1.3571109  1.0794258  0.9532644 ]
