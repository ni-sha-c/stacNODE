time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.04%, model saved.
Epoch: 0 Train: 169306.62500 Test: 3877.40698
Epoch: 80 Train: 46182.94922 Test: 2060.51123
Epoch: 160 Train: 38282.51562 Test: 1929.13904
Epoch: 240 Train: 37522.79297 Test: 1367.47827
Epoch 320: New minimal relative error: 66.26%, model saved.
Epoch: 320 Train: 36320.59766 Test: 1355.61194
Epoch: 400 Train: 38404.42578 Test: 1520.79077
Epoch: 480 Train: 35938.34766 Test: 1456.61462
Epoch: 560 Train: 39188.91406 Test: 1357.55212
Epoch: 640 Train: 37804.30078 Test: 1450.22815
Epoch: 720 Train: 36224.54297 Test: 1127.48230
Epoch: 800 Train: 40219.21094 Test: 1571.54321
Epoch: 880 Train: 41266.64453 Test: 1642.13330
Epoch: 960 Train: 36919.12109 Test: 1463.21143
Epoch: 1040 Train: 37931.36328 Test: 1364.73071
Epoch: 1120 Train: 34516.00000 Test: 1262.20862
Epoch: 1200 Train: 34999.25391 Test: 1179.61768
Epoch: 1280 Train: 31828.54297 Test: 1083.28479
Epoch: 1360 Train: 25833.90430 Test: 584.69977
Epoch: 1440 Train: 27678.33203 Test: 795.28210
Epoch: 1520 Train: 23823.74023 Test: 730.76556
Epoch: 1600 Train: 28543.92578 Test: 913.94989
Epoch: 1680 Train: 28219.80859 Test: 866.59625
Epoch: 1760 Train: 21997.57812 Test: 544.37073
Epoch: 1840 Train: 24952.39062 Test: 557.84290
Epoch: 1920 Train: 21628.85938 Test: 481.76785
Epoch: 2000 Train: 19240.69922 Test: 384.72754
Epoch: 2080 Train: 13992.21191 Test: 225.15450
Epoch 2160: New minimal relative error: 62.62%, model saved.
Epoch: 2160 Train: 11048.73535 Test: 192.55452
Epoch: 2240 Train: 8082.19336 Test: 135.47702
Epoch: 2320 Train: 3884.27832 Test: 30.18846
Epoch 2400: New minimal relative error: 25.98%, model saved.
Epoch: 2400 Train: 4044.78833 Test: 34.57158
Epoch: 2480 Train: 4135.60742 Test: 45.15208
Epoch: 2560 Train: 4192.19531 Test: 47.27998
Epoch 2640: New minimal relative error: 24.29%, model saved.
Epoch: 2640 Train: 2312.88354 Test: 25.12183
Epoch: 2720 Train: 1883.77136 Test: 24.28694
Epoch: 2800 Train: 1717.58203 Test: 27.32008
Epoch 2880: New minimal relative error: 8.39%, model saved.
Epoch: 2880 Train: 1475.95740 Test: 6.58303
Epoch: 2960 Train: 1433.58118 Test: 7.73926
Epoch 3040: New minimal relative error: 4.94%, model saved.
Epoch: 3040 Train: 1326.53857 Test: 5.81331
Epoch: 3120 Train: 1140.68018 Test: 5.19037
Epoch: 3200 Train: 1098.66138 Test: 4.25488
Epoch: 3280 Train: 1281.59668 Test: 10.99203
Epoch: 3360 Train: 1299.74268 Test: 8.37986
Epoch: 3440 Train: 1482.53894 Test: 16.51414
Epoch: 3520 Train: 1070.39807 Test: 17.66819
Epoch: 3600 Train: 974.77679 Test: 4.82155
Epoch: 3680 Train: 818.41437 Test: 2.57662
Epoch: 3760 Train: 715.51837 Test: 1.94089
Epoch: 3840 Train: 669.52655 Test: 3.02803
Epoch: 3920 Train: 600.28333 Test: 2.18894
Epoch: 4000 Train: 572.26874 Test: 2.61269
Epoch: 4080 Train: 541.42041 Test: 2.58886
Epoch: 4160 Train: 670.16541 Test: 9.78251
Epoch: 4240 Train: 630.38049 Test: 7.82838
Epoch: 4320 Train: 659.05798 Test: 3.73545
Epoch: 4400 Train: 630.40216 Test: 4.51203
Epoch: 4480 Train: 622.00922 Test: 4.12776
Epoch: 4560 Train: 543.41425 Test: 3.16987
Epoch: 4640 Train: 536.72705 Test: 2.07220
Epoch: 4720 Train: 672.38440 Test: 4.07661
Epoch: 4800 Train: 604.72632 Test: 2.53709
Epoch 4880: New minimal relative error: 3.92%, model saved.
Epoch: 4880 Train: 540.90900 Test: 1.47044
Epoch: 4960 Train: 562.14197 Test: 2.23381
Epoch: 5040 Train: 572.52112 Test: 4.18231
Epoch: 5120 Train: 577.97491 Test: 2.55735
Epoch: 5200 Train: 528.46185 Test: 1.70569
Epoch: 5280 Train: 567.47595 Test: 2.47288
Epoch: 5360 Train: 529.14935 Test: 1.96706
Epoch: 5440 Train: 602.93457 Test: 8.21310
Epoch: 5520 Train: 525.02814 Test: 2.01834
Epoch: 5600 Train: 455.52423 Test: 2.79852
Epoch: 5680 Train: 525.08978 Test: 3.90657
Epoch: 5760 Train: 668.65918 Test: 11.50625
Epoch: 5840 Train: 645.46936 Test: 4.41137
Epoch: 5920 Train: 540.19055 Test: 2.39074
Epoch: 6000 Train: 594.55176 Test: 5.71146
Epoch: 6080 Train: 516.04486 Test: 2.81290
Epoch: 6160 Train: 526.18530 Test: 3.72298
Epoch: 6240 Train: 532.18488 Test: 2.09225
Epoch: 6320 Train: 581.24475 Test: 3.43384
Epoch: 6400 Train: 553.07819 Test: 3.06689
Epoch: 6480 Train: 512.74878 Test: 3.59808
Epoch: 6560 Train: 467.55185 Test: 6.19391
Epoch: 6640 Train: 441.45370 Test: 2.13302
Epoch: 6720 Train: 469.59872 Test: 2.79803
Epoch: 6800 Train: 484.43842 Test: 3.15764
Epoch: 6880 Train: 436.08615 Test: 1.57170
Epoch: 6960 Train: 622.21674 Test: 6.80659
Epoch: 7040 Train: 470.09192 Test: 3.70341
Epoch: 7120 Train: 467.42084 Test: 1.51820
Epoch: 7200 Train: 523.21454 Test: 3.06827
Epoch: 7280 Train: 491.28638 Test: 4.03285
Epoch: 7360 Train: 442.03485 Test: 3.94718
Epoch: 7440 Train: 422.69223 Test: 2.56528
Epoch: 7520 Train: 437.21353 Test: 1.47471
Epoch: 7600 Train: 496.51425 Test: 2.84649
Epoch: 7680 Train: 497.76123 Test: 3.53352
Epoch: 7760 Train: 475.96350 Test: 3.17383
Epoch: 7840 Train: 564.80750 Test: 3.45849
Epoch: 7920 Train: 546.80420 Test: 11.56851
Epoch: 7999 Train: 481.40820 Test: 3.83247
Training Loss: tensor(481.4082)
Test Loss: tensor(3.8325)
Learned LE: [ 8.79331946e-01 -1.00669013e-02 -1.45425825e+01]
True LE: [ 8.6636758e-01 -5.3637787e-03 -1.4534098e+01]
Relative Error: [5.8616934 6.331531  6.622113  6.9260993 6.880296  6.768007  6.604252
 6.46673   6.253796  5.9529433 5.765332  5.6493    5.3685513 5.0752416
 4.836228  4.6883383 4.6873198 4.7740808 4.890184  5.0206585 5.0712714
 4.91307   4.8225546 4.825076  4.838862  4.9360495 5.1040244 5.3089523
 5.5645175 5.7543726 6.003895  6.299709  6.3757496 6.375467  6.2186255
 5.8014865 5.3836937 5.0685835 4.8774242 4.8831897 4.7238355 4.504802
 4.4067054 4.344973  4.160198  3.9180882 3.5800402 3.3521738 3.1496427
 2.928769  2.7263641 2.6153653 2.6095061 2.7912989 3.075229  3.3848736
 3.668913  3.9875429 4.2849135 4.5711365 4.6559343 4.97074   5.573807
 6.143888  6.43016   6.662031  6.6153183 6.4983478 6.3024335 6.104225
 5.8667555 5.573488  5.3586373 5.190363  4.9137545 4.661465  4.4680123
 4.32431   4.243673  4.3508735 4.5211735 4.6483707 4.6011357 4.4524126
 4.3868713 4.424544  4.4515758 4.504974  4.7409773 4.999877  5.293899
 5.5015225 5.7678576 6.0352807 6.180345  6.114801  5.935218  5.5081015
 5.035312  4.690929  4.5683813 4.5013366 4.3326044 4.1343946 4.0727606
 3.9574413 3.7865088 3.6000583 3.281535  3.0911534 2.9275312 2.6989796
 2.5553079 2.5319195 2.5496528 2.7104404 2.9879253 3.25333   3.4801762
 3.7787042 4.042322  4.3072433 4.4127984 4.7052755 5.308535  5.905663
 6.178845  6.342945  6.289404  6.181422  5.9967303 5.814752  5.541082
 5.193018  4.979009  4.7400813 4.453488  4.2600536 4.110475  3.9932127
 3.9308774 3.9997156 4.1512713 4.3208427 4.2570972 4.106003  4.014125
 4.072979  4.121063  4.139129  4.3204265 4.682472  4.994886  5.2600093
 5.5186567 5.745613  5.936036  5.841241  5.6581297 5.245788  4.7663302
 4.406823  4.291283  4.147258  3.9562876 3.8264327 3.6969452 3.6455777
 3.4847703 3.3455477 3.0398943 2.862356  2.7383132 2.541938  2.47008
 2.4771943 2.4904172 2.6101305 2.8483028 3.0750806 3.292607  3.5208187
 3.8167517 4.14795   4.287437  4.4934435 4.9822164 5.58651   5.90481
 6.0093846 5.965854  5.8370423 5.652397  5.4817495 5.214048  4.909884
 4.639147  4.346652  4.049871  3.862079  3.7663805 3.767246  3.6870189
 3.6388342 3.7982152 4.0021024 3.9459972 3.8246577 3.7609181 3.8011854
 3.8348563 3.8616788 3.9864738 4.2869267 4.682723  5.0129876 5.3070836
 5.477254  5.686747  5.612817  5.412369  5.011073  4.580777  4.2123103
 4.0593038 3.8574154 3.6316655 3.5773091 3.4041417 3.3757672 3.2608378
 3.1644027 2.939473  2.7691443 2.6432784 2.4429762 2.3756728 2.3921137
 2.4245887 2.4949658 2.6697938 2.8574383 3.0515335 3.3473487 3.6511426
 4.094889  4.198305  4.4008527 4.660776  5.1714034 5.559536  5.632953
 5.6356554 5.5107713 5.319439  5.134672  4.8102427 4.508348  4.235258
 3.8407867 3.602903  3.4230437 3.4574564 3.4917529 3.4451132 3.3628392
 3.4952972 3.7321706 3.6789472 3.571625  3.5381243 3.6002371 3.6575363
 3.6358376 3.725593  3.8917634 4.2654505 4.7505183 5.145316  5.1320324
 5.1973033 5.215812  5.0373187 4.7996974 4.383897  4.111994  3.8961575
 3.6644788 3.4209967 3.3676643 3.2009146 3.2028158 3.1215358 3.0737693
 2.937172  2.8280747 2.7084363 2.4333467 2.3170373 2.353625  2.436181
 2.4878342 2.57532   2.6726012 2.893913  3.1434867 3.456992  3.9628282
 4.04791   4.242752  4.400398  4.710893  5.115575  5.2350583 5.23424
 5.160579  4.979753  4.6688666 4.332719  4.0490136 3.8574018 3.4997978
 3.1726832 2.9669929 2.931351  2.9568145 3.0419388 3.0176277 3.1528282
 3.3968077 3.4559853 3.3668585 3.314099  3.3970854 3.4277842 3.4213598
 3.394175  3.5945072 3.8953352 4.3712745 4.673465  4.729174  4.711052
 4.851802  4.620854  4.365285  4.103632  3.9387994 3.8465564 3.5973482
 3.3274028 3.2281895 3.1059837 3.0650651 2.998469  2.971258  2.927771
 2.901713  2.771022  2.492114  2.3691716 2.424549  2.5677812 2.6393757
 2.6527991 2.6601021 2.7927604 2.9777513 3.1725347 3.6309    3.817798
 3.9465911 4.1075377 4.2671313 4.5249963 4.8404646 4.8621373 4.7437353
 4.5124087 4.238718  3.9872284 3.6802344 3.5643008 3.265034  2.9488745
 2.6517022 2.402508  2.33206   2.3800654 2.483572  2.6745474 3.011949
 3.1429317 3.0739841 3.0618212 3.0171432 3.148532  3.216229  3.160186
 3.2261896]
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.04%, model saved.
Epoch: 0 Train: 169306.62500 Test: 3877.40698
Epoch: 80 Train: 46182.94922 Test: 2060.51123
Epoch: 160 Train: 38282.51562 Test: 1929.13904
Epoch: 240 Train: 37522.79297 Test: 1367.47827
Epoch 320: New minimal relative error: 66.26%, model saved.
Epoch: 320 Train: 36320.59766 Test: 1355.61194
Epoch: 400 Train: 38404.42578 Test: 1520.79077
Epoch: 480 Train: 35938.34766 Test: 1456.61462
Epoch: 560 Train: 39188.91406 Test: 1357.55212
Epoch: 640 Train: 37804.30078 Test: 1450.22815
Epoch: 720 Train: 36224.54297 Test: 1127.48230
Epoch: 800 Train: 40219.21094 Test: 1571.54321
Epoch: 880 Train: 41266.64453 Test: 1642.13330
Epoch: 960 Train: 36919.12109 Test: 1463.21143
Epoch: 1040 Train: 37931.36328 Test: 1364.73071
Epoch: 1120 Train: 34516.00000 Test: 1262.20862
Epoch: 1200 Train: 34999.25391 Test: 1179.61768
Epoch: 1280 Train: 31828.54297 Test: 1083.28479
Epoch: 1360 Train: 25833.90430 Test: 584.69977
Epoch: 1440 Train: 27678.33203 Test: 795.28210
Epoch: 1520 Train: 23823.74023 Test: 730.76556
Epoch: 1600 Train: 28543.92578 Test: 913.94989
Epoch: 1680 Train: 28219.80859 Test: 866.59625
Epoch: 1760 Train: 21997.57812 Test: 544.37073
Epoch: 1840 Train: 24952.39062 Test: 557.84290
Epoch: 1920 Train: 21628.85938 Test: 481.76785
Epoch: 2000 Train: 19240.69922 Test: 384.72754
Epoch: 2080 Train: 13992.21191 Test: 225.15450
Epoch 2160: New minimal relative error: 62.62%, model saved.
Epoch: 2160 Train: 11048.73535 Test: 192.55452
Epoch: 2240 Train: 8082.19336 Test: 135.47702
Epoch: 2320 Train: 3884.27832 Test: 30.18846
Epoch 2400: New minimal relative error: 25.98%, model saved.
Epoch: 2400 Train: 4044.78833 Test: 34.57158
Epoch: 2480 Train: 4135.60742 Test: 45.15208
Epoch: 2560 Train: 4192.19531 Test: 47.27998
Epoch 2640: New minimal relative error: 24.29%, model saved.
Epoch: 2640 Train: 2312.88354 Test: 25.12183
Epoch: 2720 Train: 1883.77136 Test: 24.28694
Epoch: 2800 Train: 1717.58203 Test: 27.32008
Epoch 2880: New minimal relative error: 8.39%, model saved.
Epoch: 2880 Train: 1475.95740 Test: 6.58303
Epoch: 2960 Train: 1433.58118 Test: 7.73926
Epoch 3040: New minimal relative error: 4.94%, model saved.
Epoch: 3040 Train: 1326.53857 Test: 5.81331
Epoch: 3120 Train: 1140.68018 Test: 5.19037
Epoch: 3200 Train: 1098.66138 Test: 4.25488
Epoch: 3280 Train: 1281.59668 Test: 10.99203
Epoch: 3360 Train: 1299.74268 Test: 8.37986
Epoch: 3440 Train: 1482.53894 Test: 16.51414
Epoch: 3520 Train: 1070.39807 Test: 17.66819
Epoch: 3600 Train: 974.77679 Test: 4.82155
Epoch: 3680 Train: 818.41437 Test: 2.57662
Epoch: 3760 Train: 715.51837 Test: 1.94089
Epoch: 3840 Train: 669.52655 Test: 3.02803
Epoch: 3920 Train: 600.28333 Test: 2.18894
Epoch: 4000 Train: 572.26874 Test: 2.61269
Epoch: 4080 Train: 541.42041 Test: 2.58886
Epoch: 4160 Train: 670.16541 Test: 9.78251
Epoch: 4240 Train: 630.38049 Test: 7.82838
Epoch: 4320 Train: 659.05798 Test: 3.73545
Epoch: 4400 Train: 630.40216 Test: 4.51203
Epoch: 4480 Train: 622.00922 Test: 4.12776
Epoch: 4560 Train: 543.41425 Test: 3.16987
Epoch: 4640 Train: 536.72705 Test: 2.07220
Epoch: 4720 Train: 672.38440 Test: 4.07661
Epoch: 4800 Train: 604.72632 Test: 2.53709
Epoch 4880: New minimal relative error: 3.92%, model saved.
Epoch: 4880 Train: 540.90900 Test: 1.47044
Epoch: 4960 Train: 562.14197 Test: 2.23381
Epoch: 5040 Train: 572.52112 Test: 4.18231
Epoch: 5120 Train: 577.97491 Test: 2.55735
Epoch: 5200 Train: 528.46185 Test: 1.70569
Epoch: 5280 Train: 567.47595 Test: 2.47288
Epoch: 5360 Train: 529.14935 Test: 1.96706
Epoch: 5440 Train: 602.93457 Test: 8.21310
Epoch: 5520 Train: 525.02814 Test: 2.01834
Epoch: 5600 Train: 455.52423 Test: 2.79852
Epoch: 5680 Train: 525.08978 Test: 3.90657
Epoch: 5760 Train: 668.65918 Test: 11.50625
Epoch: 5840 Train: 645.46936 Test: 4.41137
Epoch: 5920 Train: 540.19055 Test: 2.39074
Epoch: 6000 Train: 594.55176 Test: 5.71146
Epoch: 6080 Train: 516.04486 Test: 2.81290
Epoch: 6160 Train: 526.18530 Test: 3.72298
Epoch: 6240 Train: 532.18488 Test: 2.09225
Epoch: 6320 Train: 581.24475 Test: 3.43384
Epoch: 6400 Train: 553.07819 Test: 3.06689
Epoch: 6480 Train: 512.74878 Test: 3.59808
Epoch: 6560 Train: 467.55185 Test: 6.19391
Epoch: 6640 Train: 441.45370 Test: 2.13302
Epoch: 6720 Train: 469.59872 Test: 2.79803
Epoch: 6800 Train: 484.43842 Test: 3.15764
Epoch: 6880 Train: 436.08615 Test: 1.57170
Epoch: 6960 Train: 622.21674 Test: 6.80659
Epoch: 7040 Train: 470.09192 Test: 3.70341
Epoch: 7120 Train: 467.42084 Test: 1.51820
Epoch: 7200 Train: 523.21454 Test: 3.06827
Epoch: 7280 Train: 491.28638 Test: 4.03285
Epoch: 7360 Train: 442.03485 Test: 3.94718
Epoch: 7440 Train: 422.69223 Test: 2.56528
Epoch: 7520 Train: 437.21353 Test: 1.47471
Epoch: 7600 Train: 496.51425 Test: 2.84649
Epoch: 7680 Train: 497.76123 Test: 3.53352
Epoch: 7760 Train: 475.96350 Test: 3.17383
Epoch: 7840 Train: 564.80750 Test: 3.45849
Epoch: 7920 Train: 546.80420 Test: 11.56851
Epoch: 7999 Train: 481.40820 Test: 3.83247
Training Loss: tensor(481.4082)
Test Loss: tensor(3.8325)
Learned LE: [ 8.79331946e-01 -1.00669013e-02 -1.45425825e+01]
True LE: [ 8.6636758e-01 -5.3637787e-03 -1.4534098e+01]
Relative Error: [5.8616934 6.331531  6.622113  6.9260993 6.880296  6.768007  6.604252
 6.46673   6.253796  5.9529433 5.765332  5.6493    5.3685513 5.0752416
 4.836228  4.6883383 4.6873198 4.7740808 4.890184  5.0206585 5.0712714
 4.91307   4.8225546 4.825076  4.838862  4.9360495 5.1040244 5.3089523
 5.5645175 5.7543726 6.003895  6.299709  6.3757496 6.375467  6.2186255
 5.8014865 5.3836937 5.0685835 4.8774242 4.8831897 4.7238355 4.504802
 4.4067054 4.344973  4.160198  3.9180882 3.5800402 3.3521738 3.1496427
 2.928769  2.7263641 2.6153653 2.6095061 2.7912989 3.075229  3.3848736
 3.668913  3.9875429 4.2849135 4.5711365 4.6559343 4.97074   5.573807
 6.143888  6.43016   6.662031  6.6153183 6.4983478 6.3024335 6.104225
 5.8667555 5.573488  5.3586373 5.190363  4.9137545 4.661465  4.4680123
 4.32431   4.243673  4.3508735 4.5211735 4.6483707 4.6011357 4.4524126
 4.3868713 4.424544  4.4515758 4.504974  4.7409773 4.999877  5.293899
 5.5015225 5.7678576 6.0352807 6.180345  6.114801  5.935218  5.5081015
 5.035312  4.690929  4.5683813 4.5013366 4.3326044 4.1343946 4.0727606
 3.9574413 3.7865088 3.6000583 3.281535  3.0911534 2.9275312 2.6989796
 2.5553079 2.5319195 2.5496528 2.7104404 2.9879253 3.25333   3.4801762
 3.7787042 4.042322  4.3072433 4.4127984 4.7052755 5.308535  5.905663
 6.178845  6.342945  6.289404  6.181422  5.9967303 5.814752  5.541082
 5.193018  4.979009  4.7400813 4.453488  4.2600536 4.110475  3.9932127
 3.9308774 3.9997156 4.1512713 4.3208427 4.2570972 4.106003  4.014125
 4.072979  4.121063  4.139129  4.3204265 4.682472  4.994886  5.2600093
 5.5186567 5.745613  5.936036  5.841241  5.6581297 5.245788  4.7663302
 4.406823  4.291283  4.147258  3.9562876 3.8264327 3.6969452 3.6455777
 3.4847703 3.3455477 3.0398943 2.862356  2.7383132 2.541938  2.47008
 2.4771943 2.4904172 2.6101305 2.8483028 3.0750806 3.292607  3.5208187
 3.8167517 4.14795   4.287437  4.4934435 4.9822164 5.58651   5.90481
 6.0093846 5.965854  5.8370423 5.652397  5.4817495 5.214048  4.909884
 4.639147  4.346652  4.049871  3.862079  3.7663805 3.767246  3.6870189
 3.6388342 3.7982152 4.0021024 3.9459972 3.8246577 3.7609181 3.8011854
 3.8348563 3.8616788 3.9864738 4.2869267 4.682723  5.0129876 5.3070836
 5.477254  5.686747  5.612817  5.412369  5.011073  4.580777  4.2123103
 4.0593038 3.8574154 3.6316655 3.5773091 3.4041417 3.3757672 3.2608378
 3.1644027 2.939473  2.7691443 2.6432784 2.4429762 2.3756728 2.3921137
 2.4245887 2.4949658 2.6697938 2.8574383 3.0515335 3.3473487 3.6511426
 4.094889  4.198305  4.4008527 4.660776  5.1714034 5.559536  5.632953
 5.6356554 5.5107713 5.319439  5.134672  4.8102427 4.508348  4.235258
 3.8407867 3.602903  3.4230437 3.4574564 3.4917529 3.4451132 3.3628392
 3.4952972 3.7321706 3.6789472 3.571625  3.5381243 3.6002371 3.6575363
 3.6358376 3.725593  3.8917634 4.2654505 4.7505183 5.145316  5.1320324
 5.1973033 5.215812  5.0373187 4.7996974 4.383897  4.111994  3.8961575
 3.6644788 3.4209967 3.3676643 3.2009146 3.2028158 3.1215358 3.0737693
 2.937172  2.8280747 2.7084363 2.4333467 2.3170373 2.353625  2.436181
 2.4878342 2.57532   2.6726012 2.893913  3.1434867 3.456992  3.9628282
 4.04791   4.242752  4.400398  4.710893  5.115575  5.2350583 5.23424
 5.160579  4.979753  4.6688666 4.332719  4.0490136 3.8574018 3.4997978
 3.1726832 2.9669929 2.931351  2.9568145 3.0419388 3.0176277 3.1528282
 3.3968077 3.4559853 3.3668585 3.314099  3.3970854 3.4277842 3.4213598
 3.394175  3.5945072 3.8953352 4.3712745 4.673465  4.729174  4.711052
 4.851802  4.620854  4.365285  4.103632  3.9387994 3.8465564 3.5973482
 3.3274028 3.2281895 3.1059837 3.0650651 2.998469  2.971258  2.927771
 2.901713  2.771022  2.492114  2.3691716 2.424549  2.5677812 2.6393757
 2.6527991 2.6601021 2.7927604 2.9777513 3.1725347 3.6309    3.817798
 3.9465911 4.1075377 4.2671313 4.5249963 4.8404646 4.8621373 4.7437353
 4.5124087 4.238718  3.9872284 3.6802344 3.5643008 3.265034  2.9488745
 2.6517022 2.402508  2.33206   2.3800654 2.483572  2.6745474 3.011949
 3.1429317 3.0739841 3.0618212 3.0171432 3.148532  3.216229  3.160186
 3.2261896]
