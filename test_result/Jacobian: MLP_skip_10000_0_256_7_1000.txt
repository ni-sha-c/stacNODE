time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.32%, model saved.
Epoch: 0 Train: 59678.57031 Test: 3810.32520
Epoch: 100 Train: 15778.45508 Test: 1488.04590
Epoch: 200 Train: 14797.56348 Test: 1494.87231
Epoch 300: New minimal relative error: 92.03%, model saved.
Epoch: 300 Train: 14170.05859 Test: 1308.74841
Epoch: 400 Train: 13854.62988 Test: 1084.28394
Epoch: 500 Train: 13184.77637 Test: 1202.10962
Epoch: 600 Train: 14674.42480 Test: 1277.38623
Epoch 700: New minimal relative error: 78.49%, model saved.
Epoch: 700 Train: 10157.40625 Test: 750.64832
Epoch: 800 Train: 9486.17188 Test: 622.35260
Epoch: 900 Train: 8768.66016 Test: 604.72662
Epoch: 1000 Train: 7809.14600 Test: 470.55743
Epoch: 1100 Train: 5444.62891 Test: 238.83667
Epoch: 1200 Train: 3328.54517 Test: 137.69362
Epoch 1300: New minimal relative error: 31.37%, model saved.
Epoch: 1300 Train: 1852.87878 Test: 42.40490
Epoch: 1400 Train: 1259.42542 Test: 20.45526
Epoch: 1500 Train: 1339.07568 Test: 39.82309
Epoch 1600: New minimal relative error: 17.62%, model saved.
Epoch: 1600 Train: 1057.80481 Test: 20.76101
Epoch: 1700 Train: 863.19489 Test: 18.13265
Epoch: 1800 Train: 796.95233 Test: 22.33888
Epoch: 1900 Train: 697.19720 Test: 19.87706
Epoch 2000: New minimal relative error: 17.21%, model saved.
Epoch: 2000 Train: 659.67236 Test: 25.37898
Epoch 2100: New minimal relative error: 9.83%, model saved.
Epoch: 2100 Train: 524.86584 Test: 11.19643
Epoch: 2200 Train: 813.85406 Test: 62.93061
Epoch: 2300 Train: 521.40479 Test: 7.89059
Epoch 2400: New minimal relative error: 9.19%, model saved.
Epoch: 2400 Train: 479.61359 Test: 8.21724
Epoch: 2500 Train: 481.48944 Test: 26.89779
Epoch: 2600 Train: 363.58490 Test: 3.81376
Epoch: 2700 Train: 452.17426 Test: 5.44915
Epoch: 2800 Train: 353.59784 Test: 4.08406
Epoch: 2900 Train: 296.71661 Test: 4.41939
Epoch: 3000 Train: 266.41144 Test: 2.79467
Epoch: 3100 Train: 227.63123 Test: 1.41470
Epoch: 3200 Train: 230.41129 Test: 1.50585
Epoch: 3300 Train: 246.24014 Test: 1.83583
Epoch: 3400 Train: 225.46408 Test: 1.79677
Epoch: 3500 Train: 242.25742 Test: 2.62690
Epoch 3600: New minimal relative error: 8.80%, model saved.
Epoch: 3600 Train: 211.57512 Test: 1.39500
Epoch 3700: New minimal relative error: 8.44%, model saved.
Epoch: 3700 Train: 205.73404 Test: 1.40687
Epoch: 3800 Train: 213.92844 Test: 1.66152
Epoch 3900: New minimal relative error: 5.89%, model saved.
Epoch: 3900 Train: 182.44278 Test: 1.21140
Epoch 4000: New minimal relative error: 5.05%, model saved.
Epoch: 4000 Train: 170.04211 Test: 0.85142
Epoch: 4100 Train: 179.56679 Test: 1.34413
Epoch 4200: New minimal relative error: 1.64%, model saved.
Epoch: 4200 Train: 164.77208 Test: 0.79650
Epoch: 4300 Train: 153.11981 Test: 2.01160
Epoch: 4400 Train: 166.07576 Test: 8.75150
Epoch: 4500 Train: 149.16682 Test: 0.87263
Epoch: 4600 Train: 157.28072 Test: 1.71314
Epoch: 4700 Train: 184.70111 Test: 1.86395
Epoch: 4800 Train: 152.97365 Test: 1.17519
Epoch: 4900 Train: 155.93846 Test: 0.93125
Epoch: 5000 Train: 147.33978 Test: 2.16919
Epoch: 5100 Train: 155.09778 Test: 1.55518
Epoch: 5200 Train: 145.81386 Test: 1.14163
Epoch: 5300 Train: 165.28896 Test: 6.34544
Epoch: 5400 Train: 131.45285 Test: 0.67912
Epoch: 5500 Train: 127.36698 Test: 1.23181
Epoch: 5600 Train: 139.15614 Test: 0.90328
Epoch: 5700 Train: 151.61986 Test: 1.10309
Epoch: 5800 Train: 139.69118 Test: 0.92297
Epoch: 5900 Train: 144.31262 Test: 5.06242
Epoch: 6000 Train: 130.48836 Test: 0.81832
Epoch: 6100 Train: 125.97831 Test: 1.04686
Epoch: 6200 Train: 124.72514 Test: 1.04491
Epoch: 6300 Train: 127.58519 Test: 0.71867
Epoch: 6400 Train: 151.89003 Test: 5.59555
Epoch: 6500 Train: 126.65510 Test: 0.57679
Epoch: 6600 Train: 125.78909 Test: 0.58564
Epoch: 6700 Train: 123.30585 Test: 0.55996
Epoch: 6800 Train: 125.35748 Test: 0.61948
Epoch: 6900 Train: 134.20110 Test: 0.83115
Epoch: 7000 Train: 190.35451 Test: 1.54533
Epoch: 7100 Train: 159.22182 Test: 0.96658
Epoch: 7200 Train: 177.62038 Test: 1.21614
Epoch: 7300 Train: 165.69141 Test: 3.88957
Epoch: 7400 Train: 193.70250 Test: 1.53196
Epoch: 7500 Train: 206.51102 Test: 1.48702
Epoch: 7600 Train: 189.15022 Test: 1.67034
Epoch: 7700 Train: 175.51813 Test: 1.18593
Epoch: 7800 Train: 192.74493 Test: 1.63171
Epoch: 7900 Train: 164.02554 Test: 2.84393
Epoch: 8000 Train: 163.01428 Test: 1.11741
Epoch: 8100 Train: 180.50470 Test: 1.56132
Epoch: 8200 Train: 152.52806 Test: 0.81099
Epoch: 8300 Train: 147.29997 Test: 0.77458
Epoch: 8400 Train: 180.60121 Test: 1.27379
Epoch: 8500 Train: 158.93927 Test: 1.12745
Epoch: 8600 Train: 160.71823 Test: 1.00322
Epoch: 8700 Train: 154.02481 Test: 0.98905
Epoch: 8800 Train: 124.77721 Test: 0.67368
Epoch: 8900 Train: 133.26059 Test: 1.03661
Epoch: 9000 Train: 141.23805 Test: 0.86824
Epoch: 9100 Train: 118.02421 Test: 0.57032
Epoch: 9200 Train: 130.70186 Test: 0.80701
Epoch: 9300 Train: 162.77336 Test: 1.19101
Epoch: 9400 Train: 148.40482 Test: 0.86100
Epoch: 9500 Train: 179.51149 Test: 1.55088
Epoch: 9600 Train: 147.46219 Test: 0.74774
Epoch: 9700 Train: 137.72684 Test: 0.89248
Epoch: 9800 Train: 166.59189 Test: 1.19442
Epoch: 9900 Train: 155.54523 Test: 1.02852
Epoch: 9999 Train: 129.49797 Test: 0.57358
Training Loss: tensor(129.4980)
Test Loss: tensor(0.5736)
Learned LE: [  0.89332956  -0.03008504 -14.517725  ]
True LE: [ 8.70241880e-01  3.42352944e-03 -1.45431595e+01]
Relative Error: [ 5.3398123  5.4971857  5.7863398  6.1205463  6.4761505  6.9293737
  7.523763   8.269515   8.600494   8.777729   8.9949045  9.47794
 10.257605  11.307757  12.600054  14.107106  14.681585  15.194859
 15.222742  14.141115  13.159134  12.376556  11.911306  11.646449
 11.134844  10.213555   9.199733   8.523415   7.9274073  7.2546353
  6.810741   6.6713734  6.8616724  7.4618316  8.710441  10.219214
 11.255171  12.404617  13.113417  13.053571  11.99855   10.358461
  8.809341   7.710801   6.7110686  6.135503   5.893271   5.865699
  5.8472915  5.736833   5.6477747  5.5276184  5.708293   5.967083
  5.9674225  5.940862   5.899454   5.7024393  5.3508263  5.072366
  4.9215527  4.7738175  4.6545315  4.7577662  5.2156076  5.7469964
  6.04372    6.4217887  6.948589   7.3096457  7.849411   8.007454
  8.198033   8.6801     9.468758  10.5313015 11.829859  13.348554
 13.796331  14.3829365 14.474339  13.273631  12.297404  11.542394
 11.007771  10.596117   9.771598   8.935439   7.908655   7.1144834
  6.2005897  5.4912987  4.9732003  4.7568936  4.860574   5.4839516
  6.6492796  8.200475   9.812542  11.1305485 12.375883  12.256026
 11.4609585  9.751336   8.17658    6.806119   5.845908   5.337633
  5.219255   5.301933   5.2388234  5.206623   5.1326365  5.0329857
  5.158607   5.4794445  5.637679   5.5544004  5.4398417  5.2948895
  4.85734    4.4522734  4.176686   4.091513   4.0429688  4.164639
  4.6223426  5.265915   5.6795177  5.994971   6.4860187  6.620966
  7.1941605  7.2712965  7.4341946  7.9216676  8.706019   9.743321
 11.018755  12.522973  12.92384   13.517326  13.727087  12.542164
 11.573419  10.762617  10.16352    9.277754   8.459449   7.704892
  6.8535967  5.718788   4.769272   4.0274906  3.4461792  3.1469848
  3.1443582  3.717427   4.7338786  6.1668744  7.976642   9.6616745
 11.244308  11.41535   11.069515   9.296476   7.4761415  6.0213056
  5.1036725  4.662792   4.5924225  4.671892   4.7239137  4.7731686
  4.6718163  4.5196238  4.6231313  4.7941165  4.9951153  5.054684
  4.9548197  4.849314   4.475711   3.940437   3.5356927  3.3371058
  3.4336023  3.662289   4.0851626  4.6311364  5.2031713  5.5584455
  5.9189405  5.9945674  6.5249376  6.7567706  6.780582   7.2246046
  7.954056   8.948234  10.1902485 11.6484    12.070903  12.643758
 13.130018  11.875375  10.837869  10.035229   9.290836   7.9994826
  7.1790586  6.719522   6.0036907  4.6875644  3.6809676  2.907886
  2.3420155  2.013976   1.9328363  2.2675033  2.9853237  4.1701035
  5.8857894  8.12885    9.723739  10.633103  10.599666   8.86536
  6.8830347  5.3781776  4.408037   4.0253286  4.0507975  4.1050496
  4.102643   4.120179   4.103912   4.0280986  4.2180753  4.306047
  4.4415436  4.4652996  4.3861623  4.1880918  4.0199986  3.5407748
  3.0037706  2.691444   2.758706   3.2020106  3.6073153  4.0441694
  4.519324   5.106835   5.359029   5.3369417  5.777122   6.2463512
  6.340724   6.5473957  7.1919312  8.113246   9.279948  10.681588
 11.227584  11.75496   12.462382  11.292822  10.24473    9.44952
  8.474771   7.0609636  6.0944867  5.5405235  4.9916854  4.0284915
  2.9806178  2.262517   1.8495545  1.7075515  1.7341852  1.6350234
  1.6608735  2.340657   3.8337178  5.932785   8.15193    9.549968
  9.797237   8.512225   6.4027495  4.843776   3.8682857  3.4318812
  3.4135778  3.2508247  3.2857378  3.393992   3.4633803  3.4473166
  3.7326927  4.0221767  4.0221562  3.9626024  3.843075   3.6203969
  3.3305435  3.0091486  2.5698862  2.183077   2.3052762  2.605002
  3.1340263  3.5567057  3.9377491  4.390101   4.937825   4.815166
  5.166479   5.762655   5.7950015  5.9847307  6.5153584  7.2904177
  8.349746   9.668293  10.417442  10.865288  11.480088  10.90883
  9.817153   8.990863   7.9089956  6.45862    5.2774534  4.629168
  3.9848514  3.367731   2.6472833  2.0860226  1.9448168  2.1222663
  2.390894   2.166276   1.5747057  1.0342408  1.8690605  3.7312827
  5.993936   8.128999   8.751566   8.413211   6.2607207  4.5118237
  3.3825862  2.7820983  2.556501   2.5137854  2.6682117  2.793315
  2.83182    2.7794313  3.0520942  3.4848888  3.7134833  3.6511686
  3.368947   3.096801   2.749539   2.390992   1.953766   1.8989748
  1.9994704  2.1374078  2.501174   3.0169783  3.4639783  3.7676005
  4.2613964  4.5558434  4.731368   5.1442657  5.5391645  5.37761
  5.74935    6.5286317  7.4401855  8.620861   9.648986   9.93475
 10.444292  10.641004   9.509936   8.691986   7.622744   6.0997906
  4.885431   3.9947495  3.1820087  2.7590806]
