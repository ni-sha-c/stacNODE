time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.84%, model saved.
Epoch: 0 Train: 31786.53320 Test: 4513.42773
Epoch: 80 Train: 5101.46582 Test: 837.33148
Epoch 160: New minimal relative error: 37.26%, model saved.
Epoch: 160 Train: 261.21707 Test: 69.62347
Epoch: 240 Train: 192.05148 Test: 143.02692
Epoch 320: New minimal relative error: 30.74%, model saved.
Epoch: 320 Train: 145.29349 Test: 57.35028
Epoch 400: New minimal relative error: 14.08%, model saved.
Epoch: 400 Train: 23.88109 Test: 15.60583
Epoch: 480 Train: 117.00960 Test: 22.81149
Epoch 560: New minimal relative error: 9.30%, model saved.
Epoch: 560 Train: 64.00391 Test: 7.73653
Epoch: 640 Train: 15.68811 Test: 1.44760
Epoch: 720 Train: 5.49086 Test: 3.90481
Epoch 800: New minimal relative error: 5.91%, model saved.
Epoch: 800 Train: 3.51472 Test: 4.35400
Epoch: 880 Train: 28.58229 Test: 7.08936
Epoch: 960 Train: 21.04601 Test: 5.58212
Epoch: 1040 Train: 6.01165 Test: 11.20905
Epoch 1120: New minimal relative error: 3.10%, model saved.
Epoch: 1120 Train: 2.36770 Test: 0.49826
Epoch: 1200 Train: 16.35413 Test: 1.78889
Epoch: 1280 Train: 1.48680 Test: 1.31375
Epoch: 1360 Train: 9.81127 Test: 4.71778
Epoch: 1440 Train: 75.08698 Test: 30.56521
Epoch: 1520 Train: 0.95835 Test: 0.07495
Epoch 1600: New minimal relative error: 2.34%, model saved.
Epoch: 1600 Train: 0.70329 Test: 0.11994
Epoch: 1680 Train: 24.79662 Test: 11.96426
Epoch: 1760 Train: 5.49706 Test: 4.85127
Epoch: 1840 Train: 2.73460 Test: 1.09026
Epoch: 1920 Train: 3.85643 Test: 0.46621
Epoch: 2000 Train: 16.21839 Test: 7.01273
Epoch: 2080 Train: 0.38143 Test: 0.00499
Epoch: 2160 Train: 4.58011 Test: 1.09540
Epoch: 2240 Train: 6.68161 Test: 5.87221
Epoch: 2320 Train: 3.24397 Test: 1.44293
Epoch 2400: New minimal relative error: 0.33%, model saved.
Epoch: 2400 Train: 0.31519 Test: 0.00576
Epoch: 2480 Train: 13.07129 Test: 4.40471
Epoch: 2560 Train: 14.13050 Test: 6.21869
Epoch: 2640 Train: 3.48073 Test: 1.79845
Epoch: 2720 Train: 1.38578 Test: 0.22746
Epoch: 2800 Train: 6.08291 Test: 2.02441
Epoch: 2880 Train: 1.60559 Test: 0.65827
Epoch: 2960 Train: 7.55121 Test: 2.35916
Epoch: 3040 Train: 8.95148 Test: 3.13228
Epoch: 3120 Train: 5.20398 Test: 2.44440
Epoch: 3200 Train: 4.16557 Test: 2.92097
Epoch: 3280 Train: 1.20903 Test: 0.15314
Epoch: 3360 Train: 1.62076 Test: 0.79704
Epoch: 3440 Train: 8.70954 Test: 3.69870
Epoch: 3520 Train: 2.17750 Test: 0.79812
Epoch: 3600 Train: 1.17358 Test: 0.37096
Epoch: 3680 Train: 0.65932 Test: 0.29557
Epoch: 3760 Train: 1.51424 Test: 0.86761
Epoch: 3840 Train: 2.03906 Test: 0.69768
Epoch: 3920 Train: 0.23688 Test: 0.10290
Epoch: 4000 Train: 1.65524 Test: 0.48511
Epoch: 4080 Train: 3.15659 Test: 1.24133
Epoch: 4160 Train: 4.64577 Test: 1.59516
Epoch: 4240 Train: 0.35970 Test: 0.09764
Epoch: 4320 Train: 2.53379 Test: 0.95419
Epoch: 4400 Train: 2.99728 Test: 1.37413
Epoch: 4480 Train: 1.00327 Test: 0.35977
Epoch: 4560 Train: 0.37664 Test: 0.10247
Epoch: 4640 Train: 0.84421 Test: 0.28749
Epoch: 4720 Train: 1.21860 Test: 0.55771
Epoch: 4800 Train: 1.12270 Test: 0.39984
Epoch: 4880 Train: 10.01450 Test: 4.06776
Epoch: 4960 Train: 7.55862 Test: 2.52968
Epoch: 5040 Train: 1.19936 Test: 0.45648
Epoch: 5120 Train: 1.21040 Test: 0.53679
Epoch: 5200 Train: 1.25444 Test: 0.44746
Epoch: 5280 Train: 1.18946 Test: 0.41226
Epoch: 5360 Train: 0.90047 Test: 0.36190
Epoch: 5440 Train: 0.64199 Test: 0.18490
Epoch: 5520 Train: 1.55502 Test: 0.79086
Epoch: 5600 Train: 7.82601 Test: 4.03582
Epoch: 5680 Train: 0.23928 Test: 0.09201
Epoch: 5760 Train: 1.49525 Test: 0.71146
Epoch: 5840 Train: 0.20253 Test: 0.02799
Epoch: 5920 Train: 0.27335 Test: 0.11132
Epoch: 6000 Train: 0.14583 Test: 0.03303
Epoch: 6080 Train: 0.12249 Test: 0.01496
Epoch: 6160 Train: 0.18760 Test: 0.04808
Epoch: 6240 Train: 0.49915 Test: 0.19732
Epoch: 6320 Train: 1.08000 Test: 0.34514
Epoch: 6400 Train: 0.63369 Test: 0.34186
Epoch: 6480 Train: 0.29035 Test: 0.07979
Epoch: 6560 Train: 0.22038 Test: 0.07327
Epoch: 6640 Train: 0.62063 Test: 0.05702
Epoch: 6720 Train: 0.14139 Test: 0.05285
Epoch: 6800 Train: 0.30009 Test: 0.10722
Epoch: 6880 Train: 1.65770 Test: 0.80714
Epoch: 6960 Train: 0.84090 Test: 0.26343
Epoch: 7040 Train: 1.38668 Test: 0.52819
Epoch: 7120 Train: 0.89087 Test: 0.36920
Epoch: 7200 Train: 0.38059 Test: 0.14129
Epoch: 7280 Train: 0.08902 Test: 0.01864
Epoch: 7360 Train: 0.08401 Test: 0.01392
Epoch: 7440 Train: 0.24601 Test: 0.08257
Epoch: 7520 Train: 0.98483 Test: 0.28068
Epoch: 7600 Train: 0.98773 Test: 0.48201
Epoch: 7680 Train: 0.07932 Test: 0.01702
Epoch: 7760 Train: 0.06704 Test: 0.00939
Epoch: 7840 Train: 2.94503 Test: 1.11876
Epoch: 7920 Train: 0.04976 Test: 0.00313
Epoch: 7999 Train: 0.10633 Test: 0.03004
Training Loss: tensor(0.1063)
Test Loss: tensor(0.0300)
Learned LE: [ 8.6531222e-01 -5.7102214e-03 -1.4552646e+01]
True LE: [ 8.6880475e-01  2.3839078e-03 -1.4543838e+01]
Relative Error: [0.24957421 0.2235457  0.19699535 0.17835136 0.17697003 0.19263011
 0.21798201 0.2450534  0.26741675 0.2812617  0.28592464 0.28370953
 0.27750307 0.2691543  0.25967675 0.2492254  0.23790222 0.22556673
 0.21294914 0.20013566 0.18728471 0.17563957 0.1682224  0.1671701
 0.17546323 0.19629565 0.21253945 0.20904325 0.20775548 0.22717078
 0.24872364 0.2442042  0.21534951 0.1898387  0.19517215 0.23363824
 0.28698167 0.32291874 0.32688946 0.3128133  0.299229   0.29431438
 0.29773024 0.30497873 0.3125429  0.3181024  0.3212232  0.32222733
 0.32198268 0.321127   0.31891268 0.31367576 0.30327466 0.28662822
 0.265389   0.24311261 0.22546719 0.2182461  0.22448462 0.23879264
 0.2489557  0.24659455 0.2318593  0.20823197 0.18128133 0.16170871
 0.16043696 0.17715637 0.20343938 0.23031987 0.2508991  0.2614392
 0.26299646 0.2587113  0.25228107 0.24526128 0.2374704  0.22835892
 0.21757284 0.20545457 0.19325767 0.18175177 0.17099285 0.16021389
 0.15087317 0.14695719 0.1523173  0.17144895 0.19278122 0.19317645
 0.19152968 0.21078876 0.23269565 0.22562084 0.19335425 0.1670732
 0.17521292 0.21417202 0.26397893 0.29215282 0.28808    0.27121666
 0.26067254 0.26180962 0.2707185  0.28187373 0.2911169  0.29647684
 0.29777157 0.29616308 0.2929318  0.28957343 0.2867298  0.28322747
 0.27646583 0.26380104 0.24472834 0.22189526 0.20026849 0.18584438
 0.18417947 0.19599026 0.21256746 0.22081633 0.21478298 0.19594198
 0.16984634 0.14844906 0.14528273 0.161405   0.18782257 0.21430454
 0.23275207 0.2400957  0.2389617  0.23395848 0.2289719  0.22483663
 0.22006075 0.212764   0.20193589 0.18867183 0.17496127 0.1630657
 0.15433656 0.14743628 0.14001012 0.13329032 0.13393918 0.14903276
 0.1741068  0.1810056  0.17831333 0.19622296 0.21953352 0.21226904
 0.17813888 0.150975   0.16060522 0.19680041 0.24097215 0.2617202
 0.25110492 0.23331405 0.22734465 0.23490576 0.2493196  0.2636611
 0.2741056  0.27910912 0.27905026 0.2748268  0.26821122 0.26125178
 0.2555438  0.25175023 0.2478232  0.24026099 0.22621734 0.20598981
 0.18323168 0.16393465 0.15384492 0.15719664 0.17322817 0.19051638
 0.19587572 0.18543576 0.16287126 0.13958432 0.13185455 0.14499049
 0.17074339 0.19688727 0.21360923 0.2183535  0.21542212 0.21096952
 0.20898929 0.20937145 0.20867035 0.2036326  0.19286533 0.17759496
 0.16076927 0.14559108 0.13531947 0.1316582  0.13147706 0.12815613
 0.12405757 0.13123922 0.15578587 0.1720969  0.16858315 0.18313508
 0.20909242 0.20496261 0.17061901 0.1418291  0.15147519 0.18162538
 0.21788064 0.23235692 0.21699746 0.19925907 0.19810295 0.21178879
 0.23079726 0.24754211 0.258631   0.2636188  0.2629767  0.2577293
 0.2490597  0.23865595 0.22885667 0.22181554 0.21770413 0.21398473
 0.2063906  0.19221967 0.1720488  0.15062071 0.13463031 0.12863518
 0.13567328 0.1545241  0.17128983 0.1734843  0.15920787 0.13610724
 0.12156499 0.12824674 0.15181017 0.17796616 0.19391875 0.19732279
 0.19371487 0.1905596  0.19243588 0.19828463 0.20222457 0.20004888
 0.19047369 0.1749667  0.15575457 0.1359898  0.1196895  0.11179285
 0.11578802 0.12493734 0.12601069 0.12451383 0.13950837 0.16396831
 0.16419013 0.1713786  0.19934587 0.2034633  0.17131127 0.14012358
 0.1467578  0.16967288 0.1947858  0.20502724 0.18655808 0.16890608
 0.17216235 0.19085687 0.21313466 0.2311914  0.2424676  0.24713674
 0.24664025 0.2420877  0.23395564 0.2228559  0.21015915 0.19844326
 0.1904135  0.18646203 0.18343465 0.17669484 0.16338149 0.14406946
 0.12438759 0.11178561 0.10854238 0.11776113 0.13859008 0.15459478
 0.15460385 0.13819192 0.11770891 0.11294568 0.13066377 0.1571558
 0.17409809 0.17776065 0.17456542 0.1726056  0.17807394 0.18868493
 0.19651817 0.19641963 0.18867582 0.17582424 0.1594684  0.14015397
 0.11930163 0.10182348 0.09586067 0.10877963 0.128514   0.1336114
 0.13461255 0.15445092 0.16664764 0.1629195  0.18772005 0.20540948
 0.18000513 0.14670452 0.14563107 0.1628055  0.17359073 0.18078062
 0.16200782 0.1433699  0.14914574 0.17136535 0.1959643  0.21463169
 0.22525617 0.22887012 0.2280121  0.2242701  0.2184605  0.21019061
 0.19903141 0.18574776 0.17292075 0.16392297 0.15993762 0.15752447
 0.15226455 0.14080332 0.12277365 0.10417841 0.09378135 0.09197021
 0.10210902 0.12411092 0.14088605 0.14081551 0.12356393 0.10547052
 0.10894275 0.1331734  0.15310428 0.159278   0.15800378 0.15691684
 0.16397747 0.17736147 0.18721935 0.1881     0.18092148 0.16980027
 0.15805909 0.14577904 0.13081126 0.112362   0.09528629 0.09013397
 0.11016283 0.14060728 0.15003917 0.1524783 ]
