time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.26%, model saved.
Epoch: 0 Train: 8977.28516 Test: 4308.50635
Epoch: 80 Train: 2569.92578 Test: 1203.98657
Epoch: 160 Train: 2373.26953 Test: 1035.58582
Epoch: 240 Train: 1721.47046 Test: 647.91321
Epoch 320: New minimal relative error: 39.03%, model saved.
Epoch: 320 Train: 850.83923 Test: 294.31036
Epoch: 400 Train: 510.51895 Test: 108.41834
Epoch: 480 Train: 302.40430 Test: 111.76887
Epoch: 560 Train: 498.35449 Test: 167.94797
Epoch 640: New minimal relative error: 36.72%, model saved.
Epoch: 640 Train: 154.84750 Test: 56.53875
Epoch: 720 Train: 168.98012 Test: 58.45866
Epoch: 800 Train: 401.19574 Test: 241.06303
Epoch 880: New minimal relative error: 13.77%, model saved.
Epoch: 880 Train: 69.13334 Test: 13.23573
Epoch 960: New minimal relative error: 6.98%, model saved.
Epoch: 960 Train: 98.76640 Test: 9.06463
Epoch: 1040 Train: 58.24177 Test: 8.15910
Epoch: 1120 Train: 62.25858 Test: 20.07619
Epoch: 1200 Train: 62.58788 Test: 5.10675
Epoch 1280: New minimal relative error: 5.54%, model saved.
Epoch: 1280 Train: 42.80147 Test: 4.85353
Epoch: 1360 Train: 36.56883 Test: 9.50228
Epoch: 1440 Train: 54.36505 Test: 17.18549
Epoch: 1520 Train: 46.10545 Test: 14.27680
Epoch: 1600 Train: 41.74351 Test: 4.99712
Epoch: 1680 Train: 35.76933 Test: 6.29099
Epoch 1760: New minimal relative error: 2.65%, model saved.
Epoch: 1760 Train: 30.14629 Test: 1.62439
Epoch: 1840 Train: 29.43413 Test: 4.58715
Epoch: 1920 Train: 24.26514 Test: 0.88081
Epoch: 2000 Train: 19.38750 Test: 0.93132
Epoch: 2080 Train: 19.04137 Test: 1.65603
Epoch: 2160 Train: 27.61794 Test: 8.10710
Epoch: 2240 Train: 23.61948 Test: 7.20478
Epoch: 2320 Train: 24.67767 Test: 6.60372
Epoch: 2400 Train: 19.96781 Test: 1.98081
Epoch: 2480 Train: 23.35714 Test: 6.14712
Epoch: 2560 Train: 19.47955 Test: 6.07770
Epoch: 2640 Train: 14.09581 Test: 1.79779
Epoch 2720: New minimal relative error: 1.91%, model saved.
Epoch: 2720 Train: 14.07775 Test: 0.30694
Epoch: 2800 Train: 22.47586 Test: 5.79809
Epoch: 2880 Train: 15.56479 Test: 2.90485
Epoch: 2960 Train: 13.58718 Test: 2.31461
Epoch: 3040 Train: 12.80667 Test: 1.35506
Epoch: 3120 Train: 11.61719 Test: 0.80158
Epoch: 3200 Train: 12.25273 Test: 1.73116
Epoch: 3280 Train: 20.57967 Test: 4.41659
Epoch: 3360 Train: 12.08882 Test: 1.53474
Epoch: 3440 Train: 12.04911 Test: 2.93341
Epoch: 3520 Train: 10.33736 Test: 0.22040
Epoch: 3600 Train: 11.13760 Test: 0.87251
Epoch: 3680 Train: 11.77493 Test: 0.85221
Epoch: 3760 Train: 10.78217 Test: 0.43271
Epoch: 3840 Train: 9.82299 Test: 0.83257
Epoch: 3920 Train: 11.44251 Test: 1.20719
Epoch: 4000 Train: 9.72739 Test: 1.03216
Epoch: 4080 Train: 9.73262 Test: 0.30195
Epoch: 4160 Train: 9.90104 Test: 0.28853
Epoch: 4240 Train: 13.74501 Test: 5.08325
Epoch 4320: New minimal relative error: 1.04%, model saved.
Epoch: 4320 Train: 8.42693 Test: 0.12066
Epoch: 4400 Train: 14.86936 Test: 3.38059
Epoch: 4480 Train: 8.25954 Test: 0.89948
Epoch: 4560 Train: 9.54836 Test: 1.15459
Epoch: 4640 Train: 8.11264 Test: 0.18654
Epoch: 4720 Train: 9.98794 Test: 2.87885
Epoch: 4800 Train: 8.00137 Test: 0.13194
Epoch 4880: New minimal relative error: 1.03%, model saved.
Epoch: 4880 Train: 7.89425 Test: 0.16300
Epoch: 4960 Train: 7.66034 Test: 0.15666
Epoch: 5040 Train: 7.99273 Test: 0.67079
Epoch: 5120 Train: 7.67721 Test: 0.58905
Epoch: 5200 Train: 7.23459 Test: 0.31594
Epoch: 5280 Train: 7.74009 Test: 0.40467
Epoch: 5360 Train: 6.93689 Test: 0.07362
Epoch: 5440 Train: 6.95271 Test: 0.12431
Epoch: 5520 Train: 7.09972 Test: 0.16316
Epoch: 5600 Train: 7.25373 Test: 0.21484
Epoch: 5680 Train: 7.32633 Test: 0.18637
Epoch: 5760 Train: 9.43934 Test: 0.94529
Epoch: 5840 Train: 10.29815 Test: 1.89221
Epoch: 5920 Train: 10.47154 Test: 1.86186
Epoch: 6000 Train: 7.52241 Test: 0.12805
Epoch: 6080 Train: 7.81181 Test: 0.32297
Epoch: 6160 Train: 6.78684 Test: 0.12647
Epoch: 6240 Train: 7.67550 Test: 0.62675
Epoch: 6320 Train: 7.62493 Test: 1.38608
Epoch: 6400 Train: 6.37751 Test: 0.13801
Epoch 6480: New minimal relative error: 0.83%, model saved.
Epoch: 6480 Train: 6.45505 Test: 0.07481
Epoch: 6560 Train: 6.89548 Test: 0.06680
Epoch: 6640 Train: 7.63669 Test: 0.48890
Epoch: 6720 Train: 6.76613 Test: 0.53850
Epoch: 6800 Train: 6.52241 Test: 0.10921
Epoch: 6880 Train: 6.66956 Test: 0.18293
Epoch: 6960 Train: 6.41511 Test: 0.14327
Epoch: 7040 Train: 7.59344 Test: 0.26123
Epoch: 7120 Train: 5.93958 Test: 0.05863
Epoch: 7200 Train: 7.50148 Test: 0.91294
Epoch: 7280 Train: 6.00713 Test: 0.36651
Epoch: 7360 Train: 5.71757 Test: 0.06922
Epoch: 7440 Train: 5.44709 Test: 0.04956
Epoch: 7520 Train: 6.06736 Test: 0.44856
Epoch: 7600 Train: 6.05508 Test: 0.80547
Epoch: 7680 Train: 5.38934 Test: 0.13940
Epoch: 7760 Train: 4.99143 Test: 0.05869
Epoch: 7840 Train: 5.60693 Test: 0.33543
Epoch: 7920 Train: 7.11709 Test: 1.03482
Epoch 7999: New minimal relative error: 0.62%, model saved.
Epoch: 7999 Train: 4.83145 Test: 0.03506
Training Loss: tensor(4.8314)
Test Loss: tensor(0.0351)
Learned LE: [ 8.6356050e-01  1.0255172e-03 -1.4537978e+01]
True LE: [ 8.6544806e-01  3.0280377e-03 -1.4538586e+01]
Relative Error: [0.44450033 0.5831338  0.6329433  0.5625517  0.497556   0.52559066
 0.5977882  0.5966477  0.6323698  0.72770166 0.8538412  0.7179645
 0.54575956 0.5593268  0.5895907  0.497063   0.34707907 0.35062048
 0.3535998  0.3593213  0.43403292 0.37155563 0.27502966 0.30323124
 0.3198109  0.4637763  0.5712103  0.62565416 0.6587043  0.8847553
 1.1152107  1.0979058  1.0058714  0.8972855  0.85036194 0.93850577
 0.66898453 0.55195636 0.6611886  0.9749937  1.162977   0.9985252
 0.5259147  0.63832116 1.0830384  1.5245312  1.9382308  1.7967712
 1.3140191  0.8726214  0.62429464 0.34171295 0.27558416 0.33968857
 0.3814842  0.4885958  0.36728126 0.20753783 0.1640305  0.22623485
 0.21934032 0.30732435 0.51203245 0.62215275 0.63701713 0.6144084
 0.5757663  0.5352497  0.5444091  0.534874   0.58823484 0.6489872
 0.63261765 0.548806   0.3740181  0.4399384  0.5351797  0.5116008
 0.38082016 0.333874   0.312264   0.27185604 0.28440788 0.29458973
 0.22365728 0.26848376 0.27188468 0.45753372 0.5902039  0.5858195
 0.6066551  0.863708   1.0973921  0.95709914 0.8033355  0.7520615
 0.6822441  0.85419697 0.60729617 0.457882   0.5120439  0.7464876
 0.9825429  0.9819097  0.7496615  0.71621215 1.0949508  1.5738215
 1.9039015  1.7219998  1.2347529  0.8786722  0.6511847  0.38310266
 0.28676853 0.36495897 0.41373217 0.4908944  0.3679405  0.19789714
 0.15393414 0.23810694 0.47002226 0.49091253 0.55932105 0.6440711
 0.7119888  0.6463788  0.6110162  0.5599068  0.5702419  0.4839629
 0.42073464 0.49647796 0.5074202  0.40814304 0.24801458 0.25525984
 0.4078797  0.46821138 0.42019272 0.31707156 0.2970126  0.2088788
 0.15928517 0.18533947 0.17811616 0.249589   0.2590921  0.3777894
 0.56120574 0.54975736 0.5038539  0.82741296 1.0951402  0.8704857
 0.715606   0.60717726 0.4823972  0.6220093  0.50580746 0.36734858
 0.398807   0.60687226 0.8373703  0.78023934 0.7554917  0.7815398
 1.0081683  1.3924947  1.6620616  1.4997736  1.184482   0.90751845
 0.6822711  0.3925222  0.2686494  0.3477489  0.42403674 0.46125385
 0.336656   0.22171322 0.16105065 0.23468453 0.4725732  0.6582544
 0.6572989  0.6390315  0.67939866 0.57786393 0.5364334  0.49740636
 0.5208998  0.5563475  0.4422032  0.44204032 0.4511457  0.45190775
 0.33066472 0.17920832 0.25063783 0.37538558 0.40243888 0.339903
 0.26439968 0.16813162 0.11699603 0.13311592 0.12228611 0.22259252
 0.2462371  0.27264923 0.42033818 0.47041556 0.44215927 0.7627869
 1.0407974  0.81513345 0.662544   0.5252634  0.33608383 0.33991385
 0.41473648 0.30749914 0.2624423  0.427175   0.61580217 0.61157715
 0.589846   0.69059634 0.8988549  1.1273595  1.3437604  1.3190126
 1.0774095  0.87537616 0.6828077  0.4876012  0.28195077 0.34081984
 0.3884584  0.37313968 0.28187597 0.20837946 0.1824537  0.19866374
 0.3362227  0.6148027  0.7643041  0.61956966 0.5283335  0.4178562
 0.35824746 0.42016572 0.4405586  0.4780395  0.43269157 0.3398174
 0.35947105 0.4738417  0.43892643 0.3596409  0.20761888 0.21428959
 0.3300271  0.3556219  0.32454592 0.17447886 0.05404115 0.04360913
 0.0980634  0.19975291 0.25168934 0.21662545 0.28621632 0.32273442
 0.38745525 0.67661947 0.90486455 0.71627617 0.5687614  0.4617135
 0.33854955 0.22079034 0.3061277  0.33829284 0.20150837 0.15897427
 0.38296452 0.47363383 0.4595128  0.5965892  0.7833899  0.9403394
 1.054568   1.0985466  0.9660757  0.7740167  0.63845015 0.5515916
 0.36160603 0.33823913 0.34166425 0.37696242 0.32840466 0.26730943
 0.24392915 0.18936528 0.22292864 0.3859997  0.5366514  0.6358607
 0.42375293 0.29869345 0.20391065 0.27914286 0.38676894 0.4365252
 0.4292951  0.28093845 0.23589882 0.36274165 0.4464225  0.42503095
 0.36410055 0.2259358  0.24072835 0.33820555 0.36041975 0.20451178
 0.13275571 0.21633871 0.08564788 0.14993241 0.14661711 0.17244607
 0.18048623 0.2371916  0.25164083 0.56876314 0.77822214 0.64376897
 0.47631854 0.39617798 0.39369947 0.25046682 0.30913928 0.4206165
 0.36010998 0.15069723 0.23330358 0.26877296 0.3817712  0.55771804
 0.6658008  0.77311754 0.901206   0.8846268  0.8191246  0.66639465
 0.58345294 0.54179806 0.45388392 0.3529414  0.27157944 0.2923885
 0.2913554  0.25955623 0.25743845 0.24115033 0.22499438 0.27649733
 0.34571612 0.42808545 0.608674   0.39589602 0.14888752 0.3039177
 0.5224914  0.53953165 0.58268535 0.46809232 0.18020403 0.1865282
 0.34665668 0.39509764 0.45513794 0.30630615 0.26029444 0.2956716
 0.3536485  0.32820472 0.2439909  0.2084463  0.32343167 0.18147908
 0.15316913 0.13853058 0.15008718 0.27904293]
