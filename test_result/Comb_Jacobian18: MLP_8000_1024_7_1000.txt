time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.97%, model saved.
Epoch: 0 Train: 58678.73828 Test: 4077.63574
Epoch 80: New minimal relative error: 92.16%, model saved.
Epoch: 80 Train: 10872.54688 Test: 1373.40698
Epoch: 160 Train: 3443.38208 Test: 530.74548
Epoch 240: New minimal relative error: 38.58%, model saved.
Epoch: 240 Train: 103.52302 Test: 45.58770
Epoch 320: New minimal relative error: 4.86%, model saved.
Epoch: 320 Train: 11.72268 Test: 3.47680
Epoch: 400 Train: 75.27484 Test: 11.78067
Epoch: 480 Train: 44.43283 Test: 7.96592
Epoch: 560 Train: 8.86153 Test: 4.17744
Epoch: 640 Train: 61.39940 Test: 3.83570
Epoch: 720 Train: 45.20684 Test: 12.13743
Epoch: 800 Train: 59.70866 Test: 9.10279
Epoch: 880 Train: 53.82003 Test: 8.60502
Epoch: 960 Train: 43.86858 Test: 9.91721
Epoch: 1040 Train: 12.46383 Test: 2.22051
Epoch: 1120 Train: 11.57499 Test: 3.64250
Epoch: 1200 Train: 3.07197 Test: 0.05761
Epoch: 1280 Train: 45.22156 Test: 6.49609
Epoch 1360: New minimal relative error: 0.89%, model saved.
Epoch: 1360 Train: 0.69311 Test: 0.03233
Epoch: 1440 Train: 22.19273 Test: 4.39577
Epoch: 1520 Train: 10.53246 Test: 1.51332
Epoch: 1600 Train: 20.55770 Test: 4.82468
Epoch: 1680 Train: 4.79493 Test: 1.35531
Epoch: 1760 Train: 8.33853 Test: 1.24368
Epoch: 1840 Train: 11.91813 Test: 2.67085
Epoch: 1920 Train: 11.23947 Test: 2.03246
Epoch: 2000 Train: 18.16277 Test: 4.29531
Epoch: 2080 Train: 4.46667 Test: 0.97351
Epoch: 2160 Train: 15.39542 Test: 3.77998
Epoch: 2240 Train: 6.45750 Test: 1.55926
Epoch: 2320 Train: 4.67086 Test: 1.03109
Epoch: 2400 Train: 12.31508 Test: 1.49262
Epoch: 2480 Train: 1.25486 Test: 0.12256
Epoch: 2560 Train: 7.42470 Test: 1.74324
Epoch: 2640 Train: 8.89627 Test: 1.34101
Epoch: 2720 Train: 6.93632 Test: 1.05410
Epoch: 2800 Train: 2.04733 Test: 0.28752
Epoch: 2880 Train: 19.47061 Test: 4.13075
Epoch: 2960 Train: 10.56739 Test: 0.72245
Epoch: 3040 Train: 1.12395 Test: 0.23708
Epoch: 3120 Train: 2.87649 Test: 0.15721
Epoch: 3200 Train: 4.58960 Test: 0.27007
Epoch: 3280 Train: 12.03807 Test: 2.74318
Epoch: 3360 Train: 15.86603 Test: 3.27630
Epoch: 3440 Train: 37.78569 Test: 7.48966
Epoch: 3520 Train: 0.14345 Test: 0.00671
Epoch: 3600 Train: 1.49445 Test: 0.20404
Epoch: 3680 Train: 2.22929 Test: 0.18832
Epoch: 3760 Train: 9.53353 Test: 0.46021
Epoch: 3840 Train: 14.25924 Test: 2.94819
Epoch: 3920 Train: 4.23272 Test: 1.09991
Epoch: 4000 Train: 0.87700 Test: 0.11044
Epoch: 4080 Train: 0.95090 Test: 0.25563
Epoch: 4160 Train: 0.70763 Test: 0.09451
Epoch: 4240 Train: 3.69414 Test: 0.89174
Epoch: 4320 Train: 13.64713 Test: 1.29062
Epoch: 4400 Train: 0.21706 Test: 0.03192
Epoch: 4480 Train: 13.49244 Test: 1.52439
Epoch 4560: New minimal relative error: 0.67%, model saved.
Epoch: 4560 Train: 0.18890 Test: 0.02305
Epoch: 4640 Train: 1.54922 Test: 0.11720
Epoch: 4720 Train: 1.67735 Test: 0.45159
Epoch 4800: New minimal relative error: 0.66%, model saved.
Epoch: 4800 Train: 0.15174 Test: 0.01359
Epoch: 4880 Train: 0.80657 Test: 0.04515
Epoch: 4960 Train: 3.81535 Test: 0.46435
Epoch: 5040 Train: 0.70379 Test: 0.11805
Epoch: 5120 Train: 3.60235 Test: 0.31550
Epoch 5200: New minimal relative error: 0.50%, model saved.
Epoch: 5200 Train: 0.09071 Test: 0.01290
Epoch: 5280 Train: 1.51867 Test: 0.11670
Epoch: 5360 Train: 0.38405 Test: 0.05319
Epoch: 5440 Train: 7.28211 Test: 0.89032
Epoch: 5520 Train: 1.44916 Test: 0.33409
Epoch: 5600 Train: 0.20584 Test: 0.04402
Epoch: 5680 Train: 1.78071 Test: 0.35393
Epoch: 5760 Train: 0.32102 Test: 0.01296
Epoch: 5840 Train: 3.33372 Test: 0.57354
Epoch: 5920 Train: 3.77828 Test: 0.62752
Epoch: 6000 Train: 3.77825 Test: 0.66709
Epoch: 6080 Train: 2.14101 Test: 0.25165
Epoch: 6160 Train: 6.17997 Test: 1.32011
Epoch: 6240 Train: 0.34285 Test: 0.07841
Epoch: 6320 Train: 0.11394 Test: 0.01918
Epoch: 6400 Train: 0.12335 Test: 0.01131
Epoch: 6480 Train: 0.12460 Test: 0.01560
Epoch: 6560 Train: 2.46393 Test: 0.39246
Epoch: 6640 Train: 0.53823 Test: 0.13490
Epoch: 6720 Train: 1.34678 Test: 0.16768
Epoch: 6800 Train: 1.64841 Test: 0.43158
Epoch: 6880 Train: 0.71720 Test: 0.12795
Epoch: 6960 Train: 0.99348 Test: 0.22264
Epoch: 7040 Train: 0.03994 Test: 0.00298
Epoch: 7120 Train: 0.13821 Test: 0.01115
Epoch 7200: New minimal relative error: 0.37%, model saved.
Epoch: 7200 Train: 0.11246 Test: 0.01198
Epoch: 7280 Train: 0.37642 Test: 0.02983
Epoch: 7360 Train: 2.48913 Test: 0.53427
Epoch: 7440 Train: 3.12348 Test: 0.69616
Epoch: 7520 Train: 0.25839 Test: 0.03406
Epoch: 7600 Train: 0.12251 Test: 0.00500
Epoch: 7680 Train: 0.85845 Test: 0.17404
Epoch: 7760 Train: 1.60583 Test: 0.35766
Epoch: 7840 Train: 0.34861 Test: 0.07614
Epoch: 7920 Train: 0.89015 Test: 0.21491
Epoch: 7999 Train: 0.41193 Test: 0.05905
Training Loss: tensor(0.4119)
Test Loss: tensor(0.0590)
Learned LE: [ 8.5666078e-01 -1.2888096e-02 -1.4508729e+01]
True LE: [ 8.4270519e-01  7.0578847e-03 -1.4523013e+01]
Relative Error: [0.51931995 0.5176576  0.5187835  0.5231187  0.5291626  0.5346568
 0.5374839  0.537443   0.5355934  0.5332796  0.5317124  0.53088677
 0.53029686 0.5287996  0.52541786 0.51934534 0.5106786  0.49983943
 0.48853123 0.47894657 0.47392052 0.47599044 0.48682058 0.50693023
 0.5357797  0.57144636 0.61096597 0.64954656 0.68047017 0.6975543
 0.7011083  0.6995121  0.7040887  0.718968   0.7382409  0.75123876
 0.7493618  0.73044765 0.69764346 0.65757346 0.6166222  0.57950455
 0.5482334  0.52274644 0.50200355 0.484601   0.4693162  0.45537344
 0.44298092 0.43243402 0.42425773 0.41925892 0.41805506 0.4208271
 0.42788434 0.4383867  0.4511883  0.46401954 0.474576   0.48114377
 0.48335934 0.4824102  0.48034158 0.4790201  0.4799895  0.4838613
 0.48949206 0.49457335 0.4973372  0.4977318  0.49723712 0.4973765
 0.4992832  0.50262886 0.50652695 0.50956595 0.5105139  0.50841254
 0.5029284  0.4944698  0.48430407 0.47458106 0.46829888 0.4681876
 0.4763224  0.49347934 0.518743   0.5504266  0.5862787  0.62283367
 0.65427816 0.6733089  0.6768561  0.67179227 0.6705609  0.6802922
 0.6958062  0.7061598  0.70279807 0.6832994  0.6510913  0.61217874
 0.5729989  0.5377862  0.50833577 0.48466033 0.4653452  0.4491119
 0.43448755 0.4207131  0.40791094 0.39622054 0.3865146  0.37937438
 0.3756172  0.3757069  0.38007557 0.3886011  0.4004743  0.414118
 0.42706603 0.43697274 0.442478   0.44404593 0.4436513  0.44309163
 0.44406733 0.44766265 0.4529905  0.45793453 0.46062082 0.46126693
 0.461722   0.46399623 0.468846   0.47590005 0.48376253 0.49086824
 0.49585834 0.49782792 0.49623692 0.49097514 0.4830214  0.474161
 0.4674245  0.46574265 0.47165012 0.4860837  0.50818485 0.535756
 0.5666974  0.5994389  0.6303303  0.6522608  0.65854776 0.65166104
 0.6445194  0.64774126 0.6587353  0.66635644 0.6616029  0.64234126
 0.6115284  0.5747453  0.53765047 0.5041895  0.476056   0.453413
 0.43529794 0.41990578 0.40603033 0.3926462  0.37959093 0.36729714
 0.3563208  0.3473698  0.34113592 0.33820042 0.33914214 0.34434736
 0.35368952 0.36630058 0.3802317  0.39289686 0.40196845 0.4067315
 0.40843076 0.40902224 0.41041523 0.4139059  0.41925627 0.42445326
 0.42725578 0.42800558 0.42885002 0.43232828 0.43932396 0.44911373
 0.46002176 0.4703367  0.4787658  0.48455024 0.4870414  0.48576695
 0.48098835 0.47418737 0.46798655 0.46548554 0.46977097 0.48208892
 0.5017835  0.52615833 0.55253357 0.5799112  0.60793555 0.63213974
 0.6437206  0.6387754  0.62692153 0.6226024  0.62820685 0.6329072
 0.626844   0.60795176 0.57925344 0.5453228  0.51062185 0.4786408
 0.4511704  0.42878246 0.4109283  0.39613178 0.38301557 0.3702329
 0.35749164 0.3450302  0.3333972  0.32320428 0.31503916 0.30933
 0.30672684 0.30782503 0.3130631  0.3224695  0.3350921  0.34889048
 0.36099705 0.36930358 0.37379506 0.37595838 0.3780217  0.38167095
 0.38738978 0.3933183  0.39686504 0.3976664  0.3983178  0.4018584
 0.40974328 0.4211938  0.43410313 0.44648248 0.45714608 0.4657742
 0.4719792  0.4750752  0.4744532  0.4708213  0.46607953 0.46357134
 0.46647462 0.477099   0.49533296 0.5182495  0.5420017  0.5645808
 0.58725303 0.61047524 0.62810254 0.6302766  0.61773056 0.60600597
 0.60495436 0.60698956 0.59989744 0.58088857 0.5540381  0.5232367
 0.49149278 0.46085262 0.4334212  0.41026053 0.39166653 0.37681308
 0.36415932 0.3521427  0.34007958 0.3280376  0.31641057 0.30580392
 0.2965639  0.28899765 0.28347024 0.2806006  0.28111583 0.28558055
 0.29423675 0.30621308 0.31940877 0.33084163 0.33859703 0.3428524
 0.34589264 0.34981206 0.35588706 0.363088   0.36838442 0.37004164
 0.37013394 0.37258    0.37977573 0.39145598 0.40532175 0.4186916
 0.43028077 0.44020537 0.44885492 0.45587984 0.46003264 0.4605284
 0.45831344 0.45634848 0.4581122  0.46664667 0.48318702 0.50584316
 0.5299235  0.5509595  0.56890875 0.5872927  0.6069129  0.61930877
 0.61431986 0.59830683 0.5891159  0.5883206  0.5817661  0.56261694
 0.536029   0.5076048  0.47894818 0.45046076 0.42294538 0.39811704
 0.37743634 0.3612374  0.3482885  0.3368185  0.3256054  0.31427416
 0.30316758 0.29279947 0.28348    0.27527016 0.26824704 0.2626696
 0.25910023 0.25842774 0.26134968 0.2683755  0.27903202 0.2911732
 0.3019102  0.3091692  0.3135111  0.31747597 0.3233729  0.3317161
 0.33975402 0.34396157 0.34437877 0.3450174  0.34980205 0.3600629
 0.37365672 0.38740608 0.39903513 0.40860575 0.41745216 0.42656773
 0.43504047 0.44076225 0.44260234 0.44205472 0.44277045 0.44841096
 0.4615489  0.4826126  0.5081695  0.53218377]
