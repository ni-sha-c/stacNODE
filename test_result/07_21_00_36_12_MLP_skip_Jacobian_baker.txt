time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 2000
num_test: 2000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 128
n_layers: 5
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 28.822961807 Test: 25.532642365
Epoch 0: New minimal relative error: 25.53%, model saved.
Epoch: 100 Train: 4.462835312 Test: 4.364212990
Epoch 100: New minimal relative error: 4.36%, model saved.
Epoch: 200 Train: 4.484554291 Test: 4.369056702
Epoch: 300 Train: 4.418465614 Test: 4.299138069
Epoch 300: New minimal relative error: 4.30%, model saved.
Epoch: 400 Train: 4.261851788 Test: 4.171113014
Epoch 400: New minimal relative error: 4.17%, model saved.
Epoch: 500 Train: 4.204287529 Test: 4.055223465
Epoch 500: New minimal relative error: 4.06%, model saved.
Epoch: 600 Train: 4.215306282 Test: 4.097412586
Epoch: 700 Train: 4.092472553 Test: 4.029457092
Epoch 700: New minimal relative error: 4.03%, model saved.
Epoch: 800 Train: 4.169472218 Test: 4.149353981
Epoch: 900 Train: 4.182719231 Test: 4.060529709
Epoch: 1000 Train: 4.094046116 Test: 4.032913208
Epoch: 1100 Train: 4.163206100 Test: 4.150074959
Epoch: 1200 Train: 4.149968147 Test: 4.078353882
Epoch: 1300 Train: 4.167017937 Test: 4.083022118
Epoch: 1400 Train: 4.213220119 Test: 4.119253635
Epoch: 1500 Train: 4.201823235 Test: 4.122243881
Epoch: 1600 Train: 4.343122959 Test: 4.245692253
Epoch: 1700 Train: 4.366219044 Test: 4.306218147
Epoch: 1800 Train: 4.361577034 Test: 4.304081917
Epoch: 1900 Train: 4.357801437 Test: 4.292355061
Epoch: 2000 Train: 4.364051819 Test: 4.351750374
Epoch: 2100 Train: 4.393461704 Test: 4.348700523
Epoch: 2200 Train: 4.378309727 Test: 4.332235336
Epoch: 2300 Train: 4.376950741 Test: 4.319690704
Epoch: 2400 Train: 4.371966362 Test: 4.319089413
Epoch: 2500 Train: 4.378049850 Test: 4.314435005
Epoch: 2600 Train: 4.383586407 Test: 4.339376450
Epoch: 2700 Train: 4.369773388 Test: 4.367505074
Epoch: 2800 Train: 4.369953156 Test: 4.371847630
Epoch: 2900 Train: 4.355445862 Test: 4.440244198
Epoch: 3000 Train: 4.358920097 Test: 4.387176514
Epoch: 3100 Train: 4.369361877 Test: 4.438863754
Epoch: 3200 Train: 4.369721413 Test: 4.415602207
Epoch: 3300 Train: 4.371257305 Test: 4.436381340
Epoch: 3400 Train: 4.370286942 Test: 4.434583664
Epoch: 3500 Train: 4.368883133 Test: 4.446949482
Epoch: 3600 Train: 4.369063377 Test: 4.403141975
Epoch: 3700 Train: 4.372599602 Test: 4.330578804
Epoch: 3800 Train: 4.374231815 Test: 4.324571609
Epoch: 3900 Train: 4.372934341 Test: 4.297504902
Epoch: 4000 Train: 4.379599571 Test: 4.311181068
Epoch: 4100 Train: 4.369746208 Test: 4.392872810
Epoch: 4200 Train: 4.368921280 Test: 4.405415535
Epoch: 4300 Train: 4.367045879 Test: 4.439184666
Epoch: 4400 Train: 4.379824638 Test: 4.388906002
Epoch: 4500 Train: 4.378272533 Test: 4.387312889
Epoch: 4600 Train: 4.363565445 Test: 4.367793560
Epoch: 4700 Train: 4.362760544 Test: 4.363036156
Epoch: 4800 Train: 4.359539032 Test: 4.354621410
Epoch: 4900 Train: 4.356072426 Test: 4.352347374
Epoch: 5000 Train: 4.358592033 Test: 4.310214996
Epoch: 5100 Train: 4.359335899 Test: 4.328154564
Epoch: 5200 Train: 4.353765965 Test: 4.324578762
Epoch: 5300 Train: 4.346790314 Test: 4.315903664
Epoch: 5400 Train: 4.340714455 Test: 4.312594414
Epoch: 5500 Train: 4.339566708 Test: 4.315784454
Epoch: 5600 Train: 4.340682030 Test: 4.327271938
Epoch: 5700 Train: 4.339594364 Test: 4.322842598
Epoch: 5800 Train: 4.369379997 Test: 4.352808952
Epoch: 5900 Train: 4.336557388 Test: 4.311553001
Epoch: 6000 Train: 4.335876942 Test: 4.315181732
Epoch: 6100 Train: 4.331433296 Test: 4.339077950
Epoch: 6200 Train: 4.327166557 Test: 4.365678787
Epoch: 6300 Train: 4.322687626 Test: 4.374316216
Epoch: 6400 Train: 4.321188927 Test: 4.380007267
Epoch: 6500 Train: 4.318875790 Test: 4.376290321
Epoch: 6600 Train: 4.313277721 Test: 4.348243713
Epoch: 6700 Train: 4.316203594 Test: 4.336902618
Epoch: 6800 Train: 4.310119629 Test: 4.319084167
Epoch: 6900 Train: 4.305866241 Test: 4.269871712
Epoch: 7000 Train: 4.309315681 Test: 4.282661915
Epoch: 7100 Train: 4.317306519 Test: 4.286501884
Epoch: 7200 Train: 4.301724434 Test: 4.226195335
Epoch: 7300 Train: 4.284023762 Test: 4.233248711
Epoch: 7400 Train: 4.267784119 Test: 4.211893082
Epoch: 7500 Train: 4.269948483 Test: 4.229752541
Epoch: 7600 Train: 4.246537209 Test: 4.158674240
Epoch: 7700 Train: 4.236917973 Test: 4.174981117
Epoch: 7800 Train: 4.237561226 Test: 4.171609879
Epoch: 7900 Train: 4.269452095 Test: 4.207738876
Epoch: 8000 Train: 4.240045071 Test: 4.177777290
Epoch: 8100 Train: 4.242111206 Test: 4.192956924
Epoch: 8200 Train: 4.268616676 Test: 4.216345787
Epoch: 8300 Train: 4.307321548 Test: 4.243976593
Epoch: 8400 Train: 4.326875687 Test: 4.264486313
Epoch: 8500 Train: 4.337101460 Test: 4.280639648
Epoch: 8600 Train: 4.347722054 Test: 4.287577629
Epoch: 8700 Train: 4.354284763 Test: 4.292908669
Epoch: 8800 Train: 4.355728149 Test: 4.293737888
Epoch: 8900 Train: 4.357549667 Test: 4.295825005
Epoch: 9000 Train: 4.357805252 Test: 4.301510811
Epoch: 9100 Train: 4.372477531 Test: 4.308528900
Epoch: 9200 Train: 4.374085426 Test: 4.311670780
Epoch: 9300 Train: 4.376392365 Test: 4.316341400
Epoch: 9400 Train: 4.379641533 Test: 4.323008537
Epoch: 9500 Train: 4.340867043 Test: 4.286417961
Epoch: 9600 Train: 4.285005093 Test: 4.240231037
Epoch: 9700 Train: 4.279577255 Test: 4.235101700
Epoch: 9800 Train: 4.285227776 Test: 4.232720375
Epoch: 9900 Train: 4.306925297 Test: 4.246412277
Epoch: 9999 Train: 4.270940781 Test: 4.214439392
Training Loss: tensor(4.2709)
Test Loss: tensor(4.2144)
True Mean x: tensor(3.6391, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.2368, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.4927, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0025, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0043)
Jacobian term Test Loss: tensor(0.0043)
Learned LE: [1.4296379  0.24723423]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
