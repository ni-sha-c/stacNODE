time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.13%, model saved.
Epoch: 0 Train: 59634.84375 Test: 4266.97217
Epoch: 100 Train: 15413.17090 Test: 1710.97217
Epoch 200: New minimal relative error: 74.70%, model saved.
Epoch: 200 Train: 14546.66797 Test: 1531.51062
Epoch: 300 Train: 15953.40234 Test: 1422.89966
Epoch: 400 Train: 13824.67676 Test: 1223.50793
Epoch 500: New minimal relative error: 64.06%, model saved.
Epoch: 500 Train: 12468.98730 Test: 1328.71533
Epoch: 600 Train: 14523.25977 Test: 1441.87878
Epoch: 700 Train: 13720.77051 Test: 1352.46265
Epoch: 800 Train: 12213.27930 Test: 881.65582
Epoch: 900 Train: 8948.21680 Test: 670.95435
Epoch: 1000 Train: 6898.03857 Test: 373.79898
Epoch: 1100 Train: 4091.87451 Test: 135.46777
Epoch: 1200 Train: 2134.28809 Test: 57.76310
Epoch 1300: New minimal relative error: 31.73%, model saved.
Epoch: 1300 Train: 1351.53845 Test: 20.33498
Epoch 1400: New minimal relative error: 23.36%, model saved.
Epoch: 1400 Train: 1066.00916 Test: 14.03953
Epoch 1500: New minimal relative error: 12.23%, model saved.
Epoch: 1500 Train: 745.71960 Test: 7.80014
Epoch 1600: New minimal relative error: 7.75%, model saved.
Epoch: 1600 Train: 563.64142 Test: 6.38487
Epoch 1700: New minimal relative error: 7.25%, model saved.
Epoch: 1700 Train: 499.67337 Test: 3.54797
Epoch: 1800 Train: 424.95227 Test: 3.06377
Epoch: 1900 Train: 424.34152 Test: 3.68170
Epoch: 2000 Train: 385.15768 Test: 5.23973
Epoch: 2100 Train: 350.05820 Test: 2.19931
Epoch: 2200 Train: 299.19672 Test: 1.62173
Epoch: 2300 Train: 299.62344 Test: 2.14818
Epoch: 2400 Train: 302.17822 Test: 2.17316
Epoch: 2500 Train: 272.92896 Test: 1.38156
Epoch 2600: New minimal relative error: 2.60%, model saved.
Epoch: 2600 Train: 271.36374 Test: 1.58233
Epoch: 2700 Train: 244.34267 Test: 1.16561
Epoch: 2800 Train: 242.45036 Test: 1.49962
Epoch: 2900 Train: 243.78607 Test: 3.06885
Epoch: 3000 Train: 234.15596 Test: 1.17046
Epoch: 3100 Train: 220.44553 Test: 1.16457
Epoch: 3200 Train: 209.16066 Test: 1.08214
Epoch: 3300 Train: 196.57971 Test: 2.45157
Epoch: 3400 Train: 198.36307 Test: 0.87954
Epoch: 3500 Train: 190.75684 Test: 0.60743
Epoch: 3600 Train: 200.63368 Test: 0.83555
Epoch 3700: New minimal relative error: 2.59%, model saved.
Epoch: 3700 Train: 233.36258 Test: 1.42438
Epoch: 3800 Train: 208.30098 Test: 1.03476
Epoch: 3900 Train: 207.01949 Test: 1.07470
Epoch: 4000 Train: 208.24602 Test: 0.96222
Epoch: 4100 Train: 213.89276 Test: 1.53677
Epoch: 4200 Train: 194.43553 Test: 0.91243
Epoch: 4300 Train: 192.93372 Test: 1.07379
Epoch: 4400 Train: 208.78497 Test: 1.21806
Epoch: 4500 Train: 194.07968 Test: 0.94147
Epoch: 4600 Train: 205.29039 Test: 1.04687
Epoch: 4700 Train: 183.27335 Test: 0.96520
Epoch: 4800 Train: 186.69606 Test: 0.75338
Epoch: 4900 Train: 188.65250 Test: 0.93040
Epoch: 5000 Train: 184.17343 Test: 0.80129
Epoch: 5100 Train: 194.81686 Test: 1.16530
Epoch: 5200 Train: 190.79620 Test: 1.05351
Epoch: 5300 Train: 180.42116 Test: 0.95402
Epoch: 5400 Train: 194.55835 Test: 1.09635
Epoch: 5500 Train: 185.43755 Test: 1.00877
Epoch: 5600 Train: 186.62801 Test: 0.97362
Epoch: 5700 Train: 193.84770 Test: 1.08265
Epoch: 5800 Train: 194.65575 Test: 1.18696
Epoch: 5900 Train: 190.67177 Test: 1.01147
Epoch: 6000 Train: 183.53268 Test: 0.84572
Epoch: 6100 Train: 158.47684 Test: 0.64096
Epoch: 6200 Train: 146.34830 Test: 0.39787
Epoch: 6300 Train: 139.91464 Test: 0.39312
Epoch: 6400 Train: 144.90236 Test: 0.45401
Epoch: 6500 Train: 152.97743 Test: 0.65627
Epoch: 6600 Train: 157.89349 Test: 0.81799
Epoch 6700: New minimal relative error: 2.25%, model saved.
Epoch: 6700 Train: 146.54448 Test: 0.51958
Epoch: 6800 Train: 149.21109 Test: 0.58383
Epoch: 6900 Train: 159.95027 Test: 0.72558
Epoch: 7000 Train: 150.91731 Test: 0.57883
Epoch: 7100 Train: 145.18527 Test: 0.51347
Epoch: 7200 Train: 149.05229 Test: 0.72389
Epoch: 7300 Train: 152.52193 Test: 0.62805
Epoch: 7400 Train: 137.51668 Test: 0.50715
Epoch: 7500 Train: 135.38010 Test: 0.48582
Epoch: 7600 Train: 146.32857 Test: 0.64155
Epoch: 7700 Train: 136.60089 Test: 0.50336
Epoch: 7800 Train: 136.95920 Test: 0.51349
Epoch: 7900 Train: 129.99178 Test: 0.43126
Epoch: 8000 Train: 149.38208 Test: 0.90880
Epoch: 8100 Train: 130.34192 Test: 0.44302
Epoch: 8200 Train: 127.64137 Test: 0.63282
Epoch: 8300 Train: 122.91717 Test: 0.43237
Epoch: 8400 Train: 117.70086 Test: 0.42051
Epoch: 8500 Train: 136.84897 Test: 0.83647
Epoch: 8600 Train: 113.63196 Test: 0.31897
Epoch 8700: New minimal relative error: 1.86%, model saved.
Epoch: 8700 Train: 113.71840 Test: 0.28719
Epoch: 8800 Train: 107.29258 Test: 0.24542
Epoch: 8900 Train: 115.36914 Test: 0.46896
Epoch: 9000 Train: 121.49993 Test: 0.52844
Epoch: 9100 Train: 108.31091 Test: 0.37789
Epoch: 9200 Train: 109.61550 Test: 0.47554
Epoch: 9300 Train: 103.85418 Test: 0.35793
Epoch 9400: New minimal relative error: 1.74%, model saved.
Epoch: 9400 Train: 105.22263 Test: 0.36077
Epoch: 9500 Train: 95.61248 Test: 0.34887
Epoch: 9600 Train: 95.90047 Test: 0.27056
Epoch: 9700 Train: 91.63898 Test: 0.26660
Epoch: 9800 Train: 94.80117 Test: 0.24423
Epoch: 9900 Train: 95.14960 Test: 0.26522
Epoch: 9999 Train: 91.82886 Test: 0.24828
Training Loss: tensor(91.8289)
Test Loss: tensor(0.2483)
Learned LE: [  0.89939266  -0.0266389  -14.542635  ]
True LE: [ 8.6603558e-01 -2.2113859e-03 -1.4551199e+01]
Relative Error: [8.369269   8.152236   7.992974   7.8293176  7.7077136  7.6225953
 7.5794516  7.6931796  8.03546    8.583752   9.119143   8.819599
 7.6895885  6.164794   4.8283195  3.504798   2.4891522  1.133384
 0.86101943 2.314986   3.521629   4.3995423  5.0299263  5.4694386
 5.7630105  6.0642147  6.3192396  6.6025987  6.972436   6.817607
 6.4664693  6.087641   5.8313947  5.6054225  5.366684   5.1562886
 5.017883   4.927331   4.807776   4.536604   4.1129956  3.6125374
 2.973602   2.5598278  2.8325536  3.5064712  4.0100565  3.7757165
 3.8698494  3.9466777  3.911409   4.0818634  4.463586   5.0711255
 5.1849666  5.026247   5.1143517  5.3403416  5.6340346  6.047221
 6.81431    7.4074955  7.764096   7.5637655  7.299688   7.099401
 7.0166245  7.0882583  6.981071   7.041812   7.346401   7.7842646
 8.320438   8.638757   8.15484    6.4855905  5.030998   3.634347
 2.5454953  1.4066252  0.48283985 1.9854441  3.2661088  4.2441435
 4.8758154  5.263384   5.480761   5.6091185  5.773077   5.964311
 6.262145   6.179957   5.8510885  5.4421415  5.247972   5.123618
 4.9332995  4.79119    4.6873474  4.682389   4.6738424  4.602375
 4.2945404  3.696924   2.985787   2.430082   2.5822232  3.4064293
 3.983372   3.7979066  3.726273   3.5090487  3.4510362  3.6349602
 4.0571303  4.648913   4.4286346  4.2413416  4.289102   4.5519166
 4.9100256  5.3625355  5.97222    6.1187673  6.4555554  6.4647236
 6.463889   6.1757436  6.0219493  6.0132318  6.1443377  6.4805055
 6.7808113  7.0801005  7.4701777  8.001793   8.20294    7.0488043
 5.5160646  3.9926193  2.7909577  1.8996859  0.23481779 1.5064994
 2.8340344  3.8867545  4.6318107  5.0486593  5.219186   5.2365823
 5.2285933  5.30317    5.5009747  5.5199485  5.2240534  4.8021665
 4.604252   4.510232   4.4334426  4.3875213  4.2845454  4.3502584
 4.511581   4.5998034  4.328518   3.8090248  3.1296768  2.408408
 2.292762   3.261369   3.9493132  3.8108177  3.428567   3.163186
 3.0911822  3.2454062  3.6486518  4.0321565  3.7644577  3.64323
 3.6282067  3.8626864  4.28035    4.604613   4.8136463  4.958767
 5.2065125  5.4586473  5.4636045  5.4801335  5.273636   5.1597724
 5.203595   5.4668484  5.9081903  6.492129   6.750041   7.1163597
 7.679938   7.935777   6.2728605  4.58729    3.2260685  2.2050385
 0.8016449  0.7822497  2.2175589  3.340169   4.204065   4.730262
 4.9328117  4.950151   4.784945   4.6687946  4.7250757  4.807456
 4.5340924  4.1463957  3.9103494  3.8382678  3.8822083  3.8884766
 3.8157802  3.903583   4.214575   4.393182   4.2462773  3.8675134
 3.234319   2.5187476  2.1065853  2.747825   3.7301536  3.7916536
 3.264012   2.8288836  2.6971092  2.7238302  3.1423335  3.4021761
 3.1060016  3.0023272  3.0598185  3.2901216  3.6704116  3.6638281
 3.8893561  3.988252   4.1742325  4.478528   4.680612   4.6240234
 4.6796913  4.591462   4.524044   4.663636   4.9471803  5.3439655
 5.8794556  6.4120107  6.8757787  7.4031763  7.269613   5.4849906
 3.9055266  2.702855   1.743062   0.2981102  1.3620254  2.6525714
 3.5919514  4.277901   4.6221786  4.6257315  4.462227   4.1709857
 4.002996   3.9711316  4.2571497  4.579491   4.534425   4.3615146
 3.9719536  3.441083   3.3272803  3.3175058  3.7156968  4.0048146
 4.0579157  3.8560932  3.3437676  2.68375    2.128131   2.2690735
 3.3501287  3.6864514  3.1508868  2.639254   2.3135078  2.3542411
 2.634418   2.8649437  2.621337   2.5137658  2.5470192  2.7875597
 2.8734336  2.8373508  3.1213274  3.2393787  3.4014418  3.6071382
 3.9658296  4.087851   4.029077   4.067051   4.1690283  4.1498075
 4.248152   4.4516997  4.7618136  5.2968984  6.075695   6.635202
 7.1277714  6.7218623  4.942416   3.4649684  2.3912816  1.1828935
 0.64818364 1.6973919  2.8256905  3.6245115  4.146465   4.354724
 4.1852617  3.8535655  3.5059657  3.4471183  4.395672   4.9841647
 5.1634374  5.093042   4.8341794  4.383696   3.6744258  2.779814
 3.0089874  3.399318   3.6972945  3.6865451  3.378012   2.8902264
 2.2898645  2.04808    2.707533   3.73229    3.0854604  2.589899
 2.219773   2.0501268  2.2508628  2.5751326  2.1802557  2.0737238
 2.1530054  2.3739314  2.283347   2.233917   2.372336   2.5854254
 2.846048   3.0480764  3.2682483  3.6943798  3.715049   3.5869799
 3.69658    3.9977942  3.8534377  3.8077812  3.8448412  4.0586805
 4.5646467  5.3004546  6.222506   6.889986   6.338609   4.81388
 3.3840814  2.3562086  1.0566148  0.92805314 1.8373888  2.7991931
 3.4755514  3.856392   3.9745796  3.6769907 ]
