time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 98.92%, model saved.
Epoch: 0 Train: 3511.11865 Test: 3980.11182
Epoch 80: New minimal relative error: 54.19%, model saved.
Epoch: 80 Train: 127.58975 Test: 136.93332
Epoch 160: New minimal relative error: 37.54%, model saved.
Epoch: 160 Train: 21.61329 Test: 19.80048
Epoch 240: New minimal relative error: 33.19%, model saved.
Epoch: 240 Train: 34.72041 Test: 12.93795
Epoch 320: New minimal relative error: 19.39%, model saved.
Epoch: 320 Train: 9.23763 Test: 8.21602
Epoch: 400 Train: 8.38606 Test: 6.95646
Epoch: 480 Train: 5.25488 Test: 4.48151
Epoch 560: New minimal relative error: 8.11%, model saved.
Epoch: 560 Train: 4.56680 Test: 3.21856
Epoch: 640 Train: 12.92310 Test: 13.56704
Epoch: 720 Train: 2.37788 Test: 1.48316
Epoch 800: New minimal relative error: 7.52%, model saved.
Epoch: 800 Train: 2.09658 Test: 1.26380
Epoch: 880 Train: 2.70250 Test: 1.64712
Epoch: 960 Train: 6.11484 Test: 4.68362
Epoch: 1040 Train: 13.21152 Test: 12.13800
Epoch 1120: New minimal relative error: 7.09%, model saved.
Epoch: 1120 Train: 1.23244 Test: 0.73277
Epoch: 1200 Train: 1.37495 Test: 0.95292
Epoch: 1280 Train: 4.99920 Test: 4.94368
Epoch: 1360 Train: 0.92210 Test: 0.57048
Epoch: 1440 Train: 2.81003 Test: 2.92213
Epoch: 1520 Train: 2.73690 Test: 2.76980
Epoch: 1600 Train: 1.01800 Test: 1.02740
Epoch 1680: New minimal relative error: 6.13%, model saved.
Epoch: 1680 Train: 1.13577 Test: 0.64125
Epoch: 1760 Train: 0.97860 Test: 0.81108
Epoch: 1840 Train: 0.71146 Test: 0.54283
Epoch 1920: New minimal relative error: 6.06%, model saved.
Epoch: 1920 Train: 0.67511 Test: 0.52577
Epoch: 2000 Train: 5.30888 Test: 3.68832
Epoch 2080: New minimal relative error: 3.40%, model saved.
Epoch: 2080 Train: 0.48293 Test: 0.29772
Epoch: 2160 Train: 0.46835 Test: 0.29717
Epoch: 2240 Train: 0.47594 Test: 0.33437
Epoch: 2320 Train: 0.51435 Test: 0.37143
Epoch: 2400 Train: 0.75407 Test: 0.67524
Epoch: 2480 Train: 0.88883 Test: 0.78596
Epoch: 2560 Train: 1.40561 Test: 1.60438
Epoch: 2640 Train: 1.61752 Test: 1.90531
Epoch: 2720 Train: 0.58716 Test: 0.47609
Epoch: 2800 Train: 0.42286 Test: 0.35042
Epoch: 2880 Train: 0.42182 Test: 0.31048
Epoch: 2960 Train: 0.97558 Test: 1.17457
Epoch: 3040 Train: 1.58738 Test: 1.43795
Epoch: 3120 Train: 0.30395 Test: 0.21376
Epoch: 3200 Train: 0.27867 Test: 0.19532
Epoch: 3280 Train: 0.27342 Test: 0.20269
Epoch: 3360 Train: 0.36363 Test: 0.29954
Epoch: 3440 Train: 0.34479 Test: 0.29892
Epoch: 3520 Train: 0.25875 Test: 0.20167
Epoch: 3600 Train: 0.65827 Test: 0.59267
Epoch: 3680 Train: 0.37434 Test: 0.33728
Epoch: 3760 Train: 0.29852 Test: 0.22908
Epoch: 3840 Train: 0.73981 Test: 0.59355
Epoch: 3920 Train: 1.18508 Test: 0.96318
Epoch: 4000 Train: 1.13007 Test: 1.03143
Epoch: 4080 Train: 0.98357 Test: 1.16191
Epoch: 4160 Train: 0.19678 Test: 0.16740
Epoch 4240: New minimal relative error: 2.37%, model saved.
Epoch: 4240 Train: 0.17790 Test: 0.14227
Epoch: 4320 Train: 0.17683 Test: 0.13919
Epoch: 4400 Train: 0.22516 Test: 0.21450
Epoch: 4480 Train: 0.24550 Test: 0.24257
Epoch: 4560 Train: 0.78897 Test: 0.60012
Epoch: 4640 Train: 0.27252 Test: 0.28913
Epoch: 4720 Train: 0.18247 Test: 0.17149
Epoch: 4800 Train: 0.24306 Test: 0.24953
Epoch: 4880 Train: 0.29156 Test: 0.32978
Epoch: 4960 Train: 0.13637 Test: 0.11258
Epoch: 5040 Train: 0.18270 Test: 0.16039
Epoch: 5120 Train: 0.13860 Test: 0.11649
Epoch: 5200 Train: 0.13695 Test: 0.11617
Epoch: 5280 Train: 0.13286 Test: 0.12043
Epoch: 5360 Train: 0.12516 Test: 0.10921
Epoch: 5440 Train: 0.12165 Test: 0.10548
Epoch: 5520 Train: 0.13458 Test: 0.12686
Epoch: 5600 Train: 0.19507 Test: 0.20774
Epoch: 5680 Train: 0.13551 Test: 0.11520
Epoch: 5760 Train: 0.56003 Test: 0.70305
Epoch: 5840 Train: 0.10892 Test: 0.09673
Epoch: 5920 Train: 0.14590 Test: 0.10880
Epoch: 6000 Train: 0.10472 Test: 0.09409
Epoch: 6080 Train: 0.13466 Test: 0.12195
Epoch: 6160 Train: 0.11797 Test: 0.11648
Epoch: 6240 Train: 0.13734 Test: 0.10816
Epoch: 6320 Train: 0.14975 Test: 0.13545
Epoch: 6400 Train: 0.35894 Test: 0.40705
Epoch: 6480 Train: 0.10839 Test: 0.10869
Epoch: 6560 Train: 0.09529 Test: 0.09181
Epoch: 6640 Train: 0.09606 Test: 0.09303
Epoch: 6720 Train: 0.10481 Test: 0.10288
Epoch: 6800 Train: 0.14289 Test: 0.15344
Epoch: 6880 Train: 0.13058 Test: 0.13467
Epoch: 6960 Train: 0.09592 Test: 0.09566
Epoch: 7040 Train: 0.20383 Test: 0.20826
Epoch: 7120 Train: 0.12117 Test: 0.12191
Epoch: 7200 Train: 0.10818 Test: 0.10924
Epoch: 7280 Train: 0.08003 Test: 0.07764
Epoch: 7360 Train: 0.08211 Test: 0.07774
Epoch: 7440 Train: 0.08667 Test: 0.08345
Epoch: 7520 Train: 0.07675 Test: 0.07552
Epoch: 7600 Train: 0.08343 Test: 0.08280
Epoch 7680: New minimal relative error: 1.75%, model saved.
Epoch: 7680 Train: 0.07465 Test: 0.07355
Epoch: 7760 Train: 0.07440 Test: 0.07339
Epoch: 7840 Train: 0.07997 Test: 0.07787
Epoch: 7920 Train: 0.07389 Test: 0.07604
Epoch: 7999 Train: 0.07091 Test: 0.07096
Training Loss: tensor(0.0709)
Test Loss: tensor(0.0710)
Learned LE: [ 0.80715173  0.01393585 -5.1789412 ]
True LE: [ 8.70693088e-01  2.41031451e-03 -1.45509405e+01]
Relative Error: [3.5284388  4.0517445  4.1566315  3.7693624  2.2264993  1.0184934
 0.6509911  0.57587904 0.6573141  1.556642   1.7490765  1.6503714
 1.3849993  1.3143882  1.2551454  1.6282897  2.6658638  3.9584742
 5.190462   5.810116   5.930103   5.8460193  5.553715   5.243449
 4.979603   5.077231   5.6327896  4.682336   4.0016656  2.701185
 1.5464714  1.0865418  1.5060006  2.014204   2.107991   2.3010938
 2.7066994  2.9530098  2.7273078  2.5791295  2.4859498  2.550736
 2.4824247  2.2512767  2.0062637  1.9969972  1.830054   1.8628275
 1.977369   1.9831522  2.0902538  2.2548354  2.4317465  2.560978
 2.4598906  2.3792455  2.346999   2.6676755  3.0169013  3.3421035
 3.619469   3.4905515  2.9702923  3.3554394  4.0142403  3.9266353
 3.1472187  1.4822307  0.7832438  0.5009604  0.58008534 1.7631363
 2.1131244  1.5386708  1.0327325  0.9079253  0.8758148  1.1289583
 2.0373528  3.3084757  4.8084803  5.94982    6.100084   5.9418197
 5.6337333  5.189509   4.724253   4.3408146  4.3810143  4.9391704
 4.009746   3.4227788  2.3347774  1.3826715  1.1508012  2.0023239
 1.9557478  2.0145588  2.254466   2.7018023  2.5604064  2.1888483
 2.1003337  2.0628595  1.9444098  1.7050538  1.4870301  1.5648792
 1.7725432  1.6424639  1.6654724  1.5601524  1.5467957  1.5633576
 1.7257748  1.9171125  2.0784853  1.9736143  2.047056   2.1518826
 2.300075   2.5846083  2.958052   3.2447197  3.110086   2.5705822
 3.0429988  3.6915276  3.7998657  2.7391589  1.258884   0.683563
 0.5353403  1.5314955  2.408843   1.3222013  0.7723279  0.73527586
 0.85254365 1.5035278  2.5140698  3.8943787  4.959705   5.809921
 6.317001   6.1677313  5.8325706  5.2812324  4.792809   4.1545997
 3.6830273  3.6997988  4.2670064  3.389789   3.2501307  2.0667117
 1.3074073  1.374737   1.9158794  1.7680185  1.9199483  2.1965652
 2.4897895  1.9597511  1.552881   1.6704463  1.5914619  1.3432925
 1.1824144  1.2137069  1.4958187  1.5060772  1.4870281  1.4082779
 1.2254919  1.0536724  1.0872728  1.2769942  1.5736878  1.7450998
 1.4347684  1.5007558  1.6520089  2.0441213  2.3415604  2.7163298
 2.968327   2.9789963  2.1460273  2.5002546  2.918878   3.0270228
 2.1325457  0.9469397  0.5742902  1.3915067  2.7507293  1.3227825
 0.5605341  0.5077513  1.420208   2.1297255  3.5146859  4.7838993
 5.882255   6.5225034  6.5624943  6.3558564  5.8841476  5.3251376
 4.779642   4.271706   3.591213   3.1273584  2.9621758  3.5711412
 2.9096942  2.9642956  1.8647524  1.1732702  1.4639543  1.75235
 1.6725891  1.8023394  2.2157478  1.8613977  1.2400284  1.0733249
 1.2968328  1.0769916  0.9533458  0.8753988  0.9494704  1.3541961
 1.399648   1.1645488  0.9412997  0.82646394 0.7982324  0.87459254
 0.96243566 1.3829684  1.5921547  1.2460916  1.1203215  1.16085
 1.5491301  2.0830576  2.6402426  2.6371412  2.4872987  1.5897535
 1.8385237  2.096395   2.1516948  1.5676339  0.6362107  0.5938212
 2.027337   2.008971   0.35320407 0.29466653 1.4723525  2.5585852
 3.8263063  4.7249975  5.4115443  5.861059   6.1386695  6.4345284
 6.031176   5.621155   4.946185   4.1668553  3.615006   3.1446989
 2.5547109  2.1841717  2.4340596  2.379431   2.5313237  1.681401
 0.9371735  1.277613   1.4291646  1.4065119  1.6383505  1.970366
 1.2724954  0.66404796 0.7536991  0.83578897 0.7829749  0.6546695
 0.5699612  0.6516954  1.1855211  1.0121126  0.71427447 0.56396073
 0.5574496  0.5986505  0.6109624  0.7524099  1.1722956  1.1731863
 1.1719196  1.3102235  0.94627243 1.0511961  1.5758326  2.043609
 2.323621   2.0897782  1.3488486  1.0716506  1.157683   1.2713054
 0.86970806 0.32622692 0.9671604  1.3755354  0.9545372  0.82523346
 1.03133    2.1959095  2.40776    3.3188527  4.3261037  4.4934034
 4.2396965  4.1712065  4.3411913  4.0252194  4.0973186  4.2402673
 3.6656775  2.901809   2.4285886  2.1924725  1.552057   1.28023
 1.83161    1.9234674  1.4516876  0.69730675 0.8671563  1.0409068
 1.0522138  1.3405185  1.4912708  0.7681722  0.4137867  0.45765823
 0.5728563  0.53195506 0.49873474 0.26026085 0.41073918 0.74232095
 0.74606144 0.39563388 0.28281143 0.40124017 0.43993926 0.43237376
 0.5999732  0.90601987 0.8117267  0.8439399  0.9825578  1.0445657
 0.8097954  0.70887214 1.1993966  1.7419822  1.670436   1.0796934
 0.523361   0.36995137 0.3059813  0.37455985 0.2972565  1.0328592
 0.8666206  0.37650383 1.934819   2.380239   1.7639508  1.8484056
 2.7512858  3.3831258  2.9477096  2.5738795  2.1512532  2.2336857
 2.2871814  2.0268998  1.9503382  2.052631  ]
