time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 3000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 500.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 17.904979706 Test: 14.485218048
Epoch 0: New minimal relative error: 14.49%, model saved.
Epoch: 30 Train: 3.728192091 Test: 3.724215031
Epoch 30: New minimal relative error: 3.72%, model saved.
Epoch: 60 Train: 3.659184933 Test: 3.664321899
Epoch 60: New minimal relative error: 3.66%, model saved.
Epoch: 90 Train: 3.507531166 Test: 3.508632421
Epoch 90: New minimal relative error: 3.51%, model saved.
Epoch: 120 Train: 3.464227200 Test: 3.464659691
Epoch 120: New minimal relative error: 3.46%, model saved.
Epoch: 150 Train: 3.440059662 Test: 3.449679852
Epoch 150: New minimal relative error: 3.45%, model saved.
Epoch: 180 Train: 3.471803904 Test: 3.475443602
Epoch: 210 Train: 3.471779346 Test: 3.477288723
Epoch: 240 Train: 3.414230347 Test: 3.426830053
Epoch 240: New minimal relative error: 3.43%, model saved.
Epoch: 270 Train: 3.418054104 Test: 3.427484035
Epoch: 300 Train: 3.413393736 Test: 3.423502922
Epoch 300: New minimal relative error: 3.42%, model saved.
Epoch: 330 Train: 3.412647963 Test: 3.420677185
Epoch 330: New minimal relative error: 3.42%, model saved.
Epoch: 360 Train: 3.403785706 Test: 3.414842367
Epoch 360: New minimal relative error: 3.41%, model saved.
Epoch: 390 Train: 3.409631729 Test: 3.438352108
Epoch: 420 Train: 3.432369709 Test: 3.435384989
Epoch: 450 Train: 3.370038509 Test: 3.385481596
Epoch 450: New minimal relative error: 3.39%, model saved.
Epoch: 480 Train: 3.331605434 Test: 3.340178251
Epoch 480: New minimal relative error: 3.34%, model saved.
Epoch: 510 Train: 3.372747421 Test: 3.378189802
Epoch: 540 Train: 3.369398117 Test: 3.394499302
Epoch: 570 Train: 3.377302885 Test: 3.399637461
Epoch: 600 Train: 3.372050285 Test: 3.400408268
Epoch: 630 Train: 3.402195930 Test: 3.424702168
Epoch: 660 Train: 3.373327971 Test: 3.389381170
Epoch: 690 Train: 3.392541409 Test: 3.407381773
Epoch: 720 Train: 3.398555756 Test: 3.421377182
Epoch: 750 Train: 3.414934397 Test: 3.434594631
Epoch: 780 Train: 3.396864891 Test: 3.423144341
Epoch: 810 Train: 3.397905111 Test: 3.415681124
Epoch: 840 Train: 3.405455112 Test: 3.417038918
Epoch: 870 Train: 3.372214794 Test: 3.398326874
Epoch: 900 Train: 3.390474796 Test: 3.421074867
Epoch: 930 Train: 3.358427048 Test: 3.391533375
Epoch: 960 Train: 3.372461796 Test: 3.391894817
Epoch: 990 Train: 3.356294155 Test: 3.378265381
Epoch: 1020 Train: 3.355707169 Test: 3.380363703
Epoch: 1050 Train: 3.345685005 Test: 3.374257088
Epoch: 1080 Train: 3.371621609 Test: 3.384091139
Epoch: 1110 Train: 3.355849266 Test: 3.381095409
Epoch: 1140 Train: 3.255731821 Test: 3.302620411
Epoch 1140: New minimal relative error: 3.30%, model saved.
Epoch: 1170 Train: 3.404974222 Test: 3.435481071
Epoch: 1200 Train: 3.403547764 Test: 3.432182312
Epoch: 1230 Train: 3.402200699 Test: 3.425240993
Epoch: 1260 Train: 3.402471066 Test: 3.428946972
Epoch: 1290 Train: 3.403601646 Test: 3.434794426
Epoch: 1320 Train: 3.401906967 Test: 3.422839642
Epoch: 1350 Train: 3.400382757 Test: 3.424702406
Epoch: 1380 Train: 3.407320976 Test: 3.428262234
Epoch: 1410 Train: 3.405884504 Test: 3.424351692
Epoch: 1440 Train: 3.405079365 Test: 3.416956902
Epoch: 1470 Train: 3.391482115 Test: 3.408394337
Epoch: 1500 Train: 3.397612572 Test: 3.414684296
Epoch: 1530 Train: 3.395757675 Test: 3.405994415
Epoch: 1560 Train: 3.393781900 Test: 3.404925108
Epoch: 1590 Train: 3.400995970 Test: 3.412899733
Epoch: 1620 Train: 3.399823666 Test: 3.412408352
Epoch: 1650 Train: 3.391070366 Test: 3.404737473
Epoch: 1680 Train: 3.390694618 Test: 3.406778336
Epoch: 1710 Train: 3.392253876 Test: 3.408452272
Epoch: 1740 Train: 3.402114391 Test: 3.414358377
Epoch: 1770 Train: 3.411194563 Test: 3.423051357
Epoch: 1800 Train: 3.416245222 Test: 3.428859711
Epoch: 1830 Train: 3.421430349 Test: 3.438533306
Epoch: 1860 Train: 3.422370672 Test: 3.441618919
Epoch: 1890 Train: 3.424345016 Test: 3.448334217
Epoch: 1920 Train: 3.423461199 Test: 3.444040298
Epoch: 1950 Train: 3.419805765 Test: 3.436276913
Epoch: 1980 Train: 3.413660765 Test: 3.431195974
Epoch: 2010 Train: 3.411555290 Test: 3.428328991
Epoch: 2040 Train: 3.408921719 Test: 3.425445080
Epoch: 2070 Train: 3.407675266 Test: 3.421501160
Epoch: 2100 Train: 3.406112194 Test: 3.422580957
Epoch: 2130 Train: 3.405260086 Test: 3.420689106
Epoch: 2160 Train: 3.403038025 Test: 3.420625210
Epoch: 2190 Train: 3.403167725 Test: 3.425149441
Epoch: 2220 Train: 3.401986122 Test: 3.424768209
Epoch: 2250 Train: 3.403061152 Test: 3.423058510
Epoch: 2280 Train: 3.407911777 Test: 3.430810452
Epoch: 2310 Train: 3.403716326 Test: 3.426939011
Epoch: 2340 Train: 3.432865381 Test: 3.454616547
Epoch: 2370 Train: 3.454228163 Test: 3.471014023
Epoch: 2400 Train: 3.445098162 Test: 3.463188887
Epoch: 2430 Train: 3.451310635 Test: 3.469991207
Epoch: 2460 Train: 3.436103582 Test: 3.452723026
Epoch: 2490 Train: 3.417919636 Test: 3.432076454
Epoch: 2520 Train: 3.410748243 Test: 3.428852320
Epoch: 2550 Train: 3.408500433 Test: 3.429456472
Epoch: 2580 Train: 3.408603668 Test: 3.430832863
Epoch: 2610 Train: 3.409729242 Test: 3.430536270
Epoch: 2640 Train: 3.410799980 Test: 3.433710337
Epoch: 2670 Train: 3.407010078 Test: 3.430287361
Epoch: 2700 Train: 3.407079697 Test: 3.424136162
Epoch: 2730 Train: 3.344367504 Test: 3.387036085
Epoch: 2760 Train: 3.342364311 Test: 3.368217468
Epoch: 2790 Train: 3.351017237 Test: 3.361991405
Epoch: 2820 Train: 3.285559416 Test: 3.322353840
Epoch: 2850 Train: 3.359045506 Test: 3.369366169
Epoch: 2880 Train: 3.352765083 Test: 3.360712290
Epoch: 2910 Train: 3.355835438 Test: 3.373641491
Epoch: 2940 Train: 3.369279861 Test: 3.381956100
Epoch: 2970 Train: 3.382818222 Test: 3.391613007
Epoch: 2999 Train: 3.392594337 Test: 3.402985573
Training Loss: tensor(3.3926)
Test Loss: tensor(3.4030)
True Mean x: tensor(3.1152, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.2425, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0022)
Jacobian term Test Loss: tensor(0.0022)
Learned LE: [13.548669   2.3265076]
True LE: tensor([ 0.6931, -0.7176], dtype=torch.float64)
