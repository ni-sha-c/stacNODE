time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.07%, model saved.
Epoch: 0 Train: 3771.87769 Test: 4269.30469
Epoch 80: New minimal relative error: 57.50%, model saved.
Epoch: 80 Train: 177.34424 Test: 167.61755
Epoch 160: New minimal relative error: 21.66%, model saved.
Epoch: 160 Train: 17.06323 Test: 13.39751
Epoch: 240 Train: 8.41108 Test: 6.33977
Epoch: 320 Train: 6.00170 Test: 4.35425
Epoch 400: New minimal relative error: 18.41%, model saved.
Epoch: 400 Train: 5.03021 Test: 3.75447
Epoch 480: New minimal relative error: 13.49%, model saved.
Epoch: 480 Train: 5.87064 Test: 4.30274
Epoch: 560 Train: 6.04387 Test: 7.59294
Epoch: 640 Train: 4.53359 Test: 4.47707
Epoch: 720 Train: 10.41188 Test: 10.66660
Epoch: 800 Train: 4.91040 Test: 4.34607
Epoch: 880 Train: 3.73310 Test: 3.15065
Epoch: 960 Train: 1.61009 Test: 1.08383
Epoch 1040: New minimal relative error: 12.41%, model saved.
Epoch: 1040 Train: 1.36031 Test: 0.82749
Epoch: 1120 Train: 1.25485 Test: 0.81572
Epoch 1200: New minimal relative error: 8.50%, model saved.
Epoch: 1200 Train: 1.16167 Test: 0.83146
Epoch: 1280 Train: 3.91096 Test: 3.73014
Epoch: 1360 Train: 1.80992 Test: 1.72778
Epoch: 1440 Train: 2.03084 Test: 2.41547
Epoch: 1520 Train: 0.61897 Test: 0.33830
Epoch 1600: New minimal relative error: 7.51%, model saved.
Epoch: 1600 Train: 0.64399 Test: 0.40320
Epoch: 1680 Train: 1.15746 Test: 0.95380
Epoch: 1760 Train: 0.70877 Test: 0.47954
Epoch: 1840 Train: 1.18395 Test: 1.01627
Epoch: 1920 Train: 0.44292 Test: 0.31031
Epoch: 2000 Train: 0.42363 Test: 0.26114
Epoch: 2080 Train: 0.37883 Test: 0.22184
Epoch: 2160 Train: 2.20692 Test: 2.51457
Epoch: 2240 Train: 1.02066 Test: 0.85725
Epoch: 2320 Train: 0.50426 Test: 0.35730
Epoch: 2400 Train: 0.39217 Test: 0.25038
Epoch: 2480 Train: 0.32755 Test: 0.21902
Epoch: 2560 Train: 0.58631 Test: 0.59242
Epoch: 2640 Train: 0.25939 Test: 0.16089
Epoch: 2720 Train: 0.26054 Test: 0.16708
Epoch: 2800 Train: 1.03729 Test: 1.00552
Epoch: 2880 Train: 0.64790 Test: 0.68075
Epoch 2960: New minimal relative error: 7.21%, model saved.
Epoch: 2960 Train: 0.44534 Test: 0.30120
Epoch: 3040 Train: 0.85565 Test: 0.85827
Epoch: 3120 Train: 0.38346 Test: 0.20504
Epoch: 3200 Train: 0.41764 Test: 0.35539
Epoch: 3280 Train: 0.20988 Test: 0.15204
Epoch: 3360 Train: 0.19830 Test: 0.14349
Epoch: 3440 Train: 0.68469 Test: 0.61330
Epoch: 3520 Train: 0.88061 Test: 0.94033
Epoch: 3600 Train: 0.40477 Test: 0.35993
Epoch: 3680 Train: 0.33267 Test: 0.29551
Epoch: 3760 Train: 0.38190 Test: 0.40067
Epoch: 3840 Train: 0.32329 Test: 0.23278
Epoch: 3920 Train: 0.19513 Test: 0.14803
Epoch: 4000 Train: 0.23741 Test: 0.20972
Epoch: 4080 Train: 0.31680 Test: 0.29906
Epoch 4160: New minimal relative error: 5.52%, model saved.
Epoch: 4160 Train: 0.17572 Test: 0.12058
Epoch: 4240 Train: 0.18654 Test: 0.10740
Epoch: 4320 Train: 0.14087 Test: 0.09760
Epoch: 4400 Train: 0.14758 Test: 0.10938
Epoch: 4480 Train: 0.17353 Test: 0.13696
Epoch: 4560 Train: 0.69843 Test: 0.74502
Epoch: 4640 Train: 0.21601 Test: 0.19474
Epoch: 4720 Train: 0.14986 Test: 0.11574
Epoch 4800: New minimal relative error: 3.51%, model saved.
Epoch: 4800 Train: 0.13554 Test: 0.09728
Epoch: 4880 Train: 0.13135 Test: 0.09471
Epoch: 4960 Train: 0.15427 Test: 0.12440
Epoch: 5040 Train: 0.21132 Test: 0.17804
Epoch: 5120 Train: 0.55692 Test: 0.46747
Epoch: 5200 Train: 0.14776 Test: 0.12204
Epoch: 5280 Train: 0.11770 Test: 0.08911
Epoch: 5360 Train: 0.20171 Test: 0.18329
Epoch: 5440 Train: 0.10906 Test: 0.07909
Epoch: 5520 Train: 0.11056 Test: 0.08787
Epoch: 5600 Train: 0.45172 Test: 0.39160
Epoch: 5680 Train: 0.45493 Test: 0.45328
Epoch: 5760 Train: 0.15461 Test: 0.13847
Epoch: 5840 Train: 0.12720 Test: 0.10422
Epoch: 5920 Train: 0.16150 Test: 0.14384
Epoch: 6000 Train: 0.12553 Test: 0.11106
Epoch: 6080 Train: 0.09775 Test: 0.07252
Epoch: 6160 Train: 0.24946 Test: 0.29632
Epoch: 6240 Train: 0.09438 Test: 0.07079
Epoch: 6320 Train: 0.10320 Test: 0.08251
Epoch: 6400 Train: 0.09404 Test: 0.07261
Epoch: 6480 Train: 0.09155 Test: 0.07016
Epoch: 6560 Train: 0.11253 Test: 0.07550
Epoch: 6640 Train: 0.08868 Test: 0.06766
Epoch: 6720 Train: 0.08937 Test: 0.06869
Epoch: 6800 Train: 0.08859 Test: 0.06810
Epoch: 6880 Train: 0.10878 Test: 0.09951
Epoch: 6960 Train: 0.08449 Test: 0.06570
Epoch: 7040 Train: 0.08682 Test: 0.06785
Epoch: 7120 Train: 0.08603 Test: 0.06848
Epoch: 7200 Train: 0.08191 Test: 0.06483
Epoch: 7280 Train: 0.08095 Test: 0.06391
Epoch: 7360 Train: 0.08060 Test: 0.06339
Epoch: 7440 Train: 0.08249 Test: 0.06594
Epoch: 7520 Train: 0.07814 Test: 0.06254
Epoch: 7600 Train: 0.08108 Test: 0.07045
Epoch: 7680 Train: 0.07645 Test: 0.06141
Epoch: 7760 Train: 0.10895 Test: 0.10882
Epoch: 7840 Train: 0.07494 Test: 0.06072
Epoch: 7920 Train: 0.09130 Test: 0.07789
Epoch: 7999 Train: 0.07530 Test: 0.06145
Training Loss: tensor(0.0753)
Test Loss: tensor(0.0615)
Learned LE: [ 0.770206    0.06352782 -4.002791  ]
True LE: [ 8.5137665e-01  3.8236845e-03 -1.4526369e+01]
Relative Error: [2.9351325  2.7336185  2.4609973  2.2496808  1.985601   1.90725
 1.848829   1.8964179  2.0506597  2.2947516  2.525946   2.7119257
 2.8310575  2.789032   2.7662032  2.7703323  2.7359157  2.4591029
 2.0468743  1.6616534  1.3448362  1.4290466  1.9552803  2.4793017
 3.169785   3.1844068  2.57323    2.0761125  1.4931568  1.2029955
 1.233205   1.4104295  1.7676228  2.0155175  2.321977   2.2471817
 2.3159456  2.4364462  2.3372507  2.2353625  2.0357955  1.9023498
 1.8053972  1.8392228  2.032492   2.0210276  1.7924299  1.6573813
 1.5682068  1.5778866  1.6279266  1.6935248  1.749059   1.802569
 1.8355044  1.8530681  1.8619604  1.9792279  2.289545   2.5411932
 2.5200467  2.4923081  2.6271262  2.57811    2.204192   1.96214
 1.7070336  1.7409779  1.71893    1.7560283  1.8902272  2.1264439
 2.3802004  2.6033213  2.6771505  2.6867428  2.7175527  2.7651842
 2.6948864  2.5456715  2.1373992  1.5687886  1.1610923  1.1438658
 1.5320626  2.0254452  2.7250881  2.9435444  2.4086843  1.9960179
 1.4074471  1.15073    1.1576751  1.2149233  1.4718825  1.691003
 1.9478433  1.9854215  2.0222762  2.1139293  1.9838071  1.9340657
 1.7530584  1.7056283  1.6409047  1.6723597  1.778679   1.8558509
 1.7280432  1.59516    1.5157881  1.4650042  1.4717824  1.5776485
 1.6893947  1.7390106  1.7670496  1.8357071  1.871291   1.9706937
 2.272092   2.5326009  2.5438375  2.3979752  2.415624   2.3425865
 2.0345128  1.7275449  1.4822291  1.5936339  1.5464547  1.5688643
 1.6381813  1.9028172  2.2069066  2.3831277  2.4598708  2.4692948
 2.5498521  2.6932075  2.703399   2.5195153  2.2042468  1.6141163
 1.0881404  0.87171096 1.0486488  1.4740918  2.1837757  2.6786058
 2.2276073  1.8421786  1.2939799  1.074081   1.104317   1.1163037
 1.2160009  1.4154617  1.6319817  1.815222   1.7614646  1.7615553
 1.6411718  1.6654726  1.5044072  1.5236671  1.5083895  1.5656959
 1.6411781  1.7742819  1.7365569  1.5639031  1.4480726  1.3880795
 1.3887569  1.4372206  1.5697168  1.6324166  1.6465547  1.7265698
 1.8027333  2.0160747  2.2593493  2.6061742  2.608841   2.4189997
 2.3171906  2.1353114  1.8903112  1.5595081  1.3398204  1.4198064
 1.3482177  1.3185372  1.4040563  1.6095068  1.8808414  2.0705435
 2.1422791  2.185656   2.307586   2.4976265  2.5846517  2.5602095
 2.286965   1.832809   1.199601   0.74697703 0.64088374 0.9898291
 1.6220626  2.3604023  2.0229414  1.6568975  1.2282395  0.9490896
 1.0315472  1.0878791  1.0943261  1.1707274  1.4391807  1.6129119
 1.4862663  1.4293418  1.3363396  1.3548616  1.2530124  1.2706251
 1.3506117  1.3684857  1.4853846  1.6958568  1.7565526  1.5393671
 1.3597137  1.2813113  1.2588899  1.2906405  1.3519871  1.5221722
 1.5982592  1.6435087  1.7073033  1.8549187  2.2425587  2.6272063
 2.6928105  2.4827878  2.2545414  1.9808482  1.6585847  1.4067478
 1.2506824  1.2668446  1.1737907  1.0526097  1.1113551  1.2617872
 1.4859073  1.5999784  1.7126026  1.8191047  1.9269115  2.1716492
 2.4306636  2.5499756  2.4562907  2.1159809  1.78202    1.036611
 0.5771434  0.47030225 0.95024246 1.7815003  1.8341216  1.401872
 1.1091585  0.870799   0.90627104 1.0633764  1.1166736  1.0538137
 1.2395847  1.3864844  1.2323773  1.1261483  1.0847145  1.0634773
 1.024111   1.0664926  1.255173   1.2223468  1.3156632  1.552012
 1.7243505  1.5102882  1.3016953  1.2018005  1.1644019  1.1592867
 1.205442   1.3190408  1.543877   1.6253965  1.6637707  1.689953
 1.9518293  2.3201034  2.6926246  2.6119192  2.3828661  1.9994928
 1.5421509  1.2621617  1.1880528  1.1586872  1.0559428  0.890252
 0.81909335 0.92399055 1.0056113  1.1201311  1.3342568  1.4556049
 1.4668543  1.8415782  2.1493833  2.2969441  2.3567572  2.2503452
 2.0846303  1.6775079  0.94399863 0.49952665 0.29779157 0.9760689
 1.715577   1.4864663  1.1579485  0.8651424  0.7698921  0.95984125
 1.1119832  1.0259452  0.9605828  1.1675419  1.0685872  0.8981431
 0.8844544  0.8024434  0.82783335 0.85858023 1.070376   1.1026245
 1.1869309  1.4002227  1.5072268  1.4080291  1.2235208  1.121484
 1.0445455  1.0144728  1.0555258  1.1645848  1.2829894  1.4786084
 1.5687761  1.5282995  1.5929325  1.8588209  2.282579   2.4966633
 2.5533025  2.165836   1.655124   1.1717558  1.05618    1.02567
 0.93879616 0.825547   0.70275974 0.6644054  0.65464073 0.70833695
 0.9389742  1.1238211  1.1890358  1.5876184  1.9644859  2.123994
 2.1439967  2.035626   2.123339   1.9760927  1.5256294  0.82937014
 0.4360679  0.43237904 1.1511554  1.409716  ]
