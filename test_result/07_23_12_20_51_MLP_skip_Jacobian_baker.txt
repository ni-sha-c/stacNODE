time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 200.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 13.450345993 Test: 10.909854889
Epoch 0: New minimal relative error: 10.91%, model saved.
Epoch: 100 Train: 3.143898964 Test: 3.233902693
Epoch 100: New minimal relative error: 3.23%, model saved.
Epoch: 200 Train: 2.897603512 Test: 2.980619431
Epoch 200: New minimal relative error: 2.98%, model saved.
Epoch: 300 Train: 2.724904537 Test: 2.685206413
Epoch 300: New minimal relative error: 2.69%, model saved.
Epoch: 400 Train: 2.771435022 Test: 3.176804066
Epoch: 500 Train: 2.714462757 Test: 2.786813736
Epoch: 600 Train: 2.895738840 Test: 2.961035728
Epoch: 700 Train: 2.669458866 Test: 2.771814823
Epoch: 800 Train: 2.779385567 Test: 2.881327152
Epoch: 900 Train: 2.687085867 Test: 2.789255381
Epoch: 1000 Train: 2.804358959 Test: 2.866027832
Epoch: 1100 Train: 2.924983501 Test: 3.165204048
Epoch: 1200 Train: 2.844050884 Test: 2.932580948
Epoch: 1300 Train: 2.809770584 Test: 2.918534756
Epoch: 1400 Train: 2.704140663 Test: 2.808019638
Epoch: 1500 Train: 2.770694733 Test: 2.897616863
Epoch: 1600 Train: 2.766920805 Test: 2.938950062
Epoch: 1700 Train: 2.920686245 Test: 3.042540312
Epoch: 1800 Train: 2.922108650 Test: 2.986783981
Epoch: 1900 Train: 2.856408596 Test: 2.958077192
Epoch: 2000 Train: 2.825954199 Test: 2.892942667
Epoch: 2100 Train: 2.913208246 Test: 2.999049664
Epoch: 2200 Train: 2.935667515 Test: 3.020749807
Epoch: 2300 Train: 2.936478138 Test: 3.020584345
Epoch: 2400 Train: 2.946600914 Test: 3.013423681
Epoch: 2500 Train: 2.920624971 Test: 2.999310493
Epoch: 2600 Train: 2.919272423 Test: 2.988887787
Epoch: 2700 Train: 2.936334133 Test: 3.018238544
Epoch: 2800 Train: 2.920159578 Test: 3.029349327
Epoch: 2900 Train: 2.968367577 Test: 3.092595100
Epoch: 3000 Train: 2.868284225 Test: 2.985442877
Epoch: 3100 Train: 2.879758358 Test: 2.989508152
Epoch: 3200 Train: 2.834115028 Test: 2.961256266
Epoch: 3300 Train: 2.874617815 Test: 2.989973545
Epoch: 3400 Train: 2.885271549 Test: 3.039268017
Epoch: 3500 Train: 2.853004932 Test: 2.982601166
Epoch: 3600 Train: 2.882352829 Test: 3.009305477
Epoch: 3700 Train: 2.946313858 Test: 3.017782688
Epoch: 3800 Train: 2.872287512 Test: 3.008105755
Epoch: 3900 Train: 2.882647991 Test: 3.004950047
Epoch: 4000 Train: 2.854740143 Test: 2.976622581
Epoch: 4100 Train: 2.823932648 Test: 2.970704079
Epoch: 4200 Train: 2.906533003 Test: 3.026278496
Epoch: 4300 Train: 2.899147987 Test: 3.032147884
Epoch: 4400 Train: 2.898243904 Test: 3.020920753
Epoch: 4500 Train: 2.936818600 Test: 3.053894281
Epoch: 4600 Train: 2.657377243 Test: 2.719489574
Epoch: 4700 Train: 2.755495071 Test: 2.854017258
Epoch: 4800 Train: 2.884882212 Test: 2.983413219
Epoch: 4900 Train: 2.898488998 Test: 2.994250774
Epoch: 5000 Train: 2.914416075 Test: 3.003776550
Epoch: 5100 Train: 2.914140463 Test: 3.024282217
Epoch: 5200 Train: 2.934365749 Test: 3.035843372
Epoch: 5300 Train: 2.948529243 Test: 3.054242134
Epoch: 5400 Train: 2.934772968 Test: 3.034262180
Epoch: 5500 Train: 2.956238270 Test: 3.065088511
Epoch: 5600 Train: 2.962394238 Test: 3.070905924
Epoch: 5700 Train: 2.964264154 Test: 3.077885151
Epoch: 5800 Train: 2.967833519 Test: 3.082248688
Epoch: 5900 Train: 2.969841480 Test: 3.080738306
Epoch: 6000 Train: 2.966574669 Test: 3.082128048
Epoch: 6100 Train: 2.970644474 Test: 3.085737228
Epoch: 6200 Train: 2.974344254 Test: 3.088198185
Epoch: 6300 Train: 2.977552891 Test: 3.095723391
Epoch: 6400 Train: 2.980391502 Test: 3.091610432
Epoch: 6500 Train: 2.974482059 Test: 3.086036682
Epoch: 6600 Train: 2.976933956 Test: 3.085268974
Epoch: 6700 Train: 2.977026939 Test: 3.082473278
Epoch: 6800 Train: 2.971896648 Test: 3.088569641
Epoch: 6900 Train: 2.968929529 Test: 3.079234600
Epoch: 7000 Train: 2.969309092 Test: 3.082043648
Epoch: 7100 Train: 2.977029800 Test: 3.098773003
Epoch: 7200 Train: 2.953977585 Test: 3.063201427
Epoch: 7300 Train: 2.758929729 Test: 2.919276237
Epoch: 7400 Train: 2.861936092 Test: 2.988691807
Epoch: 7500 Train: 2.920080185 Test: 3.017351627
Epoch: 7600 Train: 2.937736273 Test: 3.029248238
Epoch: 7700 Train: 2.958974600 Test: 3.036525249
Epoch: 7800 Train: 2.972403049 Test: 3.054040670
Epoch: 7900 Train: 2.973620653 Test: 3.059438944
Epoch: 8000 Train: 2.975483894 Test: 3.064122677
Epoch: 8100 Train: 2.976541519 Test: 3.063262463
Epoch: 8200 Train: 2.972314835 Test: 3.065452099
Epoch: 8300 Train: 2.970890045 Test: 3.064146519
Epoch: 8400 Train: 2.969221115 Test: 3.062126160
Epoch: 8500 Train: 2.965292454 Test: 3.059445381
Epoch: 8600 Train: 2.966072559 Test: 3.058983803
Epoch: 8700 Train: 2.962470770 Test: 3.057309628
Epoch: 8800 Train: 2.962448597 Test: 3.051847696
Epoch: 8900 Train: 2.960006952 Test: 3.059978485
Epoch: 9000 Train: 2.962744951 Test: 3.063471317
Epoch: 9100 Train: 2.837448359 Test: 2.933285952
Epoch: 9200 Train: 2.935946465 Test: 3.036224842
Epoch: 9300 Train: 2.937086105 Test: 3.031584263
Epoch: 9400 Train: 2.934047222 Test: 3.035042524
Epoch: 9500 Train: 2.940261364 Test: 3.040458441
Epoch: 9600 Train: 2.945901394 Test: 3.043856144
Epoch: 9700 Train: 2.944858074 Test: 3.044343710
Epoch: 9800 Train: 2.945940495 Test: 3.046018839
Epoch: 9900 Train: 2.952424765 Test: 3.044303894
Epoch: 9999 Train: 2.951201916 Test: 3.055060863
Training Loss: tensor(2.9512)
Test Loss: tensor(3.0551)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.1282, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0046, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0074)
Jacobian term Test Loss: tensor(0.0077)
Learned LE: [1.4001579  0.28374466]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
