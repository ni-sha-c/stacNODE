time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.47%, model saved.
Epoch: 0 Train: 32043.29297 Test: 3915.38477
Epoch: 80 Train: 9494.88086 Test: 1624.03772
Epoch 160: New minimal relative error: 78.48%, model saved.
Epoch: 160 Train: 8553.88477 Test: 1326.71448
Epoch: 240 Train: 7946.07861 Test: 1106.95618
Epoch 320: New minimal relative error: 55.70%, model saved.
Epoch: 320 Train: 7927.50049 Test: 1157.72083
Epoch: 400 Train: 7524.07129 Test: 1117.03174
Epoch 480: New minimal relative error: 54.58%, model saved.
Epoch: 480 Train: 6094.09131 Test: 757.26379
Epoch: 560 Train: 5427.31055 Test: 556.81274
Epoch: 640 Train: 4387.24707 Test: 402.66458
Epoch: 720 Train: 3525.80957 Test: 277.53015
Epoch: 800 Train: 2127.01636 Test: 99.60873
Epoch 880: New minimal relative error: 36.28%, model saved.
Epoch: 880 Train: 1103.35376 Test: 36.97573
Epoch 960: New minimal relative error: 14.17%, model saved.
Epoch: 960 Train: 831.59753 Test: 20.92174
Epoch: 1040 Train: 689.29565 Test: 115.42950
Epoch: 1120 Train: 526.72687 Test: 9.83747
Epoch: 1200 Train: 428.88644 Test: 7.17252
Epoch: 1280 Train: 395.24078 Test: 7.75519
Epoch 1360: New minimal relative error: 13.14%, model saved.
Epoch: 1360 Train: 337.26285 Test: 4.75992
Epoch: 1440 Train: 334.97580 Test: 8.44955
Epoch 1520: New minimal relative error: 13.01%, model saved.
Epoch: 1520 Train: 292.88480 Test: 3.52133
Epoch: 1600 Train: 274.21811 Test: 5.08480
Epoch: 1680 Train: 257.30121 Test: 5.96420
Epoch 1760: New minimal relative error: 12.99%, model saved.
Epoch: 1760 Train: 255.87555 Test: 4.20916
Epoch: 1840 Train: 239.25569 Test: 5.64955
Epoch 1920: New minimal relative error: 12.62%, model saved.
Epoch: 1920 Train: 234.03755 Test: 4.00633
Epoch 2000: New minimal relative error: 5.66%, model saved.
Epoch: 2000 Train: 209.75917 Test: 2.26528
Epoch: 2080 Train: 192.13802 Test: 1.87161
Epoch: 2160 Train: 190.78745 Test: 2.57174
Epoch: 2240 Train: 188.79881 Test: 2.55557
Epoch: 2320 Train: 164.31406 Test: 1.59259
Epoch: 2400 Train: 165.08392 Test: 1.56023
Epoch: 2480 Train: 158.29329 Test: 1.39694
Epoch: 2560 Train: 155.31448 Test: 1.98732
Epoch: 2640 Train: 151.99562 Test: 3.97410
Epoch: 2720 Train: 150.00682 Test: 1.26185
Epoch: 2800 Train: 144.03535 Test: 8.16294
Epoch: 2880 Train: 128.96025 Test: 1.10925
Epoch: 2960 Train: 130.51834 Test: 1.18568
Epoch: 3040 Train: 119.31453 Test: 0.92638
Epoch 3120: New minimal relative error: 4.06%, model saved.
Epoch: 3120 Train: 116.50738 Test: 1.30101
Epoch: 3200 Train: 112.18366 Test: 0.99518
Epoch: 3280 Train: 129.44003 Test: 1.34324
Epoch: 3360 Train: 119.21929 Test: 0.95670
Epoch: 3440 Train: 106.05961 Test: 0.76280
Epoch: 3520 Train: 111.62671 Test: 1.37202
Epoch: 3600 Train: 93.25379 Test: 0.59742
Epoch: 3680 Train: 89.99622 Test: 0.55449
Epoch: 3760 Train: 86.43660 Test: 1.67727
Epoch: 3840 Train: 83.23865 Test: 1.22478
Epoch: 3920 Train: 80.49821 Test: 1.92430
Epoch: 4000 Train: 70.58560 Test: 1.86453
Epoch 4080: New minimal relative error: 2.96%, model saved.
Epoch: 4080 Train: 72.29939 Test: 0.35709
Epoch: 4160 Train: 72.03622 Test: 0.48324
Epoch: 4240 Train: 67.89934 Test: 0.32806
Epoch: 4320 Train: 82.69534 Test: 1.85334
Epoch: 4400 Train: 59.82278 Test: 0.22960
Epoch: 4480 Train: 75.80644 Test: 10.37907
Epoch: 4560 Train: 55.07726 Test: 0.18373
Epoch: 4640 Train: 51.66079 Test: 0.15394
Epoch: 4720 Train: 50.65180 Test: 0.41208
Epoch: 4800 Train: 51.22037 Test: 0.42299
Epoch: 4880 Train: 49.85453 Test: 0.16628
Epoch: 4960 Train: 50.37440 Test: 0.17889
Epoch: 5040 Train: 51.43328 Test: 0.18734
Epoch: 5120 Train: 56.91666 Test: 0.25126
Epoch: 5200 Train: 61.76790 Test: 0.37741
Epoch: 5280 Train: 57.13482 Test: 0.27869
Epoch: 5360 Train: 55.12690 Test: 0.31440
Epoch: 5440 Train: 54.58799 Test: 0.30361
Epoch: 5520 Train: 52.51461 Test: 0.41442
Epoch: 5600 Train: 52.40295 Test: 0.31527
Epoch: 5680 Train: 59.44316 Test: 7.23035
Epoch: 5760 Train: 49.18293 Test: 0.20527
Epoch: 5840 Train: 46.15567 Test: 0.17760
Epoch 5920: New minimal relative error: 2.83%, model saved.
Epoch: 5920 Train: 44.40309 Test: 0.18524
Epoch: 6000 Train: 44.89677 Test: 0.24821
Epoch: 6080 Train: 44.26764 Test: 0.21374
Epoch: 6160 Train: 42.62368 Test: 0.23355
Epoch: 6240 Train: 51.80970 Test: 0.41969
Epoch: 6320 Train: 55.36226 Test: 0.37402
Epoch: 6400 Train: 56.36811 Test: 0.33504
Epoch: 6480 Train: 57.78546 Test: 0.36555
Epoch: 6560 Train: 56.08952 Test: 0.35675
Epoch: 6640 Train: 62.42723 Test: 0.43832
Epoch: 6720 Train: 64.31905 Test: 0.56642
Epoch: 6800 Train: 55.33305 Test: 0.34729
Epoch: 6880 Train: 55.49066 Test: 0.23661
Epoch: 6960 Train: 52.49353 Test: 0.36076
Epoch: 7040 Train: 50.27770 Test: 0.20898
Epoch: 7120 Train: 50.79002 Test: 0.21204
Epoch: 7200 Train: 48.47203 Test: 0.18263
Epoch: 7280 Train: 48.27498 Test: 0.61752
Epoch: 7360 Train: 45.17197 Test: 0.15895
Epoch: 7440 Train: 42.16304 Test: 0.14965
Epoch: 7520 Train: 40.94371 Test: 0.15928
Epoch 7600: New minimal relative error: 2.62%, model saved.
Epoch: 7600 Train: 41.43634 Test: 0.14242
Epoch: 7680 Train: 40.81307 Test: 0.17224
Epoch: 7760 Train: 44.41783 Test: 0.29288
Epoch: 7840 Train: 42.54841 Test: 0.17688
Epoch: 7920 Train: 42.16153 Test: 0.18784
Epoch: 7999 Train: 44.70905 Test: 0.23867
Training Loss: tensor(44.7090)
Test Loss: tensor(0.2387)
Learned LE: [ 8.4100616e-01  1.4250376e-02 -1.4505211e+01]
True LE: [ 8.5023159e-01 -2.0245370e-03 -1.4525001e+01]
Relative Error: [7.5835543  7.56093    7.4951253  7.4018188  7.163328   6.83956
 6.57608    6.430413   6.2520547  5.9749575  5.594627   5.237091
 5.128942   4.8731966  4.5171347  4.14931    3.7099655  3.239831
 2.8753488  2.6086473  2.4230042  2.2941382  2.234967   2.2970145
 2.3762684  2.567933   2.8705976  3.2950773  3.8070042  4.3714533
 4.605627   4.470543   4.080147   3.7360384  3.4827616  3.2088935
 2.2938342  1.930472   2.1665752  2.6823728  3.215854   3.6622748
 3.9635286  4.1670175  4.3543816  4.498045   4.560118   4.5550776
 4.50106    4.462252   4.411815   4.3580275  4.319802   4.3276663
 4.3997784  4.553881   4.767291   5.135908   5.591858   6.1083035
 6.582576   6.740024   6.833124   6.8471646  6.843923   6.804622
 6.542932   6.28141    6.205826   6.155286   5.9942236  5.695073
 5.301485   5.0349126  4.8786674  4.6010685  4.2682524  3.8816206
 3.3840768  2.8653574  2.4208903  2.1649866  2.1085954  2.227352
 2.1436586  1.9685613  2.0311925  2.2504659  2.5724685  2.990259
 3.4748456  4.01561    4.5092173  4.5157633  4.0667963  3.7066147
 3.4356613  3.0305395  2.129647   1.8649155  2.1955304  2.7330716
 3.2502525  3.6223545  3.8435564  3.9736776  4.091723   4.180112
 4.1837163  4.1143327  4.0103593  3.9241037  3.810217   3.7038803
 3.6137152  3.5748847  3.6276665  3.7813396  4.029816   4.399135
 4.8766394  5.4252625  5.922056   5.9663563  6.07492    6.1592894
 6.18713    6.125937   5.8655853  5.7687817  5.867786   5.8825326
 5.723624   5.42517    4.953325   4.8166423  4.6544495  4.4229283
 4.0639234  3.6285021  3.0791836  2.4915411  2.008585   1.7918633
 1.8706924  1.8864738  1.8166131  1.6974462  1.7376188  2.013344
 2.3385358  2.7260718  3.179001   3.6316693  4.080263   4.473975
 4.0806813  3.6908486  3.4026635  2.8769505  2.0313005  1.8549036
 2.238905   2.788982   3.2563374  3.5441546  3.717503   3.7751594
 3.8265803  3.8424969  3.783451   3.6520813  3.5086703  3.3643384
 3.1942585  3.0244334  2.878148   2.7976947  2.8160362  2.954387
 3.2493937  3.6259203  4.124941   4.712185   5.1551986  5.202178
 5.3051405  5.376938   5.4483814  5.3892384  5.250795   5.354156
 5.52082    5.532105   5.4139605  5.136288   4.69699    4.634751
 4.5398927  4.286009   3.8926113  3.4045928  2.7926304  2.1429772
 1.6420451  1.547338   1.4989812  1.4108682  1.4154109  1.4751025
 1.5813805  1.8393688  2.1478121  2.5026524  2.9198313  3.2657516
 3.6787548  4.0328884  4.119601   3.6939404  3.3587449  2.7760432
 1.9608444  1.8564132  2.2910237  2.836707   3.205847   3.445053
 3.565977   3.5642538  3.5488043  3.4966083  3.3788714  3.1958063
 3.012977   2.8058355  2.5730047  2.3345068  2.1389418  2.012085
 1.9852386  2.1089413  2.40738    2.8326814  3.332183   3.9623187
 4.349511   4.346804   4.4249663  4.578864   4.726017   4.685091
 4.7702     4.9740973  5.159438   5.19587    5.0829625  4.781862
 4.499157   4.5047407  4.415852   4.171917   3.738678   3.1975071
 2.529115   1.8176258  1.3355176  1.2521112  1.1202574  0.99352163
 1.0730597  1.1775929  1.4717714  1.7039796  1.8986685  2.1954315
 2.6222506  2.937388   3.3088288  3.6251643  3.903824   3.7278128
 3.3249583  2.7002935  1.9093454  1.8676883  2.3446746  2.756677
 3.08502    3.307789   3.3951647  3.3431537  3.251407   3.1366236
 2.9707522  2.7442102  2.4983904  2.222491   1.9638493  1.6896071
 1.4819636  1.3239462  1.2554897  1.4331717  1.8040594  2.3214889
 2.800973   3.4341052  3.6138852  3.4564369  3.6050503  3.8388464
 4.0313253  4.1250553  4.3020983  4.5840874  4.799762   4.8750954
 4.635229   4.2503724  4.134681   4.232149   4.129449   3.8874555
 3.5203264  3.0168247  2.2924254  1.5184823  1.1184689  0.9771367
 0.8208855  0.7065689  0.75736886 0.94514304 1.297709   1.3634107
 1.5060308  1.7421013  2.1172664  2.448658   2.973374   3.2535677
 3.5248525  3.8049564  3.3068678  2.6536894  1.8694347  1.8814392
 2.246025   2.638538   2.978146   3.1869478  3.240182   3.1364958
 2.9790616  2.7985704  2.5600545  2.2581165  2.0150716  1.7058293
 1.3930459  1.1152462  0.8799786  0.81995034 0.8597316  0.97233504
 1.2064829  1.7350532  2.2847295  2.8723288  2.9625762  2.8428488
 2.8668025  3.1181076  3.3771713  3.6549032  3.8705192  4.174521
 4.400983   4.345635   4.1068926  3.723485   3.7840984  3.9326577
 3.8868263  3.6175609  3.2038383  2.7127714  2.076781   1.2680826
 1.052613   0.837968   0.57394546 0.48382318 0.48203123 0.74969167
 0.9914974  1.1447394  1.2118857  1.3680171 ]
