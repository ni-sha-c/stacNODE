time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.80%, model saved.
Epoch: 0 Train: 4038.39282 Test: 4172.43652
Epoch 100: New minimal relative error: 59.33%, model saved.
Epoch: 100 Train: 88.99368 Test: 64.80079
Epoch 200: New minimal relative error: 43.04%, model saved.
Epoch: 200 Train: 10.18205 Test: 8.92355
Epoch: 300 Train: 4.99429 Test: 10.73371
Epoch: 400 Train: 11.76146 Test: 11.84669
Epoch 500: New minimal relative error: 40.28%, model saved.
Epoch: 500 Train: 5.90791 Test: 5.42368
Epoch 600: New minimal relative error: 22.93%, model saved.
Epoch: 600 Train: 1.48082 Test: 1.75788
Epoch: 700 Train: 2.97683 Test: 1.64022
Epoch 800: New minimal relative error: 18.01%, model saved.
Epoch: 800 Train: 3.22990 Test: 4.29427
Epoch: 900 Train: 4.15339 Test: 6.07821
Epoch: 1000 Train: 7.88041 Test: 4.24056
Epoch: 1100 Train: 0.58963 Test: 0.70020
Epoch 1200: New minimal relative error: 11.09%, model saved.
Epoch: 1200 Train: 7.97041 Test: 10.64992
Epoch: 1300 Train: 1.71975 Test: 3.14198
Epoch: 1400 Train: 1.61700 Test: 0.91301
Epoch: 1500 Train: 3.85669 Test: 3.91959
Epoch: 1600 Train: 4.41186 Test: 6.41234
Epoch: 1700 Train: 0.88408 Test: 1.59600
Epoch: 1800 Train: 2.10800 Test: 2.67373
Epoch: 1900 Train: 3.77978 Test: 5.48878
Epoch: 2000 Train: 3.22032 Test: 3.56105
Epoch: 2100 Train: 2.28134 Test: 2.85784
Epoch: 2200 Train: 3.01558 Test: 2.84194
Epoch: 2300 Train: 1.23020 Test: 1.90160
Epoch: 2400 Train: 0.48621 Test: 1.07166
Epoch: 2500 Train: 1.71313 Test: 2.66966
Epoch: 2600 Train: 1.85499 Test: 3.10676
Epoch: 2700 Train: 1.33930 Test: 2.12520
Epoch: 2800 Train: 1.70974 Test: 1.10079
Epoch: 2900 Train: 1.43144 Test: 1.58441
Epoch: 3000 Train: 0.27415 Test: 0.72667
Epoch: 3100 Train: 0.52350 Test: 1.03862
Epoch: 3200 Train: 0.46025 Test: 0.97258
Epoch 3300: New minimal relative error: 6.84%, model saved.
Epoch: 3300 Train: 0.66058 Test: 0.93011
Epoch: 3400 Train: 0.05784 Test: 0.34825
Epoch: 3500 Train: 0.96579 Test: 1.74875
Epoch: 3600 Train: 0.84508 Test: 1.09223
Epoch: 3700 Train: 1.42936 Test: 1.62051
Epoch: 3800 Train: 0.76311 Test: 0.95701
Epoch: 3900 Train: 1.83388 Test: 2.32164
Epoch: 4000 Train: 0.60294 Test: 1.09226
Epoch: 4100 Train: 0.74933 Test: 0.87596
Epoch: 4200 Train: 0.82915 Test: 1.22865
Epoch: 4300 Train: 0.02833 Test: 0.28135
Epoch: 4400 Train: 0.39523 Test: 0.63231
Epoch: 4500 Train: 0.70383 Test: 0.78131
Epoch: 4600 Train: 1.83409 Test: 2.78830
Epoch: 4700 Train: 0.98013 Test: 1.65237
Epoch: 4800 Train: 0.17333 Test: 0.49890
Epoch: 4900 Train: 1.92971 Test: 1.91915
Epoch: 5000 Train: 1.07999 Test: 1.56144
Epoch: 5100 Train: 0.18994 Test: 0.50436
Epoch: 5200 Train: 1.50619 Test: 1.98258
Epoch: 5300 Train: 0.03160 Test: 0.29851
Epoch: 5400 Train: 0.16392 Test: 0.42686
Epoch: 5500 Train: 0.56136 Test: 0.82750
Epoch: 5600 Train: 1.00271 Test: 1.13813
Epoch: 5700 Train: 0.78701 Test: 0.78322
Epoch: 5800 Train: 0.17449 Test: 0.52321
Epoch: 5900 Train: 0.35616 Test: 0.66314
Epoch: 6000 Train: 0.17498 Test: 0.48195
Epoch: 6100 Train: 0.01871 Test: 0.27461
Epoch: 6200 Train: 0.36234 Test: 0.71692
Epoch: 6300 Train: 0.37125 Test: 0.55140
Epoch: 6400 Train: 0.75892 Test: 1.49118
Epoch: 6500 Train: 0.99931 Test: 1.01923
Epoch: 6600 Train: 0.72175 Test: 0.87725
Epoch: 6700 Train: 0.08883 Test: 0.41189
Epoch: 6800 Train: 0.34467 Test: 0.62151
Epoch: 6900 Train: 0.43037 Test: 0.68768
Epoch: 7000 Train: 0.01839 Test: 0.28634
Epoch: 7100 Train: 0.22614 Test: 0.50634
Epoch: 7200 Train: 0.14527 Test: 0.38362
Epoch: 7300 Train: 0.03408 Test: 0.32016
Epoch: 7400 Train: 0.49078 Test: 0.94311
Epoch: 7500 Train: 0.16115 Test: 0.37187
Epoch: 7600 Train: 0.42672 Test: 0.68926
Epoch: 7700 Train: 0.02826 Test: 0.28355
Epoch: 7800 Train: 0.11278 Test: 0.39912
Epoch: 7900 Train: 0.07495 Test: 0.33286
Epoch: 8000 Train: 0.11391 Test: 0.39916
Epoch: 8100 Train: 0.35503 Test: 0.50112
Epoch: 8200 Train: 0.15333 Test: 0.47211
Epoch: 8300 Train: 0.01032 Test: 0.27189
Epoch: 8400 Train: 0.01711 Test: 0.28795
Epoch: 8500 Train: 0.24876 Test: 0.56394
Epoch: 8600 Train: 0.14205 Test: 0.42073
Epoch: 8700 Train: 0.01998 Test: 0.28071
Epoch: 8800 Train: 0.09746 Test: 0.37689
Epoch: 8900 Train: 0.08027 Test: 0.30775
Epoch: 9000 Train: 0.15767 Test: 0.45852
Epoch: 9100 Train: 0.14166 Test: 0.35426
Epoch: 9200 Train: 0.15868 Test: 0.48057
Epoch: 9300 Train: 0.00709 Test: 0.27473
Epoch: 9400 Train: 0.19011 Test: 0.46833
Epoch: 9500 Train: 0.01831 Test: 0.28200
Epoch: 9600 Train: 0.02067 Test: 0.28635
Epoch: 9700 Train: 0.01381 Test: 0.28591
Epoch: 9800 Train: 0.21184 Test: 0.56415
Epoch: 9900 Train: 0.10107 Test: 0.36574
Epoch: 9999 Train: 0.01023 Test: 0.26992
Training Loss: tensor(0.0102)
Test Loss: tensor(0.2699)
Learned LE: [ 0.91563874  0.01704183 -5.0172615 ]
True LE: [ 8.7300473e-01  8.8386647e-03 -1.4555059e+01]
Relative Error: [27.437132  28.950373  30.502537  32.09993   33.73879   35.398094
 37.03355   38.575855  39.940125  41.04813   41.85018   42.330532
 42.487064  42.30549   41.749672  40.774582  39.357334  37.541016
 35.453373  33.28092   31.244883  29.597706  28.58031   28.317514
 28.723495  29.5205    30.37983   31.077381  31.557158  31.886852
 32.17292   32.500137  32.90289   33.361618  33.814857  34.168453
 34.294983  34.032936  33.203197  31.655151  29.352846  26.473326
 23.36815   20.33349   17.480383  14.88719   12.753953  11.33314
 10.722383  10.760315  11.166258  11.724255  12.3505    13.087152
 14.0138445 15.147633  16.432627  17.789413  19.157734  20.510008
 21.845871  23.180788  24.535194  25.927847  27.372477  28.876438
 30.436724  32.03405   33.625904  35.144203  36.503014  37.621037
 38.44784   38.972626  39.202053  39.12686   38.70847   37.891758
 36.634384  34.951958  32.953777  30.824762  28.794422  27.130869
 26.098558  25.84112   26.270061  27.08416   27.927326  28.561914
 28.93927   29.14976   29.326021  29.571285  29.92391   30.35967
 30.815994  31.206718  31.420776  31.315495  30.723206  29.488594
 27.550056  25.05593   22.344725  19.68506   17.134525  14.735026
 12.68376   11.240515  10.505221  10.328272  10.462359  10.733213
 11.088942  11.592459  12.334782  13.3261385 14.48893   15.722637
 16.95495   18.156202  19.330633  20.50175   21.698425  22.945553
 24.260874  25.653616  27.122313  28.64933   30.193905  31.6895
 33.049343  34.187656  35.049522  35.626762  35.936783  35.97982
 35.716877  35.08395   34.01944   32.505726  30.62118   28.54263
 26.506409  24.795307  23.701134  23.399765  23.818132  24.642942
 25.489292  26.091745  26.392483  26.495758  26.561806  26.718456
 27.0107    27.409563  27.851984  28.258919  28.533926  28.556305
 28.176624  27.242453  25.669128  23.56492   21.25124   18.978806
 16.742413  14.530678  12.536655  11.049125  10.199548   9.848319
  9.752263   9.763924   9.871448  10.165941  10.752603  11.632674
 12.701136  13.830857  14.935432  15.982601  16.98154   17.965494
 18.97479   20.044712  21.19988   22.453976  23.808722  25.249233
 26.73714   28.20699   29.57146   30.738937  31.645754  32.280777
 32.67239   32.837143  32.742058  32.315273  31.47805   30.178686
 28.449404  26.443645  24.400488  22.614576  21.406286  20.99968
 21.362354  22.187603  23.063133  23.680286  23.947475  23.966248
 23.922354  23.9777    24.192957  24.536816  24.947182  25.352882
 25.668339  25.790945  25.59318   24.931808  23.703318  21.969965
 20.03555   18.164461  16.285961  14.290756  12.34932   10.792347
  9.821202   9.329596   9.056141   8.844234   8.718729   8.815288
  9.265949  10.065387  11.073078  12.128003  13.125922  14.029661
 14.851414  15.632979  16.427938  17.285675  18.242126  19.318829
 20.524057  21.849035  23.260015  24.693424  26.060835  27.264515
 28.226904  28.924963  29.392878  29.670807  29.74438   29.53915
 28.960674  27.928226  26.418062  24.532822  22.501358  20.62573
 19.251427  18.664162  18.904894  19.702585  20.62823   21.31799
 21.617243  21.595142  21.44983   21.386307  21.501013  21.76662
 22.120398  22.503534  22.842028  23.044289  23.001612  22.583344
 21.670338  20.266676  18.64884   17.15475   15.7020855 14.009418
 12.156324  10.514386   9.395937   8.779781   8.390609   8.015126
  7.6735682  7.5585194  7.8648434  8.599512   9.577765  10.5942
 11.52119   12.312613  12.978129  13.5628805 14.131078  14.74802
 15.465797  16.318344  17.324224  18.485926  19.780802  21.151646
 22.509853  23.750717  24.780111  25.55141   26.089897  26.45944
 26.682606  26.699272  26.401276  25.6843    24.475422  22.79691
 20.831236  18.878418  17.299904  16.4466    16.469025  17.176151
 18.146368  18.962448  19.381416  19.397297  19.186125  18.990732
 18.975048  19.136951  19.405537  19.731411  20.063242  20.32176
 20.411682  20.215137  19.60142   18.502548  17.104275  15.857787
 14.847829  13.61527   11.970839  10.269708   8.963156   8.20353
  7.7485843  7.3031197  6.800323   6.4439573  6.544628   7.1911483
  8.153377   9.165292  10.068221  10.800092  11.360535  11.788013
 12.148304  12.519093  12.9711    13.555871  14.305989  15.238796
 16.351316  17.605131  18.919245  20.183622  21.286465  22.149055
 22.764502  23.20076   23.529509  23.73767   23.721386  23.351276
 22.521051  21.173998  19.387941  17.422157  15.641475  14.446375
 14.123024  14.623701  15.578525  16.537119 ]
