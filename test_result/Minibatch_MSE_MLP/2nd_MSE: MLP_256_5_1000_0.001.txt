time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 3752.31445 Test: 4066.05054
Epoch 100: New minimal relative error: 80.97%, model saved.
Epoch: 100 Train: 79.67831 Test: 71.53097
Epoch 200: New minimal relative error: 21.68%, model saved.
Epoch: 200 Train: 34.12165 Test: 14.05240
Epoch 300: New minimal relative error: 17.14%, model saved.
Epoch: 300 Train: 5.13036 Test: 4.26693
Epoch: 400 Train: 4.69496 Test: 5.49940
Epoch: 500 Train: 3.78367 Test: 3.76977
Epoch: 600 Train: 7.98605 Test: 8.48508
Epoch: 700 Train: 1.57469 Test: 1.02803
Epoch 800: New minimal relative error: 14.33%, model saved.
Epoch: 800 Train: 0.82308 Test: 0.65973
Epoch: 900 Train: 0.75130 Test: 0.83281
Epoch 1000: New minimal relative error: 7.44%, model saved.
Epoch: 1000 Train: 0.88770 Test: 1.39405
Epoch: 1100 Train: 0.83918 Test: 0.76698
Epoch: 1200 Train: 2.79480 Test: 4.46285
Epoch: 1300 Train: 0.93838 Test: 0.84808
Epoch: 1400 Train: 4.68480 Test: 5.55302
Epoch: 1500 Train: 0.29891 Test: 0.29513
Epoch: 1600 Train: 1.09568 Test: 4.33275
Epoch: 1700 Train: 6.81346 Test: 10.28205
Epoch: 1800 Train: 0.60305 Test: 0.20823
Epoch: 1900 Train: 2.10674 Test: 1.65287
Epoch: 2000 Train: 3.58435 Test: 3.56532
Epoch 2100: New minimal relative error: 7.37%, model saved.
Epoch: 2100 Train: 2.92640 Test: 1.38480
Epoch: 2200 Train: 0.20299 Test: 0.26380
Epoch: 2300 Train: 0.14693 Test: 0.13464
Epoch: 2400 Train: 0.20327 Test: 0.22101
Epoch: 2500 Train: 0.14596 Test: 0.11791
Epoch: 2600 Train: 0.13384 Test: 0.12742
Epoch 2700: New minimal relative error: 5.93%, model saved.
Epoch: 2700 Train: 0.20184 Test: 0.12067
Epoch: 2800 Train: 0.16598 Test: 0.41440
Epoch: 2900 Train: 0.19113 Test: 0.29797
Epoch: 3000 Train: 6.36058 Test: 7.17023
Epoch: 3100 Train: 0.15144 Test: 0.17805
Epoch: 3200 Train: 0.88676 Test: 1.15072
Epoch: 3300 Train: 0.24758 Test: 0.18532
Epoch: 3400 Train: 0.10325 Test: 0.10278
Epoch: 3500 Train: 0.10115 Test: 0.10943
Epoch: 3600 Train: 0.08186 Test: 0.07954
Epoch: 3700 Train: 0.11391 Test: 0.12436
Epoch: 3800 Train: 0.09571 Test: 0.10118
Epoch: 3900 Train: 0.07312 Test: 0.07192
Epoch: 4000 Train: 0.19831 Test: 0.21829
Epoch: 4100 Train: 0.13413 Test: 0.10653
Epoch: 4200 Train: 0.07061 Test: 0.08240
Epoch: 4300 Train: 0.26142 Test: 0.28347
Epoch: 4400 Train: 0.17652 Test: 0.17420
Epoch: 4500 Train: 0.26215 Test: 0.33467
Epoch: 4600 Train: 0.19888 Test: 0.35846
Epoch: 4700 Train: 0.30413 Test: 0.32523
Epoch: 4800 Train: 1.31660 Test: 0.63110
Epoch: 4900 Train: 0.29865 Test: 0.41791
Epoch: 5000 Train: 1.60450 Test: 1.11425
Epoch: 5100 Train: 1.34912 Test: 0.72313
Epoch: 5200 Train: 0.75433 Test: 1.00608
Epoch: 5300 Train: 1.25284 Test: 1.34109
Epoch: 5400 Train: 0.06124 Test: 0.07249
Epoch: 5500 Train: 0.04614 Test: 0.05171
Epoch: 5600 Train: 1.27634 Test: 1.32879
Epoch: 5700 Train: 0.66688 Test: 0.68750
Epoch: 5800 Train: 2.59898 Test: 2.84343
Epoch: 5900 Train: 0.04207 Test: 0.04844
Epoch: 6000 Train: 0.93885 Test: 0.96411
Epoch 6100: New minimal relative error: 2.48%, model saved.
Epoch: 6100 Train: 0.08921 Test: 0.17818
Epoch: 6200 Train: 1.99643 Test: 2.35325
Epoch: 6300 Train: 0.03715 Test: 0.04330
Epoch: 6400 Train: 0.05899 Test: 0.06815
Epoch: 6500 Train: 0.10307 Test: 0.13334
Epoch: 6600 Train: 0.03496 Test: 0.04115
Epoch: 6700 Train: 0.06025 Test: 0.04878
Epoch: 6800 Train: 0.03370 Test: 0.04002
Epoch: 6900 Train: 0.90661 Test: 0.40961
Epoch: 7000 Train: 1.00863 Test: 0.63938
Epoch: 7100 Train: 0.03143 Test: 0.03770
Epoch: 7200 Train: 0.03260 Test: 0.03792
Epoch: 7300 Train: 1.73821 Test: 1.99957
Epoch: 7400 Train: 0.02971 Test: 0.03600
Epoch: 7500 Train: 0.27951 Test: 0.24125
Epoch: 7600 Train: 0.03021 Test: 0.03774
Epoch: 7700 Train: 0.02802 Test: 0.03430
Epoch: 7800 Train: 0.04270 Test: 0.06913
Epoch: 7900 Train: 0.02648 Test: 0.03282
Epoch: 8000 Train: 0.03031 Test: 0.03858
Epoch: 8100 Train: 0.05548 Test: 0.05615
Epoch: 8200 Train: 0.02512 Test: 0.03144
Epoch: 8300 Train: 0.02548 Test: 0.03145
Epoch: 8400 Train: 0.05752 Test: 0.19414
Epoch: 8500 Train: 0.02532 Test: 0.03716
Epoch: 8600 Train: 0.02357 Test: 0.02986
Epoch: 8700 Train: 0.02342 Test: 0.02936
Epoch: 8800 Train: 0.02219 Test: 0.02830
Epoch: 8900 Train: 0.02250 Test: 0.02870
Epoch: 9000 Train: 0.06088 Test: 0.09051
Epoch: 9100 Train: 0.02107 Test: 0.02707
Epoch: 9200 Train: 0.02155 Test: 0.02753
Epoch: 9300 Train: 0.02029 Test: 0.02620
Epoch: 9400 Train: 0.02848 Test: 0.03177
Epoch: 9500 Train: 0.14164 Test: 0.20190
Epoch: 9600 Train: 0.01937 Test: 0.02516
Epoch: 9700 Train: 0.02853 Test: 0.02670
Epoch: 9800 Train: 0.01872 Test: 0.02444
Epoch: 9900 Train: 0.09549 Test: 0.09657
Epoch: 9999 Train: 0.66848 Test: 0.74074
Training Loss: tensor(0.6685)
Test Loss: tensor(0.7407)
Learned LE: [ 9.096750e-01 -3.895538e-03 -5.035611e+00]
True LE: [ 8.7293535e-01 -5.8514234e-03 -1.4545120e+01]
Relative Error: [0.7394449  0.98268026 1.1794198  1.3193696  1.4981762  1.7185754
 1.7731491  1.490292   1.1314939  1.0368844  1.1622374  1.4460572
 1.6020218  1.382317   0.76793855 0.46469504 0.50241107 0.39280802
 0.3304529  0.45322877 0.57857114 0.6305334  0.63443613 0.7032223
 0.8466825  1.0154405  1.1768128  1.2638998  1.2245849  1.0704907
 1.1734266  1.4640019  1.5488238  1.4100153  1.4339592  1.2807848
 1.0081645  0.88394517 0.999553   1.0467428  1.0766188  1.2562706
 1.3551646  1.4047163  1.2665497  1.0244659  0.7929926  0.70017624
 0.74241316 0.80413616 0.8666552  0.87406796 0.842686   0.907039
 1.0501379  1.0669706  1.0990624  1.0585117  0.8238599  0.70621675
 0.68566424 0.6699108  0.65727735 0.7355423  0.90776765 1.0428307
 1.0814381  1.1181496  1.2375255  1.3138446  1.2199209  1.0278203
 0.9716152  1.0159041  1.149126   1.2805201  1.2738022  1.0405781
 0.6034778  0.5026189  0.37667552 0.3455566  0.47256714 0.58911234
 0.67972314 0.74971634 0.8738933  0.9951222  1.0322616  1.0873781
 1.2339249  1.3672209  1.3353963  1.1684715  1.2495873  1.2413124
 1.1264086  1.163902   1.2605014  1.1057805  0.81836265 0.85882276
 0.9834309  0.923313   1.0715145  1.215892   1.258066   1.1039824
 0.7796364  0.49078897 0.39775378 0.5258715  0.6801994  0.8154556
 0.86709446 0.8870853  1.0199361  1.1074698  1.0852627  1.1582513
 1.0145481  0.8008806  0.7700029  0.8182686  0.8614196  0.86674213
 0.89121544 1.0086988  1.1318632  1.1379423  1.0599802  1.0287514
 1.0313052  0.9074904  0.8089451  0.9239121  1.0470176  1.0460622
 1.0682054  1.0797237  1.0257219  0.87253106 0.644013   0.4837947
 0.41159275 0.4462594  0.6058562  0.7403777  0.83064836 0.936184
 1.0240357  0.99522525 0.9471665  1.0425683  1.2581466  1.4292122
 1.3776177  1.0773581  0.956468   0.9357853  0.93038845 1.0999132
 1.2127703  1.0408584  0.74441797 0.80107564 0.9712766  1.0294704
 1.1807665  1.1713516  1.009671   0.7461734  0.40186405 0.21689199
 0.29995787 0.51500756 0.68406045 0.76503086 0.85514617 1.0192001
 1.0526599  1.0124087  1.1219511  1.0106659  0.8267691  0.79620576
 0.86707056 0.9745326  1.0574753  1.083632   1.1234238  1.2355422
 1.3005288  1.2388922  1.1258346  1.0726271  0.99699646 0.7741238
 0.68194133 0.91472214 1.1605624  1.135989   1.0392208  0.939128
 0.83223283 0.7112564  0.6874873  0.4807951  0.41451836 0.4884157
 0.7449906  0.8733267  0.899869   0.9198144  0.84657    0.74609554
 0.76446354 0.9640972  1.2119955  1.3351779  1.3504803  0.99076796
 0.7281552  0.7740375  0.8053914  1.0386819  1.2573892  1.0830492
 0.8330097  0.7488891  1.0446066  1.2164253  1.266596   1.1039891
 0.75151044 0.48148906 0.2840618  0.18423606 0.40991497 0.58925176
 0.6192853  0.6739099  0.8169381  0.9273784  0.86061376 0.91684824
 0.97669077 0.85812855 0.85126066 0.88061    0.9638776  1.0952317
 1.2087898  1.2416643  1.2468067  1.3115975  1.3338405  1.2378098
 1.1175069  1.0670185  1.0801643  0.9675435  0.90072435 0.97991705
 1.1317254  1.2958294  1.1802378  0.9410517  0.7355014  0.635151
 0.61114436 0.54085934 0.41518942 0.4819423  0.71297514 0.7525484
 0.72797847 0.6851489  0.51013666 0.36890417 0.45440075 0.7056382
 0.9620378  1.170255   1.2746884  1.1265647  0.7632693  0.7179534
 0.72969973 0.71551734 1.217982   1.1877859  1.0089039  0.8420499
 0.9555758  1.2514879  1.4008213  1.171333   0.75301296 0.32917005
 0.2651127  0.27442974 0.54521245 0.71401    0.6194181  0.61908245
 0.6636011  0.74222213 0.7432587  0.82841635 0.8415889  0.80178607
 0.867279   0.9343952  0.971311   1.0110313  1.0984635  1.1911416
 1.2261468  1.2535961  1.2969317  1.226412   1.0498033  0.89736843
 0.81569487 0.8444813  0.9446091  1.186364   1.3086385  1.264382
 1.4112594  1.3907596  1.0031174  0.710251   0.71808076 0.61189157
 0.42658663 0.5474     0.56894547 0.51169974 0.4196672  0.38274905
 0.28163552 0.14364035 0.06730372 0.32492235 0.5713762  0.65803367
 0.8627054  1.023934   1.0388774  0.8797086  0.8809853  0.79848534
 0.655668   1.1771867  1.124191   0.98241705 0.8631653  0.9858514
 1.1980758  1.4392096  1.0695287  0.6362911  0.33103043 0.42554423
 0.6562861  0.93491346 0.82431614 0.72673714 0.64034474 0.6097916
 0.6178058  0.7243677  0.8711596  0.76684415 0.7564062  0.79507166
 0.8083029  0.8018499  0.84469503 0.958341   1.0869924  1.1466838
 1.1575478  1.1855665  1.1678516  1.0355614  0.8782407  0.7677927
 0.7018481  0.649733   0.74953115 1.1478102  1.3971254  1.5323546
 1.6982701  1.5785383  0.96394557 0.7382414 ]
