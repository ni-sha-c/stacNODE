time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.68%, model saved.
Epoch: 0 Train: 4142.35986 Test: 3886.22412
Epoch 100: New minimal relative error: 27.76%, model saved.
Epoch: 100 Train: 21.31861 Test: 20.89419
Epoch 200: New minimal relative error: 21.30%, model saved.
Epoch: 200 Train: 5.87609 Test: 5.60516
Epoch: 300 Train: 3.67902 Test: 3.52757
Epoch: 400 Train: 11.38570 Test: 12.68775
Epoch 500: New minimal relative error: 14.28%, model saved.
Epoch: 500 Train: 6.78437 Test: 7.69782
Epoch: 600 Train: 2.16981 Test: 2.54290
Epoch: 700 Train: 3.75678 Test: 3.21639
Epoch: 800 Train: 3.56166 Test: 2.86067
Epoch: 900 Train: 1.13405 Test: 1.35983
Epoch: 1000 Train: 0.44097 Test: 0.38430
Epoch 1100: New minimal relative error: 10.32%, model saved.
Epoch: 1100 Train: 1.20427 Test: 1.33101
Epoch: 1200 Train: 0.83839 Test: 0.74129
Epoch 1300: New minimal relative error: 9.86%, model saved.
Epoch: 1300 Train: 0.48536 Test: 0.56176
Epoch: 1400 Train: 6.44392 Test: 8.33083
Epoch: 1500 Train: 1.50765 Test: 2.28402
Epoch: 1600 Train: 0.86341 Test: 0.47998
Epoch: 1700 Train: 0.67340 Test: 0.40934
Epoch: 1800 Train: 0.62710 Test: 0.82276
Epoch: 1900 Train: 1.90022 Test: 2.61045
Epoch: 2000 Train: 1.00730 Test: 2.07564
Epoch: 2100 Train: 0.49865 Test: 0.65061
Epoch: 2200 Train: 2.23387 Test: 2.66884
Epoch: 2300 Train: 1.30322 Test: 1.00249
Epoch: 2400 Train: 3.72746 Test: 4.40447
Epoch: 2500 Train: 0.70395 Test: 0.42428
Epoch: 2600 Train: 0.26358 Test: 0.22976
Epoch: 2700 Train: 0.57489 Test: 0.53791
Epoch: 2800 Train: 0.94659 Test: 0.88898
Epoch: 2900 Train: 0.56657 Test: 0.67036
Epoch: 3000 Train: 0.11794 Test: 0.12557
Epoch: 3100 Train: 0.18085 Test: 0.19353
Epoch: 3200 Train: 0.13014 Test: 0.10557
Epoch: 3300 Train: 0.09115 Test: 0.08729
Epoch: 3400 Train: 0.21133 Test: 0.23997
Epoch: 3500 Train: 0.93148 Test: 0.47796
Epoch: 3600 Train: 0.18549 Test: 0.18015
Epoch: 3700 Train: 0.40224 Test: 0.52484
Epoch: 3800 Train: 1.03704 Test: 1.11711
Epoch: 3900 Train: 0.08089 Test: 0.08327
Epoch: 4000 Train: 0.08307 Test: 0.09549
Epoch: 4100 Train: 0.11151 Test: 0.08559
Epoch: 4200 Train: 0.17920 Test: 0.19623
Epoch: 4300 Train: 0.73552 Test: 0.92598
Epoch: 4400 Train: 0.05863 Test: 0.08878
Epoch: 4500 Train: 0.17470 Test: 0.17901
Epoch: 4600 Train: 0.07611 Test: 0.10163
Epoch: 4700 Train: 0.05473 Test: 0.06197
Epoch 4800: New minimal relative error: 8.45%, model saved.
Epoch: 4800 Train: 0.08595 Test: 0.08388
Epoch: 4900 Train: 3.04062 Test: 3.45202
Epoch: 5000 Train: 0.04867 Test: 0.05800
Epoch: 5100 Train: 0.29187 Test: 0.24739
Epoch: 5200 Train: 0.08069 Test: 0.09934
Epoch: 5300 Train: 0.04597 Test: 0.05548
Epoch: 5400 Train: 0.26308 Test: 0.34778
Epoch: 5500 Train: 0.23169 Test: 0.27759
Epoch: 5600 Train: 1.00347 Test: 0.51394
Epoch: 5700 Train: 0.08250 Test: 0.08032
Epoch: 5800 Train: 0.04830 Test: 0.05714
Epoch: 5900 Train: 0.40788 Test: 0.48717
Epoch: 6000 Train: 0.10326 Test: 0.16446
Epoch: 6100 Train: 0.18030 Test: 0.20714
Epoch: 6200 Train: 0.10266 Test: 0.12354
Epoch: 6300 Train: 0.03776 Test: 0.05193
Epoch: 6400 Train: 0.04112 Test: 0.05049
Epoch: 6500 Train: 0.03562 Test: 0.04744
Epoch: 6600 Train: 0.23137 Test: 0.30440
Epoch: 6700 Train: 0.03821 Test: 0.04807
Epoch: 6800 Train: 0.03609 Test: 0.04895
Epoch: 6900 Train: 0.07342 Test: 0.10228
Epoch: 7000 Train: 0.06275 Test: 0.08060
Epoch: 7100 Train: 0.14182 Test: 0.18601
Epoch: 7200 Train: 0.11385 Test: 0.12492
Epoch: 7300 Train: 0.03754 Test: 0.04048
Epoch: 7400 Train: 0.03032 Test: 0.04059
Epoch: 7500 Train: 0.11325 Test: 0.20758
Epoch: 7600 Train: 0.02760 Test: 0.03829
Epoch: 7700 Train: 0.02787 Test: 0.03836
Epoch: 7800 Train: 1.39700 Test: 1.40679
Epoch: 7900 Train: 0.02628 Test: 0.03729
Epoch: 8000 Train: 0.03114 Test: 0.04244
Epoch: 8100 Train: 0.27445 Test: 0.29843
Epoch: 8200 Train: 0.02519 Test: 0.03591
Epoch: 8300 Train: 0.02659 Test: 0.03850
Epoch: 8400 Train: 0.02484 Test: 0.03625
Epoch: 8500 Train: 0.02482 Test: 0.03554
Epoch: 8600 Train: 0.02403 Test: 0.03495
Epoch: 8700 Train: 0.02418 Test: 0.03681
Epoch: 8800 Train: 0.02304 Test: 0.03428
Epoch: 8900 Train: 0.02265 Test: 0.03333
Epoch: 9000 Train: 0.02803 Test: 0.03704
Epoch: 9100 Train: 0.02173 Test: 0.03251
Epoch: 9200 Train: 0.02186 Test: 0.03328
Epoch: 9300 Train: 0.06378 Test: 0.09274
Epoch: 9400 Train: 0.02096 Test: 0.03179
Epoch: 9500 Train: 0.04171 Test: 0.05895
Epoch: 9600 Train: 0.02038 Test: 0.03113
Epoch: 9700 Train: 0.03770 Test: 0.09993
Epoch: 9800 Train: 0.01980 Test: 0.03050
Epoch: 9900 Train: 0.02193 Test: 0.03297
Epoch: 9999 Train: 0.01931 Test: 0.02999
Training Loss: tensor(0.0193)
Test Loss: tensor(0.0300)
Learned LE: [ 0.85899997 -0.02685734 -5.5291157 ]
True LE: [ 8.4803832e-01  2.0820780e-05 -1.4529957e+01]
Relative Error: [3.1155531  2.87894    2.7025075  2.6007497  2.5546815  2.5157733
 2.4584446  2.4327314  2.5198693  2.7326677  3.0300565  3.3833482
 3.785228   4.234494   4.7350216  5.2969832  5.9244905  6.5959272
 7.2609982  7.8638425  8.369152   8.766538   9.052386   9.2051935
 9.143249   8.728549   7.891345   6.7129188  5.326161   3.878785
 2.7005334  2.425384   3.1750016  4.3665767  5.7880106  7.054495
 7.780503   8.269798   8.591504   8.749179   8.826502   8.88141
 8.8909855  8.823566   8.695812   8.526398   8.307819   8.071458
 7.877436   7.6716447  6.951132   5.9610405  5.2798886  4.7031345
 4.1371713  3.689889   3.297107   3.010099   2.9398565  2.9931185
 2.9646387  2.7639976  2.4833984  2.2592957  2.1386142  2.1095357
 2.1260529  2.1138468  2.0364988  1.9607666  2.0012753  2.180984
 2.4499683  2.7763362  3.1524558  3.5739248  4.040832   4.5645795
 5.158851   5.8138747  6.483789   7.1069813  7.6360235  8.054126
 8.367171   8.575271   8.614719   8.335217   7.6351314  6.5775476
 5.2660637  3.8009682  2.4787178  2.0565119  2.8655336  4.1200185
 5.52998    6.6147103  7.305781   7.8075213  8.030214   8.088318
 8.106227   8.124136   8.096169   7.9705772  7.7597675  7.542601
 7.4003706  7.313877   7.192975   7.0362315  6.735689   5.778747
 4.9184003  4.252036   3.5979078  3.087043   2.6700828  2.3789759
 2.3350296  2.4259198  2.423048   2.2355783  1.9732941  1.7985874
 1.7518408  1.7994474  1.8803697  1.8983195  1.8015549  1.6618948
 1.6291068  1.7507993  1.9700155  2.248599   2.580107   2.960942
 3.3878474  3.8657765  4.407611   5.0149064  5.656536   6.2747545
 6.8097167  7.229421   7.5459113  7.786507   7.9208784  7.8012333
 7.285681   6.408297   5.2470455  3.850635   2.4390216  1.7349836
 2.5173905  3.8694906  5.264054   6.1635747  6.8368564  7.291708
 7.453175   7.488766   7.472838   7.430333   7.3369355  7.1633844
 6.8907857  6.5620503  6.3534355  6.3954434  6.5482855  6.5014496
 6.3080335  5.7824535  4.741846   3.9512668  3.1952546  2.595297
 2.1593194  1.8592173  1.8134707  1.9065456  1.918148   1.7785747
 1.5901996  1.4918358  1.516161   1.6247151  1.7612017  1.8178847
 1.7209411  1.5277976  1.4088775  1.4518212  1.6047428  1.8182943
 2.0843081  2.405433   2.7825582  3.2125566  3.6935139  4.2266755
 4.800944   5.3749657  5.8840327  6.2811127  6.5778003  6.823394
 7.029298   7.077572   6.7836046  6.1290765  5.174672   3.928123
 2.536398   1.5112332  2.0459878  3.5119898  4.988046   5.7730074
 6.358955   6.7212954  6.8976326  6.9730453  6.9501705  6.857134
 6.6689863  6.3970685  6.0786138  5.7102027  5.40902    5.388607
 5.661888   5.9581275  5.8438325  5.5431123  4.7816954  3.8012803
 2.95647    2.2208996  1.7515467  1.4518046  1.397191   1.4623142
 1.4628692  1.3770251  1.2963864  1.2869219  1.3644885  1.5068151
 1.6766012  1.7765763  1.7083392  1.4964634  1.3067126  1.2661145
 1.347304   1.4933695  1.6843505  1.9240067  2.230206   2.6062753
 3.0294933  3.4816504  3.9610882  4.4499893  4.893574   5.244138
 5.5106306  5.7417164  5.9783654  6.167793   6.1148777  5.7163477
 5.011667   3.9782312  2.6975489  1.4902577  1.4970591  2.9242582
 4.575074   5.430913   5.9111     6.175521   6.365735   6.470521
 6.452588   6.3569503  6.1567936  5.800846   5.3536158  4.9321694
 4.6269784  4.514363   4.6637278  5.012704   5.321854   5.096249
 4.69268    3.8081863  2.8770978  2.0089352  1.4356232  1.1274065
 1.0752565  1.1203036  1.0798985  1.0185866  1.0371861  1.1185772
 1.2296902  1.3759857  1.5377954  1.6645802  1.6493944  1.4643337
 1.2444113  1.1428801  1.1618135  1.2531444  1.3860795  1.5449847
 1.7532334  2.05026    2.4157224  2.798756   3.1841912  3.5685341
 3.918916   4.2026887  4.43379    4.645355   4.8653784  5.1116004
 5.264152   5.1390204  4.7189007  3.967609   2.8878899  1.6811109
 1.0596464  2.1979015  3.8841765  5.0318885  5.520046   5.7207546
 5.9268613  6.033847   6.0045757  5.905021   5.7264395  5.399521
 4.912962   4.33528    3.9229538  3.78742    3.7917898  3.9728339
 4.329326   4.6523137  4.3537126  3.8729396  2.940231   1.9935108
 1.2318623  0.8750743  0.79356956 0.86774266 0.80589914 0.7056126
 0.77395236 0.9396083  1.0705243  1.186305   1.2985632  1.4102545
 1.4605653  1.3513176  1.144527   1.0165174  0.9943462  1.0398637
 1.137367   1.2573291  1.3877362  1.5784869  1.8621154  2.186655
 2.4980521  2.786671   3.0440414  3.259319   3.4463122  3.6345162
 3.8209803  4.022594   4.250005   4.3561144 ]
