time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.94%, model saved.
Epoch: 0 Train: 4178.01367 Test: 4177.87012
Epoch 100: New minimal relative error: 89.95%, model saved.
Epoch: 100 Train: 65.94640 Test: 90.15405
Epoch 200: New minimal relative error: 30.67%, model saved.
Epoch: 200 Train: 21.82079 Test: 20.66717
Epoch: 300 Train: 26.41348 Test: 41.28718
Epoch 400: New minimal relative error: 29.41%, model saved.
Epoch: 400 Train: 3.56370 Test: 8.02250
Epoch: 500 Train: 9.28403 Test: 19.16453
Epoch: 600 Train: 4.55782 Test: 6.52268
Epoch: 700 Train: 1.23874 Test: 4.86120
Epoch 800: New minimal relative error: 16.18%, model saved.
Epoch: 800 Train: 3.08063 Test: 6.13598
Epoch: 900 Train: 0.82485 Test: 4.24401
Epoch: 1000 Train: 2.46772 Test: 4.71570
Epoch: 1100 Train: 0.77669 Test: 3.97009
Epoch: 1200 Train: 0.64134 Test: 3.88705
Epoch: 1300 Train: 8.11776 Test: 9.06910
Epoch: 1400 Train: 0.40531 Test: 3.63459
Epoch: 1500 Train: 1.45548 Test: 4.84107
Epoch: 1600 Train: 0.89917 Test: 3.70570
Epoch: 1700 Train: 0.24515 Test: 3.25047
Epoch: 1800 Train: 0.21065 Test: 3.04629
Epoch: 1900 Train: 0.17559 Test: 3.60575
Epoch 2000: New minimal relative error: 16.17%, model saved.
Epoch: 2000 Train: 0.19675 Test: 3.64285
Epoch: 2100 Train: 4.36932 Test: 9.45717
Epoch: 2200 Train: 0.32641 Test: 4.13881
Epoch 2300: New minimal relative error: 14.02%, model saved.
Epoch: 2300 Train: 0.14920 Test: 3.64350
Epoch: 2400 Train: 0.37305 Test: 3.86197
Epoch: 2500 Train: 0.24922 Test: 3.61219
Epoch: 2600 Train: 0.14243 Test: 3.57321
Epoch: 2700 Train: 0.31690 Test: 3.75798
Epoch: 2800 Train: 2.66409 Test: 6.47543
Epoch: 2900 Train: 0.18379 Test: 3.68241
Epoch: 3000 Train: 0.27381 Test: 3.64403
Epoch: 3100 Train: 0.09486 Test: 3.48840
Epoch 3200: New minimal relative error: 11.60%, model saved.
Epoch: 3200 Train: 0.10575 Test: 3.47902
Epoch: 3300 Train: 1.22797 Test: 5.03688
Epoch: 3400 Train: 3.17516 Test: 7.05634
Epoch: 3500 Train: 3.75318 Test: 5.99443
Epoch: 3600 Train: 0.64553 Test: 3.92632
Epoch: 3700 Train: 0.20493 Test: 3.68897
Epoch: 3800 Train: 0.57352 Test: 3.94430
Epoch: 3900 Train: 0.28434 Test: 3.65501
Epoch: 4000 Train: 0.22791 Test: 3.65806
Epoch: 4100 Train: 0.22168 Test: 3.65907
Epoch: 4200 Train: 0.05421 Test: 3.44093
Epoch: 4300 Train: 0.07219 Test: 3.48886
Epoch: 4400 Train: 3.38067 Test: 7.11504
Epoch: 4500 Train: 0.32064 Test: 3.79263
Epoch: 4600 Train: 0.05746 Test: 3.45220
Epoch: 4700 Train: 0.09395 Test: 3.51855
Epoch: 4800 Train: 0.04199 Test: 3.43763
Epoch: 4900 Train: 0.04449 Test: 3.50242
Epoch: 5000 Train: 0.17804 Test: 3.47975
Epoch: 5100 Train: 0.04502 Test: 3.53933
Epoch: 5200 Train: 0.04979 Test: 3.43592
Epoch: 5300 Train: 0.03779 Test: 3.48628
Epoch: 5400 Train: 1.31892 Test: 4.76737
Epoch: 5500 Train: 0.03817 Test: 3.45412
Epoch: 5600 Train: 1.14013 Test: 4.82230
Epoch 5700: New minimal relative error: 10.19%, model saved.
Epoch: 5700 Train: 0.04171 Test: 3.45932
Epoch: 5800 Train: 0.03211 Test: 3.47131
Epoch: 5900 Train: 0.04924 Test: 3.52673
Epoch: 6000 Train: 0.03722 Test: 3.46813
Epoch: 6100 Train: 0.03190 Test: 3.48910
Epoch: 6200 Train: 0.05747 Test: 3.53168
Epoch: 6300 Train: 0.47177 Test: 4.10252
Epoch: 6400 Train: 1.49099 Test: 4.84668
Epoch: 6500 Train: 0.03335 Test: 3.48959
Epoch: 6600 Train: 0.02722 Test: 3.50775
Epoch: 6700 Train: 0.06120 Test: 3.58761
Epoch: 6800 Train: 0.21153 Test: 3.60823
Epoch: 6900 Train: 0.31026 Test: 3.70113
Epoch: 7000 Train: 0.02406 Test: 3.49616
Epoch: 7100 Train: 0.02778 Test: 3.56689
Epoch: 7200 Train: 0.02338 Test: 3.51239
Epoch: 7300 Train: 0.02653 Test: 3.52164
Epoch: 7400 Train: 0.12518 Test: 3.66767
Epoch: 7500 Train: 0.02223 Test: 3.52606
Epoch: 7600 Train: 1.35154 Test: 4.11910
Epoch: 7700 Train: 0.02164 Test: 3.55104
Epoch: 7800 Train: 0.02061 Test: 3.53951
Epoch: 7900 Train: 0.02299 Test: 3.58691
Epoch: 8000 Train: 0.05226 Test: 3.56990
Epoch: 8100 Train: 0.01964 Test: 3.55413
Epoch: 8200 Train: 0.02205 Test: 3.60080
Epoch: 8300 Train: 0.01909 Test: 3.56673
Epoch: 8400 Train: 0.08292 Test: 3.62816
Epoch: 8500 Train: 0.02050 Test: 3.57485
Epoch: 8600 Train: 0.01865 Test: 3.59541
Epoch: 8700 Train: 0.01787 Test: 3.57860
Epoch: 8800 Train: 0.01948 Test: 3.61661
Epoch: 8900 Train: 0.01740 Test: 3.58873
Epoch: 9000 Train: 0.04023 Test: 3.63411
Epoch: 9100 Train: 0.01702 Test: 3.59713
Epoch: 9200 Train: 0.01866 Test: 3.59408
Epoch: 9300 Train: 0.12204 Test: 3.82256
Epoch: 9400 Train: 0.01621 Test: 3.59759
Epoch: 9500 Train: 0.02219 Test: 3.65792
Epoch: 9600 Train: 0.01582 Test: 3.60205
Epoch: 9700 Train: 0.02072 Test: 3.60047
Epoch: 9800 Train: 0.03158 Test: 3.67003
Epoch: 9900 Train: 0.01512 Test: 3.60944
Epoch: 9999 Train: 0.25669 Test: 3.99848
Training Loss: tensor(0.2567)
Test Loss: tensor(3.9985)
Learned LE: [ 0.86139023 -0.014933   -5.040505  ]
True LE: [ 8.5678452e-01 -3.5800252e-03 -1.4525732e+01]
Relative Error: [0.5501022  0.88811743 1.6240088  2.2662268  2.781709   3.1990025
 3.562067   3.892635   4.1754513  4.3873386  4.5334315  4.647789
 4.767952   4.9079237  5.048845   5.151923   5.182662   5.133201
 5.0378437  4.966865   5.0095377  5.223174   5.4447837  5.5771923
 5.7768493  6.1582813  6.788785   7.277354   7.314369   7.232258
 6.9361343  6.3436613  5.682641   5.100951   4.6422386  4.2925572
 4.028626   3.8565834  3.794217   3.8153832  3.8513103  3.8652024
 3.865468   3.8607185  3.8482666  3.8306146  3.8240678  3.8529754
 3.9360783  4.0760326  4.257795   4.451589   4.614464   4.6942697
 4.6640835  4.5416956  4.3273196  4.007081   3.5485382  2.9432616
 2.2429996  1.4513382  0.72762793 0.87911606 1.5795424  2.2333243
 2.765507   3.1887202  3.5445466  3.866148   4.146676   4.3588567
 4.501818   4.6152134  4.750241   4.930997   5.1313205  5.2877326
 5.3388724  5.263708   5.102419   4.9433937  4.8802233  4.992531
 5.220935   5.331948   5.43839    5.6841316  6.214268   6.7399726
 6.8001323  6.755576   6.5359464  5.9968452  5.375761   4.8278027
 4.402001   4.065956   3.7824347  3.5719254  3.4807618  3.4953363
 3.5309405  3.5426757  3.5479438  3.5573137  3.5595174  3.5474226
 3.5328488  3.5438612  3.6105082  3.746799   3.9421656  4.1669803
 4.3777575  4.5158486  4.525848   4.418876   4.244939   3.9995656
 3.6299415  3.0974853  2.4675333  1.7658985  1.0645415  0.90399504
 1.4638824  2.118266   2.6813447  3.1256416  3.476644   3.780769
 4.0534577  4.2669578  4.4047494  4.5014358  4.62274    4.8164625
 5.068992   5.3018413  5.4183507  5.364209   5.163904   4.91935
 4.7529855  4.7467647  4.928976   5.082417   5.119173   5.2299714
 5.5975842  6.14775    6.2701306  6.2317066  6.111229   5.650217
 5.078639   4.563373   4.1661806  3.840245   3.5352328  3.2824821
 3.1534693  3.1583567  3.2000928  3.213111   3.219913   3.2380152
 3.2516115  3.2450671  3.223762   3.2140632  3.253083   3.370591
 3.5683923  3.8186717  4.0773435  4.2879343  4.377927   4.3103747
 4.156462   3.97622    3.7162018  3.288106   2.739441   2.1487458
 1.5075597  1.0658747  1.3060663  1.9112002  2.5130165  3.0008328
 3.360543   3.6439896  3.901877   4.1220264  4.264725   4.3405066
 4.4187446  4.5766325  4.837085   5.136642   5.3519187  5.381437
 5.2103443  4.9133625  4.6363525  4.5097375  4.5783215  4.7786455
 4.823004   4.8259315  4.9874663  5.4688525  5.731836   5.659658
 5.639354   5.2874336  4.7776384  4.294197   3.9142852  3.596559
 3.2867088  3.0119333  2.8450177  2.8290706  2.880916   2.9021711
 2.905578   2.9208786  2.935402   2.9279046  2.8980477  2.8681068
 2.8737576  2.953769   3.1302984  3.3887408  3.6844034  3.962317
 4.15466    4.1829987  4.0541425  3.898335   3.7426043  3.4614503
 3.0111973  2.5209541  1.9945681  1.4371331  1.2240186  1.6193718
 2.2368855  2.7930698  3.19505    3.471511   3.7050977  3.9290457
 4.0961156  4.1701694  4.196862   4.272123   4.4667583  4.769649
 5.076585   5.2368307  5.1727753  4.921741   4.5739965  4.298044
 4.229836   4.359036   4.520078   4.482437   4.4669533  4.7189646
 5.1390076  5.0829434  5.085946   4.8917675  4.457523   4.0126977
 3.6389463  3.3263783  3.0409951  2.7901762  2.6096294  2.555509
 2.6014645  2.6329262  2.6269822  2.6232529  2.6260188  2.6114395
 2.5698812  2.5185432  2.4903502  2.5221617  2.6461968  2.8747673
 3.1802351  3.5053823  3.7879553  3.9519858  3.9220839  3.7615075
 3.6387746  3.5156984  3.225873   2.8201575  2.4215004  1.9401401
 1.4340394  1.3427528  1.8351644  2.4641683  2.957273   3.2731473
 3.4882703  3.693746   3.892367   4.0090504  4.020271   4.0026608
 4.065288   4.2634087  4.5673847  4.860028   4.9710407  4.8511186
 4.5787654  4.2109146  3.932529   3.9089012  4.066997   4.1735535
 4.0801845  4.058847   4.3719997  4.567637   4.43999    4.431344
 4.1043835  3.7107344  3.3492699  3.03273    2.7865458  2.6113079
 2.4788117  2.4039009  2.4029276  2.409649   2.3739657  2.326121
 2.2985466  2.2780635  2.2376277  2.1761656  2.119825   2.1052618
 2.1637273  2.3169794  2.572912   2.9027112  3.2423952  3.5224118
 3.6622055  3.5925899  3.4216042  3.3413715  3.2550688  2.9948792
 2.6817026  2.3696866  1.913225   1.4123296  1.3848643  1.9639051
 2.5857513  3.017125   3.2708743  3.4462924  3.6367948  3.8237586
 3.9074936  3.867962   3.7989962  3.813372   3.961679   4.230015
 4.5151234  4.621534   4.4989996  4.2537365  3.8910673  3.5726337
 3.5463967  3.6968508  3.7730434  3.635376  ]
