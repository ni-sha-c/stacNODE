time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.37%, model saved.
Epoch: 0 Train: 3963.90259 Test: 3782.82690
Epoch 100: New minimal relative error: 17.82%, model saved.
Epoch: 100 Train: 44.72904 Test: 62.36820
Epoch: 200 Train: 15.12477 Test: 22.10418
Epoch: 300 Train: 6.36113 Test: 10.39025
Epoch: 400 Train: 20.49619 Test: 28.61158
Epoch 500: New minimal relative error: 15.17%, model saved.
Epoch: 500 Train: 2.51032 Test: 6.81317
Epoch 600: New minimal relative error: 14.12%, model saved.
Epoch: 600 Train: 1.43586 Test: 4.85080
Epoch: 700 Train: 10.44431 Test: 15.65416
Epoch: 800 Train: 1.08741 Test: 3.93115
Epoch: 900 Train: 8.26958 Test: 9.30850
Epoch: 1000 Train: 4.94852 Test: 5.92339
Epoch: 1100 Train: 1.68076 Test: 3.65923
Epoch: 1200 Train: 0.74482 Test: 3.48003
Epoch 1300: New minimal relative error: 9.95%, model saved.
Epoch: 1300 Train: 0.49858 Test: 3.16017
Epoch 1400: New minimal relative error: 6.96%, model saved.
Epoch: 1400 Train: 1.03829 Test: 3.28107
Epoch: 1500 Train: 0.42091 Test: 2.98466
Epoch: 1600 Train: 0.83508 Test: 2.96720
Epoch: 1700 Train: 0.35184 Test: 2.88314
Epoch: 1800 Train: 0.33978 Test: 2.84998
Epoch: 1900 Train: 0.32851 Test: 2.94176
Epoch: 2000 Train: 0.57281 Test: 3.15782
Epoch: 2100 Train: 0.61945 Test: 3.25066
Epoch: 2200 Train: 0.35885 Test: 2.92600
Epoch: 2300 Train: 0.23757 Test: 2.85369
Epoch: 2400 Train: 0.18690 Test: 2.76600
Epoch: 2500 Train: 0.23335 Test: 2.79472
Epoch: 2600 Train: 0.30290 Test: 2.89516
Epoch: 2700 Train: 0.21646 Test: 2.90022
Epoch: 2800 Train: 0.21646 Test: 2.95240
Epoch: 2900 Train: 1.05901 Test: 3.05366
Epoch: 3000 Train: 0.80578 Test: 2.99184
Epoch: 3100 Train: 0.12140 Test: 2.82623
Epoch: 3200 Train: 0.12141 Test: 2.86657
Epoch: 3300 Train: 0.15047 Test: 2.88868
Epoch: 3400 Train: 0.21983 Test: 2.97443
Epoch: 3500 Train: 0.13435 Test: 2.90286
Epoch: 3600 Train: 0.10410 Test: 2.85136
Epoch: 3700 Train: 0.10704 Test: 2.84629
Epoch: 3800 Train: 0.09477 Test: 2.83159
Epoch: 3900 Train: 0.15915 Test: 2.88517
Epoch: 4000 Train: 0.10391 Test: 2.85799
Epoch: 4100 Train: 0.09180 Test: 2.83171
Epoch: 4200 Train: 0.08770 Test: 2.83840
Epoch: 4300 Train: 0.31752 Test: 3.06130
Epoch: 4400 Train: 0.08284 Test: 2.85718
Epoch: 4500 Train: 0.81754 Test: 3.27011
Epoch: 4600 Train: 0.15182 Test: 2.95716
Epoch: 4700 Train: 0.07264 Test: 2.86786
Epoch: 4800 Train: 0.07260 Test: 2.90605
Epoch: 4900 Train: 0.38162 Test: 3.28509
Epoch: 5000 Train: 0.06750 Test: 2.90563
Epoch: 5100 Train: 0.06641 Test: 2.93525
Epoch: 5200 Train: 2.79392 Test: 5.30443
Epoch: 5300 Train: 0.06274 Test: 2.95208
Epoch: 5400 Train: 0.18091 Test: 3.14667
Epoch: 5500 Train: 0.07975 Test: 3.01157
Epoch 5600: New minimal relative error: 6.89%, model saved.
Epoch: 5600 Train: 0.05860 Test: 2.98545
Epoch: 5700 Train: 0.06188 Test: 3.00932
Epoch: 5800 Train: 0.05585 Test: 3.00285
Epoch: 5900 Train: 0.05817 Test: 3.06143
Epoch: 6000 Train: 0.81014 Test: 3.98513
Epoch: 6100 Train: 0.05341 Test: 3.06282
Epoch: 6200 Train: 0.05252 Test: 3.08822
Epoch: 6300 Train: 0.05067 Test: 3.08633
Epoch: 6400 Train: 0.06003 Test: 3.12740
Epoch: 6500 Train: 0.05108 Test: 3.11481
Epoch: 6600 Train: 0.04881 Test: 3.12926
Epoch: 6700 Train: 0.04743 Test: 3.14507
Epoch: 6800 Train: 0.33240 Test: 3.30294
Epoch: 6900 Train: 0.04563 Test: 3.16485
Epoch: 7000 Train: 0.10456 Test: 3.27492
Epoch: 7100 Train: 0.04416 Test: 3.19225
Epoch: 7200 Train: 0.04367 Test: 3.20764
Epoch: 7300 Train: 0.04256 Test: 3.21061
Epoch: 7400 Train: 0.04283 Test: 3.23328
Epoch: 7500 Train: 0.34979 Test: 3.68550
Epoch: 7600 Train: 0.07262 Test: 3.29418
Epoch: 7700 Train: 0.04012 Test: 3.27057
Epoch: 7800 Train: 0.03927 Test: 3.27583
Epoch: 7900 Train: 0.04273 Test: 3.29740
Epoch: 8000 Train: 0.03831 Test: 3.31004
Epoch: 8100 Train: 0.06608 Test: 3.35072
Epoch: 8200 Train: 0.03714 Test: 3.34170
Epoch: 8300 Train: 0.03676 Test: 3.35823
Epoch: 8400 Train: 0.03600 Test: 3.36691
Epoch: 8500 Train: 0.04331 Test: 3.38806
Epoch: 8600 Train: 0.03509 Test: 3.39811
Epoch: 8700 Train: 0.03478 Test: 3.41503
Epoch: 8800 Train: 0.08371 Test: 3.47117
Epoch: 8900 Train: 0.03363 Test: 3.43407
Epoch: 9000 Train: 0.03804 Test: 3.45119
Epoch: 9100 Train: 0.03285 Test: 3.46009
Epoch: 9200 Train: 0.05635 Test: 3.50504
Epoch: 9300 Train: 0.03205 Test: 3.47589
Epoch: 9400 Train: 0.03152 Test: 3.48199
Epoch: 9500 Train: 0.03220 Test: 3.50236
Epoch: 9600 Train: 0.03085 Test: 3.50397
Epoch: 9700 Train: 0.03067 Test: 3.51610
Epoch: 9800 Train: 0.03009 Test: 3.52679
Epoch: 9900 Train: 0.03033 Test: 3.53818
Epoch: 9999 Train: 0.02981 Test: 3.54408
Training Loss: tensor(0.0298)
Test Loss: tensor(3.5441)
Learned LE: [ 0.910864   0.0174624 -5.0342045]
True LE: [ 8.7474191e-01 -5.6506670e-04 -1.4554231e+01]
Relative Error: [1.2734663  1.2171699  1.1225985  1.0007656  0.8167481  0.585981
 0.41414854 0.421706   0.5415385  0.51165843 0.44686696 0.50004065
 0.6258584  0.7189569  0.76608056 0.7744786  0.63665783 0.28965664
 0.4184816  0.7404101  0.6119891  0.46473408 0.52781016 0.658761
 0.7611967  0.81412625 0.8616621  0.9847381  1.0127538  0.8955154
 0.83380675 0.6648066  0.5618713  0.63518625 0.59560525 0.7329349
 0.8007715  0.7613097  0.6435589  0.37603253 0.16578862 0.27126437
 0.22173072 0.23138669 0.22297089 0.24107629 0.24705108 0.14288674
 0.16510426 0.35198545 0.48848996 0.6046767  0.55694324 0.34222078
 0.19649553 0.20554972 0.30011317 0.43603364 0.5857477  0.72498405
 0.83479    0.90140796 0.9188897  0.88807076 0.8157816  0.74659836
 0.6563982  0.52390796 0.43020433 0.50879085 0.64881897 0.59334576
 0.49184626 0.4634499  0.60195756 0.75727195 0.8234473  0.8698806
 0.83031255 0.5771102  0.3846418  0.65147    0.5809446  0.40661114
 0.32882473 0.26097658 0.3589233  0.4406202  0.46890354 0.5158134
 0.71327496 0.869349   0.7402476  0.6723959  0.5828115  0.408865
 0.52883726 0.5041212  0.6169101  0.6645434  0.5348722  0.2936016
 0.09610008 0.29888567 0.32484552 0.31469378 0.29011795 0.21481435
 0.20532624 0.19744067 0.06448854 0.17119189 0.29019943 0.37520492
 0.4975332  0.5399826  0.42028823 0.24737802 0.16256143 0.18130864
 0.27588105 0.38837248 0.489547   0.56497824 0.60134816 0.58880407
 0.53575474 0.4690393  0.4308001  0.37104028 0.3246914  0.4348882
 0.66770625 0.7124548  0.6163291  0.5720192  0.5921605  0.75741994
 0.8444933  0.88658667 0.91419035 0.7643024  0.44747922 0.5873243
 0.7100092  0.6654364  0.57118356 0.4266781  0.19692671 0.11148313
 0.15769921 0.18631758 0.24618609 0.46265805 0.7596604  0.66830635
 0.5504878  0.5192899  0.4277961  0.5182022  0.473628   0.41959706
 0.50587213 0.36905524 0.20955168 0.31056625 0.2886395  0.27955565
 0.35852578 0.33087456 0.24224474 0.20502138 0.12107544 0.06230924
 0.13655876 0.17969824 0.22748989 0.3218715  0.4718264  0.5046431
 0.3358996  0.19456743 0.13227636 0.16542134 0.22953767 0.2809754
 0.3159269  0.33044595 0.30757996 0.24634951 0.17924093 0.15736206
 0.14722814 0.24533615 0.50686246 0.74045396 0.7115029  0.69326675
 0.6895439  0.7404049  0.8574389  0.87070894 0.9109981  0.8786228
 0.588792   0.42261901 0.65651274 0.7570024  0.7571348  0.6845409
 0.5906631  0.3858185  0.21770307 0.15547408 0.1170003  0.03830029
 0.21140955 0.5404008  0.6573977  0.47205248 0.42687798 0.53283685
 0.519869   0.52791226 0.2662078  0.35098404 0.29948118 0.35954678
 0.2951118  0.25396308 0.18352637 0.27333018 0.24676812 0.14669895
 0.15153103 0.12650494 0.12944005 0.1163648  0.09504806 0.12553641
 0.18996301 0.3502177  0.5332423  0.454648   0.25581583 0.14015499
 0.12699163 0.15469223 0.15256093 0.12259847 0.1209307  0.13166939
 0.08518967 0.11281908 0.13197571 0.20245157 0.3205775  0.5654773
 0.7391381  0.69204676 0.7403825  0.7683753  0.82086706 0.8914697
 0.85895455 0.90550894 0.8402276  0.55210876 0.5485825  0.67568696
 0.6879037  0.6628524  0.6265731  0.60211855 0.46568447 0.29058152
 0.20701542 0.18443654 0.14489797 0.10988522 0.29205388 0.54664195
 0.49357015 0.36101118 0.40133464 0.5983359  0.53981584 0.40592933
 0.1488335  0.3357049  0.44201446 0.19105253 0.2753099  0.16311672
 0.15160038 0.17647313 0.12680246 0.1685913  0.07669841 0.18751407
 0.16286656 0.10553936 0.10617438 0.1537804  0.2384722  0.3957538
 0.52892596 0.37125936 0.17334391 0.10991632 0.13175184 0.131847
 0.07556601 0.04326159 0.11806557 0.13601814 0.22483625 0.30751762
 0.35976687 0.4483305  0.604956   0.71624744 0.5940614  0.65608436
 0.7121123  0.7268873  0.8122416  0.79866123 0.8768631  0.8948813
 0.74558455 0.8106697  0.87738913 0.7447528  0.48079556 0.35001493
 0.32281864 0.32760718 0.4694637  0.5556567  0.5669695  0.5457804
 0.42914724 0.21716182 0.23537377 0.47364974 0.4267816  0.3090224
 0.35550058 0.59269726 0.49232602 0.3881033  0.18599424 0.4699714
 0.12040118 0.23341717 0.21714999 0.13652271 0.12380689 0.21850553
 0.3120996  0.27550498 0.12176982 0.22367197 0.15898404 0.10731541
 0.16616939 0.19833925 0.29538444 0.31755927 0.4357059  0.29095915
 0.11239854 0.08533881 0.09858017 0.10641617 0.11167804 0.12675202
 0.13586318 0.19308783 0.3268119  0.42687157 0.5057642  0.6186796
 0.7451097  0.5554943  0.40968493 0.5257539  0.51547176 0.45758435
 0.484773   0.6019424  0.83877045 0.8838488  1.0226301  1.2059499
 1.2018154  0.9131564  0.63339883 0.608747  ]
