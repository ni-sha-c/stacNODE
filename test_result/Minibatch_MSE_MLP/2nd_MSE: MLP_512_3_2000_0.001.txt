time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.25%, model saved.
Epoch: 0 Train: 4066.59741 Test: 3948.89087
Epoch 100: New minimal relative error: 17.18%, model saved.
Epoch: 100 Train: 46.09220 Test: 64.75483
Epoch: 200 Train: 12.26039 Test: 13.51845
Epoch: 300 Train: 4.89084 Test: 6.02060
Epoch: 400 Train: 43.92196 Test: 38.58026
Epoch: 500 Train: 17.10927 Test: 17.97237
Epoch 600: New minimal relative error: 16.14%, model saved.
Epoch: 600 Train: 1.31673 Test: 2.19104
Epoch: 700 Train: 3.55673 Test: 3.73741
Epoch 800: New minimal relative error: 11.36%, model saved.
Epoch: 800 Train: 0.78552 Test: 1.62839
Epoch: 900 Train: 1.14479 Test: 2.18823
Epoch: 1000 Train: 0.52950 Test: 1.15804
Epoch: 1100 Train: 0.40826 Test: 0.98570
Epoch: 1200 Train: 0.35369 Test: 0.90472
Epoch: 1300 Train: 0.31559 Test: 0.81269
Epoch: 1400 Train: 0.63770 Test: 1.24278
Epoch: 1500 Train: 1.37352 Test: 1.24942
Epoch 1600: New minimal relative error: 10.94%, model saved.
Epoch: 1600 Train: 0.50781 Test: 0.85440
Epoch: 1700 Train: 0.42728 Test: 0.67411
Epoch: 1800 Train: 8.26067 Test: 8.66652
Epoch: 1900 Train: 0.26704 Test: 0.60493
Epoch: 2000 Train: 0.16790 Test: 0.55531
Epoch: 2100 Train: 0.47506 Test: 0.81137
Epoch: 2200 Train: 0.80530 Test: 0.99760
Epoch: 2300 Train: 0.18391 Test: 0.57684
Epoch: 2400 Train: 0.50996 Test: 0.88492
Epoch: 2500 Train: 1.84623 Test: 0.98870
Epoch: 2600 Train: 0.20817 Test: 0.57069
Epoch: 2700 Train: 0.18519 Test: 0.55316
Epoch: 2800 Train: 0.13943 Test: 0.46088
Epoch: 2900 Train: 0.23080 Test: 0.56256
Epoch: 3000 Train: 0.43389 Test: 0.84622
Epoch: 3100 Train: 0.18078 Test: 0.49016
Epoch: 3200 Train: 0.09578 Test: 0.39854
Epoch: 3300 Train: 1.95612 Test: 1.31381
Epoch: 3400 Train: 0.08255 Test: 0.38212
Epoch: 3500 Train: 0.08344 Test: 0.38122
Epoch: 3600 Train: 0.07943 Test: 0.37647
Epoch: 3700 Train: 0.07837 Test: 0.39420
Epoch: 3800 Train: 0.96936 Test: 1.57229
Epoch: 3900 Train: 0.07023 Test: 0.36024
Epoch: 4000 Train: 0.12534 Test: 0.42128
Epoch: 4100 Train: 0.07511 Test: 0.36155
Epoch: 4200 Train: 0.06789 Test: 0.34992
Epoch: 4300 Train: 0.06640 Test: 0.35087
Epoch: 4400 Train: 0.70460 Test: 1.12339
Epoch: 4500 Train: 0.08219 Test: 0.36566
Epoch: 4600 Train: 0.05976 Test: 0.33449
Epoch: 4700 Train: 0.07299 Test: 0.34950
Epoch: 4800 Train: 0.06310 Test: 0.33563
Epoch: 4900 Train: 0.05659 Test: 0.32717
Epoch: 5000 Train: 0.05959 Test: 0.33158
Epoch: 5100 Train: 0.05111 Test: 0.32094
Epoch: 5200 Train: 0.06168 Test: 0.32624
Epoch: 5300 Train: 0.05880 Test: 0.32601
Epoch: 5400 Train: 0.06074 Test: 0.32815
Epoch: 5500 Train: 0.04649 Test: 0.31013
Epoch: 5600 Train: 0.05163 Test: 0.36538
Epoch: 5700 Train: 0.04513 Test: 0.30823
Epoch 5800: New minimal relative error: 10.04%, model saved.
Epoch: 5800 Train: 0.05072 Test: 0.31085
Epoch: 5900 Train: 0.04316 Test: 0.30194
Epoch: 6000 Train: 0.10083 Test: 0.32657
Epoch: 6100 Train: 0.04161 Test: 0.29896
Epoch: 6200 Train: 0.05780 Test: 0.35058
Epoch: 6300 Train: 0.05397 Test: 0.30893
Epoch: 6400 Train: 0.03895 Test: 0.29088
Epoch: 6500 Train: 0.04306 Test: 0.29689
Epoch: 6600 Train: 0.03761 Test: 0.28859
Epoch: 6700 Train: 0.28344 Test: 0.63337
Epoch: 6800 Train: 0.03665 Test: 0.28986
Epoch 6900: New minimal relative error: 5.89%, model saved.
Epoch: 6900 Train: 0.07809 Test: 0.34842
Epoch: 7000 Train: 0.03549 Test: 0.28543
Epoch: 7100 Train: 0.03460 Test: 0.28202
Epoch: 7200 Train: 0.03510 Test: 0.28542
Epoch: 7300 Train: 0.03358 Test: 0.28065
Epoch: 7400 Train: 0.06929 Test: 0.31324
Epoch: 7500 Train: 0.04194 Test: 0.28816
Epoch: 7600 Train: 0.08041 Test: 0.35271
Epoch: 7700 Train: 0.03164 Test: 0.27650
Epoch: 7800 Train: 0.03242 Test: 0.27808
Epoch: 7900 Train: 0.36588 Test: 0.62724
Epoch: 8000 Train: 0.04834 Test: 0.29736
Epoch: 8100 Train: 0.20067 Test: 0.44880
Epoch: 8200 Train: 0.03894 Test: 0.27660
Epoch: 8300 Train: 0.02916 Test: 0.27197
Epoch: 8400 Train: 0.03378 Test: 0.27502
Epoch: 8500 Train: 0.03331 Test: 0.27451
Epoch: 8600 Train: 0.03588 Test: 0.27789
Epoch: 8700 Train: 0.02764 Test: 0.26828
Epoch: 8800 Train: 0.03051 Test: 0.27057
Epoch: 8900 Train: 0.02710 Test: 0.26734
Epoch: 9000 Train: 0.04631 Test: 0.28942
Epoch: 9100 Train: 0.02629 Test: 0.26513
Epoch: 9200 Train: 0.03211 Test: 0.26819
Epoch: 9300 Train: 0.03150 Test: 0.27345
Epoch: 9400 Train: 0.02542 Test: 0.26529
Epoch: 9500 Train: 0.03886 Test: 0.26781
Epoch: 9600 Train: 0.02488 Test: 0.26592
Epoch: 9700 Train: 0.02553 Test: 0.26667
Epoch: 9800 Train: 0.10550 Test: 0.36575
Epoch: 9900 Train: 0.02405 Test: 0.26521
Epoch: 9999 Train: 0.02473 Test: 0.26887
Training Loss: tensor(0.0247)
Test Loss: tensor(0.2689)
Learned LE: [ 0.9233122  -0.02895746 -5.3597517 ]
True LE: [ 8.5323465e-01 -4.0212837e-03 -1.4531522e+01]
Relative Error: [ 2.324192    2.2868738   2.3548517   2.5267544   2.8036208   3.1854894
  3.666434    4.2315965   4.8501925   5.483029    6.1379504   6.910669
  7.854192    8.82595     9.599899   10.071432   10.257677   10.177689
  9.841389    9.32951     8.8100195   8.4315405   8.2112255   8.084921
  8.04056     8.0866375   8.117618    7.950457    7.477697    6.762071
  5.9801316   5.227864    4.489077    3.812816    3.3252132   3.0795596
  2.926031    2.7487707   2.548093    2.3450007   2.148569    2.0186596
  2.0601995   2.2733028   2.5553496   2.8512757   3.1446178   3.396428
  3.5555568   3.6048915   3.5598524   3.441314    3.2656708   3.088226
  2.9621282   2.8449984   2.7005286   2.6001246   2.5932596   2.5517051
  2.3522859   2.0912473   1.8992635   1.8141799   1.8362346   1.9557284
  2.167215    2.4733114   2.874993    3.3630962   3.9114692   4.4820642
  5.0916305   5.8605185   6.835078    7.8212156   8.578708    9.053045
  9.298043    9.3044615   9.027119    8.529549    8.015593    7.6685214
  7.496047    7.403787    7.3753433   7.4366546   7.4787693   7.3121285
  6.837829    6.125368    5.3575068   4.652484    3.976634    3.2995348
  2.7761276   2.543847    2.4281118   2.2722015   2.0789754   1.8782368
  1.6876206   1.5715606   1.6373552   1.8496974   2.079196    2.293226
  2.524712    2.7749367   2.9821877   3.091673    3.095993    3.0148664
  2.8651977   2.6908836   2.5549107   2.4258618   2.2691932   2.1696477
  2.2025118   2.2429163   2.0999079   1.8294225   1.5936071   1.4580572
  1.4278027   1.4815507   1.6072522   1.8164223   2.1220343   2.5211422
  2.990243    3.4896333   4.036398    4.774874    5.7579665   6.754515
  7.513502    8.01547     8.3276005   8.415627    8.20534     7.7479362
  7.260526    6.9327517   6.767102    6.666857    6.6206975   6.676124
  6.730722    6.5941386   6.1692824   5.5124383   4.7915277   4.1218247
  3.5198085   2.9075584   2.3505776   2.0984347   2.0132637   1.885993
  1.7064754   1.5051981   1.3245019   1.2283517   1.315241    1.5231144
  1.7031146   1.8336881   1.9818727   2.2012877   2.444319    2.6172998
  2.680735    2.6456287   2.5336375   2.372037    2.2271287   2.0905342
  1.9305036   1.8339319   1.888851    2.0021179   1.9408315   1.6855952
  1.4195082   1.2383366   1.1538279   1.135255    1.160613    1.2514747
  1.4421546   1.7403213   2.1222968   2.5428813   3.0052176   3.668766
  4.617544    5.610898    6.3885674   6.9453573   7.3321857   7.4899807
  7.3504157   6.9681892   6.5438805   6.2289314   6.0269294   5.884277
  5.796464    5.819901    5.870298    5.7685604   5.4304786   4.879557
  4.2539134   3.6451108   3.083705    2.5742016   2.0591354   1.7453527
  1.6826763   1.5766315   1.4285085   1.2326221   1.0634587   0.9831609
  1.0738269   1.2797307   1.4358929   1.5114508   1.5789464   1.7253273
  1.958823    2.177889    2.2997494   2.3140538   2.2490768   2.121164
  1.9782256   1.8399705   1.6836619   1.5924791   1.6488204   1.8024971
  1.8400981   1.6370965   1.363826    1.1525557   1.0253539   0.95664656
  0.9059986   0.8767617   0.92298985  1.0909543   1.3691171   1.703606
  2.065436    2.6022346   3.4540582   4.408851    5.194577    5.8103266
  6.284345    6.516952    6.449938    6.1657147   5.848546    5.5683866
  5.306322    5.0945396   4.9545894   4.9296737   4.9583263   4.863642
  4.5909457   4.1611857   3.6772158   3.1946297   2.7020211   2.2397673
  1.8512186   1.4893665   1.4180217   1.332324    1.2171739   1.0610199
  0.9035112   0.82149154  0.8860323   1.0945563   1.2701714   1.3429933
  1.3703232   1.4277971   1.5759977   1.7892406   1.9535122   2.0120318
  1.9844049   1.905299    1.7914971   1.6668506   1.5199949   1.4322639
  1.4768964   1.6232057   1.7464489   1.6390219   1.3841453   1.1632491
  1.0144821   0.9340534   0.8796402   0.8055361   0.7208998   0.70073944
  0.814917    1.0365692   1.2924018   1.6640035   2.348859    3.216056
  3.9732635   4.6021924   5.1459584   5.4779587   5.5071135   5.3178835
  5.1191187   4.92386     4.622736    4.3122067   4.0999537   4.016101
  4.029999    3.9550848   3.7200122   3.3473318   2.9693105   2.6652877
  2.3414683   1.9575841   1.6011887   1.3601642   1.1447438   1.1435175
  1.0075035   0.9365565   0.81927145  0.720638    0.7308494   0.92283326
  1.1573905   1.2892891   1.3395817   1.3571161   1.3859317   1.4984818
  1.6584827   1.7503825   1.740545    1.6848465   1.618481    1.5415434
  1.4247787   1.3340259   1.3608644   1.4653984   1.6121106   1.632791
  1.4307506   1.2136912   1.06356     0.98301375  0.95363504  0.91587967
  0.8229849   0.68858975  0.5902396   0.61392474  0.73913836  0.93545073
  1.3899083   2.1117053   2.815745    3.3971033   3.9326377   4.3383317
  4.5037627   4.421974    4.2871017   4.1979322   3.9519722   3.5606277
  3.232665    3.0548398   3.0257533   2.9994848 ]
