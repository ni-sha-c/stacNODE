time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.55%, model saved.
Epoch: 0 Train: 3884.69409 Test: 4003.52319
Epoch 100: New minimal relative error: 59.92%, model saved.
Epoch: 100 Train: 22.04027 Test: 19.29675
Epoch: 200 Train: 50.63327 Test: 33.09602
Epoch: 300 Train: 26.42965 Test: 33.49604
Epoch: 400 Train: 8.59912 Test: 11.26496
Epoch 500: New minimal relative error: 57.89%, model saved.
Epoch: 500 Train: 8.28610 Test: 12.36539
Epoch 600: New minimal relative error: 33.22%, model saved.
Epoch: 600 Train: 9.22132 Test: 8.10694
Epoch 700: New minimal relative error: 30.00%, model saved.
Epoch: 700 Train: 2.72921 Test: 1.61290
Epoch 800: New minimal relative error: 21.74%, model saved.
Epoch: 800 Train: 6.78548 Test: 8.93959
Epoch: 900 Train: 1.34578 Test: 2.20124
Epoch: 1000 Train: 4.75810 Test: 4.35419
Epoch: 1100 Train: 0.22509 Test: 0.33053
Epoch: 1200 Train: 5.31666 Test: 6.77831
Epoch: 1300 Train: 10.38191 Test: 10.96991
Epoch: 1400 Train: 2.78023 Test: 2.91800
Epoch: 1500 Train: 2.85000 Test: 3.19566
Epoch: 1600 Train: 0.90028 Test: 0.75814
Epoch: 1700 Train: 1.33461 Test: 1.70606
Epoch: 1800 Train: 0.77232 Test: 0.98293
Epoch: 1900 Train: 3.42413 Test: 4.22949
Epoch: 2000 Train: 0.75856 Test: 1.04074
Epoch 2100: New minimal relative error: 15.91%, model saved.
Epoch: 2100 Train: 1.37861 Test: 0.91019
Epoch: 2200 Train: 0.82549 Test: 1.03599
Epoch: 2300 Train: 1.63292 Test: 2.22677
Epoch: 2400 Train: 2.74090 Test: 3.36756
Epoch: 2500 Train: 0.27564 Test: 0.33773
Epoch: 2600 Train: 2.05786 Test: 2.22045
Epoch: 2700 Train: 1.27278 Test: 1.24036
Epoch: 2800 Train: 2.33520 Test: 2.88963
Epoch: 2900 Train: 2.21803 Test: 2.31798
Epoch: 3000 Train: 4.82678 Test: 5.34983
Epoch: 3100 Train: 0.71445 Test: 0.67854
Epoch: 3200 Train: 1.77179 Test: 2.15281
Epoch: 3300 Train: 0.09888 Test: 0.10947
Epoch 3400: New minimal relative error: 11.46%, model saved.
Epoch: 3400 Train: 2.68439 Test: 3.00240
Epoch: 3500 Train: 0.84092 Test: 0.95729
Epoch: 3600 Train: 1.11878 Test: 1.08612
Epoch: 3700 Train: 0.53306 Test: 0.60953
Epoch: 3800 Train: 0.27407 Test: 0.30814
Epoch: 3900 Train: 0.56393 Test: 0.72453
Epoch: 4000 Train: 1.12760 Test: 1.05727
Epoch: 4100 Train: 1.46040 Test: 1.28980
Epoch: 4200 Train: 0.26189 Test: 0.31679
Epoch: 4300 Train: 0.50761 Test: 0.64930
Epoch: 4400 Train: 1.41732 Test: 1.10229
Epoch: 4500 Train: 1.31118 Test: 0.96416
Epoch: 4600 Train: 0.40918 Test: 0.39583
Epoch: 4700 Train: 0.56864 Test: 0.71598
Epoch: 4800 Train: 0.61611 Test: 0.62452
Epoch: 4900 Train: 0.95698 Test: 1.28049
Epoch: 5000 Train: 0.28980 Test: 0.39802
Epoch: 5100 Train: 0.76706 Test: 1.06768
Epoch: 5200 Train: 0.03063 Test: 0.06547
Epoch: 5300 Train: 0.44245 Test: 0.57466
Epoch: 5400 Train: 0.18164 Test: 0.22514
Epoch: 5500 Train: 0.20672 Test: 0.25079
Epoch: 5600 Train: 0.38455 Test: 0.38671
Epoch: 5700 Train: 0.15681 Test: 0.20544
Epoch: 5800 Train: 0.10293 Test: 0.20422
Epoch: 5900 Train: 0.62957 Test: 0.86477
Epoch: 6000 Train: 0.29669 Test: 0.33773
Epoch: 6100 Train: 0.02104 Test: 0.06335
Epoch: 6200 Train: 0.18502 Test: 0.21228
Epoch: 6300 Train: 0.30128 Test: 0.40723
Epoch: 6400 Train: 0.19925 Test: 0.28581
Epoch: 6500 Train: 0.07844 Test: 0.11033
Epoch: 6600 Train: 0.43069 Test: 0.53379
Epoch: 6700 Train: 0.33331 Test: 0.30886
Epoch: 6800 Train: 0.10466 Test: 0.14329
Epoch: 6900 Train: 0.13573 Test: 0.16477
Epoch: 7000 Train: 0.11254 Test: 0.12447
Epoch: 7100 Train: 0.27001 Test: 0.31471
Epoch: 7200 Train: 0.20833 Test: 0.26640
Epoch: 7300 Train: 0.03246 Test: 0.06793
Epoch: 7400 Train: 0.11029 Test: 0.08235
Epoch: 7500 Train: 0.86705 Test: 1.09166
Epoch: 7600 Train: 0.03207 Test: 0.07221
Epoch: 7700 Train: 0.23254 Test: 0.30731
Epoch: 7800 Train: 0.13626 Test: 0.15649
Epoch: 7900 Train: 0.11764 Test: 0.17339
Epoch: 8000 Train: 0.13849 Test: 0.19994
Epoch: 8100 Train: 0.06322 Test: 0.10568
Epoch: 8200 Train: 0.03628 Test: 0.07086
Epoch: 8300 Train: 0.15104 Test: 0.18320
Epoch: 8400 Train: 0.04250 Test: 0.07387
Epoch: 8500 Train: 0.03964 Test: 0.05458
Epoch: 8600 Train: 0.11735 Test: 0.14207
Epoch: 8700 Train: 0.08438 Test: 0.08871
Epoch: 8800 Train: 0.03826 Test: 0.06870
Epoch: 8900 Train: 0.02590 Test: 0.05130
Epoch: 9000 Train: 0.08626 Test: 0.10605
Epoch: 9100 Train: 0.02070 Test: 0.04610
Epoch: 9200 Train: 0.02950 Test: 0.06414
Epoch: 9300 Train: 0.04546 Test: 0.05846
Epoch: 9400 Train: 0.07407 Test: 0.08618
Epoch: 9500 Train: 0.00750 Test: 0.03149
Epoch: 9600 Train: 0.01324 Test: 0.03642
Epoch: 9700 Train: 0.22800 Test: 0.27582
Epoch: 9800 Train: 0.03839 Test: 0.06406
Epoch: 9900 Train: 0.13241 Test: 0.18457
Epoch: 9999 Train: 0.00786 Test: 0.03318
Training Loss: tensor(0.0079)
Test Loss: tensor(0.0332)
Learned LE: [ 0.8404192   0.08885321 -5.1225896 ]
True LE: [ 8.75663280e-01  9.60881915e-03 -1.45660515e+01]
Relative Error: [ 5.293147   5.654299   6.327153   7.129412   7.711912   7.8199043
  7.3911505  6.54053    5.5549445  4.952081   5.3355756  6.7461743
  8.7092905 10.800271  12.742406  14.389388  15.641196  15.740376
 15.083012  14.228505  13.240293  12.230772  11.266815  10.369192
  9.537899   8.762685   8.016465   7.2674403  6.5238824  5.8588257
  5.3573613  5.0501814  4.8861403  4.8325834  4.9703546  5.3689857
  5.9840126  6.687685   7.3327804  7.8084216  8.060999   8.07093
  7.8340874  7.3739142  6.755949   6.075868   5.429372   4.883711
  4.4784727  4.2319136  3.7891414  3.2724674  3.020442   2.9648902
  3.0511315  3.2320647  3.4682908  3.7290373  3.997127   4.270956
  4.538932   4.752082   4.9337974  5.2986846  5.99587    6.8313704
  7.4380026  7.561766   7.1531305  6.3306055  5.3519483  4.6588106
  4.8190036  5.980623   7.7428193  9.688526  11.535762  13.122219
 14.356585  14.468248  13.828842  12.98812   12.014755  11.030145
 10.102462   9.245946   8.455877   7.7241645  7.024814   6.31525
  5.5870733  4.9136376  4.399507   4.088813   3.9043531  3.773068
  3.7930324  4.0896983  4.651496   5.346725   6.0041027  6.4930677
  6.759082   6.788815   6.5786414  6.1502857  5.571036   4.940348
  4.3547096  3.8763418  3.5317566  3.3444684  3.0325484  2.6605294
  2.5456738  2.5934973  2.7392724  2.9420795  3.1754549  3.4190588
  3.6614478  3.9067812  4.155874   4.3652687  4.5359793  4.8772516
  5.56837    6.4249477  7.0643516  7.226298   6.8677416  6.1096
  5.1805305  4.433065   4.3502703  5.1860733  6.6758394  8.430181
 10.1610365 11.683559  12.920342  13.136848  12.539472  11.735485
 10.793768   9.846054   8.968339   8.169268   7.4355245  6.7596664
  6.120898   5.47058    4.778577   4.1097093  3.5948687  3.3063538
  3.1422775  2.954183   2.8293388  2.9692519  3.4366965  4.108396
  4.7796288  5.2869086  5.5702734  5.625705   5.452859   5.071663
  4.550167   3.9900105  3.4844208  3.0833647  2.7970965  2.6394687
  2.4549112  2.2364972  2.2453449  2.363904   2.5317707  2.723969
  2.9300165  3.138941   3.342622   3.5470545  3.764388   3.964122
  4.118817   4.408993   5.0561604  5.916387   6.5974736  6.820344
  6.5392146  5.8763933  5.0374327  4.289756   3.9901805  4.439989
  5.5687466  7.064766   8.640268  10.085254  11.316512  11.739007
 11.212693  10.468693   9.573856   8.670358   7.8522215  7.1288457
  6.4717507  5.866154   5.2979074  4.7224627  4.090489   3.442902
  2.938194   2.699962   2.6202931  2.4442294  2.1902008  2.1107407
  2.4059362  3.012924   3.6893427  4.2178836  4.520392   4.604783
  4.4794493  4.1642246  3.7281997  3.2734535  2.883348   2.5829463
  2.3586123  2.202761   2.0911582  2.0166104  2.1174917  2.265368
  2.4203722  2.5763369  2.7371855  2.8999097  3.0568683  3.211577
  3.3836787  3.5647159  3.7047067  3.923205   4.4801764  5.3147154
  6.0415373  6.347202   6.16721    5.622335   4.9057164  4.2200723
  3.7877786  3.8545218  4.53057    5.6709385  7.0222034  8.3542595
  9.540247  10.2403755  9.845106   9.182507   8.351679   7.4936867
  6.733073   6.098536   5.5481997  5.042611   4.5582695  4.0630355
  3.510426   2.906207   2.4175184  2.2437067  2.311681   2.2577126
  1.9869593  1.7104594  1.7290719  2.1619823  2.7940853  3.332692
  3.6506448  3.7607555  3.688207   3.4570992  3.1381714  2.8308797
  2.5971212  2.4227886  2.2665217  2.1114419  1.9705737  1.9751914
  2.1270823  2.2688642  2.3880782  2.493538   2.6002336  2.7119899
  2.820635   2.9231708  3.039081   3.1855042  3.3158512  3.4608603
  3.879097   4.637503   5.4004273  5.8073955  5.7494345  5.3359523
  4.758625   4.185934   3.7374628  3.5317664  3.722392   4.3845334
  5.398192   6.5416994  7.6289005  8.539145   8.434972   7.864216
  7.1223392  6.3136973  5.5916543  5.032953   4.6089983  4.2584558
  3.913168   3.5112042  3.0353696  2.4952173  2.0225441  1.8970715
  2.128534   2.2855184  2.1480067  1.8516524  1.655464   1.7748545
  2.217959   2.7076898  3.0228202  3.1447504  3.11799    2.9769666
  2.7918112  2.64992    2.5802307  2.5302956  2.4416008  2.3034008
  2.1047082  2.048533   2.2170293  2.3363569  2.412433   2.4649854
  2.5170777  2.5793087  2.6446798  2.7016664  2.759388   2.8505433
  2.9664326  3.0629427  3.3143535  3.921602   4.6809254  5.197465
  5.2817183  5.0069885  4.5676646  4.130551   3.7641003  3.470805
  3.300739   3.416492   3.9355528  4.755358   5.666325   6.54761
  6.952694   6.4906373  5.870945   5.1375113  4.441088   3.9157853
  3.5816689  3.3959718  3.2904181  3.1009052]
