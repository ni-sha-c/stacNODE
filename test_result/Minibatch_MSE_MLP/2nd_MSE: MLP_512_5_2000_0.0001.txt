time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.81%, model saved.
Epoch: 0 Train: 3737.57812 Test: 4091.84717
Epoch 100: New minimal relative error: 85.02%, model saved.
Epoch: 100 Train: 43.76110 Test: 45.50066
Epoch 200: New minimal relative error: 52.15%, model saved.
Epoch: 200 Train: 18.30991 Test: 15.39119
Epoch 300: New minimal relative error: 29.91%, model saved.
Epoch: 300 Train: 2.21834 Test: 4.24562
Epoch: 400 Train: 9.29012 Test: 9.07020
Epoch 500: New minimal relative error: 23.94%, model saved.
Epoch: 500 Train: 2.64232 Test: 3.69312
Epoch 600: New minimal relative error: 14.61%, model saved.
Epoch: 600 Train: 0.81883 Test: 1.48333
Epoch: 700 Train: 2.43214 Test: 2.52726
Epoch: 800 Train: 7.80677 Test: 12.28462
Epoch: 900 Train: 0.43445 Test: 0.69730
Epoch: 1000 Train: 1.02909 Test: 1.12846
Epoch: 1100 Train: 0.78919 Test: 1.45135
Epoch: 1200 Train: 0.81338 Test: 1.09028
Epoch: 1300 Train: 6.68129 Test: 6.54091
Epoch: 1400 Train: 2.55620 Test: 3.06117
Epoch: 1500 Train: 0.65414 Test: 1.14746
Epoch: 1600 Train: 2.14673 Test: 1.82151
Epoch: 1700 Train: 8.74986 Test: 13.12052
Epoch: 1800 Train: 0.31368 Test: 0.83300
Epoch: 1900 Train: 0.78288 Test: 1.14760
Epoch: 2000 Train: 6.70332 Test: 6.78023
Epoch: 2100 Train: 1.09613 Test: 1.45619
Epoch: 2200 Train: 0.27641 Test: 0.60225
Epoch: 2300 Train: 5.36333 Test: 3.33978
Epoch: 2400 Train: 0.26569 Test: 0.74584
Epoch: 2500 Train: 0.39810 Test: 0.77782
Epoch: 2600 Train: 1.71414 Test: 1.82324
Epoch: 2700 Train: 0.27114 Test: 0.67022
Epoch: 2800 Train: 0.41458 Test: 0.43492
Epoch: 2900 Train: 0.08735 Test: 0.33809
Epoch: 3000 Train: 0.81156 Test: 1.06787
Epoch: 3100 Train: 0.49217 Test: 0.73559
Epoch: 3200 Train: 0.16639 Test: 0.35129
Epoch: 3300 Train: 0.05855 Test: 0.26322
Epoch: 3400 Train: 0.08460 Test: 0.28552
Epoch: 3500 Train: 0.78265 Test: 0.89444
Epoch: 3600 Train: 0.29810 Test: 0.45424
Epoch 3700: New minimal relative error: 7.44%, model saved.
Epoch: 3700 Train: 0.16604 Test: 0.40792
Epoch: 3800 Train: 0.05741 Test: 0.28321
Epoch: 3900 Train: 0.09045 Test: 0.28334
Epoch: 4000 Train: 0.34523 Test: 0.60267
Epoch: 4100 Train: 0.04095 Test: 0.25255
Epoch: 4200 Train: 1.72196 Test: 2.00630
Epoch: 4300 Train: 0.09345 Test: 0.29398
Epoch: 4400 Train: 0.47777 Test: 0.76943
Epoch: 4500 Train: 0.65698 Test: 0.96153
Epoch: 4600 Train: 0.61972 Test: 0.82886
Epoch: 4700 Train: 0.21309 Test: 0.42743
Epoch: 4800 Train: 0.10420 Test: 0.30561
Epoch: 4900 Train: 0.13289 Test: 0.33288
Epoch: 5000 Train: 0.62036 Test: 0.71793
Epoch: 5100 Train: 0.75617 Test: 1.07808
Epoch: 5200 Train: 0.18684 Test: 0.39963
Epoch: 5300 Train: 0.10606 Test: 0.29373
Epoch: 5400 Train: 0.02733 Test: 0.23438
Epoch: 5500 Train: 0.21267 Test: 0.44424
Epoch: 5600 Train: 0.79228 Test: 0.95276
Epoch: 5700 Train: 0.74692 Test: 1.05394
Epoch: 5800 Train: 0.42619 Test: 0.66742
Epoch: 5900 Train: 0.06671 Test: 0.27156
Epoch: 6000 Train: 1.63072 Test: 1.66970
Epoch: 6100 Train: 0.32350 Test: 0.60637
Epoch: 6200 Train: 0.76937 Test: 1.24511
Epoch: 6300 Train: 0.17478 Test: 0.36662
Epoch: 6400 Train: 0.01924 Test: 0.21256
Epoch: 6500 Train: 0.01806 Test: 0.21814
Epoch: 6600 Train: 0.09451 Test: 0.28626
Epoch: 6700 Train: 0.03080 Test: 0.24280
Epoch: 6800 Train: 0.02372 Test: 0.23122
Epoch: 6900 Train: 0.07312 Test: 0.25059
Epoch: 7000 Train: 0.10787 Test: 0.28183
Epoch: 7100 Train: 0.06170 Test: 0.26288
Epoch: 7200 Train: 0.23496 Test: 0.35450
Epoch: 7300 Train: 0.18027 Test: 0.40534
Epoch: 7400 Train: 0.24669 Test: 0.36951
Epoch: 7500 Train: 0.01349 Test: 0.21150
Epoch: 7600 Train: 0.02135 Test: 0.21784
Epoch: 7700 Train: 0.01435 Test: 0.21035
Epoch: 7800 Train: 0.01271 Test: 0.20980
Epoch: 7900 Train: 0.02290 Test: 0.22740
Epoch: 8000 Train: 0.01229 Test: 0.20739
Epoch: 8100 Train: 0.01381 Test: 0.21280
Epoch: 8200 Train: 0.02623 Test: 0.27480
Epoch: 8300 Train: 0.31141 Test: 0.47196
Epoch: 8400 Train: 0.03856 Test: 0.27677
Epoch: 8500 Train: 0.01561 Test: 0.20995
Epoch: 8600 Train: 0.01141 Test: 0.20593
Epoch: 8700 Train: 0.01196 Test: 0.20511
Epoch: 8800 Train: 0.01171 Test: 0.20948
Epoch: 8900 Train: 0.01044 Test: 0.20103
Epoch: 9000 Train: 0.01033 Test: 0.20521
Epoch: 9100 Train: 0.03408 Test: 0.22564
Epoch: 9200 Train: 0.01249 Test: 0.21052
Epoch: 9300 Train: 0.49729 Test: 0.89974
Epoch: 9400 Train: 0.00968 Test: 0.20272
Epoch: 9500 Train: 0.15437 Test: 0.33731
Epoch: 9600 Train: 0.00987 Test: 0.20070
Epoch: 9700 Train: 0.08570 Test: 0.24292
Epoch: 9800 Train: 0.00902 Test: 0.20014
Epoch: 9900 Train: 0.41940 Test: 0.73638
Epoch: 9999 Train: 0.00888 Test: 0.19816
Training Loss: tensor(0.0089)
Test Loss: tensor(0.1982)
Learned LE: [ 0.93914044  0.02029693 -4.6752086 ]
True LE: [ 8.7028557e-01 -5.8021755e-03 -1.4540033e+01]
Relative Error: [ 4.6444836  4.6778293  4.687516   4.6689277  4.628136   4.559979
  4.4505615  4.315004   4.223452   4.2465997  4.3631396  4.466892
  4.458464   4.294787   3.982007   3.5578756  3.089416   2.6846838
  2.4976616  2.657827   3.150585   3.8434803  4.5831923  5.2545915
  5.818245   6.295803   6.7244105  7.132199   7.5398273  7.970557
  8.449481   8.986254   9.54822   10.045181  10.3506    10.376737
 10.125387   9.638849   8.958147   8.166682   7.395417   6.766566
  6.3466644  6.1380997  6.104482   6.193973   6.348516   6.5068016
  6.6137686  6.633438   6.5563927  6.3961506  6.1778383  5.925547
  5.6554847  5.37639    5.0959926  4.829623   4.6025114  4.4427075
  4.366044   4.3646426  4.4077005  4.454272   4.47367    4.458888
  4.414842   4.3328466  4.195851   4.028795   3.9260347  3.9695182
  4.1167345  4.2387104  4.2342615  4.069272   3.7566793  3.3383646
  2.8858836  2.5178368  2.4003744  2.6510246  3.217781   3.9292448
  4.610278   5.1752462  5.6403127  6.0563107  6.454376   6.844053
  7.231135   7.633107   8.08133    8.600087   9.173363   9.71733
 10.087854  10.151504   9.875888   9.327351   8.591653   7.7781296
  7.029507   6.4614215  6.114782   5.968862   5.980495   6.0998983
  6.2682943  6.4209385  6.50041    6.4751463  6.346178   6.139332
  5.8879294  5.6193676  5.3471265  5.0737786  4.8007402  4.538731
  4.312077   4.151441   4.0766406  4.0826817  4.1396155  4.204189
  4.2406178  4.2382007  4.1998787  4.1121073  3.9526515  3.7547503
  3.6379073  3.695957   3.866101   4.00098    4.002036   3.8431413
  3.541212   3.1379051  2.7062368  2.372869   2.3150816  2.6390421
  3.2486212  3.9230764  4.494624   4.948988   5.3516593  5.747532
  6.1447115  6.5354986  6.91382    7.290759   7.700744   8.18351
  8.742584   9.309331   9.7424     9.876854   9.614897   9.015134
  8.228279   7.4016104  6.684519   6.177044   5.8904595  5.787716
  5.8277507  5.9651313  6.1392255  6.2777295  6.3203106  6.24229
  6.0573773  5.804592   5.525634   5.248757   4.9834146  4.72511
  4.467516   4.215568   3.991961   3.8300343  3.754554   3.7651722
  3.8351548  3.9211552  3.9820712  4.001481   3.979548   3.8972392
  3.72412    3.4972634  3.3599312  3.4214802  3.6042874  3.7483025
  3.7589762  3.616547   3.3372037  2.957654   2.5480423  2.240877
  2.2268393  2.602141   3.2113147  3.7938318  4.246098   4.6327844
  5.019945   5.420154   5.824287   6.22005    6.5961895  6.953736
  7.324084   7.759971   8.285677   8.84749    9.315766   9.529405
  9.330714   8.708668   7.874854   7.040389   6.3571954  5.9012322
  5.65514    5.5751333  5.628933   5.7769337  5.9521275  6.071593
  6.0721407  5.9375987  5.6972427  5.403061   5.103698   4.8271437
  4.5774302  4.3421702  4.107235   3.8703725  3.6510303  3.4851472
  3.4038029  3.412981   3.4919336  3.5999138  3.6912575  3.7418106
  3.747917   3.685256   3.5117574  3.2607987  3.094427   3.143489
  3.3266377  3.4764957  3.502785   3.3883672  3.1438398  2.7958136
  2.406766   2.1135743  2.1241176  2.523431   3.0846052  3.5521832
  3.9272952  4.3022413  4.698453   5.101993   5.503432   5.8968186
  6.271535   6.617402   6.9518104  7.3339624  7.8139253  8.355969
  8.831339   9.09803    9.000853   8.4116335  7.5396647  6.696783
  6.042578   5.6211867  5.3922067  5.315606   5.3726306  5.5280843
  5.7045403  5.8045073  5.7623434  5.5729737  5.281754   4.953595
  4.6423807  4.3744287  4.1474     3.9416816  3.7349517  3.5172143
  3.3027773  3.1287777  3.0339954  3.0327978  3.1120217  3.2379086
  3.3624642  3.4519334  3.4971912  3.470005   3.3146276  3.049707
  2.8458464  2.8627193  3.0317261  3.1838794  3.2312047  3.1556559
  2.9574697  2.6478271  2.277728   1.9848447  1.9981325  2.3904154
  2.8698926  3.2478337  3.6103458  4.0084567  4.41591    4.8121033
  5.196017   5.57219    5.93779    6.2767572  6.5840154  6.907945
  7.326808   7.839567   8.315289   8.599527   8.594937   8.11581
  7.234536   6.3750834  5.735709   5.3259816  5.090525   4.999584
  5.0519214  5.2159047  5.3982077  5.4830804  5.4028673  5.1658187
  4.8331847  4.4811034  4.1672745  3.91506    3.7159986  3.5436456
  3.3687835  3.1737156  2.965052   2.7786043  2.6612408  2.6383824
  2.705417   2.8383381  2.9928536  3.1247618  3.2170553  3.2405071
  3.1258383  2.8653183  2.6211836  2.5853324  2.7228286  2.8708673
  2.9416413  2.9126167  2.7701228  2.5064483  2.1558008  1.8506597
  1.8432734  2.2009664  2.5954633  2.9404576  3.3388808  3.7704873
  4.1859236  4.568261   4.922957   5.2640076]
