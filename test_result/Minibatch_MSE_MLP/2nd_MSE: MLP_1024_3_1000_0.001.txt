time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.15%, model saved.
Epoch: 0 Train: 3968.90234 Test: 3920.09912
Epoch 100: New minimal relative error: 31.54%, model saved.
Epoch: 100 Train: 27.23454 Test: 26.32449
Epoch: 200 Train: 20.09760 Test: 17.32025
Epoch 300: New minimal relative error: 30.21%, model saved.
Epoch: 300 Train: 3.62881 Test: 3.23098
Epoch: 400 Train: 2.22933 Test: 2.10527
Epoch: 500 Train: 2.37121 Test: 2.47636
Epoch 600: New minimal relative error: 19.05%, model saved.
Epoch: 600 Train: 4.27784 Test: 4.25554
Epoch: 700 Train: 6.80238 Test: 7.56569
Epoch: 800 Train: 4.91484 Test: 6.97767
Epoch: 900 Train: 7.50452 Test: 3.45817
Epoch: 1000 Train: 0.69938 Test: 0.43366
Epoch: 1100 Train: 3.31593 Test: 2.05948
Epoch 1200: New minimal relative error: 15.13%, model saved.
Epoch: 1200 Train: 0.92553 Test: 0.64467
Epoch: 1300 Train: 0.54847 Test: 0.37723
Epoch: 1400 Train: 0.32929 Test: 0.24651
Epoch: 1500 Train: 1.23703 Test: 0.35466
Epoch: 1600 Train: 1.37816 Test: 0.50509
Epoch: 1700 Train: 0.87659 Test: 1.19117
Epoch: 1800 Train: 0.17370 Test: 0.15535
Epoch 1900: New minimal relative error: 7.98%, model saved.
Epoch: 1900 Train: 0.22697 Test: 0.23593
Epoch: 2000 Train: 5.60173 Test: 6.60645
Epoch: 2100 Train: 4.99155 Test: 5.13002
Epoch: 2200 Train: 0.89668 Test: 0.79316
Epoch: 2300 Train: 1.00182 Test: 0.70320
Epoch: 2400 Train: 0.75520 Test: 1.48116
Epoch: 2500 Train: 1.37823 Test: 1.22625
Epoch: 2600 Train: 0.75577 Test: 0.59728
Epoch: 2700 Train: 1.23101 Test: 1.74003
Epoch: 2800 Train: 0.09632 Test: 0.09033
Epoch: 2900 Train: 0.10136 Test: 0.09736
Epoch: 3000 Train: 0.09771 Test: 0.09090
Epoch: 3100 Train: 0.09936 Test: 0.08628
Epoch: 3200 Train: 5.69049 Test: 6.93582
Epoch: 3300 Train: 0.07882 Test: 0.07453
Epoch: 3400 Train: 0.28040 Test: 0.20541
Epoch: 3500 Train: 3.18926 Test: 4.48035
Epoch: 3600 Train: 0.07138 Test: 0.06828
Epoch: 3700 Train: 0.61104 Test: 0.58030
Epoch: 3800 Train: 0.06478 Test: 0.06236
Epoch: 3900 Train: 0.07710 Test: 0.07994
Epoch: 4000 Train: 0.06739 Test: 0.06680
Epoch: 4100 Train: 0.12868 Test: 0.10870
Epoch: 4200 Train: 0.42604 Test: 0.49739
Epoch: 4300 Train: 0.10573 Test: 0.09879
Epoch: 4400 Train: 5.17681 Test: 5.08836
Epoch: 4500 Train: 0.05307 Test: 0.05213
Epoch: 4600 Train: 0.05282 Test: 0.05154
Epoch: 4700 Train: 0.07680 Test: 0.06627
Epoch: 4800 Train: 0.41983 Test: 0.55898
Epoch: 4900 Train: 0.26227 Test: 0.35151
Epoch: 5000 Train: 0.06250 Test: 0.07738
Epoch: 5100 Train: 0.08245 Test: 0.10263
Epoch: 5200 Train: 0.04602 Test: 0.04578
Epoch: 5300 Train: 0.92405 Test: 1.38061
Epoch: 5400 Train: 0.04152 Test: 0.04203
Epoch: 5500 Train: 0.04846 Test: 0.06148
Epoch: 5600 Train: 0.04006 Test: 0.04131
Epoch: 5700 Train: 0.03918 Test: 0.04027
Epoch: 5800 Train: 0.04110 Test: 0.04322
Epoch: 5900 Train: 0.16746 Test: 0.45207
Epoch: 6000 Train: 0.03609 Test: 0.03720
Epoch: 6100 Train: 0.03704 Test: 0.03731
Epoch: 6200 Train: 0.03528 Test: 0.03713
Epoch: 6300 Train: 1.41891 Test: 1.81553
Epoch: 6400 Train: 0.03344 Test: 0.03496
Epoch: 6500 Train: 0.03331 Test: 0.03548
Epoch: 6600 Train: 0.03468 Test: 0.03747
Epoch: 6700 Train: 0.55332 Test: 0.65136
Epoch: 6800 Train: 0.03991 Test: 0.04750
Epoch: 6900 Train: 0.02997 Test: 0.03195
Epoch: 7000 Train: 0.05315 Test: 0.07278
Epoch: 7100 Train: 0.02916 Test: 0.03119
Epoch: 7200 Train: 0.38189 Test: 0.09415
Epoch: 7300 Train: 0.02836 Test: 0.03044
Epoch: 7400 Train: 0.41559 Test: 0.61982
Epoch: 7500 Train: 0.02703 Test: 0.02932
Epoch: 7600 Train: 0.24772 Test: 0.12805
Epoch: 7700 Train: 0.02607 Test: 0.02850
Epoch: 7800 Train: 0.02700 Test: 0.02946
Epoch: 7900 Train: 1.12677 Test: 1.18559
Epoch: 8000 Train: 0.02479 Test: 0.02721
Epoch: 8100 Train: 0.02476 Test: 0.02769
Epoch: 8200 Train: 0.05716 Test: 0.05691
Epoch: 8300 Train: 0.02352 Test: 0.02614
Epoch: 8400 Train: 0.02351 Test: 0.02652
Epoch: 8500 Train: 0.03382 Test: 0.03888
Epoch: 8600 Train: 0.78339 Test: 0.77476
Epoch: 8700 Train: 0.02215 Test: 0.02496
Epoch: 8800 Train: 0.02287 Test: 0.02511
Epoch: 8900 Train: 0.02427 Test: 0.02935
Epoch: 9000 Train: 0.03097 Test: 0.03702
Epoch: 9100 Train: 0.02099 Test: 0.02385
Epoch: 9200 Train: 0.02150 Test: 0.02412
Epoch: 9300 Train: 0.11259 Test: 0.05780
Epoch: 9400 Train: 0.02007 Test: 0.02298
Epoch: 9500 Train: 0.01982 Test: 0.02308
Epoch: 9600 Train: 0.03598 Test: 0.04641
Epoch: 9700 Train: 0.01921 Test: 0.02219
Epoch: 9800 Train: 0.04264 Test: 0.03801
Epoch: 9900 Train: 0.01878 Test: 0.02183
Epoch: 9999 Train: 0.01880 Test: 0.02170
Training Loss: tensor(0.0188)
Test Loss: tensor(0.0217)
Learned LE: [ 0.8675746  -0.06624225 -4.487265  ]
True LE: [ 8.4656787e-01  1.0235406e-02 -1.4528775e+01]
Relative Error: [3.9722092  4.4068475  4.608635   4.4991226  4.020296   3.1051412
 1.9374443  1.3076667  2.184324   3.595894   4.9208255  5.920757
 6.73295    7.145862   7.371556   7.5619063  7.5072103  7.2068706
 6.862169   6.597001   6.4138346  6.1916013  6.000033   5.951638
 5.985605   6.053646   6.0161514  5.7572308  5.1868873  4.4980836
 3.866592   3.407454   3.339478   3.3936217  3.505724   3.5480504
 2.740374   2.2612185  2.2553124  2.373455   2.4071894  2.3690355
 2.3490713  2.3872778  2.4344304  2.5524101  2.7922544  3.1050525
 3.3895965  3.579795   3.724363   3.8208394  3.8703706  3.9096138
 3.9297938  3.8999867  3.796249   3.6032348  3.3293302  3.034454
 2.838837   2.8828056  3.2274835  3.7393105  4.1824827  4.391078
 4.250175   3.614317   2.5352733  1.2783818  0.8603722  2.1171212
 3.567144   4.751537   5.705492   6.4231277  6.820316   6.9982753
 6.9679565  6.7347264  6.3649573  5.9957447  5.70212    5.537967
 5.3416553  5.1557856  5.0891337  5.111902   5.2432356  5.319501
 5.0702915  4.502416   3.8700912  3.300741   2.9526222  2.979565
 3.1054375  3.2610881  3.0150378  2.1897838  1.9919189  2.1458707
 2.2399175  2.2094357  2.159154   2.172624   2.254414   2.3372238
 2.4922042  2.7384105  3.0222676  3.229624   3.3423543  3.4066465
 3.4225266  3.430591   3.449318   3.4598112  3.444938   3.3872128
 3.258906   3.0372279  2.7585723  2.5514514  2.5705934  2.8968308
 3.4124098  3.8783748  4.108186   3.9081216  3.2066576  2.1712925
 0.9169055  0.5956617  1.9845028  3.3877633  4.497874   5.429245
 6.1509533  6.553895   6.5345945  6.4185953  6.156164   5.7784543
 5.378183   5.010776   4.799195   4.692223   4.4971347  4.347365
 4.2990236  4.3083453  4.481541   4.3936768  3.8983397  3.335558
 2.8330238  2.491858   2.5390756  2.769413   2.94717    2.5375185
 1.8495634  1.8140603  2.0084207  2.0454526  1.9858162  1.9581752
 2.0160303  2.1407804  2.2452383  2.4214876  2.6569595  2.890997
 3.025616   3.0686295  3.04735    2.9943533  2.9559133  2.9468696
 2.9583578  2.9858322  3.0139964  2.991003   2.8437915  2.5812514
 2.3448653  2.3039846  2.5751286  3.0884721  3.5905018  3.8331878
 3.5987835  2.9594817  2.0396118  0.8938917  0.4191488  1.6865879
 3.0366056  4.188496   5.1007595  5.871625   6.2668347  6.256554
 6.157045   5.914883   5.532795   5.0910807  4.6061974  4.2318764
 4.1232786  4.0680213  3.8334668  3.5734582  3.3737705  3.3767507
 3.6448696  3.3637233  2.7913635  2.3560781  1.968502   1.9722468
 2.2937074  2.4795117  2.1918745  1.5937049  1.607883   1.7879866
 1.8107917  1.7420964  1.745066   1.8470751  2.0162094  2.1375852
 2.3139286  2.5276556  2.699859   2.7717023  2.7581832  2.6806738
 2.5815043  2.5029817  2.4754016  2.4976377  2.567556   2.6772008
 2.7592266  2.7041194  2.4868608  2.2203467  2.08379    2.238974
 2.7242157  3.2968142  3.6140919  3.4515982  2.9239593  2.118328
 1.1414517  0.49986598 1.2732193  2.4864697  3.7573059  4.707172
 5.527177   6.0157757  6.1490426  6.010459   5.730917   5.284377
 4.8157396  4.282967   3.8254976  3.542039   3.5067458  3.439736
 3.1269565  2.6521678  2.3812346  2.5161572  2.8007593  2.2695289
 1.8256605  1.4117452  1.2675827  1.6273448  1.8526134  1.8968638
 1.3419741  1.3374462  1.4851809  1.5437592  1.4809493  1.4929582
 1.6260108  1.8127414  1.9737875  2.1207862  2.306133   2.4303763
 2.4433522  2.3992887  2.3136365  2.2110312  2.1166666  2.084767
 2.1212523  2.20048    2.3194492  2.4403741  2.4706476  2.35175
 2.100399   1.851201   1.7942125  2.1042051  2.734      3.2729845
 3.357576   3.0533664  2.4030955  1.5586966  0.8855118  0.9547379
 1.7403255  3.0721374  4.133558   5.070327   5.6269813  5.9104943
 5.7638     5.3938346  4.985904   4.50266    3.9668462  3.3918731
 3.0739462  2.9354057  2.8965101  2.6704292  2.3865778  1.900003
 1.6698732  1.7138156  1.9437639  1.4068786  1.0024834  0.6928174
 0.7553468  1.1110977  1.2249917  1.1000924  0.997649   1.1334614
 1.211865   1.2328578  1.218638   1.3095354  1.4768547  1.6685022
 1.80664    1.9438004  2.0491197  2.044886   1.9637991  1.902408
 1.855953   1.7911013  1.7604381  1.8048408  1.8835064  1.9437902
 1.9895151  2.0131667  1.991465   1.8759977  1.6480021  1.4183381
 1.3719194  1.7170707  2.3890142  2.8176877  2.825027   2.5014048
 1.8983374  1.27265    1.0147688  1.1862009  1.9182004  3.220968
 4.2548504  4.9786525  5.382977   5.3682437  4.9895515  4.655061
 4.285883   3.7955675  3.2148032  2.6531353 ]
