time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.59%, model saved.
Epoch: 0 Train: 3868.11084 Test: 4072.43628
Epoch 100: New minimal relative error: 31.58%, model saved.
Epoch: 100 Train: 51.39540 Test: 50.87494
Epoch 200: New minimal relative error: 18.06%, model saved.
Epoch: 200 Train: 14.04010 Test: 22.71064
Epoch 300: New minimal relative error: 17.36%, model saved.
Epoch: 300 Train: 2.39409 Test: 1.07140
Epoch: 400 Train: 1.03776 Test: 0.98487
Epoch: 500 Train: 8.12898 Test: 4.17145
Epoch: 600 Train: 2.28153 Test: 3.65992
Epoch 700: New minimal relative error: 17.16%, model saved.
Epoch: 700 Train: 1.04347 Test: 2.43985
Epoch: 800 Train: 6.00647 Test: 3.40399
Epoch: 900 Train: 7.14078 Test: 9.57094
Epoch: 1000 Train: 10.08865 Test: 9.66469
Epoch: 1100 Train: 1.31848 Test: 1.30293
Epoch: 1200 Train: 1.00128 Test: 0.68483
Epoch: 1300 Train: 5.77278 Test: 6.62644
Epoch: 1400 Train: 1.06484 Test: 1.03032
Epoch: 1500 Train: 10.01081 Test: 10.09054
Epoch: 1600 Train: 2.62061 Test: 1.82031
Epoch: 1700 Train: 1.54354 Test: 1.46048
Epoch 1800: New minimal relative error: 11.98%, model saved.
Epoch: 1800 Train: 0.25274 Test: 0.20387
Epoch 1900: New minimal relative error: 8.07%, model saved.
Epoch: 1900 Train: 0.27742 Test: 0.30184
Epoch: 2000 Train: 0.39281 Test: 0.54565
Epoch: 2100 Train: 1.75913 Test: 1.18148
Epoch: 2200 Train: 0.75722 Test: 0.77939
Epoch: 2300 Train: 0.08204 Test: 0.09078
Epoch: 2400 Train: 0.18657 Test: 0.22302
Epoch: 2500 Train: 0.27543 Test: 0.15628
Epoch: 2600 Train: 0.31446 Test: 0.44497
Epoch: 2700 Train: 2.04747 Test: 1.91865
Epoch: 2800 Train: 0.35719 Test: 0.25995
Epoch: 2900 Train: 0.88052 Test: 1.12843
Epoch: 3000 Train: 1.16756 Test: 1.39296
Epoch: 3100 Train: 0.24201 Test: 0.10714
Epoch: 3200 Train: 0.24993 Test: 0.33067
Epoch: 3300 Train: 0.10526 Test: 0.10738
Epoch: 3400 Train: 0.89214 Test: 0.97887
Epoch: 3500 Train: 1.50339 Test: 1.93987
Epoch: 3600 Train: 1.65343 Test: 1.85782
Epoch: 3700 Train: 0.17081 Test: 0.24060
Epoch: 3800 Train: 0.17441 Test: 0.18209
Epoch: 3900 Train: 0.60183 Test: 0.76959
Epoch: 4000 Train: 1.55604 Test: 1.38279
Epoch: 4100 Train: 0.19918 Test: 0.21276
Epoch: 4200 Train: 0.39915 Test: 0.44027
Epoch: 4300 Train: 0.12772 Test: 0.16548
Epoch 4400: New minimal relative error: 5.95%, model saved.
Epoch: 4400 Train: 1.51326 Test: 1.71659
Epoch: 4500 Train: 0.42976 Test: 0.45642
Epoch: 4600 Train: 0.58620 Test: 0.58278
Epoch: 4700 Train: 0.20162 Test: 0.21932
Epoch: 4800 Train: 0.08470 Test: 0.12528
Epoch: 4900 Train: 1.04586 Test: 1.18740
Epoch: 5000 Train: 0.09711 Test: 0.13798
Epoch: 5100 Train: 0.07782 Test: 0.10935
Epoch: 5200 Train: 0.05583 Test: 0.08721
Epoch: 5300 Train: 0.21956 Test: 0.27286
Epoch: 5400 Train: 0.34503 Test: 0.35248
Epoch: 5500 Train: 0.24792 Test: 0.23401
Epoch: 5600 Train: 0.02962 Test: 0.09494
Epoch: 5700 Train: 0.40794 Test: 0.43682
Epoch: 5800 Train: 0.28653 Test: 0.31260
Epoch: 5900 Train: 0.23561 Test: 0.27184
Epoch: 6000 Train: 0.09280 Test: 0.13227
Epoch: 6100 Train: 0.08418 Test: 0.13961
Epoch: 6200 Train: 0.07157 Test: 0.10643
Epoch: 6300 Train: 0.65407 Test: 0.79463
Epoch: 6400 Train: 0.43125 Test: 0.61053
Epoch: 6500 Train: 0.03793 Test: 0.05240
Epoch: 6600 Train: 0.02361 Test: 0.04427
Epoch: 6700 Train: 0.33949 Test: 0.29441
Epoch: 6800 Train: 0.51133 Test: 0.69227
Epoch: 6900 Train: 0.76547 Test: 0.73884
Epoch: 7000 Train: 0.12477 Test: 0.13396
Epoch: 7100 Train: 0.05482 Test: 0.09202
Epoch: 7200 Train: 0.01674 Test: 0.03776
Epoch: 7300 Train: 0.01422 Test: 0.03398
Epoch: 7400 Train: 0.74693 Test: 0.80030
Epoch: 7500 Train: 0.24445 Test: 0.28095
Epoch: 7600 Train: 0.03701 Test: 0.05700
Epoch: 7700 Train: 0.25504 Test: 0.28563
Epoch: 7800 Train: 0.05462 Test: 0.07925
Epoch 7900: New minimal relative error: 4.03%, model saved.
Epoch: 7900 Train: 0.05252 Test: 0.07330
Epoch: 8000 Train: 0.18562 Test: 0.23667
Epoch: 8100 Train: 0.07930 Test: 0.10193
Epoch: 8200 Train: 0.03439 Test: 0.06009
Epoch: 8300 Train: 0.04542 Test: 0.06211
Epoch: 8400 Train: 0.07847 Test: 0.09523
Epoch: 8500 Train: 0.21271 Test: 0.26027
Epoch: 8600 Train: 0.06481 Test: 0.06378
Epoch: 8700 Train: 0.06836 Test: 0.09185
Epoch: 8800 Train: 0.05674 Test: 0.08389
Epoch: 8900 Train: 0.03833 Test: 0.05439
Epoch: 9000 Train: 0.01366 Test: 0.03502
Epoch: 9100 Train: 0.09331 Test: 0.15040
Epoch: 9200 Train: 0.02951 Test: 0.05133
Epoch: 9300 Train: 0.03309 Test: 0.05670
Epoch 9400: New minimal relative error: 4.02%, model saved.
Epoch: 9400 Train: 0.03347 Test: 0.06852
Epoch: 9500 Train: 0.01237 Test: 0.03034
Epoch: 9600 Train: 0.01119 Test: 0.02824
Epoch: 9700 Train: 0.01291 Test: 0.03294
Epoch: 9800 Train: 0.00962 Test: 0.02737
Epoch: 9900 Train: 0.01042 Test: 0.02831
Epoch: 9999 Train: 0.03927 Test: 0.07126
Training Loss: tensor(0.0393)
Test Loss: tensor(0.0713)
Learned LE: [ 0.8918936   0.05721965 -5.166174  ]
True LE: [ 8.864499e-01 -4.057737e-03 -1.456205e+01]
Relative Error: [0.50190973 0.59594595 0.7535662  0.8760232  0.9331119  1.0175562
 1.196221   1.395164   1.5217241  1.5319618  1.4257017  1.2227044
 0.9536836  0.6931388  0.65960705 0.92426294 1.244477   1.5187359
 1.8008327  2.1922014  2.7375536  3.3861616  4.0192733  4.508731
 4.783459   4.862408   4.8122716  4.6643686  4.4300103  4.163458
 3.9475415  3.832176   3.782106   3.7667036  3.7178442  3.533758
 3.227706   2.8796148  2.5546608  2.262456   2.0004354  1.7817385
 1.595878   1.4314836  1.2799363  1.1093713  0.90858656 0.71996737
 0.5847951  0.49345964 0.4161024  0.3461669  0.29819196 0.28529778
 0.31417796 0.3773654  0.44927305 0.5004369  0.51293206 0.4964682
 0.48383376 0.48699933 0.50839573 0.57941175 0.69089276 0.77705663
 0.7742299  0.7290551  0.80392855 0.9880846  1.1463295  1.1998439
 1.1450263  1.0158297  0.8491167  0.6512937  0.46619618 0.46109813
 0.6151213  0.75356823 0.8807982  1.0982023  1.4577633  1.912001
 2.3732045  2.7535563  2.9902735  3.081166   3.0875354  3.045363
 2.9305065  2.7481923  2.5559201  2.4252357  2.3690526  2.3684616
 2.4104958  2.3676279  2.194132   1.9613914  1.7435813  1.5625771
 1.4050012  1.2911216  1.215213   1.1425467  1.0706625  0.97569513
 0.82469016 0.6493132  0.50925046 0.41438293 0.33919305 0.2754315
 0.23756705 0.23521358 0.26395327 0.31060046 0.35984397 0.396718
 0.40831816 0.39842016 0.39901218 0.43054694 0.47004324 0.51746035
 0.59232825 0.6717632  0.69067055 0.6000681  0.5193733  0.6232486
 0.8053632  0.91327655 0.91017497 0.83738256 0.7715392  0.7220206
 0.6287264  0.5023081  0.42050257 0.39101058 0.37123936 0.40104198
 0.5633518  0.81349844 1.0557852  1.2551868  1.3942648  1.4545034
 1.4600168  1.4673431  1.4697213  1.4295844  1.3559548  1.2905941
 1.2767714  1.3052287  1.3857572  1.458385   1.4106364  1.2840868
 1.1478715  1.0418599  0.95404136 0.89594173 0.8945392  0.88860065
 0.86351    0.83807844 0.76357883 0.63141185 0.5074989  0.42715037
 0.369502   0.3234287  0.30322745 0.31478986 0.34397584 0.37092748
 0.3858802  0.39099655 0.384259   0.3632229  0.3459921  0.36446905
 0.40691006 0.43749046 0.46908316 0.52625465 0.58743423 0.5780553
 0.4559858  0.3809911  0.5068969  0.66636825 0.72277784 0.68361706
 0.6496137  0.70603585 0.75608337 0.7167597  0.61879486 0.5322102
 0.48422426 0.43377322 0.3671478  0.391469   0.47630793 0.5078928
 0.46413207 0.3945348  0.3313194  0.30726072 0.36054927 0.4668388
 0.54189235 0.5699273  0.58181864 0.597766   0.6684036  0.79103327
 0.84639555 0.8093797  0.7451959  0.6818919  0.6374522  0.6008427
 0.62373906 0.67465603 0.6645981  0.66130346 0.6715103  0.6176331
 0.5280621  0.46940804 0.43516004 0.39977995 0.375581   0.38268313
 0.4183868  0.45495346 0.46354043 0.44553593 0.41703907 0.38080874
 0.34136847 0.3222517  0.34370354 0.37395063 0.37671113 0.3782538
 0.42147642 0.49177104 0.49953717 0.39670113 0.33635113 0.45360112
 0.57538944 0.5871959  0.53221697 0.5695245  0.7120756  0.7945344
 0.7732137  0.70333874 0.66065395 0.64213055 0.5751875  0.5127954
 0.5815613  0.7189953  0.7755761  0.71539485 0.61280537 0.5295039
 0.46479097 0.42396462 0.47127876 0.48805577 0.4599169  0.42344475
 0.34151226 0.3798483  0.48534065 0.50304085 0.49047744 0.47606426
 0.44598764 0.42457852 0.42248896 0.49823964 0.5325744  0.4853846
 0.5090344  0.5563575  0.5327047  0.49508038 0.4879642  0.4741336
 0.4404704  0.41641033 0.4271891  0.46974054 0.50449914 0.49699974
 0.46223962 0.42108703 0.37331    0.3277606  0.30769777 0.32650495
 0.34651324 0.32703692 0.2976071  0.32217604 0.40725762 0.45642248
 0.3939561  0.33968103 0.43078935 0.52371156 0.5113538  0.45530498
 0.53235203 0.6926802  0.7752585  0.77034116 0.7400441  0.7428863
 0.72813547 0.65217745 0.65476406 0.79919046 0.95105004 0.991596
 0.91519535 0.80625546 0.714097   0.6224132  0.5397133  0.55225587
 0.5395462  0.47074026 0.44795614 0.32305267 0.28302816 0.34098363
 0.3336739  0.3481059  0.38173363 0.36791337 0.34978563 0.3647874
 0.45554972 0.44693494 0.35848284 0.40310514 0.48122704 0.48626667
 0.4887742  0.51431775 0.507222   0.45836377 0.41137925 0.40359938
 0.437774   0.46966946 0.45784992 0.42887628 0.39892256 0.35688335
 0.31621963 0.29688436 0.31332833 0.3368565  0.31899303 0.27565977
 0.26713517 0.3301876  0.41074017 0.39421567 0.33721605 0.39843455
 0.49204132 0.4905927  0.44266033 0.4995667  0.6305234  0.7005662
 0.7142498  0.7258882  0.7635518  0.74919003 0.68585986 0.7498029
 0.8778388  0.9272932  0.8927221  0.7609674 ]
