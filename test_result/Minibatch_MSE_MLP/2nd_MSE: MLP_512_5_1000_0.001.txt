time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.96%, model saved.
Epoch: 0 Train: 4059.49121 Test: 3890.57690
Epoch 100: New minimal relative error: 32.77%, model saved.
Epoch: 100 Train: 39.10974 Test: 55.64059
Epoch: 200 Train: 18.19262 Test: 39.08882
Epoch: 300 Train: 3.27972 Test: 15.51887
Epoch 400: New minimal relative error: 16.79%, model saved.
Epoch: 400 Train: 4.30668 Test: 15.59590
Epoch: 500 Train: 1.99835 Test: 13.73407
Epoch: 600 Train: 3.34649 Test: 17.12513
Epoch: 700 Train: 2.42006 Test: 16.18782
Epoch: 800 Train: 4.55389 Test: 17.25062
Epoch: 900 Train: 11.92263 Test: 24.73898
Epoch: 1000 Train: 0.31814 Test: 15.73793
Epoch: 1100 Train: 8.39156 Test: 26.37210
Epoch: 1200 Train: 0.39992 Test: 17.26488
Epoch: 1300 Train: 0.42515 Test: 18.99094
Epoch: 1400 Train: 0.76771 Test: 19.24368
Epoch: 1500 Train: 0.32016 Test: 19.85310
Epoch: 1600 Train: 0.11767 Test: 20.83194
Epoch: 1700 Train: 0.70292 Test: 24.39628
Epoch: 1800 Train: 2.23027 Test: 27.02452
Epoch: 1900 Train: 4.13464 Test: 28.65862
Epoch 2000: New minimal relative error: 15.70%, model saved.
Epoch: 2000 Train: 0.35981 Test: 26.04293
Epoch: 2100 Train: 0.11845 Test: 27.35373
Epoch: 2200 Train: 2.70719 Test: 31.28410
Epoch 2300: New minimal relative error: 15.01%, model saved.
Epoch: 2300 Train: 0.12992 Test: 31.19186
Epoch: 2400 Train: 1.58273 Test: 33.12637
Epoch: 2500 Train: 2.95411 Test: 36.64611
Epoch: 2600 Train: 0.95021 Test: 36.81740
Epoch: 2700 Train: 0.19226 Test: 38.50834
Epoch: 2800 Train: 1.46822 Test: 41.05345
Epoch: 2900 Train: 0.88800 Test: 42.18793
Epoch: 3000 Train: 0.60148 Test: 43.79521
Epoch: 3100 Train: 0.15903 Test: 45.80803
Epoch: 3200 Train: 0.05720 Test: 46.79996
Epoch: 3300 Train: 0.43202 Test: 50.12752
Epoch: 3400 Train: 0.84613 Test: 53.74892
Epoch: 3500 Train: 1.19973 Test: 56.33087
Epoch: 3600 Train: 3.51459 Test: 61.16625
Epoch: 3700 Train: 0.18347 Test: 56.11789
Epoch: 3800 Train: 1.62293 Test: 62.34227
Epoch: 3900 Train: 0.95441 Test: 65.15323
Epoch: 4000 Train: 0.11638 Test: 68.86779
Epoch: 4100 Train: 0.12150 Test: 71.18753
Epoch: 4200 Train: 1.60610 Test: 75.00017
Epoch: 4300 Train: 1.12747 Test: 77.29145
Epoch: 4400 Train: 0.33307 Test: 79.90196
Epoch: 4500 Train: 0.27529 Test: 84.62282
Epoch: 4600 Train: 0.11757 Test: 86.68274
Epoch: 4700 Train: 0.26717 Test: 89.35265
Epoch: 4800 Train: 0.46143 Test: 95.40143
Epoch: 4900 Train: 0.23361 Test: 96.82085
Epoch: 5000 Train: 1.21882 Test: 101.55970
Epoch: 5100 Train: 0.02900 Test: 103.93013
Epoch: 5200 Train: 0.09167 Test: 109.72168
Epoch: 5300 Train: 0.07931 Test: 110.22770
Epoch: 5400 Train: 0.43892 Test: 115.86657
Epoch: 5500 Train: 0.17195 Test: 119.80495
Epoch: 5600 Train: 0.62101 Test: 123.45879
Epoch: 5700 Train: 0.07291 Test: 128.48503
Epoch: 5800 Train: 0.05656 Test: 131.63824
Epoch: 5900 Train: 0.09377 Test: 133.87791
Epoch: 6000 Train: 0.11384 Test: 136.13301
Epoch: 6100 Train: 0.05469 Test: 142.02074
Epoch: 6200 Train: 0.34441 Test: 145.84586
Epoch: 6300 Train: 0.04928 Test: 150.31010
Epoch: 6400 Train: 0.13278 Test: 150.62148
Epoch: 6500 Train: 0.07249 Test: 156.16501
Epoch: 6600 Train: 0.49188 Test: 160.57921
Epoch: 6700 Train: 2.06406 Test: 163.81873
Epoch: 6800 Train: 0.01595 Test: 165.62852
Epoch: 6900 Train: 0.01555 Test: 168.55739
Epoch: 7000 Train: 0.02521 Test: 173.42860
Epoch: 7100 Train: 0.09620 Test: 175.81511
Epoch: 7200 Train: 0.15007 Test: 179.19611
Epoch: 7300 Train: 0.03740 Test: 183.00902
Epoch: 7400 Train: 0.03572 Test: 185.64189
Epoch: 7500 Train: 0.13978 Test: 189.60101
Epoch: 7600 Train: 0.02307 Test: 192.17836
Epoch: 7700 Train: 0.03017 Test: 194.00943
Epoch: 7800 Train: 0.22688 Test: 197.68167
Epoch: 7900 Train: 0.02927 Test: 201.40822
Epoch: 8000 Train: 0.14131 Test: 203.41682
Epoch: 8100 Train: 0.55141 Test: 206.33888
Epoch: 8200 Train: 0.46765 Test: 208.18379
Epoch: 8300 Train: 0.07453 Test: 212.04411
Epoch: 8400 Train: 0.04092 Test: 213.98888
Epoch: 8500 Train: 0.13490 Test: 216.77956
Epoch: 8600 Train: 0.15169 Test: 219.24210
Epoch: 8700 Train: 0.33413 Test: 219.87004
Epoch: 8800 Train: 0.20516 Test: 224.97159
Epoch: 8900 Train: 0.04557 Test: 224.13203
Epoch: 9000 Train: 0.11192 Test: 227.11374
Epoch: 9100 Train: 0.56990 Test: 228.23077
Epoch: 9200 Train: 0.07030 Test: 232.06789
Epoch: 9300 Train: 0.04199 Test: 232.53058
Epoch: 9400 Train: 0.11457 Test: 235.89761
Epoch: 9500 Train: 0.02449 Test: 235.35208
Epoch: 9600 Train: 0.13192 Test: 239.68677
Epoch: 9700 Train: 0.02130 Test: 240.42880
Epoch: 9800 Train: 0.03716 Test: 239.77489
Epoch: 9900 Train: 0.01141 Test: 242.81491
Epoch: 9999 Train: 0.00933 Test: 243.83658
Training Loss: tensor(0.0093)
Test Loss: tensor(243.8366)
Learned LE: [ 0.94110125 -0.00522352 -4.525716  ]
True LE: [ 8.5328448e-01 -8.5304421e-04 -1.4524837e+01]
Relative Error: [14.818123   15.376024   15.765039   15.88667    15.6606245  15.043438
 14.036946   12.688583   11.09158     9.384081    7.7470675   6.3732276
  5.4031305   4.8607564   4.5962367   4.4207907   4.2808967   4.1745296
  4.074544    3.9427037   3.7562616   3.515095    3.2362375   2.9503922
  2.6939125   2.500016    2.3766956   2.2823594   2.1661057   2.0410562
  1.9661013   1.9685249   2.0336723   2.1500986   2.3648403   2.737124
  3.2760854   3.9156322   4.585173    5.256032    5.919372    6.5645022
  7.1736603   7.720864    8.189814    8.60211     9.006546    9.430453
  9.85283    10.223513   10.500707   10.674799   10.766332   10.805808
 10.820289   10.835978   10.885596   11.006258   11.231623   11.583299
 12.063119   12.644956   13.27095    13.85728    14.306471   14.5195675
 14.410992   13.924368   13.041775   11.788827   10.242268    8.542638
  6.9066257   5.5891795   4.768568    4.428173    4.3240213   4.2473474
  4.1823974   4.1357374   4.0643206   3.9222631   3.7076886   3.4413419
  3.1392322   2.8330219   2.5740914   2.4092636   2.3535664   2.349294
  2.2807856   2.0585515   1.7439585   1.5026278   1.4193572   1.468358
  1.6381636   1.9625902   2.4357293   2.9970603   3.579871    4.1774297
  4.798798    5.428426    6.032393    6.5721436   7.0213327   7.39723
  7.7615895   8.165779    8.605041    9.028547    9.376871    9.614071
  9.739622    9.776529    9.75487     9.709277    9.682092    9.718698
  9.858183   10.127522   10.536894   11.0711775  11.68181    12.288896
 12.795719   13.104172   13.127568   12.800794   12.0877285  10.986675
  9.542814    7.8788185   6.236946    4.944811    4.2370415   4.0600677
  4.0827274   4.085642    4.088921    4.0996366   4.0472584   3.8833585
  3.6457684   3.3807058   3.0861807   2.7634003   2.4711072   2.2888515
  2.2561636   2.3307197   2.3959079   2.3069155   1.9403353   1.4244499
  1.0532907   0.94452184  1.0441402   1.3348764   1.7308632   2.1820562
  2.6489253   3.148672    3.714016    4.3285785   4.9317164   5.4663615
  5.899566    6.239599    6.550451    6.9069943   7.3339157   7.79575
  8.222585    8.546623    8.733114    8.787788    8.741701    8.639022
  8.533711    8.4810505   8.526431    8.700958    9.02222     9.488438
 10.065212   10.680708   11.238353   11.638684   11.795572   11.642245
 11.132597   10.241568    8.972123    7.3985415   5.751762    4.4250584
  3.7534454   3.6915503   3.828536    3.9123514   3.9915926   4.073102
  4.051698    3.879102    3.6295824   3.3708959   3.1015859   2.7926493
  2.4604588   2.2018516   2.1128387   2.1957953   2.3483102   2.4306247
  2.2842474   1.7669599   1.1012846   0.68007934  0.59904456  0.83288306
  1.1917192   1.5023388   1.8156672   2.1879632   2.670205    3.2577248
  3.8663044   4.4027133   4.827104    5.142742    5.4008555   5.684731
  6.055271    6.516405    7.009816    7.4397974   7.7235146   7.8319073
  7.787956    7.6440597   7.46634     7.32207     7.265754    7.333582
  7.548838    7.922045    8.438144    9.041275    9.63896    10.126032
 10.411109   10.427908   10.131256    9.489259    8.471529    7.079193
  5.4685693   4.041287    3.2843854   3.2732852   3.5278413   3.7043836
  3.8669991   4.040431    4.0847874   3.9462388   3.713686    3.457621
  3.1927733   2.9005413   2.5724475   2.2404268   2.013452    1.9921348
  2.1412826   2.3374033   2.4089477   2.1797152   1.5603164   0.8521242
  0.4543664   0.4586165   0.8012586   1.032873    1.1367241   1.3369596
  1.702553    2.2265294   2.834055    3.3870022   3.809737    4.11301
  4.3371696   4.5463243   4.816451    5.208992    5.719108    6.2501087
  6.66699     6.881502    6.887199    6.7359633   6.5050306   6.2743793
  6.110877    6.060424    6.1521387   6.4054804   6.824504    7.3787465
  7.992129    8.558275    8.97093     9.15163     9.056665    8.664221
  7.944796    6.842208    5.384119    3.8576105   2.8516207   2.7667263
  3.1375415   3.4292154   3.6683533   3.9436982   4.096407    4.045314
  3.879058    3.6491196   3.3867676   3.0937915   2.7621753   2.4192398
  2.0920749   1.8696228   1.8625302   2.0239568   2.2510962   2.3128572
  2.0094757   1.3888273   0.73498684  0.41008288  0.49902168  0.7801276
  0.7602152   0.6696499   0.86655337  1.2822518   1.8391165   2.4193594
  2.8668196   3.1615782   3.3662307   3.530805    3.6976542   3.948557
  4.372398    4.9413533   5.4978685   5.875183    6.0014215   5.903507
  5.661299    5.3675003   5.102824    4.926097    4.8776174   4.985975
  5.268277    5.7192307   6.294367    6.9061995   7.442671    7.7976236
  7.9022384   7.7364125   7.300213    6.5480123   5.4057736   3.9476786
  2.6026144   2.1713736   2.5949345   3.051042    3.3553925   3.6974769
  3.9808087   4.0543647   4.0067763   3.8694196   3.6545396   3.385345
  3.057864    2.6873276   2.3254445   1.9978281 ]
