time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.49%, model saved.
Epoch: 0 Train: 4082.30347 Test: 4180.05127
Epoch: 100 Train: 47.80540 Test: 42.17756
Epoch 200: New minimal relative error: 27.18%, model saved.
Epoch: 200 Train: 7.48234 Test: 7.12123
Epoch: 300 Train: 8.73935 Test: 14.54203
Epoch: 400 Train: 2.85471 Test: 2.38427
Epoch 500: New minimal relative error: 21.62%, model saved.
Epoch: 500 Train: 2.11845 Test: 1.52759
Epoch: 600 Train: 1.82301 Test: 1.53418
Epoch: 700 Train: 0.97807 Test: 0.65378
Epoch: 800 Train: 0.98956 Test: 0.69820
Epoch: 900 Train: 11.58987 Test: 7.27204
Epoch 1000: New minimal relative error: 18.38%, model saved.
Epoch: 1000 Train: 1.21687 Test: 1.17638
Epoch: 1100 Train: 1.51383 Test: 1.17854
Epoch: 1200 Train: 0.36320 Test: 0.24590
Epoch: 1300 Train: 4.61903 Test: 5.29508
Epoch: 1400 Train: 1.24882 Test: 1.03340
Epoch: 1500 Train: 0.26074 Test: 0.17090
Epoch: 1600 Train: 2.90021 Test: 3.76050
Epoch 1700: New minimal relative error: 13.33%, model saved.
Epoch: 1700 Train: 0.44558 Test: 0.47232
Epoch: 1800 Train: 3.06231 Test: 3.47785
Epoch 1900: New minimal relative error: 9.03%, model saved.
Epoch: 1900 Train: 0.31076 Test: 0.37374
Epoch: 2000 Train: 0.39050 Test: 0.33268
Epoch: 2100 Train: 0.86797 Test: 0.69918
Epoch: 2200 Train: 0.32017 Test: 0.28384
Epoch: 2300 Train: 0.38726 Test: 0.20933
Epoch: 2400 Train: 0.15221 Test: 0.11764
Epoch: 2500 Train: 0.35525 Test: 0.39329
Epoch: 2600 Train: 0.96029 Test: 1.14808
Epoch: 2700 Train: 3.46845 Test: 3.09663
Epoch: 2800 Train: 2.41987 Test: 2.03512
Epoch: 2900 Train: 1.43909 Test: 1.46908
Epoch: 3000 Train: 0.35536 Test: 0.45156
Epoch: 3100 Train: 0.26845 Test: 0.16119
Epoch: 3200 Train: 0.45448 Test: 0.77433
Epoch: 3300 Train: 0.96520 Test: 0.65802
Epoch: 3400 Train: 0.64503 Test: 0.43105
Epoch: 3500 Train: 0.08808 Test: 0.07622
Epoch: 3600 Train: 0.14175 Test: 0.09685
Epoch: 3700 Train: 0.07740 Test: 0.05847
Epoch: 3800 Train: 0.08846 Test: 0.07354
Epoch: 3900 Train: 1.18682 Test: 1.56227
Epoch: 4000 Train: 0.07132 Test: 0.05519
Epoch: 4100 Train: 0.06917 Test: 0.05448
Epoch: 4200 Train: 1.64051 Test: 1.81893
Epoch: 4300 Train: 0.69870 Test: 0.87637
Epoch: 4400 Train: 0.07451 Test: 0.06511
Epoch: 4500 Train: 0.08511 Test: 0.08495
Epoch: 4600 Train: 0.12430 Test: 0.10617
Epoch: 4700 Train: 0.12921 Test: 0.13490
Epoch: 4800 Train: 0.12902 Test: 0.14197
Epoch: 4900 Train: 0.06950 Test: 0.06492
Epoch: 5000 Train: 0.38067 Test: 0.47891
Epoch: 5100 Train: 0.07563 Test: 0.07224
Epoch: 5200 Train: 0.05259 Test: 0.04390
Epoch: 5300 Train: 0.04965 Test: 0.04143
Epoch: 5400 Train: 0.57731 Test: 0.56843
Epoch: 5500 Train: 0.04976 Test: 0.04099
Epoch: 5600 Train: 0.28568 Test: 0.19098
Epoch: 5700 Train: 0.08643 Test: 0.14429
Epoch: 5800 Train: 0.07688 Test: 0.08478
Epoch: 5900 Train: 1.27910 Test: 1.10377
Epoch: 6000 Train: 0.04214 Test: 0.03644
Epoch: 6100 Train: 0.04195 Test: 0.03571
Epoch: 6200 Train: 0.05168 Test: 0.04673
Epoch: 6300 Train: 0.03971 Test: 0.03470
Epoch: 6400 Train: 0.04117 Test: 0.03580
Epoch: 6500 Train: 0.28947 Test: 0.21670
Epoch: 6600 Train: 0.05058 Test: 0.03542
Epoch: 6700 Train: 0.13349 Test: 0.09424
Epoch: 6800 Train: 0.48695 Test: 0.64140
Epoch: 6900 Train: 0.04426 Test: 0.04229
Epoch: 7000 Train: 0.53499 Test: 0.54311
Epoch: 7100 Train: 0.04538 Test: 0.04583
Epoch: 7200 Train: 0.03430 Test: 0.03178
Epoch: 7300 Train: 0.35589 Test: 0.42652
Epoch 7400: New minimal relative error: 7.71%, model saved.
Epoch: 7400 Train: 0.03289 Test: 0.03032
Epoch: 7500 Train: 0.03287 Test: 0.03045
Epoch: 7600 Train: 0.06568 Test: 0.06996
Epoch: 7700 Train: 0.05608 Test: 0.06192
Epoch: 7800 Train: 0.03071 Test: 0.02881
Epoch: 7900 Train: 0.03256 Test: 0.02986
Epoch: 8000 Train: 0.03039 Test: 0.02891
Epoch: 8100 Train: 0.02929 Test: 0.02782
Epoch: 8200 Train: 0.04976 Test: 0.04323
Epoch: 8300 Train: 0.02933 Test: 0.02822
Epoch: 8400 Train: 0.02849 Test: 0.02739
Epoch: 8500 Train: 0.03680 Test: 0.02860
Epoch: 8600 Train: 0.02720 Test: 0.02625
Epoch: 8700 Train: 0.03263 Test: 0.02920
Epoch: 8800 Train: 0.03051 Test: 0.03559
Epoch: 8900 Train: 0.02598 Test: 0.02550
Epoch: 9000 Train: 0.02650 Test: 0.02538
Epoch: 9100 Train: 0.16329 Test: 0.20274
Epoch: 9200 Train: 0.02501 Test: 0.02477
Epoch: 9300 Train: 0.03131 Test: 0.02718
Epoch: 9400 Train: 0.02439 Test: 0.02437
Epoch: 9500 Train: 0.18774 Test: 0.08984
Epoch: 9600 Train: 0.07326 Test: 0.06943
Epoch: 9700 Train: 0.02353 Test: 0.02381
Epoch: 9800 Train: 0.02324 Test: 0.02352
Epoch: 9900 Train: 0.02289 Test: 0.02327
Epoch: 9999 Train: 0.03437 Test: 0.02622
Training Loss: tensor(0.0344)
Test Loss: tensor(0.0262)
Learned LE: [ 0.9208921   0.04472483 -5.4375243 ]
True LE: [ 8.6025399e-01  1.0003110e-02 -1.4544204e+01]
Relative Error: [5.689053   5.9780703  5.378969   4.1016674  2.582829   1.2742653
 0.81652385 1.2929171  1.7033825  1.8494503  1.9391758  1.9581277
 1.733225   1.6046343  1.6659098  1.3093342  0.59230024 0.9473307
 1.632292   2.393139   3.1207042  3.574688   3.6783144  3.7953124
 4.024762   4.2652373  4.5023227  4.777288   5.193466   5.5884757
 5.753421   5.3857455  4.547234   4.0935864  4.0760045  3.7353494
 3.5322745  3.88596    4.5016537  4.752284   4.6448174  4.4899693
 4.3826017  3.8805702  3.491921   3.4211178  3.4139621  3.281777
 3.0372386  2.9189878  3.031185   3.2366145  3.5094879  3.7116847
 3.7362392  3.5970666  3.41427    3.173536   2.6623132  2.1479907
 2.2501822  3.0359757  4.132204   5.0282307  5.2079477  4.4854913
 3.154946   1.715504   0.66888887 0.8453708  1.3672755  1.5995423
 1.6097234  1.603466   1.3779465  1.2074859  1.5320779  1.7547684
 1.3104595  0.75089556 1.06579    1.8506252  2.7166634  3.4130816
 3.6085465  3.495631   3.5364275  3.6818337  3.8401086  3.991545
 4.1957617  4.6104465  5.078572   5.017618   4.555413   3.8369265
 3.4656432  3.4735513  3.2197614  3.117899   3.557494   3.9138389
 3.7442536  3.5036564  3.4439106  3.3073769  2.8840241  2.7874355
 2.8456643  2.797297   2.624118   2.4325814  2.471837   2.6565363
 2.8651752  3.0882378  3.1528447  2.9879112  2.723427   2.5220704
 2.1303992  1.4461076  1.1174634  1.436136   2.207427   3.2266393
 4.102922   4.2937016  3.5708923  2.2853055  1.0074589  0.41626176
 0.9593177  1.3600187  1.4282132  1.3549159  1.2127849  0.8880966
 0.9797053  1.4623756  1.6167469  1.2921766  1.0304888  1.3209642
 2.1652465  3.0116887  3.5032668  3.4519882  3.3213544  3.3976252
 3.4970388  3.5061758  3.4445696  3.4884763  3.8705773  4.4197516
 4.2807894  3.8095944  3.23545    2.8376281  2.8294895  2.7378461
 2.709955   3.1457663  3.184285   2.7588902  2.4802713  2.5335782
 2.3740613  2.1888847  2.2554948  2.2873578  2.2213814  2.0544064
 1.9228789  2.0946243  2.345987   2.5423853  2.683277   2.5974176
 2.2814305  1.9617802  1.674073   1.1671093  0.6450081  0.48587546
 0.67481136 1.2194433  2.063661   2.9375088  3.2934227  2.7580433
 1.6380367  0.5699208  0.39725417 0.91106164 1.2014065  1.1804519
 1.072705   0.8963774  0.7510973  0.8814197  1.2011548  1.3030237
 1.2425745  1.2648108  1.5581294  2.356236   3.126338   3.483557
 3.429928   3.2592483  3.2336519  3.2604098  3.1924822  3.010084
 2.9141033  3.135832   3.5511308  3.4816337  3.0735455  2.7045033
 2.21332    2.1266553  2.1932292  2.250823   2.6512644  2.4781308
 1.953588   1.6634548  1.7390661  1.6309695  1.6779596  1.7303193
 1.7085562  1.6141375  1.4197986  1.2967966  1.549122   1.8892785
 2.0900688  2.1844516  2.0223083  1.6849405  1.3784828  1.0175171
 0.58921885 0.38570002 0.30016163 0.19965293 0.30815622 0.8075295
 1.5993482  2.2754827  2.2100554  1.3794092  0.4585301  0.23849067
 0.6515893  0.8904863  0.86106694 0.76334256 0.7692072  0.8952206
 0.89915955 1.0028819  0.97780174 1.0824236  1.3747013  1.6858459
 2.334234   3.189167   3.679116   3.6404426  3.3446937  3.1709096
 3.072913   2.825786   2.4903286  2.3173964  2.4337983  2.6439862
 2.644916   2.2140164  2.131822   1.6872417  1.3545123  1.3340728
 1.6205238  2.0194407  1.890252   1.3148941  1.04976    1.0860742
 1.0946006  1.1767008  1.209529   1.1438315  1.031206   0.856839
 0.71752775 0.9032951  1.2775165  1.456792   1.5047054  1.3905247
 1.1429174  0.9036543  0.62356573 0.43285605 0.37587896 0.3583094
 0.3179362  0.33915702 0.3208825  0.3700782  1.1925709  1.8125249
 1.5144917  0.6571344  0.15882714 0.39543575 0.57501924 0.5881074
 0.48612556 0.68982553 1.087076   1.0561913  1.070825   1.0077462
 1.1277603  1.4760789  1.8534806  2.2102606  2.9987633  3.7203562
 3.62723    3.2193675  2.857408   2.824099   2.7389932  2.464227
 2.1132712  1.8317146  1.7141553  1.7183509  1.5415877  1.1903689
 1.280185   0.82826984 0.6154072  0.62843764 1.0957644  1.3121872
 0.74936086 0.5209419  0.5645858  0.8538561  0.687674   0.7077252
 0.7545805  0.6290657  0.46859938 0.32870212 0.29465503 0.60668117
 0.85291576 0.8411172  0.74545044 0.66055644 0.54866695 0.35871565
 0.29247653 0.36190018 0.34548104 0.31464627 0.36528617 0.5780028
 0.59999263 0.25926548 0.9822362  1.594845   1.2277479  0.38492417
 0.33561978 0.4552207  0.4624953  0.40871155 0.5803922  1.0839779
 1.4250679  1.2374506  1.2415     1.26455    1.7718736  2.1036801
 2.2759426  2.7018523  2.8943126  2.7948432 ]
