time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 3966.91406 Test: 4090.42603
Epoch 80: New minimal relative error: 70.59%, model saved.
Epoch: 80 Train: 111.46104 Test: 128.83604
Epoch 160: New minimal relative error: 48.36%, model saved.
Epoch: 160 Train: 49.50916 Test: 40.24155
Epoch: 240 Train: 70.63578 Test: 120.39580
Epoch 320: New minimal relative error: 45.14%, model saved.
Epoch: 320 Train: 9.92404 Test: 11.22345
Epoch: 400 Train: 3.38007 Test: 4.72417
Epoch: 480 Train: 3.06331 Test: 3.55697
Epoch 560: New minimal relative error: 16.62%, model saved.
Epoch: 560 Train: 4.04741 Test: 7.06907
Epoch: 640 Train: 2.90518 Test: 2.66952
Epoch 720: New minimal relative error: 7.53%, model saved.
Epoch: 720 Train: 1.55205 Test: 2.51413
Epoch: 800 Train: 4.03179 Test: 5.02062
Epoch: 880 Train: 3.06845 Test: 4.21364
Epoch: 960 Train: 4.43662 Test: 5.61864
Epoch: 1040 Train: 3.92983 Test: 5.07795
Epoch 1120: New minimal relative error: 7.20%, model saved.
Epoch: 1120 Train: 1.82913 Test: 2.31332
Epoch: 1200 Train: 2.79286 Test: 3.87317
Epoch 1280: New minimal relative error: 4.79%, model saved.
Epoch: 1280 Train: 0.44275 Test: 1.03810
Epoch: 1360 Train: 1.68288 Test: 2.02964
Epoch 1440: New minimal relative error: 4.18%, model saved.
Epoch: 1440 Train: 0.27105 Test: 0.78920
Epoch: 1520 Train: 0.30083 Test: 0.99604
Epoch: 1600 Train: 0.27142 Test: 0.82894
Epoch: 1680 Train: 0.21022 Test: 0.69060
Epoch: 1760 Train: 0.28122 Test: 0.66164
Epoch: 1840 Train: 0.31216 Test: 0.75190
Epoch: 1920 Train: 0.19047 Test: 0.57902
Epoch: 2000 Train: 0.15529 Test: 0.56016
Epoch 2080: New minimal relative error: 3.05%, model saved.
Epoch: 2080 Train: 0.26776 Test: 0.80797
Epoch: 2160 Train: 0.71388 Test: 1.35235
Epoch: 2240 Train: 6.66100 Test: 4.87404
Epoch: 2320 Train: 0.79098 Test: 0.87287
Epoch: 2400 Train: 0.38293 Test: 0.68102
Epoch: 2480 Train: 1.65313 Test: 1.55339
Epoch: 2560 Train: 1.74080 Test: 1.68515
Epoch: 2640 Train: 0.50341 Test: 0.78092
Epoch: 2720 Train: 0.41676 Test: 0.75263
Epoch: 2800 Train: 2.20276 Test: 3.05577
Epoch: 2880 Train: 2.18683 Test: 3.27959
Epoch: 2960 Train: 6.99329 Test: 4.69033
Epoch: 3040 Train: 0.07283 Test: 0.35320
Epoch: 3120 Train: 0.22471 Test: 0.48067
Epoch: 3200 Train: 0.68161 Test: 0.90161
Epoch: 3280 Train: 0.06909 Test: 0.32683
Epoch: 3360 Train: 0.26925 Test: 0.59081
Epoch: 3440 Train: 0.11996 Test: 0.36487
Epoch 3520: New minimal relative error: 2.91%, model saved.
Epoch: 3520 Train: 0.05942 Test: 0.29755
Epoch: 3600 Train: 0.11728 Test: 0.49238
Epoch: 3680 Train: 1.18183 Test: 0.93597
Epoch: 3760 Train: 0.06270 Test: 0.28338
Epoch: 3840 Train: 0.05786 Test: 0.29675
Epoch: 3920 Train: 0.20880 Test: 0.33665
Epoch: 4000 Train: 0.10273 Test: 0.33757
Epoch 4080: New minimal relative error: 2.62%, model saved.
Epoch: 4080 Train: 0.37174 Test: 0.42049
Epoch: 4160 Train: 0.45536 Test: 0.64682
Epoch: 4240 Train: 0.14630 Test: 0.37369
Epoch: 4320 Train: 0.15622 Test: 0.37290
Epoch: 4400 Train: 0.23604 Test: 0.44686
Epoch: 4480 Train: 0.04403 Test: 0.24733
Epoch: 4560 Train: 0.86332 Test: 0.87763
Epoch 4640: New minimal relative error: 2.10%, model saved.
Epoch: 4640 Train: 0.03879 Test: 0.23144
Epoch: 4720 Train: 0.03683 Test: 0.22450
Epoch: 4800 Train: 0.04846 Test: 0.24090
Epoch: 4880 Train: 0.03328 Test: 0.21043
Epoch: 4960 Train: 0.20334 Test: 0.36945
Epoch: 5040 Train: 0.03325 Test: 0.21479
Epoch: 5120 Train: 0.03384 Test: 0.22106
Epoch: 5200 Train: 2.31837 Test: 2.97752
Epoch: 5280 Train: 0.03139 Test: 0.20405
Epoch: 5360 Train: 0.02927 Test: 0.18922
Epoch: 5440 Train: 0.04740 Test: 0.22099
Epoch: 5520 Train: 1.43065 Test: 0.65197
Epoch: 5600 Train: 0.02891 Test: 0.19293
Epoch: 5680 Train: 0.36426 Test: 0.50609
Epoch: 5760 Train: 0.02698 Test: 0.18258
Epoch: 5840 Train: 0.27103 Test: 0.53693
Epoch: 5920 Train: 0.02714 Test: 0.18652
Epoch: 6000 Train: 0.06951 Test: 0.23899
Epoch: 6080 Train: 0.02513 Test: 0.17279
Epoch: 6160 Train: 0.56662 Test: 0.29220
Epoch: 6240 Train: 0.02464 Test: 0.17218
Epoch: 6320 Train: 0.02322 Test: 0.16280
Epoch: 6400 Train: 0.13376 Test: 0.27389
Epoch: 6480 Train: 0.02273 Test: 0.15936
Epoch: 6560 Train: 0.03361 Test: 0.19014
Epoch: 6640 Train: 0.02234 Test: 0.16040
Epoch: 6720 Train: 0.02134 Test: 0.15286
Epoch: 6800 Train: 0.02370 Test: 0.17796
Epoch: 6880 Train: 0.02135 Test: 0.15338
Epoch: 6960 Train: 0.02045 Test: 0.14709
Epoch: 7040 Train: 0.14947 Test: 0.38639
Epoch: 7120 Train: 0.02031 Test: 0.14836
Epoch: 7200 Train: 0.02013 Test: 0.14359
Epoch: 7280 Train: 0.02066 Test: 0.14694
Epoch: 7360 Train: 0.01907 Test: 0.13942
Epoch: 7440 Train: 0.02379 Test: 0.14225
Epoch: 7520 Train: 0.02047 Test: 0.14287
Epoch 7600: New minimal relative error: 1.89%, model saved.
Epoch: 7600 Train: 0.01830 Test: 0.13542
Epoch: 7680 Train: 0.03064 Test: 0.13677
Epoch: 7760 Train: 0.01776 Test: 0.13143
Epoch: 7840 Train: 0.06750 Test: 0.16902
Epoch 7920: New minimal relative error: 1.89%, model saved.
Epoch: 7920 Train: 0.01748 Test: 0.13062
Epoch: 7999 Train: 0.01695 Test: 0.12668
Training Loss: tensor(0.0169)
Test Loss: tensor(0.1267)
Learned LE: [ 0.8721517  -0.01061466 -5.131051  ]
True LE: [ 8.6591327e-01  3.8335377e-03 -1.4544127e+01]
Relative Error: [0.63568246 0.8589614  1.1785345  1.5197449  1.8546801  2.1666873
 2.432464   2.6285677  2.7507594  2.819148   2.8565507  2.8634436
 2.8243248  2.7342563  2.6013975  2.424436   2.1881835  1.8914882
 1.5688596  1.2841058  1.1061082  1.0690833  1.1563563  1.292889
 1.3230674  1.122557   0.80843914 0.7179981  0.858239   0.9803284
 1.022024   1.0034257  0.9423658  0.8757644  0.8473519  0.87587965
 0.938039   0.99102473 0.99982685 0.9476427  0.8574472  0.7954659
 0.79417545 0.8088683  0.7895892  0.72392404 0.6304917  0.5411509
 0.47884914 0.47081485 0.55873287 0.727539   0.91623735 1.0841422
 1.2237148  1.3230088  1.3460116  1.2790561  1.1402975  0.95407236
 0.76291275 0.622846   0.61720365 0.81809545 1.1529664  1.5155125
 1.8491691  2.1348765  2.3609269  2.5125277  2.5868876  2.607808
 2.610478   2.6055017  2.5704558  2.4866447  2.3631346  2.2107086
 2.0125504  1.7474452  1.4381573  1.1467226  0.93991405 0.8644621
 0.9232736  1.0599005  1.1212403  0.96199566 0.69489425 0.66101485
 0.80897623 0.9237349  0.9689356  0.9223868  0.80678976 0.7106566
 0.68832964 0.7440959  0.8333592  0.89821666 0.8969703  0.81739306
 0.7131185  0.6884212  0.74414414 0.7859924  0.76120454 0.6694268
 0.54582167 0.44137618 0.38194636 0.36680943 0.43025553 0.58970433
 0.78350043 0.9561885  1.0943496  1.1957057  1.2271159  1.164333
 1.0314909  0.87552136 0.7450617  0.650897   0.61497873 0.7703339
 1.1218088  1.5105664  1.8286271  2.0528605  2.19955    2.2816985
 2.3047953  2.2871304  2.265188   2.2618818  2.25508    2.203057
 2.0975432  1.9632494  1.8057154  1.5900815  1.3105109  1.0277163
 0.807059   0.6898739  0.701934   0.8229525  0.91925013 0.81791943
 0.5907174  0.5793871  0.7217513  0.8341341  0.89284396 0.81893384
 0.6549483  0.5377083  0.52103704 0.59899676 0.7128604  0.79408324
 0.78813154 0.673775   0.53777695 0.5559089  0.67503095 0.7471131
 0.7263772  0.61933327 0.47028935 0.34210128 0.26904967 0.24718212
 0.27975476 0.4159438  0.6132126  0.7949548  0.9319322  1.0286126
 1.0691137  1.0212109  0.9032377  0.7820126  0.71497226 0.67524374
 0.6273515  0.7234302  1.0711046  1.4745431  1.7621336  1.8980858
 1.9304872  1.9167053  1.8861343  1.8460553  1.8122176  1.813368
 1.8480569  1.8653221  1.8186828  1.7128981  1.581499   1.4192592
 1.1857069  0.91838974 0.7015457  0.5631603  0.51977307 0.5989001
 0.71720254 0.681914   0.4887489  0.47666994 0.6118704  0.72038245
 0.79629105 0.70099545 0.49873117 0.36343327 0.34493423 0.43456233
 0.56725484 0.6719118  0.680941   0.5456487  0.34546643 0.3791899
 0.5495128  0.64976    0.6501471  0.55426776 0.4057631  0.2746023
 0.17909303 0.12650849 0.12648635 0.21129023 0.40038624 0.59452254
 0.73848385 0.8258787  0.8647255  0.8340973  0.73455936 0.64100105
 0.6326252  0.6591157  0.64075893 0.6934231  0.992338   1.370401
 1.6172769  1.6727905  1.5927229  1.4702809  1.3774205  1.3279717
 1.2980883  1.2911371  1.336128   1.4192971  1.4771204  1.4627784
 1.3761288  1.2510779  1.0741662  0.82747287 0.6039113  0.47044206
 0.4015704  0.42202455 0.5299854  0.5518012  0.39565474 0.36370283
 0.49589837 0.5855474  0.67580765 0.5742592  0.34952158 0.19976231
 0.1741967  0.26052365 0.3961414  0.52285117 0.56786144 0.46176752
 0.22679794 0.20326301 0.3721263  0.47334    0.5024433  0.44928518
 0.3403839  0.2514344  0.1870324  0.11605567 0.08052807 0.03532258
 0.15816547 0.3612417  0.5276732  0.620817   0.6464018  0.61901605
 0.53235626 0.4442682  0.47714612 0.57973444 0.62949187 0.6881056
 0.91018593 1.1996748  1.3845844  1.3979774  1.2634228  1.0614549
 0.8984683  0.83651704 0.82932776 0.81516653 0.8138274  0.88081473
 1.0034304  1.1141949  1.1588384  1.1090912  0.9880079  0.78665614
 0.538183   0.37369776 0.31869277 0.31920746 0.39149806 0.43936703
 0.33108285 0.2562911  0.3925589  0.43930513 0.5301658  0.45220047
 0.23420525 0.10370159 0.10373987 0.14998195 0.22608447 0.34504122
 0.4260502  0.3881213  0.24330485 0.21391529 0.25864464 0.26709706
 0.2841963  0.2836081  0.2528704  0.24252655 0.24117191 0.2232975
 0.19764131 0.16079047 0.07254245 0.10702976 0.3037465  0.43859786
 0.47489133 0.4325529  0.3465303  0.24108124 0.27568635 0.44963196
 0.58237433 0.6882204  0.86326313 1.0346649  1.1166267  1.1012424
 0.98555666 0.788143   0.5939857  0.520885   0.54629177 0.55020994
 0.49319634 0.43622828 0.4859329  0.6292361  0.78369117 0.89750475
 0.90545046 0.79602236 0.5729978  0.3294298  0.21356086 0.23714599
 0.31847703 0.36817104 0.31206715 0.18831846]
