time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 102.39%, model saved.
Epoch: 0 Train: 3454.59766 Test: 4194.26709
Epoch 100: New minimal relative error: 32.38%, model saved.
Epoch: 100 Train: 105.87651 Test: 86.48196
Epoch 200: New minimal relative error: 27.40%, model saved.
Epoch: 200 Train: 12.91400 Test: 14.85999
Epoch 300: New minimal relative error: 12.82%, model saved.
Epoch: 300 Train: 9.90582 Test: 7.97882
Epoch: 400 Train: 17.55367 Test: 14.17311
Epoch 500: New minimal relative error: 12.80%, model saved.
Epoch: 500 Train: 5.92505 Test: 4.39042
Epoch: 600 Train: 8.07765 Test: 6.51835
Epoch: 700 Train: 22.09023 Test: 12.53450
Epoch 800: New minimal relative error: 12.35%, model saved.
Epoch: 800 Train: 3.16117 Test: 2.68542
Epoch: 900 Train: 4.31713 Test: 5.02600
Epoch 1000: New minimal relative error: 11.46%, model saved.
Epoch: 1000 Train: 6.30351 Test: 4.47011
Epoch: 1100 Train: 1.89748 Test: 1.49883
Epoch: 1200 Train: 36.35897 Test: 44.77317
Epoch: 1300 Train: 6.69880 Test: 9.17745
Epoch: 1400 Train: 3.33185 Test: 3.59167
Epoch: 1500 Train: 1.71971 Test: 1.90539
Epoch: 1600 Train: 3.07860 Test: 5.29033
Epoch: 1700 Train: 1.40359 Test: 0.96478
Epoch: 1800 Train: 1.03976 Test: 0.83213
Epoch: 1900 Train: 3.04654 Test: 3.27411
Epoch 2000: New minimal relative error: 6.89%, model saved.
Epoch: 2000 Train: 1.86162 Test: 2.28782
Epoch: 2100 Train: 4.21026 Test: 3.59286
Epoch: 2200 Train: 0.66393 Test: 0.73373
Epoch: 2300 Train: 0.87365 Test: 1.85005
Epoch: 2400 Train: 1.21847 Test: 1.08922
Epoch: 2500 Train: 2.02358 Test: 4.19939
Epoch: 2600 Train: 1.47210 Test: 1.55719
Epoch: 2700 Train: 1.99812 Test: 2.35868
Epoch: 2800 Train: 2.60538 Test: 1.55482
Epoch: 2900 Train: 8.05521 Test: 9.09014
Epoch 3000: New minimal relative error: 5.96%, model saved.
Epoch: 3000 Train: 0.97652 Test: 1.06024
Epoch: 3100 Train: 0.39745 Test: 0.41279
Epoch: 3200 Train: 0.50852 Test: 0.59334
Epoch: 3300 Train: 0.21337 Test: 0.27389
Epoch: 3400 Train: 0.52931 Test: 0.69806
Epoch: 3500 Train: 6.29946 Test: 8.71157
Epoch: 3600 Train: 5.00013 Test: 5.16647
Epoch 3700: New minimal relative error: 3.84%, model saved.
Epoch: 3700 Train: 0.97940 Test: 1.14592
Epoch: 3800 Train: 0.83025 Test: 1.17690
Epoch: 3900 Train: 0.17238 Test: 0.25705
Epoch: 4000 Train: 0.89939 Test: 1.20546
Epoch: 4100 Train: 0.46946 Test: 0.78958
Epoch: 4200 Train: 0.12931 Test: 0.16309
Epoch: 4300 Train: 0.20597 Test: 0.26267
Epoch: 4400 Train: 0.44334 Test: 0.30910
Epoch: 4500 Train: 1.13674 Test: 0.33632
Epoch: 4600 Train: 0.88950 Test: 0.60530
Epoch: 4700 Train: 1.23908 Test: 1.18970
Epoch: 4800 Train: 0.38548 Test: 0.46537
Epoch: 4900 Train: 0.29533 Test: 0.28926
Epoch: 5000 Train: 0.15062 Test: 0.23218
Epoch: 5100 Train: 0.16571 Test: 0.17801
Epoch: 5200 Train: 0.10188 Test: 0.14791
Epoch: 5300 Train: 0.42870 Test: 0.73276
Epoch: 5400 Train: 0.21264 Test: 0.32797
Epoch: 5500 Train: 4.17482 Test: 6.42383
Epoch: 5600 Train: 0.07984 Test: 0.11996
Epoch 5700: New minimal relative error: 2.58%, model saved.
Epoch: 5700 Train: 0.26858 Test: 0.20668
Epoch: 5800 Train: 0.34034 Test: 0.44800
Epoch: 5900 Train: 0.17111 Test: 0.17583
Epoch: 6000 Train: 0.14459 Test: 0.20813
Epoch: 6100 Train: 0.09427 Test: 0.10689
Epoch: 6200 Train: 0.08907 Test: 0.12727
Epoch: 6300 Train: 0.15412 Test: 0.20250
Epoch: 6400 Train: 0.20300 Test: 0.23220
Epoch: 6500 Train: 0.07067 Test: 0.11094
Epoch: 6600 Train: 2.08036 Test: 2.64634
Epoch: 6700 Train: 0.10892 Test: 0.14789
Epoch: 6800 Train: 0.16882 Test: 0.26747
Epoch: 6900 Train: 0.11435 Test: 0.12506
Epoch: 7000 Train: 0.28129 Test: 0.30176
Epoch: 7100 Train: 0.11841 Test: 0.13504
Epoch: 7200 Train: 0.76169 Test: 0.67832
Epoch: 7300 Train: 0.05056 Test: 0.08701
Epoch: 7400 Train: 0.06389 Test: 0.10648
Epoch: 7500 Train: 0.06354 Test: 0.09870
Epoch: 7600 Train: 0.15532 Test: 0.19338
Epoch: 7700 Train: 0.34485 Test: 0.55073
Epoch: 7800 Train: 0.06555 Test: 0.10640
Epoch: 7900 Train: 0.13094 Test: 0.13249
Epoch: 8000 Train: 0.14022 Test: 0.16749
Epoch: 8100 Train: 0.04500 Test: 0.08502
Epoch: 8200 Train: 0.08944 Test: 0.22342
Epoch: 8300 Train: 0.14475 Test: 0.26771
Epoch: 8400 Train: 0.24489 Test: 0.38642
Epoch: 8500 Train: 0.31718 Test: 0.51515
Epoch: 8600 Train: 2.16691 Test: 2.54574
Epoch: 8700 Train: 0.59540 Test: 0.73265
Epoch: 8800 Train: 0.06500 Test: 0.12748
Epoch: 8900 Train: 0.05809 Test: 0.11188
Epoch: 9000 Train: 0.11046 Test: 0.15537
Epoch: 9100 Train: 0.08925 Test: 0.14053
Epoch: 9200 Train: 0.06016 Test: 0.09480
Epoch: 9300 Train: 0.04619 Test: 0.09079
Epoch: 9400 Train: 0.06256 Test: 0.11421
Epoch: 9500 Train: 0.09131 Test: 0.11091
Epoch: 9600 Train: 0.04355 Test: 0.08456
Epoch: 9700 Train: 0.07916 Test: 0.11947
Epoch: 9800 Train: 0.10102 Test: 0.08715
Epoch: 9900 Train: 0.13507 Test: 0.20565
Epoch: 9999 Train: 0.03303 Test: 0.07052
Training Loss: tensor(0.0330)
Test Loss: tensor(0.0705)
Learned LE: [ 0.8001746   0.07043393 -3.048359  ]
True LE: [ 8.6228448e-01  8.3236089e-03 -1.4540936e+01]
Relative Error: [1.0881926  1.1682566  1.2885687  1.22454    1.1346775  0.89926916
 0.5942018  0.57423884 0.4422533  0.8273605  1.2695357  1.3303128
 1.7971658  2.36481    2.9720826  3.3980138  3.7512436  4.286029
 4.6629615  4.7671223  4.877483   5.098387   5.3175488  5.591961
 5.8757825  5.696051   5.474695   5.338702   5.291377   5.6687536
 6.2228703  6.256      5.818677   5.42511    4.555875   3.6435678
 3.1741402  3.3079863  2.7710361  1.5414133  0.7209468  0.15462054
 0.5381543  1.2140633  1.7504519  2.098296   2.092938   1.725782
 1.666291   1.7744068  2.0273192  2.353465   2.387041   2.3877592
 2.4049838  2.2341425  2.0964832  1.7848665  1.3481008  0.86387825
 0.9580794  1.0896337  1.0513356  1.0364904  1.2347709  1.2630234
 1.0276117  0.7655277  0.5492053  0.69963044 0.53658557 0.7980625
 1.1316994  1.1959646  1.6309662  2.183392   2.7998543  3.2304876
 3.5854816  3.9615788  4.1459093  4.1160073  4.213722   4.453297
 4.8972363  5.2939663  5.513151   5.498285   5.3397427  5.0740247
 4.677694   4.8539896  5.2140584  5.557896   5.270261   4.814983
 4.1886287  3.3833876  2.8741107  2.9193385  2.4903197  1.3074347
 0.7335053  0.34150967 0.4234684  1.0462537  1.524136   1.8181043
 1.4726446  1.2275448  1.2886386  1.5107472  1.8295819  2.1198483
 2.258476   2.3251898  2.370259   2.2833018  2.1306562  1.8959277
 1.4324414  0.87004673 0.722737   0.8306134  0.92034453 0.9253656
 0.99698865 1.2215413  1.0719534  0.69111466 0.5796966  0.7514012
 0.64793384 0.8186661  1.0343952  1.1737646  1.5961164  2.0810997
 2.5589137  3.0955207  3.27426    3.4374404  3.4891644  3.507028
 3.6123629  4.1149397  4.540136   4.782423   5.021366   5.116613
 4.896747   4.8948483  4.42828    4.0505757  4.2176657  4.6200876
 4.6137977  4.350832   3.8819532  3.2258754  2.6726246  2.5895278
 2.2713263  1.1332138  0.6649899  0.5139276  0.37602547 0.78684783
 1.2476919  1.364204   0.91002864 0.8829392  1.1333292  1.3409979
 1.489192   1.6373763  1.7937075  1.8368466  1.9630181  2.039323
 2.017649   1.9407455  1.5628163  1.0229411  0.57016844 0.56256557
 0.65724295 0.71399915 0.75379276 1.0277199  1.1075171  0.7883772
 0.5165747  0.82759345 0.7271272  0.76535004 0.984147   1.2152529
 1.4993854  1.9711161  2.3965251  2.8278875  2.9122238  2.9436736
 3.0296102  3.0783942  3.200816   3.6685514  4.1349034  4.3293705
 4.429261   4.630982   4.5844874  4.434236   4.4006915  3.9545949
 3.6619515  3.4528203  3.7993164  3.870368   3.5043764  2.9512274
 2.454802   2.279781   2.079513   1.1580191  0.6810577  0.5538451
 0.4451501  0.48426697 0.8706083  0.85320675 0.44973892 0.9655355
 1.1350191  1.16866    1.2324562  1.2624631  1.3169899  1.4544055
 1.5167416  1.6725084  1.6895605  1.7382545  1.6208652  1.2210343
 0.7803091  0.5063869  0.51310235 0.48395377 0.42797527 0.6250747
 1.0725509  0.9943932  0.70457673 0.8176597  0.7739605  0.7211937
 0.9142371  1.2060579  1.3577992  1.8433292  2.2709756  2.4325287
 2.4472766  2.4441357  2.6159468  2.8487422  3.0079403  3.3254735
 3.5633605  3.7405365  4.1786213  4.161548   4.0946746  4.0193844
 3.8396788  3.6778104  3.4040372  3.0650527  2.8927255  2.9831977
 3.0266247  2.5997486  2.1926253  2.018593   1.6909659  1.1065882
 0.70290613 0.6580238  0.6493625  0.46593463 0.5189027  0.45917803
 0.56507844 0.8411019  0.88450444 0.87943006 0.82718956 0.77816087
 0.8574489  1.104944   1.1914135  1.2573442  1.425796   1.4130207
 1.4006722  1.2415574  0.9472205  0.51380086 0.34750056 0.50124115
 0.3796662  0.2803043  0.6760756  1.0006781  1.0065316  0.8999662
 1.0488869  0.8192104  0.86876565 1.142128   1.2657267  1.6407814
 1.9097762  2.03034    1.9662583  1.9890617  2.092132   2.3885918
 2.6305778  2.9844496  3.2017093  3.3290517  3.5009906  3.564692
 3.4367013  3.410367   3.3138194  3.0957665  3.04536    2.8373337
 2.474722   2.3451486  2.122269   2.1673334  1.7558137  1.5812794
 1.3876489  0.8954452  0.6494637  0.7378588  0.7385214  0.6557504
 0.39069653 0.33743313 0.526741   0.53798926 0.6231055  0.667129
 0.5812784  0.43149522 0.45121726 0.6529941  0.7955844  0.95958745
 1.1160789  1.1128377  1.1136689  0.9934874  0.941405   0.80713874
 0.35105556 0.3930117  0.50815094 0.3079814  0.22261325 0.65709597
 0.9302502  1.0210669  1.1603727  1.1846358  0.7639369  0.8959299
 1.0535983  1.315744   1.5079722  1.6315973  1.5734851  1.7809076
 2.0004895  2.2968156  2.5831444  2.428243   2.348587   2.714568
 2.9019425  2.8257551  2.7420797  2.540107  ]
