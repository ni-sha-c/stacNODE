time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.56%, model saved.
Epoch: 0 Train: 31924.03906 Test: 3970.93652
Epoch 80: New minimal relative error: 87.59%, model saved.
Epoch: 80 Train: 6023.57471 Test: 1100.34534
Epoch: 160 Train: 2135.85303 Test: 253.45753
Epoch 240: New minimal relative error: 21.65%, model saved.
Epoch: 240 Train: 307.04373 Test: 32.40736
Epoch: 320 Train: 112.47055 Test: 7.50027
Epoch: 400 Train: 70.61515 Test: 5.08674
Epoch: 480 Train: 77.26579 Test: 21.83392
Epoch 560: New minimal relative error: 12.63%, model saved.
Epoch: 560 Train: 37.50030 Test: 4.49491
Epoch: 640 Train: 20.84012 Test: 0.71297
Epoch 720: New minimal relative error: 7.69%, model saved.
Epoch: 720 Train: 17.66650 Test: 0.76364
Epoch 800: New minimal relative error: 6.74%, model saved.
Epoch: 800 Train: 13.00434 Test: 0.41133
Epoch: 880 Train: 14.79365 Test: 2.01890
Epoch: 960 Train: 11.59126 Test: 1.25697
Epoch: 1040 Train: 9.30884 Test: 0.31119
Epoch: 1120 Train: 8.25352 Test: 0.26715
Epoch 1200: New minimal relative error: 1.68%, model saved.
Epoch: 1200 Train: 7.98753 Test: 0.33914
Epoch: 1280 Train: 30.34077 Test: 3.07814
Epoch: 1360 Train: 6.33465 Test: 0.19587
Epoch: 1440 Train: 6.03671 Test: 0.21104
Epoch 1520: New minimal relative error: 1.35%, model saved.
Epoch: 1520 Train: 11.97508 Test: 1.73999
Epoch: 1600 Train: 7.69239 Test: 0.28373
Epoch: 1680 Train: 14.37417 Test: 1.21954
Epoch: 1760 Train: 5.33910 Test: 0.33123
Epoch: 1840 Train: 4.35660 Test: 0.13236
Epoch: 1920 Train: 4.66657 Test: 0.22210
Epoch: 2000 Train: 14.58344 Test: 6.99440
Epoch: 2080 Train: 6.09182 Test: 1.35584
Epoch: 2160 Train: 5.77442 Test: 1.91343
Epoch 2240: New minimal relative error: 0.62%, model saved.
Epoch: 2240 Train: 3.33315 Test: 0.12735
Epoch: 2320 Train: 5.51114 Test: 1.55631
Epoch: 2400 Train: 3.18156 Test: 0.14773
Epoch: 2480 Train: 3.09636 Test: 0.32682
Epoch: 2560 Train: 2.79692 Test: 0.09805
Epoch: 2640 Train: 2.65397 Test: 0.12646
Epoch: 2720 Train: 2.55481 Test: 0.09518
Epoch: 2800 Train: 3.45385 Test: 0.17613
Epoch: 2880 Train: 2.97809 Test: 0.67268
Epoch: 2960 Train: 2.23729 Test: 0.08474
Epoch 3040: New minimal relative error: 0.62%, model saved.
Epoch: 3040 Train: 2.18459 Test: 0.07765
Epoch: 3120 Train: 2.07470 Test: 0.09356
Epoch: 3200 Train: 2.01525 Test: 0.08079
Epoch: 3280 Train: 1.91117 Test: 0.07305
Epoch: 3360 Train: 2.19525 Test: 0.15415
Epoch: 3440 Train: 1.77953 Test: 0.07234
Epoch: 3520 Train: 1.81353 Test: 0.09118
Epoch: 3600 Train: 1.66144 Test: 0.07025
Epoch: 3680 Train: 13.77966 Test: 8.47383
Epoch: 3760 Train: 1.55824 Test: 0.06982
Epoch: 3840 Train: 4.19647 Test: 0.82907
Epoch: 3920 Train: 1.45714 Test: 0.06916
Epoch: 4000 Train: 1.48625 Test: 0.10269
Epoch: 4080 Train: 1.37316 Test: 0.06824
Epoch: 4160 Train: 3.34281 Test: 0.44371
Epoch: 4240 Train: 1.28576 Test: 0.06722
Epoch: 4320 Train: 1.34225 Test: 0.10950
Epoch: 4400 Train: 5.28496 Test: 2.76545
Epoch: 4480 Train: 1.22026 Test: 0.07778
Epoch: 4560 Train: 1.21429 Test: 0.11804
Epoch: 4640 Train: 1.11729 Test: 0.06807
Epoch: 4720 Train: 1.09834 Test: 0.07355
Epoch: 4800 Train: 1.09389 Test: 0.07075
Epoch: 4880 Train: 1.56688 Test: 0.14742
Epoch: 4960 Train: 2.29126 Test: 0.83997
Epoch: 5040 Train: 2.03177 Test: 0.90436
Epoch: 5120 Train: 1.67231 Test: 0.21275
Epoch: 5200 Train: 0.93255 Test: 0.06897
Epoch: 5280 Train: 0.90608 Test: 0.06376
Epoch: 5360 Train: 0.96082 Test: 0.08734
Epoch: 5440 Train: 0.86805 Test: 0.07525
Epoch 5520: New minimal relative error: 0.35%, model saved.
Epoch: 5520 Train: 0.83334 Test: 0.06131
Epoch: 5600 Train: 0.81156 Test: 0.05998
Epoch: 5680 Train: 0.80626 Test: 0.06524
Epoch: 5760 Train: 0.89646 Test: 0.16572
Epoch: 5840 Train: 0.75568 Test: 0.05894
Epoch: 5920 Train: 1.55763 Test: 0.22530
Epoch: 6000 Train: 1.01402 Test: 0.31726
Epoch: 6080 Train: 0.70646 Test: 0.05903
Epoch: 6160 Train: 1.87326 Test: 0.12852
Epoch: 6240 Train: 0.67758 Test: 0.05795
Epoch: 6320 Train: 0.69160 Test: 0.07414
Epoch: 6400 Train: 0.64762 Test: 0.05677
Epoch: 6480 Train: 0.93071 Test: 0.12067
Epoch: 6560 Train: 0.62152 Test: 0.05631
Epoch: 6640 Train: 0.66540 Test: 0.08004
Epoch 6720: New minimal relative error: 0.29%, model saved.
Epoch: 6720 Train: 0.59667 Test: 0.05566
Epoch: 6800 Train: 0.94779 Test: 0.15358
Epoch: 6880 Train: 0.57379 Test: 0.05532
Epoch: 6960 Train: 0.57906 Test: 0.06402
Epoch: 7040 Train: 1.35825 Test: 0.20578
Epoch: 7120 Train: 0.54137 Test: 0.05487
Epoch: 7200 Train: 0.78117 Test: 0.14342
Epoch: 7280 Train: 0.52171 Test: 0.05452
Epoch: 7360 Train: 0.51193 Test: 0.05442
Epoch: 7440 Train: 0.50490 Test: 0.05479
Epoch: 7520 Train: 0.49394 Test: 0.05434
Epoch: 7600 Train: 0.49394 Test: 0.05588
Epoch: 7680 Train: 0.47631 Test: 0.05376
Epoch: 7760 Train: 0.67745 Test: 0.08791
Epoch: 7840 Train: 0.46043 Test: 0.05376
Epoch: 7920 Train: 0.46996 Test: 0.06536
Epoch 7999: New minimal relative error: 0.22%, model saved.
Epoch: 7999 Train: 0.44445 Test: 0.05349
Training Loss: tensor(0.4445)
Test Loss: tensor(0.0535)
Learned LE: [ 8.8786000e-01 -4.3272585e-04 -1.4558660e+01]
True LE: [ 8.7996113e-01  9.5244143e-03 -1.4563325e+01]
Relative Error: [4.787301   4.8712296  4.9031215  4.8589087  4.720201   4.4820914
 4.1530666  3.752156   3.3012018  2.821051   2.339544   1.9268212
 1.7317498  1.8774234  2.2587757  2.668733   2.9785852  3.1537976
 3.2204905  3.2315063  3.2365122  3.2652016  3.3246827  3.4083319
 3.506965   3.6141663  3.725871   3.8390338  3.9507315  4.0614963
 4.1760755  4.301596   4.4433527  4.6014895  4.766886   4.914483
 5.0043726  4.999203   4.8983045  4.744842   4.573216   4.3487473
 4.000647   3.502138   2.8845975  2.2051299  1.585453   1.3360403
 1.6626756  2.2163744  2.7134385  3.0730448  3.3036177  3.4449637
 3.54198    3.6320102  3.7390277  3.8727245  4.0296016  4.198072
 4.3650374  4.519513   4.653647   4.7577696  4.8145566  4.7995844
 4.6909657  4.479383   4.1703525  3.7825472  3.3403914  2.8683972
 2.3948288  1.9819124  1.7648567  1.8681675  2.2024028  2.560252
 2.8087115  2.9181044  2.9249713  2.8898737  2.865808   2.878833
 2.9297261  3.006319   3.0978625  3.199801   3.3101063  3.4253519
 3.5392494  3.6466234  3.748735   3.8532295  3.9692342  4.103437
 4.2551055  4.406712   4.5175204  4.540024   4.458498   4.3165693
 4.165142   3.975505   3.6637378  3.1946104  2.6016123  1.9420832
 1.3379667  1.1459979  1.5561643  2.1327705  2.610564   2.9316006
 3.119531   3.224219   3.2959993  3.3740454  3.4808264  3.6216826
 3.7889473  3.9687529  4.1468945  4.3125725  4.45847    4.577784
 4.6573386  4.6742787  4.604631   4.4332933  4.159653   3.7978568
 3.3728151  2.9131753  2.449662   2.0376515  1.7942405  1.8413596
 2.1138656  2.4113162  2.597809   2.6475291  2.6042814  2.5339167
 2.4892695  2.492425   2.536726   2.6045637  2.6849096  2.776778
 2.88275    3.0011523  3.123501   3.2378733  3.3369567  3.425096
 3.514583   3.617566   3.7434034  3.8879604  4.0172043  4.074421
 4.0232882  3.8970575  3.7654452  3.6170216  3.3554225  2.928784
 2.3685017  1.7337462  1.1369839  0.96850324 1.4316242  2.0184195
 2.4742787  2.7571976  2.903515   2.9730964  3.0221672  3.0910773
 3.198576   3.3444245  3.5158198  3.6976662  3.8773384  4.045179
 4.1948147  4.3216434  4.417022   4.463202   4.436512   4.318136
 4.098891   3.7832923  3.390834   2.9516034  2.5010312  2.0907288
 1.8186159  1.7974507  1.9930114  2.2236292  2.3525567  2.3548093
 2.2763197  2.1828732  2.125718   2.1229055  2.1607645  2.2163174
 2.279099   2.3535876  2.4488823  2.5670526  2.6996832  2.8304834
 2.9414587  3.0255105  3.0929518  3.1617806  3.2488472  3.3673332
 3.5023398  3.5960186  3.588304   3.4857314  3.3700514  3.264615
 3.0682902  2.7018902  2.1871376  1.5845046  0.99099874 0.79646903
 1.2781311  1.8680269  2.3047533  2.552362   2.658591   2.6942914
 2.722635   2.7849033  2.8949547  3.0439007  3.2132094  3.3878222
 3.5579379  3.7178965  3.8629222  3.9885235  4.0892196  4.1537786
 4.1644964  4.1029053  3.95428    3.7098777  3.375811   2.9751527
 2.5451403  2.1382575  1.8376725  1.7427076  1.848555   2.006779
 2.084837   2.054846   1.9580865  1.8540157  1.7900891  1.7825239
 1.8126438  1.8521918  1.8903719  1.9387413  2.0143197  2.124691
 2.2630358  2.4127693  2.5505352  2.6531982  2.7152379  2.755191
 2.797945   2.867624   2.9805896  3.0985508  3.1447449  3.0818121
 2.977882   2.9072537  2.787601   2.5045636  2.0542104  1.4958725
 0.9112729  0.63220924 1.085672   1.67576    2.1012506  2.3204277
 2.3893228  2.3908293  2.3976588  2.4536512  2.5669336  2.7181194
 2.881751   3.042164   3.1947029  3.3385224  3.4717274  3.5895083
 3.6858256  3.753654   3.7835917  3.765146   3.687115   3.5330276
 3.2891695  2.959973   2.572478   2.1774068  1.8525134  1.6863292
 1.6970694  1.7775462  1.8086774  1.759308   1.6608067  1.5580984
 1.4910755  1.4774456  1.4984409  1.5198374  1.5278256  1.5399863
 1.5844938  1.6757542  1.8093398  1.9698294  2.1361313  2.280612
 2.373669   2.409576   2.4170668  2.4283998  2.4801455  2.586413
 2.6790283  2.6773777  2.5923853  2.535389   2.4908235  2.3151824
 1.9587997  1.463795   0.90486145 0.50382346 0.8484763  1.4341182
 1.8631836  2.0674329  2.10478    2.0701733  2.049019   2.0934296
 2.2080562  2.3610413  2.5180686  2.6628134  2.7951045  2.9194145
 3.0372906  3.1449668  3.2336555  3.2947726  3.3228402  3.3169894
 3.2803276  3.2067578  3.0722454  2.8541815  2.5524704  2.1989288
 1.8637435  1.6360465  1.557092   1.5586962  1.5419115  1.4761992
 1.3860033  1.2966976  1.231571   1.2099957  1.2210989  1.2269711
 1.2045193  1.1720915  1.1706128  1.2264832 ]
