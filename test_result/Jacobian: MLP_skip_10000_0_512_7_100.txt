time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.26%, model saved.
Epoch: 0 Train: 8977.28516 Test: 4308.50635
Epoch: 100 Train: 2562.19580 Test: 1173.02441
Epoch: 200 Train: 2603.79883 Test: 1158.65442
Epoch: 300 Train: 1202.67896 Test: 388.02863
Epoch 400: New minimal relative error: 29.43%, model saved.
Epoch: 400 Train: 511.40637 Test: 134.38033
Epoch: 500 Train: 283.75565 Test: 49.94220
Epoch: 600 Train: 339.57684 Test: 139.05899
Epoch: 700 Train: 257.68973 Test: 51.07649
Epoch: 800 Train: 143.84758 Test: 25.55287
Epoch 900: New minimal relative error: 22.93%, model saved.
Epoch: 900 Train: 91.87910 Test: 32.91153
Epoch: 1000 Train: 87.23640 Test: 24.93506
Epoch 1100: New minimal relative error: 14.41%, model saved.
Epoch: 1100 Train: 52.93803 Test: 6.81827
Epoch: 1200 Train: 75.37296 Test: 20.31183
Epoch 1300: New minimal relative error: 4.87%, model saved.
Epoch: 1300 Train: 38.42324 Test: 2.62309
Epoch: 1400 Train: 42.55677 Test: 13.36169
Epoch: 1500 Train: 51.01590 Test: 21.80644
Epoch 1600: New minimal relative error: 3.82%, model saved.
Epoch: 1600 Train: 36.94215 Test: 7.68669
Epoch: 1700 Train: 37.42098 Test: 19.01447
Epoch 1800: New minimal relative error: 2.74%, model saved.
Epoch: 1800 Train: 27.11534 Test: 2.13729
Epoch: 1900 Train: 25.00558 Test: 1.43531
Epoch: 2000 Train: 22.16432 Test: 6.51815
Epoch 2100: New minimal relative error: 1.40%, model saved.
Epoch: 2100 Train: 20.54596 Test: 2.98871
Epoch: 2200 Train: 21.55884 Test: 2.80176
Epoch: 2300 Train: 16.56104 Test: 1.04805
Epoch: 2400 Train: 16.41602 Test: 0.84005
Epoch: 2500 Train: 18.67692 Test: 6.52801
Epoch: 2600 Train: 15.30104 Test: 1.77154
Epoch: 2700 Train: 15.06777 Test: 1.65048
Epoch: 2800 Train: 15.80091 Test: 1.52078
Epoch: 2900 Train: 22.97198 Test: 6.80006
Epoch: 3000 Train: 21.43171 Test: 5.87928
Epoch: 3100 Train: 12.03786 Test: 0.72950
Epoch: 3200 Train: 12.12293 Test: 0.66525
Epoch: 3300 Train: 26.85166 Test: 11.41461
Epoch: 3400 Train: 12.47016 Test: 2.74075
Epoch: 3500 Train: 13.10616 Test: 1.24278
Epoch: 3600 Train: 15.82280 Test: 2.67263
Epoch: 3700 Train: 12.13497 Test: 1.16479
Epoch: 3800 Train: 12.87374 Test: 2.29014
Epoch: 3900 Train: 10.00050 Test: 0.81217
Epoch: 4000 Train: 18.14895 Test: 3.84759
Epoch: 4100 Train: 12.46000 Test: 1.12128
Epoch: 4200 Train: 14.17493 Test: 2.77317
Epoch: 4300 Train: 15.72278 Test: 5.21840
Epoch: 4400 Train: 13.85395 Test: 4.49371
Epoch: 4500 Train: 12.42321 Test: 0.96239
Epoch: 4600 Train: 13.00292 Test: 3.85854
Epoch: 4700 Train: 12.07887 Test: 3.57872
Epoch: 4800 Train: 16.74179 Test: 4.32749
Epoch: 4900 Train: 10.72541 Test: 3.33662
Epoch: 5000 Train: 9.45984 Test: 1.44873
Epoch: 5100 Train: 8.76825 Test: 1.10790
Epoch: 5200 Train: 8.83798 Test: 0.72310
Epoch: 5300 Train: 8.33209 Test: 2.03595
Epoch 5400: New minimal relative error: 1.14%, model saved.
Epoch: 5400 Train: 7.35910 Test: 0.06454
Epoch: 5500 Train: 7.92658 Test: 0.22422
Epoch: 5600 Train: 7.19427 Test: 0.16233
Epoch: 5700 Train: 7.09859 Test: 0.26118
Epoch: 5800 Train: 7.06860 Test: 0.13794
Epoch: 5900 Train: 7.56720 Test: 0.45710
Epoch: 6000 Train: 11.75926 Test: 3.21673
Epoch: 6100 Train: 9.77525 Test: 1.89700
Epoch: 6200 Train: 9.04540 Test: 1.38975
Epoch: 6300 Train: 6.91165 Test: 0.08574
Epoch: 6400 Train: 7.88421 Test: 0.30204
Epoch: 6500 Train: 8.43071 Test: 0.48721
Epoch 6600: New minimal relative error: 1.04%, model saved.
Epoch: 6600 Train: 7.85602 Test: 0.15273
Epoch: 6700 Train: 7.23668 Test: 0.17552
Epoch: 6800 Train: 7.40748 Test: 0.09378
Epoch: 6900 Train: 8.79101 Test: 1.00018
Epoch: 7000 Train: 6.95753 Test: 0.12441
Epoch: 7100 Train: 6.76136 Test: 0.09993
Epoch: 7200 Train: 8.68666 Test: 1.95416
Epoch: 7300 Train: 8.96945 Test: 3.22627
Epoch: 7400 Train: 8.04596 Test: 0.61075
Epoch: 7500 Train: 6.81707 Test: 0.49587
Epoch: 7600 Train: 6.53040 Test: 0.11240
Epoch: 7700 Train: 6.30066 Test: 0.06824
Epoch: 7800 Train: 6.31273 Test: 0.07575
Epoch 7900: New minimal relative error: 0.54%, model saved.
Epoch: 7900 Train: 5.92816 Test: 0.09707
Epoch: 8000 Train: 6.18256 Test: 0.17652
Epoch: 8100 Train: 6.09268 Test: 0.08969
Epoch: 8200 Train: 6.40060 Test: 0.10436
Epoch: 8300 Train: 7.17694 Test: 0.23912
Epoch: 8400 Train: 6.47300 Test: 0.97945
Epoch: 8500 Train: 6.49162 Test: 1.85504
Epoch: 8600 Train: 5.70633 Test: 0.06266
Epoch 8700: New minimal relative error: 0.53%, model saved.
Epoch: 8700 Train: 5.75247 Test: 0.06307
Epoch: 8800 Train: 5.64521 Test: 0.06081
Epoch: 8900 Train: 6.65067 Test: 0.18489
Epoch: 9000 Train: 6.23941 Test: 0.11044
Epoch: 9100 Train: 6.05137 Test: 1.35093
Epoch: 9200 Train: 5.40313 Test: 0.17518
Epoch: 9300 Train: 5.53536 Test: 0.34541
Epoch: 9400 Train: 7.81636 Test: 2.09955
Epoch: 9500 Train: 5.39920 Test: 0.38118
Epoch: 9600 Train: 5.30043 Test: 0.26223
Epoch: 9700 Train: 5.10849 Test: 0.26665
Epoch: 9800 Train: 4.60776 Test: 0.11832
Epoch: 9900 Train: 4.73780 Test: 0.05043
Epoch: 9999 Train: 4.69708 Test: 0.05013
Training Loss: tensor(4.6971)
Test Loss: tensor(0.0501)
Learned LE: [ 8.6604339e-01 -4.6925442e-03 -1.4535674e+01]
True LE: [ 8.6544806e-01  3.0280377e-03 -1.4538586e+01]
Relative Error: [0.39422187 0.55786926 0.81136835 0.65961623 0.43361956 0.2781149
 0.24448423 0.39527845 0.6534205  0.774935   0.8163818  0.54883724
 0.6653444  0.82267153 0.910729   0.69425994 0.57942325 0.44770885
 0.2514485  0.2557579  0.48014554 1.0112139  0.9156042  0.6217993
 0.43983623 0.31598026 0.1642955  0.44671282 0.6712305  0.87218356
 0.92072797 0.9110242  0.58953434 0.49625877 0.54530454 0.6228948
 0.81272554 0.78397936 0.6866473  0.49682957 0.27368602 0.43068215
 0.36284915 0.7129645  0.9881589  0.9833167  1.0319532  0.9565474
 0.73894507 0.42124012 0.32345915 0.42735314 0.5547614  0.6190829
 0.62147284 0.6634253  0.71590316 0.9552447  1.1678723  0.6783885
 0.47456893 0.3042148  0.34934345 0.51300555 0.59801066 0.47555134
 0.33902252 0.21108177 0.2532585  0.3571898  0.5744266  0.68009293
 0.70818675 0.5129594  0.55912906 0.7331265  0.69911486 0.5220606
 0.45809475 0.47186157 0.4137408  0.2733246  0.34071374 0.78576225
 0.9545489  0.62184703 0.36894464 0.32089758 0.20969875 0.4444113
 0.6436684  0.759053   0.8167687  0.6803096  0.44675016 0.3986386
 0.4974084  0.53417736 0.81369245 0.74557424 0.7160237  0.4917597
 0.13805535 0.34198916 0.2922083  0.3894636  0.63680524 0.6972872
 0.8527483  0.88510495 0.6257976  0.3748211  0.30999964 0.4043792
 0.5057004  0.5156661  0.540354   0.5014244  0.52485377 0.76070845
 0.9451035  0.556938   0.391909   0.25415793 0.26555535 0.40426022
 0.5409744  0.4893356  0.32248035 0.21013497 0.23518126 0.3725604
 0.5620432  0.6197761  0.59095246 0.48068482 0.43728068 0.6544107
 0.6121208  0.46527764 0.39203736 0.54973453 0.6078112  0.43489727
 0.31363803 0.5171465  0.96571577 0.68136    0.336582   0.28415653
 0.23784956 0.48299763 0.59005713 0.641751   0.64218336 0.5671213
 0.36810583 0.37381738 0.4922785  0.48121545 0.7684525  0.88174325
 0.793059   0.50717586 0.05391391 0.31291366 0.4856295  0.19673882
 0.21698801 0.37192705 0.62896436 0.65715617 0.4676861  0.31059045
 0.32185203 0.39531836 0.45226714 0.4915197  0.50624084 0.428773
 0.36881402 0.66810036 0.84730554 0.39275587 0.27944002 0.2834929
 0.15677993 0.252522   0.43459785 0.51241994 0.39117014 0.30160096
 0.29512015 0.44798622 0.6082568  0.6490063  0.5993009  0.43345833
 0.3821879  0.51420474 0.5087362  0.511644   0.5072133  0.60791916
 0.6632645  0.635102   0.43107858 0.33319157 0.7423241  0.6872436
 0.32181102 0.25003153 0.27632955 0.48465466 0.58476734 0.521075
 0.4886603  0.4571639  0.40689194 0.39313617 0.46587577 0.53338015
 0.7530481  0.93552506 0.7868028  0.5308933  0.06865483 0.23380832
 0.5275775  0.39220798 0.3276627  0.15069701 0.30136293 0.44113094
 0.42118523 0.33209312 0.35284603 0.3991801  0.4776992  0.52253145
 0.44285142 0.33748066 0.31614095 0.712864   0.8859992  0.41262344
 0.20621511 0.18860807 0.24710454 0.17429058 0.32370085 0.4588533
 0.43404624 0.2837096  0.33050206 0.60583377 0.7496047  0.8304662
 0.7375299  0.49391448 0.47047716 0.36765128 0.40567863 0.44522896
 0.52028227 0.6921244  0.7512488  0.7333927  0.5880507  0.3232225
 0.4575515  0.73460895 0.29876733 0.18802053 0.2837643  0.4243103
 0.54021615 0.47539654 0.37108806 0.34134907 0.49432474 0.46258807
 0.47101873 0.7360448  0.74886745 1.0030113  0.9170693  0.6786606
 0.18111676 0.22366664 0.39233103 0.49868456 0.47605872 0.39494863
 0.16966741 0.27665988 0.33167383 0.31274354 0.37965256 0.40768436
 0.46950883 0.5400679  0.41728014 0.28233674 0.27060324 0.7010587
 1.0175085  0.5518363  0.22664116 0.16478379 0.18120444 0.25465888
 0.21611084 0.3618337  0.4193863  0.3772025  0.3290907  0.3975758
 0.64412814 0.8897018  0.9046119  0.7389402  0.56814414 0.36182547
 0.408218   0.36589786 0.47596496 0.6491191  0.8278842  0.73975253
 0.6106267  0.39728895 0.24283808 0.5839133  0.400828   0.1379215
 0.26779595 0.29182607 0.416008   0.40981722 0.2600963  0.2944968
 0.4392446  0.5128788  0.63054574 0.6649399  0.7509367  0.68025607
 0.6830878  0.4650944  0.24548802 0.23880878 0.3406587  0.48427647
 0.5644355  0.5433751  0.41411525 0.1885853  0.21156804 0.20644537
 0.33409044 0.38158888 0.40864623 0.4752175  0.4023955  0.23019409
 0.21179079 0.6320586  1.0496663  0.7391056  0.25942793 0.20309947
 0.2356726  0.22233182 0.15947275 0.28263393 0.40916434 0.42874366
 0.39950767 0.36207208 0.40417564 0.6449913  0.79604524 0.820805
 0.5134605  0.41862205 0.43462646 0.4113742  0.3384567  0.4661357
 0.67476386 0.68530786 0.58511657 0.34000432 0.15860088 0.39049062
 0.63665426 0.24466911 0.21968298 0.26853856]
