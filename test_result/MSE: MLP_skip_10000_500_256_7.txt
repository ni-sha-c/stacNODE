time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 105.61%, model saved.
Epoch: 0 Train: 3710.89722 Test: 4089.77856
Epoch: 100 Train: 211.69453 Test: 215.45213
Epoch 200: New minimal relative error: 71.04%, model saved.
Epoch: 200 Train: 50.56887 Test: 46.62999
Epoch 300: New minimal relative error: 40.52%, model saved.
Epoch: 300 Train: 12.46982 Test: 13.68940
Epoch 400: New minimal relative error: 34.48%, model saved.
Epoch: 400 Train: 10.08224 Test: 12.48721
Epoch 500: New minimal relative error: 19.95%, model saved.
Epoch: 500 Train: 6.39676 Test: 8.23045
Epoch: 600 Train: 9.80034 Test: 7.62905
Epoch: 700 Train: 21.67603 Test: 10.87436
Epoch: 800 Train: 10.70303 Test: 13.69540
Epoch: 900 Train: 19.90467 Test: 19.55745
Epoch: 1000 Train: 3.13457 Test: 3.42051
Epoch 1100: New minimal relative error: 15.94%, model saved.
Epoch: 1100 Train: 2.72024 Test: 3.90617
Epoch: 1200 Train: 3.93379 Test: 4.93643
Epoch: 1300 Train: 9.79792 Test: 10.73870
Epoch: 1400 Train: 1.81680 Test: 1.91730
Epoch: 1500 Train: 1.24654 Test: 1.31891
Epoch 1600: New minimal relative error: 12.05%, model saved.
Epoch: 1600 Train: 1.98273 Test: 2.58279
Epoch: 1700 Train: 2.85193 Test: 1.44348
Epoch: 1800 Train: 2.23685 Test: 1.66246
Epoch: 1900 Train: 1.70459 Test: 1.73371
Epoch: 2000 Train: 0.81505 Test: 0.92462
Epoch: 2100 Train: 0.85466 Test: 0.97663
Epoch: 2200 Train: 1.16633 Test: 3.13339
Epoch: 2300 Train: 1.08555 Test: 2.06259
Epoch: 2400 Train: 2.43543 Test: 2.32426
Epoch: 2500 Train: 0.99423 Test: 0.87085
Epoch: 2600 Train: 0.55116 Test: 0.58128
Epoch 2700: New minimal relative error: 7.50%, model saved.
Epoch: 2700 Train: 0.49574 Test: 0.55299
Epoch 2800: New minimal relative error: 5.91%, model saved.
Epoch: 2800 Train: 0.61947 Test: 0.74199
Epoch: 2900 Train: 3.44381 Test: 3.59969
Epoch: 3000 Train: 0.88892 Test: 0.90965
Epoch: 3100 Train: 2.11614 Test: 2.40117
Epoch: 3200 Train: 4.97485 Test: 4.85182
Epoch: 3300 Train: 0.65904 Test: 0.74525
Epoch: 3400 Train: 0.44184 Test: 0.47234
Epoch: 3500 Train: 0.36100 Test: 0.41767
Epoch: 3600 Train: 0.31891 Test: 0.36893
Epoch: 3700 Train: 0.84965 Test: 1.04193
Epoch: 3800 Train: 0.37180 Test: 0.41202
Epoch: 3900 Train: 0.30075 Test: 0.35162
Epoch: 4000 Train: 2.24029 Test: 2.15323
Epoch: 4100 Train: 0.45224 Test: 0.53629
Epoch: 4200 Train: 0.42775 Test: 0.49781
Epoch: 4300 Train: 4.74154 Test: 5.13309
Epoch: 4400 Train: 2.40799 Test: 2.60683
Epoch: 4500 Train: 2.11496 Test: 2.99273
Epoch: 4600 Train: 1.70820 Test: 1.94868
Epoch: 4700 Train: 0.37412 Test: 0.47368
Epoch: 4800 Train: 3.56463 Test: 3.94470
Epoch: 4900 Train: 0.33895 Test: 0.47371
Epoch: 5000 Train: 0.39906 Test: 0.45937
Epoch: 5100 Train: 0.27730 Test: 0.33898
Epoch: 5200 Train: 0.22420 Test: 0.27881
Epoch: 5300 Train: 0.22360 Test: 0.26663
Epoch: 5400 Train: 0.22034 Test: 0.27134
Epoch: 5500 Train: 0.20969 Test: 0.26182
Epoch: 5600 Train: 0.23012 Test: 0.29201
Epoch: 5700 Train: 0.18288 Test: 0.22879
Epoch: 5800 Train: 0.18413 Test: 0.23002
Epoch: 5900 Train: 0.18118 Test: 0.22752
Epoch: 6000 Train: 0.18262 Test: 0.22536
Epoch: 6100 Train: 1.48572 Test: 1.84857
Epoch: 6200 Train: 2.59353 Test: 3.59296
Epoch: 6300 Train: 0.16210 Test: 0.20678
Epoch: 6400 Train: 1.79131 Test: 2.19340
Epoch: 6500 Train: 0.67495 Test: 0.52016
Epoch: 6600 Train: 0.54696 Test: 0.42733
Epoch: 6700 Train: 0.22903 Test: 0.25578
Epoch: 6800 Train: 0.15762 Test: 0.20113
Epoch: 6900 Train: 0.15206 Test: 0.19761
Epoch: 7000 Train: 0.14311 Test: 0.18411
Epoch 7100: New minimal relative error: 5.87%, model saved.
Epoch: 7100 Train: 0.16846 Test: 0.21375
Epoch: 7200 Train: 0.17809 Test: 0.20691
Epoch: 7300 Train: 0.15797 Test: 0.21460
Epoch: 7400 Train: 0.67018 Test: 0.85677
Epoch: 7500 Train: 0.13779 Test: 0.21597
Epoch: 7600 Train: 0.24862 Test: 0.29762
Epoch: 7700 Train: 0.14038 Test: 0.18847
Epoch: 7800 Train: 0.12571 Test: 0.16464
Epoch 7900: New minimal relative error: 4.75%, model saved.
Epoch: 7900 Train: 0.12727 Test: 0.16601
Epoch: 8000 Train: 0.20537 Test: 0.17993
Epoch: 8100 Train: 0.14710 Test: 0.19956
Epoch: 8200 Train: 0.12762 Test: 0.16533
Epoch: 8300 Train: 0.13059 Test: 0.17118
Epoch: 8400 Train: 0.12161 Test: 0.16317
Epoch: 8500 Train: 0.11526 Test: 0.15291
Epoch: 8600 Train: 0.97123 Test: 1.13822
Epoch: 8700 Train: 0.20359 Test: 0.27989
Epoch: 8800 Train: 0.21868 Test: 0.30121
Epoch: 8900 Train: 0.16897 Test: 0.22843
Epoch: 9000 Train: 0.16805 Test: 0.16675
Epoch: 9100 Train: 1.21001 Test: 1.13715
Epoch: 9200 Train: 0.10545 Test: 0.14226
Epoch: 9300 Train: 0.10607 Test: 0.14252
Epoch: 9400 Train: 0.10338 Test: 0.14008
Epoch: 9500 Train: 0.10294 Test: 0.14149
Epoch: 9600 Train: 0.13254 Test: 0.18196
Epoch: 9700 Train: 0.10000 Test: 0.13609
Epoch: 9800 Train: 0.10393 Test: 0.14107
Epoch: 9900 Train: 0.10888 Test: 0.14904
Epoch: 9999 Train: 0.21267 Test: 0.18346
Training Loss: tensor(0.2127)
Test Loss: tensor(0.1835)
Learned LE: [ 0.7014636   0.06744778 -2.3520212 ]
True LE: [ 8.74833286e-01  5.53938095e-03 -1.45564165e+01]
Relative Error: [ 3.6360092   3.293999    3.2181005   3.2737129   3.5571184   4.0138507
  3.990425    3.5721145   3.3382897   3.1571758   3.2434962   3.6403193
  4.797364    6.1300054   7.236861    8.058461    8.630785    9.427935
 10.334235   11.127259   12.163218   12.941252   13.647029   14.632098
 15.713601   16.71678    17.60583    17.460186   16.873533   16.432022
 15.359308   14.1374855  13.146532   12.393082   11.802416   11.41105
 11.162032   10.647679   10.157011    9.751708    9.088072    8.432053
  7.9052753   7.415088    6.8544      6.0023685   5.1041903   4.0275073
  3.1750765   3.066274    3.0952249   3.242896    3.4162328   3.971131
  4.7159586   5.6078415   6.52731     5.864685    5.1470447   4.550446
  3.8738732   3.654144    3.1662662   2.7541242   2.5486524   2.6234767
  2.908369    3.4166493   3.386418    2.825104    2.5823805   2.4084833
  2.3047647   2.3896356   3.1014779   4.5472407   5.787831    6.759122
  7.3916407   8.189901    9.207928   10.028698   11.145092   11.88733
 12.634354   13.549249   14.526379   15.380798   16.17513    16.293896
 15.554324   14.983797   14.386526   13.102598   12.014546   11.210814
 10.688278   10.283875   10.08192     9.587236    8.82163     8.476866
  7.8933983   7.0795827   6.722298    6.4517736   6.1341896   5.462562
  4.618097    3.4899309   2.5456655   2.3984098   2.3270977   2.498208
  2.7954485   3.3318295   3.9493914   4.6776524   5.477263    5.2419734
  4.363588    3.7185795   3.1518266   2.9162974   2.8740602   2.4604597
  2.1610153   2.1071877   2.3235133   2.791604    3.1043923   2.364194
  2.2394269   2.0231857   1.818238    1.6089522   1.7826269   2.8017566
  4.221987    5.4468646   6.393543    7.111597    8.317739    9.325281
 10.254575   10.941876   11.661661   12.452159   13.353282   14.079626
 14.740387   15.381681   14.371029   13.594991   13.171901   12.318581
 11.217933   10.304811    9.69412     9.300941    9.157031    9.065105
  8.0351715   7.2917895   6.751945    5.8170223   5.5235767   5.4587727
  5.3565474   4.8500667   4.144799    3.1414912   2.28346     1.893429
  1.6793559   1.9424067   2.255089    2.7048001   3.1963003   3.8027332
  4.4529743   4.885203    3.8712573   3.0635202   2.6198227   2.2913032
  2.3369744   2.4888039   2.145532    1.9420193   1.98333     2.3016171
  2.898684    2.257689    2.1049674   2.1516557   1.6804245   1.2045268
  0.9235811   1.504871    2.7205138   4.0820622   5.2975917   6.17527
  7.3483124   8.352986    8.865765    9.375359   10.01612    10.770906
 11.879514   12.832552   13.31413    13.751263   13.600313   12.489961
 11.805964   11.756369   10.586225    9.517481    8.716238    8.411221
  8.239857    8.296288    7.447354    6.631782    5.948121    5.0613785
  4.27352     4.3796363   4.4901404   4.2698      3.6476498   2.9290373
  2.1437786   1.6638488   1.3057685   1.4530625   1.695331    2.0467267
  2.4140298   2.9055216   3.4880395   4.160097    3.5988514   2.5708902
  2.0276701   1.7773734   1.8000735   2.0348752   2.265807    1.9956365
  1.8784285   2.0411294   2.473456    2.4958897   1.8641922   1.8769575
  2.0328498   1.5111725   1.177126    0.3418834   1.6646055   2.7172074
  4.0948243   5.142944    5.8151283   6.543055    7.196633    7.5756865
  8.06704     8.798309    9.514878   10.560831   11.566686   12.367319
 12.575138   11.792037   10.746848   10.400047   10.160887    8.925702
  7.8992977   7.4084206   7.2436585   7.1931934   6.913303    6.040746
  5.384442    4.612005    3.674247    3.23937     3.4869275   3.6029413
  3.2883759   2.7032735   1.9865507   1.5304343   1.2202231   0.9715627
  1.0719695   1.3341938   1.6343666   2.0131357   2.5431986   3.143968
  3.6347423   2.4604554   1.6648849   1.305127    1.2920288   1.5437813
  1.8505726   2.1419463   2.007796    2.1112468   2.335702    2.77371
  2.1784198   1.4075378   1.7032653   1.9169089   1.8326783   1.2484732
  0.3627463   1.8260725   2.6433096   4.1279483   4.3545623   4.919506
  5.5325537   6.01804     6.291799    6.8554497   7.4835353   8.170982
  8.982336    9.837196   10.597084   11.324091   10.194314    9.251473
  8.929189    8.561833    7.3636813   6.5264845   6.125679    6.036284
  6.0203543   5.559393    4.8786826   4.317593    3.390097    2.704229
  2.4987938   2.697087    2.854257    2.5099628   1.9733738   1.3380781
  1.2322913   0.88789535  0.5810069   0.73294747  0.9909015   1.2568498
  1.7277422   2.2312193   2.8026402   2.911126    1.9858142   1.2892345
  0.9588121   1.2381161   1.5011808   1.681358    2.1097498   2.1327124
  2.1748648   2.532826    3.0493116   2.0164392   1.236752    1.2707593
  1.8151969   2.0175707   1.2809458   0.64068085  1.9025558   2.786789
  2.9231648   3.4621649   4.177606    4.790796    5.2705674   5.612592
  6.2212343   6.5554724   6.8485923   7.187563  ]
