time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.74%, model saved.
Epoch: 0 Train: 9786.87695 Test: 4110.49805
Epoch: 100 Train: 2769.38574 Test: 1129.26880
Epoch: 200 Train: 2734.78955 Test: 1041.35229
Epoch: 300 Train: 1255.95142 Test: 335.60291
Epoch 400: New minimal relative error: 61.11%, model saved.
Epoch: 400 Train: 487.07013 Test: 102.40112
Epoch: 500 Train: 292.84280 Test: 54.91822
Epoch 600: New minimal relative error: 35.84%, model saved.
Epoch: 600 Train: 164.64076 Test: 29.87900
Epoch 700: New minimal relative error: 29.73%, model saved.
Epoch: 700 Train: 103.14763 Test: 10.10045
Epoch: 800 Train: 88.40818 Test: 17.28280
Epoch: 900 Train: 147.76379 Test: 59.72341
Epoch 1000: New minimal relative error: 17.97%, model saved.
Epoch: 1000 Train: 62.01038 Test: 10.83815
Epoch 1100: New minimal relative error: 12.45%, model saved.
Epoch: 1100 Train: 50.72649 Test: 2.73411
Epoch: 1200 Train: 45.43932 Test: 3.04320
Epoch 1300: New minimal relative error: 6.87%, model saved.
Epoch: 1300 Train: 43.26494 Test: 2.92909
Epoch: 1400 Train: 60.83900 Test: 36.58074
Epoch: 1500 Train: 68.55294 Test: 6.32599
Epoch: 1600 Train: 32.06994 Test: 1.17847
Epoch 1700: New minimal relative error: 3.48%, model saved.
Epoch: 1700 Train: 30.53749 Test: 1.13206
Epoch: 1800 Train: 52.36428 Test: 20.69505
Epoch: 1900 Train: 27.36497 Test: 1.62904
Epoch: 2000 Train: 26.38643 Test: 1.52458
Epoch: 2100 Train: 25.71076 Test: 1.83628
Epoch 2200: New minimal relative error: 2.23%, model saved.
Epoch: 2200 Train: 23.48704 Test: 0.75966
Epoch: 2300 Train: 24.63407 Test: 3.47689
Epoch: 2400 Train: 31.67557 Test: 2.83812
Epoch: 2500 Train: 21.77434 Test: 2.24848
Epoch: 2600 Train: 20.57389 Test: 0.96603
Epoch: 2700 Train: 22.75812 Test: 5.18617
Epoch: 2800 Train: 19.30355 Test: 0.66060
Epoch: 2900 Train: 18.43013 Test: 0.86009
Epoch: 3000 Train: 17.27164 Test: 0.52762
Epoch: 3100 Train: 17.56824 Test: 2.00560
Epoch: 3200 Train: 17.47753 Test: 1.07598
Epoch: 3300 Train: 19.72946 Test: 3.47952
Epoch: 3400 Train: 14.99147 Test: 0.45234
Epoch: 3500 Train: 15.75899 Test: 0.88838
Epoch: 3600 Train: 14.59597 Test: 0.41417
Epoch: 3700 Train: 20.32137 Test: 4.20006
Epoch: 3800 Train: 15.39444 Test: 0.85185
Epoch: 3900 Train: 13.60148 Test: 0.34439
Epoch: 4000 Train: 13.65601 Test: 0.45334
Epoch: 4100 Train: 13.73315 Test: 1.06043
Epoch: 4200 Train: 13.74247 Test: 0.97295
Epoch: 4300 Train: 12.50314 Test: 0.46404
Epoch: 4400 Train: 14.66724 Test: 2.64349
Epoch: 4500 Train: 14.81219 Test: 3.11550
Epoch: 4600 Train: 13.09430 Test: 0.58462
Epoch: 4700 Train: 11.75543 Test: 0.30256
Epoch: 4800 Train: 11.61929 Test: 0.38484
Epoch: 4900 Train: 12.87526 Test: 1.24961
Epoch: 5000 Train: 11.38218 Test: 0.31277
Epoch: 5100 Train: 12.87122 Test: 1.03292
Epoch: 5200 Train: 11.06026 Test: 0.51029
Epoch 5300: New minimal relative error: 1.65%, model saved.
Epoch: 5300 Train: 10.67685 Test: 0.23822
Epoch: 5400 Train: 10.63959 Test: 0.33395
Epoch: 5500 Train: 12.09733 Test: 1.16374
Epoch: 5600 Train: 10.44920 Test: 0.25827
Epoch: 5700 Train: 10.65761 Test: 0.47042
Epoch 5800: New minimal relative error: 1.07%, model saved.
Epoch: 5800 Train: 10.12906 Test: 0.44054
Epoch: 5900 Train: 10.11009 Test: 0.21980
Epoch 6000: New minimal relative error: 1.05%, model saved.
Epoch: 6000 Train: 9.99064 Test: 0.29592
Epoch: 6100 Train: 10.04965 Test: 0.23975
Epoch: 6200 Train: 10.18146 Test: 0.25265
Epoch: 6300 Train: 10.04224 Test: 0.39890
Epoch: 6400 Train: 10.28122 Test: 0.50052
Epoch: 6500 Train: 9.78187 Test: 0.20859
Epoch: 6600 Train: 9.84908 Test: 0.28077
Epoch: 6700 Train: 11.45266 Test: 2.24144
Epoch 6800: New minimal relative error: 0.81%, model saved.
Epoch: 6800 Train: 9.41372 Test: 0.20107
Epoch: 6900 Train: 9.51624 Test: 0.30070
Epoch: 7000 Train: 10.35295 Test: 0.65404
Epoch: 7100 Train: 9.88274 Test: 0.72967
Epoch: 7200 Train: 9.51413 Test: 0.41734
Epoch: 7300 Train: 9.02749 Test: 0.19939
Epoch: 7400 Train: 9.12442 Test: 0.22495
Epoch: 7500 Train: 9.22836 Test: 0.38056
Epoch: 7600 Train: 9.15397 Test: 0.18924
Epoch: 7700 Train: 8.82982 Test: 0.17860
Epoch: 7800 Train: 8.89881 Test: 0.20239
Epoch: 7900 Train: 9.09585 Test: 0.33903
Epoch: 8000 Train: 9.53178 Test: 0.61952
Epoch: 8100 Train: 8.96141 Test: 0.17816
Epoch: 8200 Train: 8.81409 Test: 0.17733
Epoch: 8300 Train: 8.70062 Test: 0.20852
Epoch: 8400 Train: 8.82167 Test: 0.18527
Epoch: 8500 Train: 8.98409 Test: 0.30694
Epoch: 8600 Train: 8.73502 Test: 0.30623
Epoch: 8700 Train: 8.87264 Test: 0.48365
Epoch: 8800 Train: 9.01194 Test: 0.41533
Epoch: 8900 Train: 8.62646 Test: 0.17349
Epoch: 9000 Train: 8.45385 Test: 0.25194
Epoch: 9100 Train: 8.49163 Test: 0.16752
Epoch: 9200 Train: 8.85699 Test: 0.19703
Epoch: 9300 Train: 8.35335 Test: 0.17191
Epoch: 9400 Train: 9.29197 Test: 0.41699
Epoch: 9500 Train: 8.34433 Test: 0.16987
Epoch: 9600 Train: 8.22841 Test: 0.17411
Epoch: 9700 Train: 8.26708 Test: 0.19078
Epoch: 9800 Train: 8.26010 Test: 0.37806
Epoch: 9900 Train: 8.23879 Test: 0.19953
Epoch: 9999 Train: 8.15667 Test: 0.18203
Training Loss: tensor(8.1567)
Test Loss: tensor(0.1820)
Learned LE: [ 8.4685844e-01 -4.4738943e-05 -1.4489299e+01]
True LE: [ 8.4429783e-01  3.6374833e-03 -1.4521419e+01]
Relative Error: [0.97120535 0.65190935 0.54776293 0.7198688  0.96521574 0.8980771
 0.7960574  0.3490355  0.19873889 0.2679175  0.52920353 0.85549223
 0.8235458  0.5497595  0.0852564  0.3376662  0.4623947  0.5508011
 0.6430006  0.9045693  1.0778229  0.92379004 0.81902707 0.70087814
 0.6078147  0.54397076 0.49937078 0.44760206 0.50128764 0.5429393
 0.4117845  0.64045995 1.0990311  1.4354594  1.7965604  1.9334714
 2.1664147  2.3310103  1.7066319  1.3888599  1.3080343  1.0104797
 0.9753537  0.66186404 0.49025473 0.48372984 0.43252772 0.14133033
 0.2390137  0.2472851  0.37301326 0.48464307 0.51177454 0.38995895
 0.43595442 0.49365443 0.63055533 0.8284604  1.114507   1.437174
 1.3831716  1.0367719  0.9388791  0.72951496 0.58262575 0.6409595
 0.6693476  0.7411298  0.6026144  0.21999688 0.20420486 0.26733625
 0.43723813 0.99126387 1.0296055  0.4675468  0.18237351 0.51135814
 0.68515635 0.7469687  0.7398054  0.91502464 0.9010704  0.8360308
 0.77716565 0.604604   0.5032102  0.50558627 0.529697   0.49414593
 0.4157972  0.46842957 0.39313906 0.46755135 0.83613527 1.0763757
 1.4584588  1.4884257  1.6138     1.9720275  1.7490432  1.3353987
 1.1773051  1.3279908  1.1506951  0.80802774 0.5638259  0.5056939
 0.46435836 0.17345774 0.15660542 0.1534081  0.27948707 0.46002594
 0.43854192 0.32580286 0.3655366  0.5202961  0.6344684  0.7483398
 0.92039335 1.127569   1.2450331  0.841781   0.84151345 0.77194643
 0.62102085 0.63354653 0.49199933 0.4636157  0.44880965 0.17240842
 0.2995323  0.3605659  0.47307402 0.96666014 1.160232   0.4451268
 0.33681527 0.63847804 0.7806907  0.7866909  0.8004127  0.8163411
 0.7998324  0.7817873  0.7303705  0.622678   0.5146538  0.4379555
 0.4629966  0.4851178  0.32322297 0.24336381 0.30630812 0.36712646
 0.6980397  0.8613104  1.1392373  1.1794472  1.179018   1.5114794
 1.8030616  1.3392588  1.1428496  1.2908874  1.3834951  0.99842745
 0.66983926 0.53787696 0.46247515 0.27179083 0.22511739 0.22417124
 0.22249722 0.41357195 0.3822612  0.25170907 0.38946414 0.62385446
 0.77490693 0.8190185  0.8660442  0.8889346  1.1147923  0.59428227
 0.6364273  0.76312596 0.665703   0.6599797  0.5335757  0.34439233
 0.39262372 0.17750144 0.36512342 0.4791894  0.50026876 0.8839457
 1.231555   0.46183285 0.44411543 0.6990786  0.8181746  0.787624
 0.7735863  0.7934746  0.7312795  0.7064583  0.6457082  0.585078
 0.51885873 0.44170207 0.43275505 0.46423542 0.27084082 0.15012422
 0.29409045 0.53510803 0.6460026  0.78191495 0.93915063 1.0161546
 0.9113522  1.1296091  1.2868584  1.4679575  1.0918809  1.1329119
 1.462637   1.2862917  0.82805103 0.5857232  0.51090276 0.3492976
 0.36540592 0.3441423  0.2588463  0.29116237 0.28241235 0.14874028
 0.39859983 0.81585264 0.8706228  0.9908977  0.9890997  0.89924407
 0.8697897  0.5011523  0.34908408 0.67642426 0.7263043  0.7808238
 0.7048385  0.5112603  0.25467718 0.20462969 0.4288175  0.5499109
 0.5462629  0.73744965 1.2988329  0.523544   0.4953661  0.7224143
 0.7842547  0.73599476 0.67821205 0.6755844  0.6316516  0.5767468
 0.5406337  0.4597458  0.45145884 0.4817065  0.5233838  0.56456906
 0.35132474 0.05322585 0.26776922 0.5544425  0.7023009  0.8578883
 0.9202692  0.7959801  0.7716137  0.80077934 0.9127097  1.1760453
 1.1711413  0.90792763 1.0994365  1.5161959  1.0613703  0.7171584
 0.649731   0.39652932 0.3439287  0.3466323  0.34623048 0.2698667
 0.21975802 0.08422144 0.28044385 0.39810273 0.5685277  0.4700958
 0.42706648 0.43401065 0.64659    0.9016896  0.21999243 0.4539075
 0.6776503  0.843509   0.97918594 0.77614117 0.34117758 0.20137739
 0.44259414 0.6018999  0.5717878  0.60512334 0.98235804 0.61991256
 0.4990264  0.727992   0.7575264  0.60991955 0.50662583 0.47525555
 0.45130932 0.4365744  0.36075482 0.3131318  0.36853164 0.55397564
 0.71512055 0.71159947 0.68226546 0.4527682  0.36391377 0.6324641
 0.79366314 0.87885034 0.98503727 0.7629721  0.71068555 0.6026037
 0.7438535  0.7365251  1.0022153  0.8451197  0.80517215 1.0567384
 1.3844684  0.89122504 0.65222245 0.36300462 0.3071497  0.35364097
 0.49459794 0.34417847 0.3143738  0.23282138 0.14111678 0.13252829
 0.22765401 0.4382416  0.22053155 0.05454098 0.06813014 0.35944766
 0.42204592 0.10108837 0.41721746 0.73667717 1.0674696  1.0023203
 0.79539967 0.24242803 0.48663867 0.6187364  0.5950618  0.5281827
 0.6777531  0.7581727  0.46712697 0.6968867  0.7810832  0.59724426
 0.26538292 0.29362494 0.29195285 0.3856069  0.48106396 0.47278148
 0.50823206 0.7732436  0.9488267  1.0521188 ]
