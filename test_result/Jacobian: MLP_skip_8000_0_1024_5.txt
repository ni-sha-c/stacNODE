time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.67%, model saved.
Epoch: 0 Train: 168952.18750 Test: 4211.62744
Epoch: 80 Train: 31090.48828 Test: 1407.63635
Epoch: 160 Train: 30033.67969 Test: 1069.79211
Epoch: 240 Train: 29102.33203 Test: 1249.21460
Epoch 320: New minimal relative error: 97.84%, model saved.
Epoch: 320 Train: 25480.87305 Test: 978.06360
Epoch: 400 Train: 23270.38086 Test: 3148.82104
Epoch: 480 Train: 30951.16406 Test: 1500.69739
Epoch: 560 Train: 27405.93555 Test: 1135.57544
Epoch: 640 Train: 31300.23047 Test: 1415.08105
Epoch: 720 Train: 31236.59766 Test: 2045.07214
Epoch 800: New minimal relative error: 64.08%, model saved.
Epoch: 800 Train: 28497.11328 Test: 1115.84497
Epoch: 880 Train: 31891.02344 Test: 1506.10840
Epoch 960: New minimal relative error: 62.20%, model saved.
Epoch: 960 Train: 29912.62891 Test: 1126.67163
Epoch: 1040 Train: 31532.88281 Test: 1282.58374
Epoch: 1120 Train: 31539.38672 Test: 2151.33032
Epoch: 1200 Train: 30741.75781 Test: 1229.47241
Epoch: 1280 Train: 29379.07617 Test: 1271.46448
Epoch: 1360 Train: 32591.86328 Test: 1453.14929
Epoch: 1440 Train: 33561.25000 Test: 1537.41052
Epoch: 1520 Train: 27836.66992 Test: 1268.81128
Epoch: 1600 Train: 35933.11719 Test: 1311.27771
Epoch: 1680 Train: 31715.37891 Test: 1349.50635
Epoch: 1760 Train: 33130.19531 Test: 1269.05273
Epoch: 1840 Train: 32404.45703 Test: 1340.80750
Epoch: 1920 Train: 31861.74414 Test: 1412.11926
Epoch: 2000 Train: 32629.98438 Test: 1257.43896
Epoch: 2080 Train: 30112.53516 Test: 1273.65942
Epoch 2160: New minimal relative error: 61.96%, model saved.
Epoch: 2160 Train: 34567.09766 Test: 1345.97437
Epoch: 2240 Train: 28741.37109 Test: 1095.22314
Epoch: 2320 Train: 34017.85547 Test: 1376.74512
Epoch: 2400 Train: 29297.54492 Test: 1206.30200
Epoch: 2480 Train: 33437.23047 Test: 1304.37061
Epoch 2560: New minimal relative error: 59.51%, model saved.
Epoch: 2560 Train: 30665.17578 Test: 1272.42834
Epoch: 2640 Train: 31319.31055 Test: 1518.82190
Epoch: 2720 Train: 31809.92188 Test: 1278.45947
Epoch: 2800 Train: 30857.44727 Test: 1293.41333
Epoch 2880: New minimal relative error: 46.27%, model saved.
Epoch: 2880 Train: 30484.78711 Test: 1235.13916
Epoch: 2960 Train: 30998.02539 Test: 1226.19165
Epoch: 3040 Train: 28497.25000 Test: 1113.77124
Epoch: 3120 Train: 30369.31055 Test: 1076.07227
Epoch: 3200 Train: 28045.37305 Test: 1084.03088
Epoch: 3280 Train: 25865.68164 Test: 1151.39099
Epoch: 3360 Train: 25657.18750 Test: 906.69843
Epoch: 3440 Train: 25591.76172 Test: 882.50763
Epoch: 3520 Train: 25017.28711 Test: 833.58191
Epoch: 3600 Train: 23324.52539 Test: 832.53741
Epoch: 3680 Train: 22469.33203 Test: 674.16498
Epoch: 3760 Train: 20769.04883 Test: 468.25943
Epoch: 3840 Train: 15837.68262 Test: 365.84583
Epoch: 3920 Train: 12717.78223 Test: 258.81802
Epoch 4000: New minimal relative error: 33.76%, model saved.
Epoch: 4000 Train: 7228.62842 Test: 100.98170
Epoch: 4080 Train: 4523.61572 Test: 53.28582
Epoch: 4160 Train: 3340.69604 Test: 25.12980
Epoch 4240: New minimal relative error: 17.53%, model saved.
Epoch: 4240 Train: 2809.69434 Test: 19.61375
Epoch 4320: New minimal relative error: 9.59%, model saved.
Epoch: 4320 Train: 2359.86841 Test: 15.84803
Epoch: 4400 Train: 2128.31055 Test: 14.87950
Epoch: 4480 Train: 1961.19812 Test: 18.75128
Epoch: 4560 Train: 1819.32141 Test: 15.61320
Epoch 4640: New minimal relative error: 6.76%, model saved.
Epoch: 4640 Train: 1668.38171 Test: 10.61675
Epoch 4720: New minimal relative error: 3.46%, model saved.
Epoch: 4720 Train: 1560.91895 Test: 7.40340
Epoch: 4800 Train: 1495.96765 Test: 6.79915
Epoch: 4880 Train: 1729.29297 Test: 22.85761
Epoch: 4960 Train: 1472.84839 Test: 9.39307
Epoch: 5040 Train: 1293.88147 Test: 6.98036
Epoch: 5120 Train: 1211.50684 Test: 6.45494
Epoch: 5200 Train: 1116.46448 Test: 4.56472
Epoch: 5280 Train: 1029.06909 Test: 12.00740
Epoch: 5360 Train: 1027.08008 Test: 5.75073
Epoch: 5440 Train: 1054.44031 Test: 5.69990
Epoch: 5520 Train: 1014.55829 Test: 10.02776
Epoch: 5600 Train: 954.59375 Test: 8.94497
Epoch: 5680 Train: 937.77509 Test: 5.43660
Epoch: 5760 Train: 892.20416 Test: 4.14909
Epoch: 5840 Train: 875.44440 Test: 3.38842
Epoch: 5920 Train: 888.70795 Test: 4.94373
Epoch: 6000 Train: 926.95630 Test: 6.66079
Epoch: 6080 Train: 867.48126 Test: 3.66306
Epoch: 6160 Train: 873.53870 Test: 5.40289
Epoch: 6240 Train: 860.52637 Test: 3.83873
Epoch: 6320 Train: 812.29852 Test: 3.46333
Epoch: 6400 Train: 828.57507 Test: 9.44510
Epoch 6480: New minimal relative error: 2.42%, model saved.
Epoch: 6480 Train: 810.21301 Test: 3.33507
Epoch: 6560 Train: 759.66559 Test: 3.49684
Epoch: 6640 Train: 790.08301 Test: 8.12322
Epoch: 6720 Train: 803.12091 Test: 4.69103
Epoch: 6800 Train: 762.68933 Test: 2.42117
Epoch: 6880 Train: 712.55200 Test: 2.75164
Epoch: 6960 Train: 764.10303 Test: 3.29712
Epoch: 7040 Train: 771.49091 Test: 4.90669
Epoch: 7120 Train: 862.64716 Test: 3.92267
Epoch: 7200 Train: 755.44305 Test: 5.71136
Epoch: 7280 Train: 768.85187 Test: 3.32614
Epoch: 7360 Train: 826.32397 Test: 6.10054
Epoch: 7440 Train: 911.73529 Test: 3.77032
Epoch: 7520 Train: 819.25110 Test: 5.19457
Epoch: 7600 Train: 840.09222 Test: 4.10047
Epoch: 7680 Train: 750.27948 Test: 3.23742
Epoch: 7760 Train: 697.94586 Test: 2.26585
Epoch: 7840 Train: 670.92413 Test: 2.46272
Epoch: 7920 Train: 684.99011 Test: 4.69683
Epoch: 7999 Train: 690.18341 Test: 3.08398
Training Loss: tensor(690.1834)
Test Loss: tensor(3.0840)
Learned LE: [  0.97059524  -0.07689421 -14.530176  ]
True LE: [ 8.6472321e-01  4.4503487e-03 -1.4540405e+01]
Relative Error: [ 3.9333076   3.7332468   3.9019752   4.416194    4.7762027   4.258448
  3.5128672   2.8516467   2.5951395   2.590644    2.6498096   3.018492
  3.507403    3.9134574   4.320711    4.6551394   4.8511887   5.037386
  5.1548934   5.266831    5.3062797   5.5310993   5.8637114   5.835328
  5.7929907   5.579672    5.6825724   5.646501    5.000031    4.41921
  3.9540238   3.5332437   3.0812447   2.6336484   2.175427    1.759136
  1.5591954   1.4076908   1.4651394   2.3217247   3.2251582   4.1860113
  5.161748    5.922736    6.5781536   7.1771197   7.9436183   8.836285
  9.099552    9.67173     9.955524   10.2629595  10.088228    9.330353
  8.186993    7.16025     6.396026    5.884258    5.378709    4.8621955
  4.512635    4.272763    3.9848118   3.7432802   3.8661144   4.269728
  4.645982    4.086502    3.1863532   2.44085     2.130477    1.992219
  2.2465723   2.7164083   3.2751818   3.802045    4.2045517   4.4661736
  4.6297746   4.765279    4.8282924   5.04627     5.068486    5.284024
  5.587867    5.503199    5.3978477   5.148192    5.2156487   5.267134
  4.776329    4.263879    3.9358153   3.5131085   3.0915828   2.622284
  2.2088182   1.8995477   1.8075023   1.7027494   1.7214501   2.5741305
  3.4195025   4.142011    4.8713717   5.664792    6.2635446   6.818627
  7.5640798   8.56859     8.850669    9.314365    9.539415    9.868951
  9.725983    8.922703    7.794933    6.8274455   6.159638    5.703656
  5.210903    4.760761    4.3786216   4.0087237   3.880322    3.690909
  3.8110342   4.1172915   4.42749     3.7200801   2.6617348   1.8425391
  1.3932852   1.5103865   2.062355    2.6102295   3.057726    3.572195
  3.9627407   4.247536    4.3841186   4.4636903   4.580223    4.7921996
  4.9100833   5.153589    5.420575    5.268108    5.053156    4.752175
  4.7173886   4.7734365   4.608335    4.1400323   3.861118    3.5952828
  3.153173    2.6862109   2.3200672   2.0375462   1.9930102   1.8998629
  1.929537    2.814799    3.6024797   4.195757    4.7637453   5.3097663
  5.9305263   6.4767904   7.1627855   8.146802    8.519872    8.866523
  9.090905    9.419768    9.324665    8.448115    7.378526    6.565877
  5.9469795   5.5668244   5.1141267   4.743446    4.16522     3.749698
  3.6984575   3.5734031   3.6955404   3.8904812   4.0353293   3.31696
  2.2476413   1.3454725   0.7392478   1.3121288   2.0493603   2.5455527
  2.971205    3.3778467   3.7095575   3.9460409   4.0993853   4.2696066
  4.506914    4.687042    4.767267    4.987785    5.281899    5.1474175
  4.856164    4.4649024   4.2565527   4.293695    4.42935     4.0790305
  3.826759    3.614096    3.186979    2.7575915   2.3607543   2.0859115
  2.0398726   1.9592061   2.0238738   2.8971336   3.664007    4.3024516
  4.652743    5.0542803   5.5267944   6.099315    6.7989655   7.6204243
  8.117395    8.378948    8.568968    8.906496    8.806348    7.958886
  6.9508724   6.327012    5.8041925   5.4294367   5.057938    4.635418
  4.0086746   3.5397968   3.4380913   3.440161    3.4625325   3.604161
  3.7190557   3.109523    2.0408113   0.90081125  0.48385915  1.4180254
  2.0619323   2.5572934   2.9344568   3.233384    3.4764163   3.6303494
  3.826433    4.2012205   4.4305463   4.5310283   4.6055613   4.8325715
  5.1062813   5.029044    4.7407746   4.261428    3.916845    3.8894997
  4.134882    4.0098004   3.7628922   3.5254967   3.166068    2.7317364
  2.37078     2.0026574   1.9483956   1.9110419   1.9844955   2.6668212
  3.5361423   4.2390237   4.652385    4.8409896   5.1929245   5.703006
  6.362971    7.0149436   7.6528974   7.8321486   7.9989715   8.300926
  8.245344    7.470396    6.5400815   6.0415235   5.668024    5.340149
  5.0618143   4.509305    3.8445458   3.2844048   3.038466    3.0955245
  3.166663    3.3861973   3.519667    3.0896974   2.0139537   0.7975434
  0.6448258   1.4642662   2.0790029   2.5617967   2.9273043   3.1181884
  3.2675645   3.4159012   3.7568555   4.088493    4.2765064   4.460933
  4.522833    4.711306    4.906221    4.8969674   4.5920196   4.131747
  3.7677798   3.7044241   3.8105018   3.9204626   3.6329494   3.3683467
  3.0798721   2.7134552   2.3661535   2.0221732   1.903571    1.8783646
  1.8729565   2.3270764   3.262763    3.9492712   4.487309    4.7331195
  4.899869    5.3014517   5.858489    6.401984    7.0303755   7.2820477
  7.393091    7.5416327   7.6165357   6.950422    6.065843    5.733221
  5.504546    5.290119    4.946379    4.336902    3.6449034   3.0560822
  2.6900277   2.579909    2.9575968   3.1605587   3.3088386   3.1573884
  2.0789602   1.0103155   0.8415641   1.4010252   1.9253999   2.3341448
  2.707877    2.9425411   3.0015192   3.397502    3.6756053   3.8953905
  4.1228485   4.308327    4.407956    4.559523    4.6665673   4.733887
  4.400415    3.8868282   3.676741    3.611385  ]
