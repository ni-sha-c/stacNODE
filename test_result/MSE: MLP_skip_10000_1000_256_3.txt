time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 103.36%, model saved.
Epoch: 0 Train: 4088.15088 Test: 3927.49365
Epoch 100: New minimal relative error: 88.33%, model saved.
Epoch: 100 Train: 207.06836 Test: 207.50890
Epoch 200: New minimal relative error: 64.33%, model saved.
Epoch: 200 Train: 22.08638 Test: 24.44148
Epoch 300: New minimal relative error: 29.40%, model saved.
Epoch: 300 Train: 9.76488 Test: 11.54834
Epoch 400: New minimal relative error: 28.97%, model saved.
Epoch: 400 Train: 6.42313 Test: 7.49456
Epoch: 500 Train: 5.10131 Test: 6.03915
Epoch: 600 Train: 4.53887 Test: 5.29651
Epoch: 700 Train: 7.99522 Test: 7.90584
Epoch 800: New minimal relative error: 22.91%, model saved.
Epoch: 800 Train: 3.35693 Test: 4.03075
Epoch: 900 Train: 3.03956 Test: 3.67032
Epoch 1000: New minimal relative error: 13.09%, model saved.
Epoch: 1000 Train: 3.39420 Test: 3.85570
Epoch: 1100 Train: 2.52679 Test: 3.08930
Epoch: 1200 Train: 7.83797 Test: 6.88780
Epoch: 1300 Train: 2.14235 Test: 2.64475
Epoch: 1400 Train: 2.02686 Test: 2.47655
Epoch: 1500 Train: 1.82067 Test: 2.27339
Epoch: 1600 Train: 1.68748 Test: 2.11529
Epoch: 1700 Train: 1.55520 Test: 1.96603
Epoch 1800: New minimal relative error: 12.58%, model saved.
Epoch: 1800 Train: 1.44204 Test: 1.82978
Epoch: 1900 Train: 1.32920 Test: 1.69921
Epoch: 2000 Train: 1.25025 Test: 1.60043
Epoch: 2100 Train: 2.39091 Test: 3.06656
Epoch: 2200 Train: 1.05161 Test: 1.37631
Epoch 2300: New minimal relative error: 10.56%, model saved.
Epoch: 2300 Train: 1.03425 Test: 1.31584
Epoch: 2400 Train: 0.90309 Test: 1.19964
Epoch: 2500 Train: 0.86527 Test: 1.13547
Epoch: 2600 Train: 0.78004 Test: 1.05138
Epoch: 2700 Train: 0.94827 Test: 1.25365
Epoch: 2800 Train: 0.68009 Test: 0.92860
Epoch: 2900 Train: 0.72838 Test: 0.97298
Epoch 3000: New minimal relative error: 8.76%, model saved.
Epoch: 3000 Train: 0.60230 Test: 0.83305
Epoch: 3100 Train: 0.72795 Test: 0.91761
Epoch: 3200 Train: 0.53843 Test: 0.75335
Epoch: 3300 Train: 0.57296 Test: 0.73755
Epoch: 3400 Train: 0.48456 Test: 0.68600
Epoch: 3500 Train: 0.46041 Test: 0.65490
Epoch: 3600 Train: 0.44048 Test: 0.62875
Epoch: 3700 Train: 0.41934 Test: 0.60350
Epoch: 3800 Train: 0.40409 Test: 0.58384
Epoch: 3900 Train: 0.38451 Test: 0.55809
Epoch: 4000 Train: 0.37444 Test: 0.54546
Epoch: 4100 Train: 0.35415 Test: 0.51848
Epoch: 4200 Train: 1.24214 Test: 1.63506
Epoch: 4300 Train: 0.32689 Test: 0.48207
Epoch: 4400 Train: 0.79376 Test: 0.87746
Epoch: 4500 Train: 0.55439 Test: 0.65592
Epoch: 4600 Train: 0.28827 Test: 0.43199
Epoch: 4700 Train: 0.27945 Test: 0.41949
Epoch: 4800 Train: 0.28155 Test: 0.42631
Epoch 4900: New minimal relative error: 8.43%, model saved.
Epoch: 4900 Train: 0.26038 Test: 0.39325
Epoch: 5000 Train: 0.35845 Test: 0.52234
Epoch: 5100 Train: 0.24594 Test: 0.37399
Epoch: 5200 Train: 0.23845 Test: 0.36212
Epoch: 5300 Train: 0.25979 Test: 0.38385
Epoch: 5400 Train: 0.22495 Test: 0.34435
Epoch: 5500 Train: 0.22128 Test: 0.33957
Epoch: 5600 Train: 0.21377 Test: 0.32880
Epoch: 5700 Train: 0.62352 Test: 0.66350
Epoch: 5800 Train: 0.20382 Test: 0.31523
Epoch: 5900 Train: 0.19917 Test: 0.30888
Epoch: 6000 Train: 0.25212 Test: 0.30559
Epoch: 6100 Train: 0.18881 Test: 0.29326
Epoch: 6200 Train: 0.22124 Test: 0.31725
Epoch 6300: New minimal relative error: 5.96%, model saved.
Epoch: 6300 Train: 0.18091 Test: 0.28167
Epoch: 6400 Train: 0.18538 Test: 0.28227
Epoch: 6500 Train: 0.30596 Test: 0.43379
Epoch: 6600 Train: 0.17084 Test: 0.26642
Epoch: 6700 Train: 0.19971 Test: 0.27903
Epoch: 6800 Train: 0.16487 Test: 0.25770
Epoch: 6900 Train: 0.16990 Test: 0.26226
Epoch: 7000 Train: 0.15986 Test: 0.25028
Epoch: 7100 Train: 0.15992 Test: 0.24784
Epoch: 7200 Train: 0.15454 Test: 0.24210
Epoch: 7300 Train: 0.15256 Test: 0.23876
Epoch: 7400 Train: 0.15001 Test: 0.23513
Epoch: 7500 Train: 0.14837 Test: 0.23286
Epoch: 7600 Train: 0.20699 Test: 0.30473
Epoch: 7700 Train: 0.14555 Test: 0.22681
Epoch: 7800 Train: 0.14188 Test: 0.22304
Epoch: 7900 Train: 0.14136 Test: 0.22094
Epoch: 8000 Train: 0.13825 Test: 0.21748
Epoch: 8100 Train: 0.13956 Test: 0.22640
Epoch: 8200 Train: 0.13486 Test: 0.21226
Epoch: 8300 Train: 0.16713 Test: 0.25986
Epoch: 8400 Train: 0.13168 Test: 0.20736
Epoch: 8500 Train: 0.26982 Test: 0.37435
Epoch: 8600 Train: 0.12866 Test: 0.20272
Epoch: 8700 Train: 0.16079 Test: 0.22869
Epoch: 8800 Train: 0.12580 Test: 0.19820
Epoch: 8900 Train: 0.12539 Test: 0.19648
Epoch: 9000 Train: 0.23428 Test: 0.33132
Epoch: 9100 Train: 0.12172 Test: 0.19212
Epoch: 9200 Train: 0.13491 Test: 0.20214
Epoch: 9300 Train: 0.12681 Test: 0.19985
Epoch: 9400 Train: 0.11801 Test: 0.18629
Epoch: 9500 Train: 0.12263 Test: 0.18736
Epoch: 9600 Train: 0.11562 Test: 0.18282
Epoch: 9700 Train: 0.11661 Test: 0.18411
Epoch: 9800 Train: 0.11336 Test: 0.17942
Epoch: 9900 Train: 0.11253 Test: 0.17782
Epoch: 9999 Train: 0.11269 Test: 0.17835
Training Loss: tensor(0.1127)
Test Loss: tensor(0.1784)
Learned LE: [ 0.89298403 -0.01362605 -4.1909733 ]
True LE: [ 8.7243831e-01  2.8090300e-03 -1.4551303e+01]
Relative Error: [6.1789737  6.143285   6.0696964  5.971701   5.7804832  5.653117
 5.321321   5.0524616  4.5498843  4.012012   3.6511078  3.6691687
 3.9053645  3.7628233  3.3509955  3.6940768  4.6524644  4.8070064
 5.078781   4.598187   4.312154   3.9770489  3.667013   3.2355945
 2.8751895  2.095069   1.545354   1.2605486  1.1224136  1.171639
 1.3963972  1.2050691  0.9066323  0.7750705  0.65736294 0.5604491
 0.53034806 0.91690433 1.4600683  2.0371835  3.0775623  4.1828485
 5.102363   5.584755   5.6788864  5.6805778  5.7954683  5.931772
 5.847555   5.71599    4.3823986  3.1436913  2.266852   1.9054062
 2.0877779  2.5507088  2.5853586  2.9708803  3.7090952  4.6551332
 4.8573503  5.3247476  5.818666   5.905414   5.640981   5.3000307
 4.996045   4.715152   4.5807247  4.5465884  4.4218717  4.242042
 3.8006232  3.554839   3.66221    3.5282862  3.0590854  3.0506184
 3.8204367  4.288885   4.5946283  4.5021644  4.0999317  3.94536
 3.2787893  2.8332474  2.3606682  1.7953947  1.3526977  0.9568355
 0.68681043 0.6211117  0.8596668  1.0978234  1.165435   1.1362973
 1.036548   0.85249954 0.62035763 0.6510766  1.1284087  1.746767
 2.470309   3.6592345  4.803195   4.9710965  4.853367   4.7911253
 4.839261   5.1282787  5.378278   5.0216246  3.6600597  2.442999
 1.539622   0.8505745  0.6831517  0.8949099  0.9040268  1.030252
 1.6098787  2.6457787  3.284024   3.9509926  4.6649704  5.1422315
 4.919878   4.6551657  4.342167   3.9386332  3.681887   3.618515
 3.8935974  3.7336633  3.634196   3.54697    3.5219028  3.379411
 3.0012217  2.5776565  2.9521756  3.8877091  3.9320688  4.4003224
 3.891505   3.641783   3.0662537  2.6648333  2.2635207  1.8416485
 1.5590725  1.2662485  1.0369942  0.7574706  0.34711063 0.7460788
 0.9671755  1.2909892  1.308972   1.3272841  1.0168098  0.6385107
 0.834817   1.5783248  1.8971626  2.9884794  3.8125887  4.153169
 4.051943   3.9808843  3.878289   3.9501586  4.193571   4.0673685
 3.5162132  2.4947758  2.0770094  1.754531   1.919492   1.6291714
 1.5142546  1.3815079  0.94941825 0.8005188  1.4245505  2.1924288
 2.9296403  3.2704294  3.876609   4.1255913  3.940457   3.47832
 3.1297877  2.9080803  2.8473556  3.1293657  3.1366394  3.070806
 3.176613   3.329871   3.0115457  2.4269729  2.3550446  2.8386078
 3.4269214  3.5597827  3.7798681  3.3242562  2.8643796  2.5928116
 2.2706046  1.931659   1.6566767  1.5105662  1.3932917  1.2682321
 0.8321557  0.35703337 0.71426594 1.0539596  1.4834138  1.5900025
 1.7143443  1.0960424  0.63772106 0.9876215  1.8749975  2.2047064
 2.638791   3.3196852  3.2717996  3.1990547  3.178383   2.9891772
 3.0564978  2.8969193  2.871356   3.0277479  2.8080041  2.8535407
 2.7354982  2.2713957  1.6559885  1.6139501  1.3530154  0.9408588
 0.7008194  0.5263598  0.8277409  1.4111093  2.1229265  2.8304102
 3.6376991  3.3594906  2.905064   2.600683   2.3748727  2.2766387
 2.4513755  2.6520514  2.596679   2.8829205  2.8445032  2.6688218
 2.1007636  2.0984063  2.5643237  2.9211266  3.1177895  3.1016583
 2.5584652  2.1800964  1.9334667  1.5292932  1.4144827  1.355705
 1.2517691  1.2134091  1.2334973  1.0603279  0.35963732 0.5127018
 1.1155015  1.7372234  1.9542925  1.900463   1.1366178  0.61207694
 1.1566043  1.9668194  1.5863914  2.056852   2.4284027  2.3652475
 2.3274872  2.2031648  2.107048   2.115401   2.0486052  2.348402
 2.771702   2.7718956  2.333226   1.8910362  1.3918736  1.5391455
 1.6007917  1.1801594  0.77293223 0.91235024 0.583831   0.6936556
 0.6467861  1.1011832  1.9676881  2.7899098  3.1004493  2.670383
 2.395339   2.1258774  1.9124014  1.9323525  2.0650263  2.1907632
 2.4346893  2.4348636  2.1691148  1.9535348  1.7817435  2.1128485
 2.421547   2.553792   2.4530163  1.8380278  1.5052145  1.1524355
 0.7322626  0.72784865 0.81826866 0.81882674 0.88523126 1.1577018
 1.0380177  0.7983067  0.23800515 0.8755692  1.4051341  1.5983518
 1.8226451  1.46376    0.5989957  1.1687604  1.5515416  1.1574979
 1.4463744  1.6535679  1.6157646  1.5012376  1.3494974  1.403304
 1.4887793  1.5024145  2.16269    2.1170614  2.164732   1.5823761
 0.9824746  0.78336066 0.96471095 1.0186301  0.98212063 0.9361228
 1.0636389  0.65352935 0.51795125 0.686906   0.8503255  1.0427282
 1.9513203  2.6150794  2.6028366  2.3049686  2.003556   1.7113453
 1.6021726  1.5621604  1.7496691  1.996113   2.0651476  1.7518702
 1.525224   1.4774779  1.5624049  1.9291075  2.0070872  1.949669
 1.6192188  1.1065116  0.52883536 0.38736692]
