time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.34%, model saved.
Epoch: 0 Train: 32152.44141 Test: 4238.16797
Epoch 80: New minimal relative error: 74.18%, model saved.
Epoch: 80 Train: 8601.74023 Test: 1469.52136
Epoch 160: New minimal relative error: 66.93%, model saved.
Epoch: 160 Train: 8080.66553 Test: 1187.82568
Epoch: 240 Train: 6937.39893 Test: 1012.74200
Epoch 320: New minimal relative error: 50.16%, model saved.
Epoch: 320 Train: 6650.09961 Test: 1103.62366
Epoch: 400 Train: 6901.12939 Test: 875.72443
Epoch: 480 Train: 4374.13135 Test: 565.67365
Epoch: 560 Train: 3284.60376 Test: 365.84766
Epoch: 640 Train: 2355.38916 Test: 168.46225
Epoch 720: New minimal relative error: 32.09%, model saved.
Epoch: 720 Train: 1098.15820 Test: 49.89923
Epoch: 800 Train: 725.28900 Test: 22.57995
Epoch 880: New minimal relative error: 23.59%, model saved.
Epoch: 880 Train: 588.11755 Test: 21.36849
Epoch 960: New minimal relative error: 17.66%, model saved.
Epoch: 960 Train: 525.29346 Test: 13.58387
Epoch 1040: New minimal relative error: 12.20%, model saved.
Epoch: 1040 Train: 465.90024 Test: 13.44086
Epoch: 1120 Train: 446.90033 Test: 8.72027
Epoch 1200: New minimal relative error: 8.42%, model saved.
Epoch: 1200 Train: 303.27124 Test: 6.01180
Epoch: 1280 Train: 313.64142 Test: 6.78985
Epoch: 1360 Train: 348.39676 Test: 19.29900
Epoch 1440: New minimal relative error: 7.10%, model saved.
Epoch: 1440 Train: 209.38469 Test: 2.65303
Epoch: 1520 Train: 214.83888 Test: 4.02155
Epoch: 1600 Train: 249.95113 Test: 9.45873
Epoch: 1680 Train: 230.17500 Test: 6.96879
Epoch: 1760 Train: 169.59749 Test: 2.98219
Epoch 1840: New minimal relative error: 5.19%, model saved.
Epoch: 1840 Train: 162.88997 Test: 2.06055
Epoch 1920: New minimal relative error: 4.66%, model saved.
Epoch: 1920 Train: 150.40637 Test: 1.53864
Epoch: 2000 Train: 144.72763 Test: 1.93471
Epoch: 2080 Train: 154.40480 Test: 5.13987
Epoch: 2160 Train: 134.39520 Test: 1.71869
Epoch: 2240 Train: 138.81393 Test: 25.78045
Epoch: 2320 Train: 124.33994 Test: 1.12506
Epoch: 2400 Train: 114.95997 Test: 0.90228
Epoch: 2480 Train: 108.57009 Test: 0.93529
Epoch: 2560 Train: 105.70601 Test: 1.78054
Epoch: 2640 Train: 111.40028 Test: 1.51882
Epoch: 2720 Train: 124.90475 Test: 6.47795
Epoch 2800: New minimal relative error: 3.98%, model saved.
Epoch: 2800 Train: 93.66263 Test: 0.73661
Epoch: 2880 Train: 91.56896 Test: 1.07491
Epoch: 2960 Train: 87.92529 Test: 1.12848
Epoch: 3040 Train: 114.34509 Test: 18.96323
Epoch: 3120 Train: 86.21556 Test: 0.67824
Epoch: 3200 Train: 78.44560 Test: 0.56586
Epoch: 3280 Train: 78.04494 Test: 1.45536
Epoch: 3360 Train: 92.43517 Test: 1.62332
Epoch 3440: New minimal relative error: 3.82%, model saved.
Epoch: 3440 Train: 73.77671 Test: 0.42486
Epoch: 3520 Train: 90.69206 Test: 16.20767
Epoch: 3600 Train: 69.06323 Test: 0.44872
Epoch 3680: New minimal relative error: 2.36%, model saved.
Epoch: 3680 Train: 62.95955 Test: 0.51418
Epoch: 3760 Train: 61.49012 Test: 1.58602
Epoch: 3840 Train: 66.76666 Test: 0.64002
Epoch: 3920 Train: 58.81350 Test: 0.96876
Epoch: 4000 Train: 54.75351 Test: 0.55304
Epoch: 4080 Train: 53.12334 Test: 0.60678
Epoch: 4160 Train: 52.45640 Test: 0.26141
Epoch: 4240 Train: 52.76141 Test: 0.58692
Epoch: 4320 Train: 51.87005 Test: 0.22979
Epoch: 4400 Train: 58.71890 Test: 6.12133
Epoch: 4480 Train: 48.43455 Test: 0.19053
Epoch: 4560 Train: 49.41477 Test: 1.17267
Epoch: 4640 Train: 49.09316 Test: 0.54467
Epoch: 4720 Train: 49.45100 Test: 0.28903
Epoch: 4800 Train: 56.59555 Test: 1.59652
Epoch: 4880 Train: 50.89477 Test: 0.35034
Epoch 4960: New minimal relative error: 1.78%, model saved.
Epoch: 4960 Train: 49.07992 Test: 0.26066
Epoch: 5040 Train: 50.19834 Test: 1.13656
Epoch: 5120 Train: 48.70506 Test: 0.38056
Epoch: 5200 Train: 47.19568 Test: 0.47565
Epoch: 5280 Train: 48.97697 Test: 0.81132
Epoch: 5360 Train: 45.68125 Test: 0.18380
Epoch: 5440 Train: 44.73349 Test: 0.22427
Epoch: 5520 Train: 49.12031 Test: 2.07773
Epoch 5600: New minimal relative error: 1.62%, model saved.
Epoch: 5600 Train: 43.95986 Test: 0.15664
Epoch: 5680 Train: 44.62814 Test: 0.23609
Epoch: 5760 Train: 45.98990 Test: 0.74808
Epoch: 5840 Train: 44.66286 Test: 0.38758
Epoch: 5920 Train: 43.04468 Test: 0.17949
Epoch: 6000 Train: 48.81443 Test: 0.65908
Epoch 6080: New minimal relative error: 1.55%, model saved.
Epoch: 6080 Train: 42.91751 Test: 0.15049
Epoch: 6160 Train: 42.23731 Test: 0.36987
Epoch: 6240 Train: 42.97660 Test: 0.16565
Epoch: 6320 Train: 43.95064 Test: 0.20257
Epoch: 6400 Train: 43.13777 Test: 0.24990
Epoch: 6480 Train: 43.78141 Test: 0.17093
Epoch: 6560 Train: 41.27861 Test: 0.18865
Epoch: 6640 Train: 40.90421 Test: 0.26916
Epoch: 6720 Train: 40.09697 Test: 0.73780
Epoch: 6800 Train: 38.94832 Test: 0.52055
Epoch: 6880 Train: 37.88520 Test: 0.16485
Epoch: 6960 Train: 38.78756 Test: 0.24083
Epoch: 7040 Train: 38.57288 Test: 0.13857
Epoch: 7120 Train: 38.25839 Test: 0.17477
Epoch: 7200 Train: 39.97555 Test: 0.35745
Epoch: 7280 Train: 41.01024 Test: 0.52553
Epoch: 7360 Train: 41.24887 Test: 0.23238
Epoch: 7440 Train: 38.51224 Test: 0.16858
Epoch: 7520 Train: 37.15495 Test: 0.14572
Epoch: 7600 Train: 41.62495 Test: 2.66840
Epoch: 7680 Train: 36.86465 Test: 0.12535
Epoch 7760: New minimal relative error: 1.41%, model saved.
Epoch: 7760 Train: 37.31909 Test: 0.13412
Epoch: 7840 Train: 36.52193 Test: 0.13199
Epoch: 7920 Train: 36.52643 Test: 0.38936
Epoch: 7999 Train: 35.71431 Test: 0.11443
Training Loss: tensor(35.7143)
Test Loss: tensor(0.1144)
Learned LE: [  0.8382492   0.0213545 -14.528283 ]
True LE: [ 8.6601180e-01 -6.5042251e-03 -1.4541409e+01]
Relative Error: [1.6600083  1.5333095  1.6434882  1.7843492  1.8491815  1.6195776
 1.6424624  1.7390177  1.7489294  1.6930391  1.5891303  1.3543298
 1.2606605  1.20628    1.0607651  0.81304497 0.7748771  0.57084376
 0.4155151  0.45745027 0.70733386 1.1967633  1.5294042  1.1326779
 0.935991   0.80923903 0.9863544  1.1551746  1.1471012  1.3115721
 1.4407358  1.4067829  1.4628117  1.437825   1.4257336  1.7260025
 2.0154126  2.1189623  2.1062615  2.2527506  2.4179392  2.7119732
 3.239027   2.979429   2.2214422  1.6217196  1.4214225  1.3758308
 1.3140417  1.1904987  0.93933696 0.6238509  0.66553646 0.97149265
 1.2368234  1.4029092  1.4104658  1.4428606  1.6027659  1.7461913
 1.8643233  1.742966   1.585672   1.5223837  1.5976516  1.7306342
 1.8253876  1.5458486  1.4412313  1.5786986  1.669848   1.6626524
 1.5762852  1.3631116  1.122664   1.0381464  1.0178266  0.81159526
 0.7844698  0.5975208  0.36501646 0.3975499  0.68695915 1.186663
 1.3758516  0.99055415 0.84687704 0.7199606  0.85443074 1.013781
 1.0551462  1.2872511  1.3906336  1.3549322  1.4000434  1.3540766
 1.2516985  1.4592057  1.8927644  2.0113182  1.9544148  2.052937
 2.167226   2.454554   2.9207568  2.7118952  2.1115184  1.4844129
 1.3198932  1.3160483  1.2698401  1.1302288  0.8883513  0.6533099
 0.76315475 1.0098481  1.1762741  1.2832282  1.3203741  1.4204879
 1.527201   1.6054254  1.6203508  1.6295692  1.6004897  1.5622503
 1.5028229  1.6305424  1.7033411  1.4148546  1.3188696  1.3713398
 1.5284828  1.5718536  1.5093257  1.3480415  1.0506194  0.8566833
 1.0242449  0.8641079  0.8338263  0.63897026 0.37140632 0.39968655
 0.6764717  1.1575276  1.2819558  0.91374534 0.8000794  0.69540995
 0.7595932  0.84700155 0.91072285 1.1411417  1.3426905  1.3029453
 1.3057214  1.1991006  1.0281286  1.1278496  1.6914778  1.8531829
 1.8010507  1.8377519  1.9239545  2.1916869  2.6199574  2.450792
 1.9743366  1.392962   1.2248998  1.243671   1.2375106  1.125012
 0.9035528  0.7085861  0.8470241  1.0470749  1.1472682  1.1627066
 1.2421423  1.3643738  1.4941354  1.5117013  1.5290139  1.6013122
 1.736081   1.633325   1.4577037  1.5804341  1.6605626  1.358333
 1.1091311  1.1927154  1.2849685  1.3770473  1.3517054  1.2884622
 1.054125   0.78202367 0.8764809  0.96003443 0.84193265 0.7187276
 0.43959162 0.39853892 0.6424515  1.1069564  1.3072963  0.912361
 0.7878698  0.6749151  0.63291276 0.7243574  0.73314893 0.9501352
 1.2274683  1.2157068  1.1440765  1.0293039  0.8116098  0.82968444
 1.3973649  1.6776731  1.578528   1.6280546  1.6724447  1.8957396
 2.2930512  2.230633   1.8729571  1.3674674  1.1302772  1.1647727
 1.2205745  1.094981   0.8948929  0.7321103  0.89690226 1.0289943
 1.0683135  1.125413   1.2233858  1.3223891  1.3852819  1.4841295
 1.472877   1.5305064  1.6750747  1.687542   1.4763919  1.3965236
 1.4213752  1.3008516  0.9815549  1.0015271  1.0438402  1.1292962
 1.0979753  1.1686834  1.0023313  0.7486438  0.68489593 1.0162605
 0.87927514 0.8337877  0.5108755  0.37403378 0.5525297  0.98159975
 1.3735034  0.9436771  0.74681616 0.71811515 0.62186223 0.6470042
 0.6259302  0.7065808  1.0464059  1.0901017  0.92641497 0.874022
 0.6258104  0.56109387 1.0888171  1.3378081  1.4017882  1.4084868
 1.4638443  1.613107   1.924884   1.9836057  1.6582826  1.3767078
 1.0368651  1.05387    1.1165705  1.0545051  0.8259387  0.6718596
 0.84489155 0.93887436 0.9757845  1.1043638  1.1587399  1.2515814
 1.3371121  1.4108934  1.4151152  1.4665569  1.4847252  1.5935751
 1.4549843  1.2276381  1.2576782  1.1938607  0.82563853 0.795274
 0.9218456  0.95862544 0.91111577 0.94052255 0.89530337 0.6739887
 0.55564994 0.7622309  0.97190374 0.87753356 0.52317613 0.32190442
 0.42749563 0.7930348  1.399917   1.0234656  0.76363283 0.7054017
 0.650042   0.64828956 0.580693   0.5571735  0.8394178  0.9746912
 0.8251857  0.6858956  0.48747218 0.35981408 0.78294265 1.1453832
 1.2388681  1.2774049  1.2662654  1.3495809  1.5615313  1.6114212
 1.4922583  1.3495805  0.99882084 0.84488213 1.0029217  0.98483366
 0.7742284  0.5885012  0.7643781  0.82851255 0.9081957  1.0393901
 1.096012   1.1380602  1.243791   1.3016644  1.3326485  1.3648388
 1.3215791  1.2827779  1.3217447  1.0625305  0.93235075 1.0155884
 0.70919555 0.5247431  0.67962736 0.807034   0.82091933 0.7475807
 0.7174795  0.5746851  0.4705377  0.45199302 0.8381229  0.9016767
 0.64166915 0.30372298 0.28075165 0.5244395  1.0493472  1.179668
 0.84923834 0.7532525  0.71674985 0.60539573]
