time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 101.96%, model saved.
Epoch: 0 Train: 60328.60938 Test: 3865.18652
Epoch: 100 Train: 15887.75586 Test: 1531.92285
Epoch: 200 Train: 18187.84180 Test: 1914.91870
Epoch: 300 Train: 13302.25879 Test: 1948.58606
Epoch 400: New minimal relative error: 96.21%, model saved.
Epoch: 400 Train: 12990.25195 Test: 1209.67200
Epoch 500: New minimal relative error: 58.79%, model saved.
Epoch: 500 Train: 15647.83984 Test: 1386.75879
Epoch: 600 Train: 13127.60742 Test: 1100.23486
Epoch 700: New minimal relative error: 53.04%, model saved.
Epoch: 700 Train: 13654.38184 Test: 1221.28003
Epoch: 800 Train: 14626.52734 Test: 1231.10437
Epoch: 900 Train: 12730.15625 Test: 1246.09692
Epoch: 1000 Train: 12226.46777 Test: 1151.46240
Epoch: 1100 Train: 10183.74414 Test: 739.91046
Epoch: 1200 Train: 9381.67188 Test: 547.67065
Epoch: 1300 Train: 7392.95703 Test: 330.15399
Epoch: 1400 Train: 5586.14355 Test: 228.58232
Epoch: 1500 Train: 4942.21045 Test: 220.19658
Epoch: 1600 Train: 2997.25610 Test: 123.34331
Epoch: 1700 Train: 2056.49976 Test: 60.02975
Epoch 1800: New minimal relative error: 10.80%, model saved.
Epoch: 1800 Train: 904.53174 Test: 21.60750
Epoch: 1900 Train: 714.57318 Test: 14.08704
Epoch: 2000 Train: 590.19733 Test: 10.46511
Epoch: 2100 Train: 428.01999 Test: 4.28125
Epoch: 2200 Train: 343.49786 Test: 6.34585
Epoch: 2300 Train: 286.42773 Test: 3.06905
Epoch: 2400 Train: 239.49193 Test: 2.49103
Epoch: 2500 Train: 218.13441 Test: 1.93422
Epoch: 2600 Train: 197.85674 Test: 1.36715
Epoch 2700: New minimal relative error: 8.65%, model saved.
Epoch: 2700 Train: 184.01831 Test: 1.21433
Epoch: 2800 Train: 166.49036 Test: 1.06653
Epoch: 2900 Train: 179.50053 Test: 1.70736
Epoch 3000: New minimal relative error: 4.83%, model saved.
Epoch: 3000 Train: 158.48190 Test: 1.19610
Epoch: 3100 Train: 155.10416 Test: 1.61624
Epoch 3200: New minimal relative error: 2.62%, model saved.
Epoch: 3200 Train: 153.43159 Test: 1.09318
Epoch: 3300 Train: 159.96034 Test: 5.04117
Epoch: 3400 Train: 152.16287 Test: 2.15041
Epoch: 3500 Train: 140.81052 Test: 1.13433
Epoch: 3600 Train: 138.90012 Test: 1.65781
Epoch: 3700 Train: 119.74614 Test: 1.37818
Epoch: 3800 Train: 114.80993 Test: 0.91802
Epoch: 3900 Train: 126.47439 Test: 1.16977
Epoch: 4000 Train: 138.66576 Test: 2.73730
Epoch: 4100 Train: 116.84651 Test: 0.97671
Epoch: 4200 Train: 112.03737 Test: 3.72148
Epoch: 4300 Train: 126.29893 Test: 3.60060
Epoch: 4400 Train: 121.87872 Test: 1.02044
Epoch: 4500 Train: 117.97385 Test: 0.96905
Epoch: 4600 Train: 119.37772 Test: 1.27939
Epoch: 4700 Train: 119.71529 Test: 2.01241
Epoch: 4800 Train: 115.84116 Test: 0.94061
Epoch: 4900 Train: 128.53349 Test: 5.29561
Epoch: 5000 Train: 119.94491 Test: 1.03465
Epoch: 5100 Train: 116.15691 Test: 1.02900
Epoch: 5200 Train: 97.31546 Test: 2.03693
Epoch: 5300 Train: 98.57762 Test: 0.79826
Epoch: 5400 Train: 100.71840 Test: 1.40346
Epoch: 5500 Train: 89.83167 Test: 0.57993
Epoch: 5600 Train: 93.92924 Test: 0.72125
Epoch: 5700 Train: 101.11449 Test: 1.57192
Epoch: 5800 Train: 97.54959 Test: 0.67662
Epoch: 5900 Train: 110.42583 Test: 1.11810
Epoch: 6000 Train: 97.16796 Test: 1.08465
Epoch: 6100 Train: 93.58346 Test: 1.05580
Epoch: 6200 Train: 95.68487 Test: 0.82107
Epoch: 6300 Train: 99.36414 Test: 0.83587
Epoch: 6400 Train: 99.06965 Test: 0.73615
Epoch: 6500 Train: 63.71242 Test: 0.30655
Epoch: 6600 Train: 64.45674 Test: 0.35969
Epoch: 6700 Train: 65.24898 Test: 0.75505
Epoch: 6800 Train: 61.54225 Test: 0.71513
Epoch: 6900 Train: 71.53640 Test: 1.73348
Epoch: 7000 Train: 58.51542 Test: 0.64534
Epoch: 7100 Train: 59.04241 Test: 1.21648
Epoch: 7200 Train: 55.15484 Test: 0.32391
Epoch: 7300 Train: 53.05136 Test: 0.36250
Epoch: 7400 Train: 52.67955 Test: 0.42217
Epoch: 7500 Train: 63.89692 Test: 0.48645
Epoch: 7600 Train: 71.50759 Test: 0.83511
Epoch: 7700 Train: 58.38004 Test: 0.32499
Epoch: 7800 Train: 69.53291 Test: 3.01481
Epoch: 7900 Train: 52.94538 Test: 0.23248
Epoch 8000: New minimal relative error: 2.15%, model saved.
Epoch: 8000 Train: 46.54194 Test: 0.20934
Epoch: 8100 Train: 49.20866 Test: 0.72525
Epoch: 8200 Train: 53.19561 Test: 0.31730
Epoch: 8300 Train: 56.03501 Test: 0.51746
Epoch: 8400 Train: 50.91117 Test: 0.22631
Epoch: 8500 Train: 82.08944 Test: 13.19868
Epoch: 8600 Train: 53.06591 Test: 0.24756
Epoch: 8700 Train: 53.57262 Test: 0.33897
Epoch: 8800 Train: 53.07962 Test: 0.86053
Epoch: 8900 Train: 48.80825 Test: 0.22240
Epoch: 9000 Train: 44.50705 Test: 0.22827
Epoch: 9100 Train: 41.61933 Test: 0.31091
Epoch: 9200 Train: 41.78035 Test: 0.17907
Epoch: 9300 Train: 44.54624 Test: 0.22090
Epoch: 9400 Train: 45.13578 Test: 0.24576
Epoch: 9500 Train: 46.58503 Test: 0.33202
Epoch: 9600 Train: 43.01791 Test: 0.33742
Epoch: 9700 Train: 39.86374 Test: 0.20578
Epoch: 9800 Train: 41.94079 Test: 0.20177
Epoch: 9900 Train: 50.86399 Test: 0.38437
Epoch: 9999 Train: 43.76002 Test: 0.22193
Training Loss: tensor(43.7600)
Test Loss: tensor(0.2219)
Learned LE: [ 8.8935542e-01  4.4435104e-03 -1.4554268e+01]
True LE: [ 8.7830925e-01 -4.4778632e-03 -1.4551398e+01]
Relative Error: [0.83380044 0.48501697 0.41208375 0.8164634  1.1791947  1.484825
 1.7646196  1.8625699  1.8698635  1.8498516  1.7392082  1.5659935
 1.4188946  1.317098   1.2725441  1.2473873  1.1985307  1.2866479
 1.5237684  1.898941   2.5298703  3.0922885  3.7018445  3.254281
 2.9033144  2.6928296  2.65481    2.624058   2.3430386  2.13852
 1.7214189  1.3397636  0.91942436 0.6730841  0.47235554 0.32259524
 0.5063385  0.7315867  0.9185619  1.2591769  1.47827    1.3072691
 0.9818987  0.73958033 0.83526206 1.3093017  1.6877321  1.9759853
 1.7250898  1.5547246  1.1045463  0.69813144 0.5675693  0.87333655
 1.1861821  1.4089833  1.7916911  1.8295907  1.9253498  1.9018022
 1.6114024  1.2774432  0.8933096  0.57031584 0.3548157  0.59285843
 0.95562947 1.1902914  1.4502809  1.7030599  1.721644   1.7451234
 1.5988274  1.4688196  1.3472517  1.2968141  1.2730767  1.2600539
 1.286136   1.1781563  1.2059721  1.4211849  1.845236   2.3545303
 2.864323   3.2930942  2.8391216  2.5289524  2.453428   2.4183352
 2.2383711  1.9643883  1.734526   1.33003    0.7973686  0.59400105
 0.5515161  0.34119898 0.39429018 0.58647126 0.74738926 1.0224326
 1.304635   1.4374194  1.1248089  0.7704204  0.5857294  0.8348151
 1.4637626  1.7532574  1.8499049  1.5134597  1.2270561  0.82184917
 0.49160463 0.53843457 0.8043289  1.0554999  1.3603165  1.4628289
 1.5846573  1.7223411  1.5145069  1.1934606  0.8720464  0.62459266
 0.37656417 0.40327743 0.60611254 0.88450706 1.0632867  1.242047
 1.4109799  1.3881502  1.4590921  1.3310771  1.1325611  1.0552042
 1.0981877  1.1429836  1.195877   1.2376847  1.1151929  1.1160496
 1.3558304  1.7096745  2.070325   2.5555632  2.9304178  2.5455303
 2.2925422  2.1603355  2.2290554  1.8300716  1.650624   1.4121602
 0.87688935 0.50642884 0.548276   0.584972   0.41816854 0.5653428
 0.64344096 0.7192911  0.95492464 1.1739584  1.1802701  0.8893367
 0.5425966  0.48159516 0.90032333 1.5487081  1.6915407  1.6329063
 1.3182594  1.0129747  0.66511023 0.55148363 0.5015664  0.6687883
 0.8698346  1.102489   1.0873513  1.3121383  1.376451   1.0950801
 0.80801815 0.60358036 0.44196498 0.34355968 0.294033   0.34999135
 0.5266781  0.72069114 0.8081056  0.9833495  0.9622327  1.0349416
 0.98024774 0.8265477  0.8085874  0.8489725  0.899672   0.9920127
 1.0707734  1.0966343  1.0598927  1.2680025  1.5013337  1.7344449
 2.148347   2.6760333  2.3312654  2.0364268  1.9184679  1.9265964
 1.5476543  1.3810741  1.1291629  0.53732103 0.3702078  0.6164405
 0.7364419  0.609117   0.63031054 0.63038695 0.606436   0.71155405
 0.88145983 0.87225753 0.6882317  0.35727215 0.48870003 0.8985986
 1.3772541  1.5513178  1.3576952  1.1964808  0.9053608  0.6841847
 0.6453397  0.501779   0.59642184 0.7437047  0.81578827 0.86208516
 0.99454224 0.9668277  0.6825148  0.48682415 0.32471684 0.21842225
 0.20891307 0.1496576  0.1504144  0.15107001 0.41240865 0.6242666
 0.8637542  0.8991409  0.95053625 0.8913015  0.73928845 0.7285685
 0.6707039  0.64633495 0.6959549  0.84848636 1.0185205  1.0626012
 1.093816   1.2748771  1.4101993  1.7000754  2.3212907  2.1674526
 1.7967469  1.7099255  1.6080607  1.3292477  1.2005774  0.871025
 0.32776585 0.20598467 0.56662476 0.756403   0.76020133 0.5817789
 0.5133383  0.45134094 0.4472726  0.6893324  0.70894617 0.55431455
 0.22949804 0.5208275  0.84359246 1.1667192  1.273515   1.1237857
 1.1370783  0.8760524  0.74452275 0.709291   0.5609886  0.4883727
 0.69270784 0.71587294 0.7301911  0.6836346  0.69237596 0.40444216
 0.19433512 0.13823226 0.15414724 0.24845001 0.48173106 0.4603115
 0.33830112 0.4175744  0.6108861  0.7492998  0.91789436 0.8597194
 0.8603803  0.73537326 0.6456242  0.63573605 0.53982913 0.5621778
 0.63105065 0.87651694 1.057507   1.1036377  1.1924559  1.1912867
 1.2426677  1.7421663  2.0545256  1.6697963  1.5632639  1.458549
 1.2804574  1.0101719  0.71044934 0.29466063 0.18531376 0.39868477
 0.6147492  0.8440595  0.6395686  0.41320658 0.3205061  0.44041052
 0.53654593 0.5105039  0.33773863 0.1799018  0.37484527 0.6893208
 0.9259355  1.0121753  0.87515104 0.950965   0.8592377  0.76021045
 0.70297325 0.6262374  0.482515   0.6743395  0.7310372  0.64076096
 0.47309723 0.6013138  0.29462838 0.10726643 0.11091117 0.32647228
 0.5360289  0.6514363  0.6804773  0.68760455 0.6774046  0.701005
 0.7936721  0.8361864  0.87020856 0.6564991  0.54497993 0.5546851
 0.6092213  0.5723775  0.46549034 0.48912674 0.65595186 0.81104004
 1.0602037  1.160235   1.0468659  1.0496521 ]
