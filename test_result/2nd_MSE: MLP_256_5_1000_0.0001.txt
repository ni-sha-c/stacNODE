time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.93%, model saved.
Epoch: 0 Train: 3893.92285 Test: 4088.77856
Epoch 100: New minimal relative error: 57.28%, model saved.
Epoch: 100 Train: 100.83040 Test: 129.34125
Epoch 200: New minimal relative error: 18.66%, model saved.
Epoch: 200 Train: 11.24041 Test: 14.08017
Epoch: 300 Train: 7.73604 Test: 11.97262
Epoch: 400 Train: 7.33058 Test: 6.27052
Epoch: 500 Train: 10.59972 Test: 8.84378
Epoch: 600 Train: 2.04632 Test: 3.62048
Epoch 700: New minimal relative error: 18.49%, model saved.
Epoch: 700 Train: 1.36206 Test: 1.94589
Epoch: 800 Train: 2.57940 Test: 4.18619
Epoch: 900 Train: 1.39848 Test: 2.50066
Epoch: 1000 Train: 1.85042 Test: 1.84378
Epoch: 1100 Train: 0.85580 Test: 1.10553
Epoch 1200: New minimal relative error: 13.00%, model saved.
Epoch: 1200 Train: 0.78638 Test: 0.98764
Epoch: 1300 Train: 0.91659 Test: 1.52740
Epoch: 1400 Train: 8.80731 Test: 7.69937
Epoch: 1500 Train: 1.99872 Test: 0.97705
Epoch: 1600 Train: 1.28544 Test: 0.80529
Epoch: 1700 Train: 0.35143 Test: 0.85169
Epoch: 1800 Train: 0.32831 Test: 0.58470
Epoch: 1900 Train: 0.26863 Test: 0.52183
Epoch: 2000 Train: 0.26096 Test: 0.55232
Epoch: 2100 Train: 0.33803 Test: 0.65967
Epoch 2200: New minimal relative error: 12.43%, model saved.
Epoch: 2200 Train: 0.22898 Test: 0.44595
Epoch: 2300 Train: 0.19150 Test: 0.41122
Epoch 2400: New minimal relative error: 9.03%, model saved.
Epoch: 2400 Train: 0.18499 Test: 0.41598
Epoch: 2500 Train: 0.18106 Test: 0.40464
Epoch: 2600 Train: 0.18778 Test: 0.38999
Epoch: 2700 Train: 0.20129 Test: 0.43021
Epoch: 2800 Train: 2.53559 Test: 4.36674
Epoch: 2900 Train: 0.14123 Test: 0.34912
Epoch: 3000 Train: 0.19266 Test: 0.43207
Epoch: 3100 Train: 0.60385 Test: 0.76844
Epoch: 3200 Train: 0.60890 Test: 0.66401
Epoch: 3300 Train: 1.72178 Test: 0.76039
Epoch: 3400 Train: 0.11221 Test: 0.30361
Epoch: 3500 Train: 0.56726 Test: 0.98891
Epoch: 3600 Train: 0.10418 Test: 0.29113
Epoch: 3700 Train: 0.35702 Test: 0.67469
Epoch: 3800 Train: 0.78469 Test: 1.40839
Epoch: 3900 Train: 0.10519 Test: 0.29402
Epoch: 4000 Train: 0.08942 Test: 0.26663
Epoch: 4100 Train: 0.11741 Test: 0.30919
Epoch: 4200 Train: 0.21047 Test: 0.45417
Epoch: 4300 Train: 0.08122 Test: 0.26527
Epoch: 4400 Train: 0.08333 Test: 0.26946
Epoch: 4500 Train: 5.25604 Test: 3.51114
Epoch: 4600 Train: 0.07126 Test: 0.24842
Epoch: 4700 Train: 0.10994 Test: 0.26874
Epoch: 4800 Train: 0.07596 Test: 0.25127
Epoch: 4900 Train: 0.10687 Test: 0.28105
Epoch: 5000 Train: 0.13037 Test: 0.41403
Epoch: 5100 Train: 0.06076 Test: 0.23493
Epoch: 5200 Train: 0.06600 Test: 0.24593
Epoch: 5300 Train: 0.07129 Test: 0.25454
Epoch: 5400 Train: 0.84463 Test: 0.39118
Epoch: 5500 Train: 0.05572 Test: 0.23559
Epoch: 5600 Train: 0.13192 Test: 0.27427
Epoch: 5700 Train: 0.10116 Test: 0.27296
Epoch: 5800 Train: 0.05207 Test: 0.23346
Epoch: 5900 Train: 0.05193 Test: 0.23939
Epoch: 6000 Train: 2.39048 Test: 3.49384
Epoch: 6100 Train: 0.04833 Test: 0.23466
Epoch: 6200 Train: 0.05417 Test: 0.24724
Epoch: 6300 Train: 0.04654 Test: 0.22548
Epoch: 6400 Train: 0.04667 Test: 0.23675
Epoch: 6500 Train: 0.04333 Test: 0.22488
Epoch: 6600 Train: 0.05168 Test: 0.24514
Epoch: 6700 Train: 0.23623 Test: 0.40602
Epoch: 6800 Train: 0.04122 Test: 0.22616
Epoch: 6900 Train: 0.06298 Test: 0.25098
Epoch: 7000 Train: 0.03989 Test: 0.22455
Epoch: 7100 Train: 0.44040 Test: 0.26747
Epoch: 7200 Train: 0.03928 Test: 0.22875
Epoch: 7300 Train: 0.06291 Test: 0.27493
Epoch: 7400 Train: 0.03697 Test: 0.22354
Epoch: 7500 Train: 0.04133 Test: 0.24292
Epoch: 7600 Train: 0.03597 Test: 0.22254
Epoch: 7700 Train: 0.06307 Test: 0.23388
Epoch: 7800 Train: 0.04544 Test: 0.22671
Epoch: 7900 Train: 0.55871 Test: 0.91063
Epoch: 8000 Train: 0.03381 Test: 0.21993
Epoch: 8100 Train: 1.08678 Test: 0.67638
Epoch: 8200 Train: 0.03286 Test: 0.21883
Epoch: 8300 Train: 0.10333 Test: 0.23254
Epoch: 8400 Train: 0.28571 Test: 0.50966
Epoch: 8500 Train: 0.03117 Test: 0.21677
Epoch: 8600 Train: 0.06477 Test: 0.25810
Epoch: 8700 Train: 0.03067 Test: 0.21571
Epoch: 8800 Train: 0.02954 Test: 0.21229
Epoch: 8900 Train: 0.02944 Test: 0.21465
Epoch: 9000 Train: 0.04096 Test: 0.21989
Epoch: 9100 Train: 0.02823 Test: 0.21100
Epoch: 9200 Train: 0.03068 Test: 0.21153
Epoch: 9300 Train: 0.02756 Test: 0.20944
Epoch: 9400 Train: 0.32277 Test: 0.62595
Epoch: 9500 Train: 0.02715 Test: 0.20841
Epoch: 9600 Train: 0.05663 Test: 0.22075
Epoch: 9700 Train: 0.02593 Test: 0.20593
Epoch: 9800 Train: 0.02625 Test: 0.20794
Epoch: 9900 Train: 0.02525 Test: 0.20484
Epoch: 9999 Train: 0.02643 Test: 0.21200
Training Loss: tensor(0.0264)
Test Loss: tensor(0.2120)
Learned LE: [ 0.8588695  -0.05838606 -4.378219  ]
True LE: [ 8.8843775e-01  4.8421249e-03 -1.4563945e+01]
Relative Error: [2.251617   2.3461998  2.4023468  2.4049976  2.358405   2.2774107
 2.170444   2.0533133  1.9598609  1.9177344  1.9187272  1.9371083
 1.9513613  1.8865354  1.8144708  1.784488   1.7982291  1.8139634
 1.7940378  1.7512634  1.7544223  1.9124398  2.2170088  2.5244718
 2.7637222  2.921369   2.9872942  2.965337   2.8957112  2.8249032
 2.7583897  2.68567    2.6331656  2.6730087  2.846275   2.7998247
 2.678954   2.7111847  2.8210285  2.9286945  2.9163651  2.8025079
 2.688452   2.597139   2.4932373  2.3440056  2.1409516  1.8606796
 1.5771009  1.4053943  1.3342646  1.2892783  1.2305845  1.177952
 1.1528968  1.1639061  1.2103903  1.2911013  1.4034206  1.5345289
 1.664616   1.7794542  1.8765311  1.9582155  2.0165157  2.030885
 1.9937828  1.9219222  1.8291343  1.7278595  1.6508294  1.634271
 1.6743958  1.7378373  1.7946498  1.7258784  1.6301149  1.6080258
 1.6504371  1.6926172  1.6865386  1.6444107  1.6194701  1.6969278
 1.9374092  2.2270532  2.4672625  2.6457384  2.755223   2.7735174
 2.6985335  2.589682   2.5029182  2.4240432  2.34225    2.306741
 2.4091234  2.538948   2.3698735  2.3205268  2.3922951  2.5022068
 2.5003452  2.3718517  2.237637   2.1319985  2.030629   1.8997867
 1.7489363  1.5515442  1.3250825  1.1822886  1.1254992  1.0870068
 1.0199932  0.94272476 0.88978803 0.88653153 0.9296037  1.0028096
 1.1054716  1.2306365  1.3502557  1.442328   1.5080888  1.56161
 1.6092561  1.6326487  1.606227   1.5378358  1.4533144  1.3671591
 1.3072119  1.318549   1.4076042  1.5278054  1.6472975  1.613045
 1.4743811  1.4131476  1.4485012  1.5094888  1.5240344  1.4909236
 1.4562563  1.4730672  1.6235557  1.8835794  2.116061   2.2883906
 2.4120102  2.485479   2.474559   2.3616717  2.227677   2.13453
 2.044322   1.9587092  1.9526316  2.1191916  2.1203163  1.962417
 1.9708958  2.0657308  2.093917   1.9673712  1.8136554  1.6871101
 1.5817078  1.4698884  1.3597534  1.2530297  1.0988634  0.9813738
 0.93219674 0.8963204  0.8226919  0.7181143  0.626847   0.6073096
 0.6732082  0.7701752  0.87327987 0.98706204 1.0892103  1.1537707
 1.1838051  1.2000222  1.2207595  1.2427405  1.2313112  1.1653843
 1.0743855  0.9913625  0.9386232  0.9629018  1.0928264  1.2794952
 1.4724696  1.542513   1.3982338  1.2626318  1.2343869  1.2927585
 1.3328416  1.3118834  1.2668037  1.2559587  1.3158205  1.5250187
 1.7662607  1.9363596  2.0405025  2.1046252  2.1442902  2.1202126
 1.9837186  1.831113   1.7312007  1.6253747  1.5435622  1.5828112
 1.7956254  1.6897496  1.5948439  1.6323957  1.6920156  1.6087251
 1.4536408  1.3076037  1.1779764  1.0688192  0.9716648  0.9347339
 0.8819823  0.80203134 0.7608091  0.71919346 0.64184254 0.5156059
 0.3772852  0.32199714 0.43375683 0.601579   0.7420449  0.84663695
 0.9156876  0.94690865 0.94750684 0.93091744 0.9125077  0.9065509
 0.9032865  0.858545   0.76331186 0.67063665 0.62178975 0.644698
 0.7729004  0.98939806 1.2383362  1.4479417  1.3986359  1.2301272
 1.1069195  1.1197922  1.1847212  1.1667771  1.0849363  1.0282373
 1.0400864  1.1577011  1.4177355  1.6275864  1.736021   1.7687114
 1.7638633  1.7581456  1.7299215  1.5872421  1.4124801  1.3002263
 1.1936296  1.1526392  1.2521154  1.4910034  1.3483654  1.2816706
 1.304771   1.304599   1.2036104  1.0663531  0.90560055 0.7546795
 0.62043166 0.5716485  0.6257096  0.6316514  0.6222111  0.5831652
 0.4953521  0.3626012  0.1914799  0.07193648 0.2783054  0.5142881
 0.7196647  0.84327555 0.8693702  0.8453944  0.81790006 0.790306
 0.7491398  0.69344395 0.6441408  0.61334366 0.563965   0.49634668
 0.4759229  0.5200841  0.61761165 0.77438927 0.989675   1.2578924
 1.4032676  1.2898853  1.130748   1.0803363  1.1788169  1.2073399
 1.0557394  0.86982596 0.79312354 0.8358391  1.0462102  1.3485972
 1.5211674  1.5635481  1.5178598  1.4318928  1.3566697  1.3113405
 1.187871   0.9828811  0.88328356 0.8550651  0.8847598  0.999385
 1.2621169  1.1259699  1.0326364  1.0331808  1.0417864  1.0011325
 0.85359514 0.6689073  0.46816772 0.26529938 0.2720248  0.4005148
 0.468519   0.49985555 0.42749453 0.30068263 0.18539286 0.15986368
 0.3022405  0.51438373 0.7538364  0.9331111  0.95943594 0.87258023
 0.781833   0.73773754 0.7172544  0.67718744 0.5905655  0.4686584
 0.38744032 0.4083653  0.5010728  0.6189259  0.7174007  0.79652894
 0.890412   1.0436884  1.295818   1.375989   1.2454515  1.1340811
 1.2356378  1.4334954  1.3952103  1.074553   0.75561076 0.6360346
 0.71652925 1.0235249  1.3565853  1.501227  ]
