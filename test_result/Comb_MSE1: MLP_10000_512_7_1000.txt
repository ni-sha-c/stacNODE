time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 0
batch_size: None
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.99%, model saved.
Epoch: 0 Train: 3888.77026 Test: 4015.55786
Epoch 100: New minimal relative error: 33.20%, model saved.
Epoch: 100 Train: 95.86976 Test: 125.29073
Epoch: 200 Train: 60.26135 Test: 84.45680
Epoch: 300 Train: 9.82499 Test: 19.97770
Epoch: 400 Train: 11.87268 Test: 12.21382
Epoch 500: New minimal relative error: 18.93%, model saved.
Epoch: 500 Train: 14.03561 Test: 23.06652
Epoch: 600 Train: 11.25205 Test: 20.15225
Epoch 700: New minimal relative error: 15.03%, model saved.
Epoch: 700 Train: 4.36887 Test: 8.26390
Epoch: 800 Train: 7.84944 Test: 15.93260
Epoch: 900 Train: 9.78538 Test: 15.66543
Epoch: 1000 Train: 1.38363 Test: 6.69844
Epoch: 1100 Train: 1.34039 Test: 6.60672
Epoch 1200: New minimal relative error: 11.65%, model saved.
Epoch: 1200 Train: 5.53561 Test: 12.49083
Epoch 1300: New minimal relative error: 11.59%, model saved.
Epoch: 1300 Train: 3.14031 Test: 10.24679
Epoch: 1400 Train: 1.24305 Test: 6.54029
Epoch 1500: New minimal relative error: 6.39%, model saved.
Epoch: 1500 Train: 0.34045 Test: 6.92620
Epoch: 1600 Train: 0.69050 Test: 7.57204
Epoch: 1700 Train: 0.44212 Test: 7.64913
Epoch: 1800 Train: 1.30430 Test: 8.89700
Epoch: 1900 Train: 3.49795 Test: 9.66489
Epoch: 2000 Train: 2.71865 Test: 11.28870
Epoch: 2100 Train: 0.98069 Test: 9.66399
Epoch: 2200 Train: 0.44236 Test: 9.60136
Epoch: 2300 Train: 4.68050 Test: 13.88081
Epoch: 2400 Train: 0.14772 Test: 10.05901
Epoch: 2500 Train: 0.60921 Test: 11.22518
Epoch: 2600 Train: 0.55823 Test: 11.26110
Epoch: 2700 Train: 1.85812 Test: 14.74513
Epoch 2800: New minimal relative error: 6.23%, model saved.
Epoch: 2800 Train: 0.61419 Test: 12.12795
Epoch 2900: New minimal relative error: 4.89%, model saved.
Epoch: 2900 Train: 0.17931 Test: 11.98592
Epoch 3000: New minimal relative error: 4.19%, model saved.
Epoch: 3000 Train: 0.26486 Test: 13.31019
Epoch: 3100 Train: 0.52343 Test: 12.79201
Epoch: 3200 Train: 0.09810 Test: 13.08359
Epoch: 3300 Train: 2.23318 Test: 15.19476
Epoch 3400: New minimal relative error: 3.03%, model saved.
Epoch: 3400 Train: 0.21022 Test: 14.95062
Epoch: 3500 Train: 0.81619 Test: 14.63603
Epoch: 3600 Train: 1.36787 Test: 16.28681
Epoch: 3700 Train: 0.10132 Test: 15.84163
Epoch: 3800 Train: 0.53972 Test: 16.67182
Epoch: 3900 Train: 0.57585 Test: 17.25056
Epoch: 4000 Train: 1.54577 Test: 18.83503
Epoch: 4100 Train: 0.26534 Test: 18.49972
Epoch: 4200 Train: 2.33406 Test: 20.10044
Epoch: 4300 Train: 3.17722 Test: 22.68425
Epoch 4400: New minimal relative error: 2.33%, model saved.
Epoch: 4400 Train: 0.04699 Test: 20.07338
Epoch: 4500 Train: 0.62539 Test: 21.02284
Epoch: 4600 Train: 0.19589 Test: 21.86193
Epoch: 4700 Train: 0.03180 Test: 21.89663
Epoch: 4800 Train: 0.24081 Test: 22.58660
Epoch: 4900 Train: 0.15956 Test: 23.08559
Epoch: 5000 Train: 0.18469 Test: 23.96181
Epoch: 5100 Train: 0.45277 Test: 24.85262
Epoch: 5200 Train: 2.23843 Test: 26.10298
Epoch: 5300 Train: 0.73412 Test: 26.63824
Epoch: 5400 Train: 0.24336 Test: 26.12844
Epoch: 5500 Train: 0.22860 Test: 27.16051
Epoch: 5600 Train: 0.28195 Test: 28.08970
Epoch: 5700 Train: 0.09333 Test: 28.63563
Epoch: 5800 Train: 0.03044 Test: 29.08841
Epoch: 5900 Train: 0.62662 Test: 30.58850
Epoch: 6000 Train: 0.65560 Test: 31.44266
Epoch: 6100 Train: 0.12435 Test: 31.06841
Epoch: 6200 Train: 0.21326 Test: 32.39010
Epoch: 6300 Train: 0.48850 Test: 32.65397
Epoch: 6400 Train: 0.28798 Test: 33.30204
Epoch: 6500 Train: 0.13145 Test: 34.06783
Epoch: 6600 Train: 0.02202 Test: 34.19226
Epoch: 6700 Train: 0.04750 Test: 35.16796
Epoch: 6800 Train: 0.03248 Test: 35.52692
Epoch: 6900 Train: 0.52928 Test: 36.69785
Epoch: 7000 Train: 0.12191 Test: 36.56770
Epoch 7100: New minimal relative error: 1.96%, model saved.
Epoch: 7100 Train: 0.04989 Test: 37.08563
Epoch: 7200 Train: 0.07960 Test: 37.60502
Epoch: 7300 Train: 0.03895 Test: 38.81047
Epoch: 7400 Train: 0.07899 Test: 38.81202
Epoch: 7500 Train: 0.46518 Test: 40.83531
Epoch: 7600 Train: 0.01671 Test: 40.03823
Epoch: 7700 Train: 0.04828 Test: 40.85060
Epoch: 7800 Train: 0.34451 Test: 40.81501
Epoch: 7900 Train: 0.09820 Test: 42.24017
Epoch: 8000 Train: 0.04573 Test: 42.50094
Epoch: 8100 Train: 0.48020 Test: 43.23047
Epoch: 8200 Train: 0.02021 Test: 43.31949
Epoch: 8300 Train: 0.06340 Test: 43.55385
Epoch: 8400 Train: 0.03149 Test: 44.53440
Epoch 8500: New minimal relative error: 1.50%, model saved.
Epoch: 8500 Train: 0.01756 Test: 46.19322
Epoch: 8600 Train: 0.02066 Test: 46.73105
Epoch: 8700 Train: 0.01197 Test: 47.45588
Epoch: 8800 Train: 0.02150 Test: 47.98888
Epoch: 8900 Train: 0.28453 Test: 48.25801
Epoch: 9000 Train: 0.01993 Test: 49.08222
Epoch: 9100 Train: 0.01104 Test: 49.03241
Epoch: 9200 Train: 0.01116 Test: 50.21075
Epoch: 9300 Train: 0.01044 Test: 50.20951
Epoch: 9400 Train: 0.36081 Test: 50.07020
Epoch: 9500 Train: 0.00997 Test: 50.90051
Epoch: 9600 Train: 0.00965 Test: 51.95791
Epoch: 9700 Train: 0.16801 Test: 50.88720
Epoch 9800: New minimal relative error: 1.24%, model saved.
Epoch: 9800 Train: 0.01040 Test: 52.49911
Epoch: 9900 Train: 0.01501 Test: 53.36828
Epoch: 9999 Train: 0.00902 Test: 53.33839
Training Loss: tensor(0.0090)
Test Loss: tensor(53.3384)
Learned LE: [ 0.9288478  -0.09997512 -5.3061285 ]
True LE: [ 8.6393088e-01  2.2896680e-03 -1.4559063e+01]
Relative Error: [0.36957026 0.40490186 0.39732397 0.27790615 0.22687048 0.28222698
 0.26722628 0.24100024 0.24675234 0.23432486 0.18803048 0.137093
 0.13372694 0.17295283 0.21886039 0.27605355 0.340485   0.3711828
 0.3436872  0.28211358 0.22886272 0.20594257 0.20260063 0.19033068
 0.1668129  0.19171037 0.2807706  0.3578298  0.4058352  0.47902235
 0.5444586  0.5066304  0.42206287 0.32228854 0.18032557 0.11682589
 0.10894468 0.03505192 0.10893758 0.1969672  0.23868778 0.24996437
 0.2612682  0.29096797 0.34384266 0.38483852 0.34903914 0.24321999
 0.15103655 0.09902765 0.11945924 0.23323965 0.31868485 0.30742225
 0.28008062 0.3030667  0.36002377 0.41067415 0.3521364  0.28835016
 0.32144392 0.39044    0.41793948 0.4229206  0.4011092  0.2845116
 0.19802952 0.29322985 0.31290805 0.29313025 0.29839092 0.28849816
 0.24349292 0.18073307 0.13923053 0.14756054 0.18617329 0.24776089
 0.32756707 0.38155    0.36877006 0.30221173 0.23419823 0.19795977
 0.19156553 0.19162907 0.18209186 0.19418441 0.26272956 0.3537696
 0.428932   0.49689323 0.5467699  0.519994   0.43190256 0.3441144
 0.20487773 0.10381231 0.1023027  0.03178664 0.09940597 0.20902638
 0.2745427  0.29004255 0.28065023 0.28780246 0.33156732 0.39261127
 0.40440568 0.3300495  0.22657534 0.16098084 0.16014092 0.24915281
 0.35998857 0.3909198  0.33804294 0.32778332 0.3504159  0.36049914
 0.3526517  0.3090778  0.34773344 0.42581847 0.4555393  0.43572196
 0.3890794  0.28451902 0.12668815 0.22529049 0.30598575 0.31963158
 0.34170026 0.3354735  0.2929823  0.23096824 0.16761522 0.12878369
 0.12874681 0.17156938 0.25987157 0.3506811  0.380984   0.33024415
 0.24650511 0.1909922  0.17628926 0.18343408 0.18605325 0.18912406
 0.22675169 0.30673352 0.40652183 0.49847817 0.5571145  0.5393844
 0.4495488  0.36379293 0.25157243 0.12236039 0.10023823 0.05643006
 0.0595491  0.1772578  0.27113384 0.3186716  0.31466395 0.29366022
 0.30200115 0.34946504 0.39083028 0.36056685 0.27190682 0.19603679
 0.17157374 0.22706833 0.33879414 0.41862205 0.3926775  0.33448148
 0.34034258 0.30676037 0.26255542 0.2800955  0.34817415 0.42432788
 0.46179202 0.43210572 0.3672235  0.2779344  0.1098865  0.07610068
 0.2244715  0.29334334 0.34821868 0.34292144 0.29892725 0.2567563
 0.20947842 0.15698764 0.11694863 0.10095828 0.14454634 0.24523185
 0.33247203 0.3431715  0.27477497 0.19382869 0.15903877 0.16536051
 0.17871025 0.18215345 0.19217215 0.23317398 0.3249578  0.44550765
 0.54361975 0.56425744 0.48377463 0.37855744 0.2923706  0.17953844
 0.10596005 0.08657048 0.0333309  0.11745106 0.21770717 0.2945381
 0.32687843 0.30954754 0.28220633 0.28899685 0.3274075  0.3392393
 0.28032994 0.2019688  0.15666881 0.17524764 0.2637257  0.37650818
 0.41893306 0.35327208 0.31873915 0.31038794 0.20057745 0.15967432
 0.2995043  0.3901095  0.4253353  0.39837077 0.32879174 0.25605324
 0.16438948 0.14168403 0.06430353 0.19736895 0.30447924 0.31355968
 0.2554467  0.2252712  0.22019145 0.19474876 0.15512314 0.12243578
 0.10080704 0.12129336 0.20463529 0.27904272 0.28341794 0.2208756
 0.15392387 0.14185363 0.1610531  0.17517094 0.17587365 0.17832226
 0.2108522  0.31868622 0.45545167 0.5471235  0.53043234 0.4166246
 0.3093411  0.22989047 0.1468679  0.10810433 0.07284243 0.0727765
 0.15099722 0.22514918 0.27890673 0.2988547  0.27305314 0.23899868
 0.23581569 0.26176703 0.25681174 0.19392957 0.14146285 0.11976238
 0.16233855 0.2549388  0.36149645 0.38230342 0.2995547  0.28707913
 0.25966927 0.15154053 0.22726604 0.32740554 0.3647127  0.33742425
 0.26247153 0.20041929 0.17260912 0.20192447 0.2173569  0.04387961
 0.20275758 0.26633286 0.21719527 0.15526241 0.15901707 0.17968345
 0.17237227 0.15015094 0.13110915 0.1132819  0.10254352 0.1508985
 0.21194297 0.22116593 0.17888916 0.13209638 0.13613832 0.16212302
 0.17783293 0.17240424 0.15667741 0.16631941 0.27676356 0.42084983
 0.5078036  0.4785877  0.35690042 0.2425712  0.18214698 0.13423276
 0.10574196 0.08440576 0.11303893 0.17304726 0.21503146 0.23689829
 0.2424077  0.20881969 0.15954392 0.13685556 0.15525688 0.15741903
 0.11180741 0.10491746 0.10919087 0.13475882 0.20166053 0.29812315
 0.32841346 0.24797899 0.2243494  0.23077242 0.22188272 0.35154188
 0.3203474  0.2855704  0.21316108 0.1045119  0.09757891 0.15664878
 0.29223672 0.27628687 0.0922846  0.17871627 0.20107806 0.15410306
 0.09509787 0.10424116 0.12409555 0.13318576 0.13853614 0.13626932
 0.12226622 0.09123763 0.09791576 0.15616055 0.1827543  0.155568
 0.12491443 0.13443016 0.16665375 0.19040371]
