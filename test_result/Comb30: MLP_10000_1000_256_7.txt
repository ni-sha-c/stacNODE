time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.02%, model saved.
Epoch: 0 Train: 3973.65747 Test: 4062.52197
Epoch: 100 Train: 131.71835 Test: 140.89165
Epoch 200: New minimal relative error: 34.22%, model saved.
Epoch: 200 Train: 41.29538 Test: 39.97921
Epoch 300: New minimal relative error: 33.13%, model saved.
Epoch: 300 Train: 11.61783 Test: 15.29572
Epoch: 400 Train: 5.50748 Test: 9.71139
Epoch 500: New minimal relative error: 23.90%, model saved.
Epoch: 500 Train: 2.66088 Test: 6.90711
Epoch 600: New minimal relative error: 21.91%, model saved.
Epoch: 600 Train: 7.68850 Test: 12.39209
Epoch: 700 Train: 5.68384 Test: 7.41281
Epoch 800: New minimal relative error: 17.74%, model saved.
Epoch: 800 Train: 13.03187 Test: 12.47236
Epoch: 900 Train: 3.14150 Test: 7.17574
Epoch: 1000 Train: 30.99002 Test: 40.02273
Epoch: 1100 Train: 2.80637 Test: 9.14725
Epoch: 1200 Train: 0.63567 Test: 5.13577
Epoch: 1300 Train: 0.33750 Test: 4.63804
Epoch: 1400 Train: 9.27184 Test: 10.41681
Epoch 1500: New minimal relative error: 13.92%, model saved.
Epoch: 1500 Train: 0.64713 Test: 5.14807
Epoch: 1600 Train: 2.67000 Test: 8.03782
Epoch: 1700 Train: 0.29553 Test: 5.00446
Epoch: 1800 Train: 4.74794 Test: 8.45740
Epoch 1900: New minimal relative error: 7.08%, model saved.
Epoch: 1900 Train: 0.21198 Test: 5.34897
Epoch: 2000 Train: 11.56144 Test: 16.26957
Epoch: 2100 Train: 0.18429 Test: 5.45332
Epoch: 2200 Train: 1.14482 Test: 6.14214
Epoch: 2300 Train: 0.17588 Test: 5.65378
Epoch: 2400 Train: 1.37624 Test: 7.07886
Epoch: 2500 Train: 0.18481 Test: 5.81882
Epoch: 2600 Train: 8.00332 Test: 16.87240
Epoch: 2700 Train: 0.39609 Test: 6.17834
Epoch: 2800 Train: 0.97960 Test: 7.60871
Epoch: 2900 Train: 0.30201 Test: 6.54767
Epoch: 3000 Train: 0.09737 Test: 6.46821
Epoch: 3100 Train: 0.19441 Test: 6.62660
Epoch: 3200 Train: 2.15723 Test: 9.09620
Epoch: 3300 Train: 0.40090 Test: 7.15148
Epoch: 3400 Train: 0.09302 Test: 7.66902
Epoch: 3500 Train: 0.53733 Test: 8.43090
Epoch: 3600 Train: 0.77252 Test: 8.61125
Epoch: 3700 Train: 0.84728 Test: 8.51437
Epoch: 3800 Train: 0.16042 Test: 8.42991
Epoch: 3900 Train: 0.36549 Test: 8.32951
Epoch: 4000 Train: 0.40064 Test: 8.65118
Epoch: 4100 Train: 0.06476 Test: 8.76867
Epoch: 4200 Train: 0.09411 Test: 8.74395
Epoch: 4300 Train: 0.05808 Test: 9.04587
Epoch: 4400 Train: 0.09502 Test: 9.43780
Epoch: 4500 Train: 0.05046 Test: 9.41779
Epoch: 4600 Train: 0.14092 Test: 9.86693
Epoch: 4700 Train: 1.93451 Test: 11.81982
Epoch: 4800 Train: 0.08168 Test: 10.52338
Epoch: 4900 Train: 0.14411 Test: 10.65771
Epoch: 5000 Train: 0.41201 Test: 11.19424
Epoch: 5100 Train: 3.56632 Test: 14.47213
Epoch: 5200 Train: 0.04470 Test: 11.33393
Epoch: 5300 Train: 0.12933 Test: 11.97243
Epoch: 5400 Train: 0.68809 Test: 11.95914
Epoch: 5500 Train: 2.04715 Test: 13.23554
Epoch: 5600 Train: 0.20147 Test: 12.72023
Epoch: 5700 Train: 0.59730 Test: 13.34314
Epoch: 5800 Train: 0.51167 Test: 13.09278
Epoch: 5900 Train: 1.37581 Test: 14.01077
Epoch: 6000 Train: 0.70643 Test: 14.24638
Epoch: 6100 Train: 0.11647 Test: 13.80927
Epoch: 6200 Train: 0.03022 Test: 14.03798
Epoch: 6300 Train: 1.11502 Test: 15.13705
Epoch: 6400 Train: 0.07438 Test: 14.43718
Epoch: 6500 Train: 1.85305 Test: 16.48450
Epoch: 6600 Train: 1.49056 Test: 16.65653
Epoch: 6700 Train: 0.31822 Test: 15.25449
Epoch: 6800 Train: 0.15983 Test: 15.57491
Epoch: 6900 Train: 0.58159 Test: 16.04881
Epoch: 7000 Train: 0.02457 Test: 15.80616
Epoch: 7100 Train: 0.02938 Test: 16.16664
Epoch: 7200 Train: 0.36845 Test: 16.43623
Epoch: 7300 Train: 0.04607 Test: 16.37789
Epoch: 7400 Train: 0.02153 Test: 16.57084
Epoch: 7500 Train: 0.02581 Test: 16.86699
Epoch: 7600 Train: 0.02703 Test: 16.93523
Epoch: 7700 Train: 0.02421 Test: 17.16231
Epoch: 7800 Train: 0.10997 Test: 17.39612
Epoch: 7900 Train: 0.01825 Test: 17.58978
Epoch: 8000 Train: 0.01819 Test: 17.55882
Epoch: 8100 Train: 0.01956 Test: 17.87206
Epoch: 8200 Train: 0.01726 Test: 18.12885
Epoch: 8300 Train: 0.23379 Test: 18.19788
Epoch: 8400 Train: 0.01683 Test: 18.38210
Epoch: 8500 Train: 0.02493 Test: 18.42452
Epoch: 8600 Train: 0.03545 Test: 18.45398
Epoch: 8700 Train: 0.01581 Test: 18.71912
Epoch: 8800 Train: 0.39302 Test: 19.17636
Epoch: 8900 Train: 0.01537 Test: 19.04784
Epoch: 9000 Train: 0.03690 Test: 19.08288
Epoch: 9100 Train: 0.78638 Test: 19.38087
Epoch: 9200 Train: 0.01472 Test: 19.33217
Epoch: 9300 Train: 0.01413 Test: 19.66166
Epoch: 9400 Train: 0.01513 Test: 19.74965
Epoch: 9500 Train: 0.01408 Test: 20.04640
Epoch: 9600 Train: 0.01385 Test: 20.12053
Epoch: 9700 Train: 0.02678 Test: 20.14816
Epoch: 9800 Train: 0.01332 Test: 20.40229
Epoch: 9900 Train: 0.01285 Test: 20.66030
Epoch: 9999 Train: 0.01351 Test: 20.57878
Training Loss: tensor(0.0135)
Test Loss: tensor(20.5788)
Learned LE: [ 0.87133014 -0.0668172  -4.1896496 ]
True LE: [ 8.6650902e-01  3.6787475e-03 -1.4544280e+01]
Relative Error: [1.4731418  1.9552473  2.3707585  2.664407   2.8519952  2.9604707
 3.0078263  3.026657   3.0493968  3.0940306  3.1919713  3.4155157
 3.8206987  4.3653955  4.931567   5.3941836  5.6827297  5.7820015
 5.703573   5.4549236  5.065041   4.628464   4.354759   3.883008
 3.4350142  3.159514   2.9646614  2.8136666  2.6949818  2.6116047
 2.5725381  2.5892756  2.6654203  2.7796485  2.872653   2.8839922
 2.8083813  2.67179    2.4637322  2.1638045  1.7983648  1.9284241
 2.5019717  2.8719392  2.964561   2.937166   2.7124472  2.3703957
 3.3745542  4.125516   4.3868766  4.1439166  3.6254709  2.950312
 2.224387   1.5708963  1.0717     0.81248796 0.74949133 0.78422356
 0.91731346 1.1842623  1.5618262  2.0075824  2.4486604  2.7886825
 2.9882953  3.080492   3.1029644  3.0707095  3.0062234  2.9330914
 2.8675427  2.844794   2.942671   3.2202249  3.6373637  4.0715933
 4.4160647  4.634743   4.726972   4.6697674  4.430147   4.04581
 3.664305   3.4974759  2.8768785  2.4690914  2.2118435  2.043555
 1.9257227  1.8465245  1.8066311  1.8077128  1.8527987  1.9439347
 2.0755908  2.198663   2.238528   2.1837656  2.0694358  1.8702401
 1.5470166  1.2447512  1.6256444  2.2176404  2.4462762  2.562741
 2.5855896  2.3832054  2.0686185  2.9522965  3.5979428  3.6940322
 3.330359   2.7817     2.127167   1.4714702  0.9310538  0.59123963
 0.5905102  0.7414332  0.9529074  1.2459209  1.5893247  1.9439787
 2.320814   2.6945229  2.9793482  3.1246963  3.160413   3.1337512
 3.061062   2.952995   2.8204632  2.6730542  2.5423002  2.5013638
 2.6211052  2.8795986  3.1598933  3.3832145  3.5544553  3.6745205
 3.6715283  3.4756403  3.134321   2.8400185  2.7588878  2.0651157
 1.6554605  1.4030219  1.2490109  1.1458653  1.090466   1.0957507
 1.1443366  1.2145994  1.2959982  1.4181756  1.5689664  1.6610152
 1.6467832  1.5606583  1.4012738  1.0814062  0.80841    1.2784919
 1.8136623  2.014293   2.2008138  2.2680483  2.1126533  1.7927585
 2.4012916  2.9739554  3.0047028  2.591991   2.0609384  1.4812378
 0.91623497 0.49003357 0.3519011  0.5709992  0.83324677 1.1094323
 1.4096112  1.7207723  2.0030818  2.279004   2.5796988  2.8517213
 3.0140061  3.055663   3.0196865  2.9371912  2.822608   2.6806264
 2.5044265  2.3083634  2.1505198  2.1135983  2.202814   2.3047318
 2.3657877  2.4560554  2.5958047  2.6757658  2.57339    2.3223064
 2.115376   2.2636888  1.548617   1.1176353  0.8763836  0.7232416
 0.57967854 0.45725638 0.48636183 0.61905307 0.7469604  0.81403285
 0.88376135 1.0129155  1.1436031  1.1850696  1.1412456  1.0557476
 0.82493293 0.45215514 0.82019734 1.3343673  1.5994363  1.8427911
 1.9444827  1.8532006  1.663722   1.7470479  2.2991936  2.40366
 2.017632   1.5048765  1.0219468  0.5822016  0.24535862 0.22653313
 0.48026118 0.77656215 1.0342884  1.2507479  1.4831108  1.6962694
 1.8683599  2.07268    2.340994   2.5896077  2.7329953  2.761022
 2.7022595  2.5889835  2.4559765  2.3055694  2.117895   1.9078857
 1.7564205  1.7221627  1.6925945  1.5467795  1.4268142  1.4863259
 1.6269809  1.6636165  1.549086   1.4505153  1.7909849  1.5362674
 1.027618   0.8605399  0.78921753 0.6738631  0.45399094 0.21257459
 0.28599507 0.47379494 0.57817775 0.5873678  0.6641549  0.77386695
 0.82471436 0.80558974 0.7822087  0.71823364 0.39595538 0.39669004
 0.77616435 1.1218276  1.4486148  1.6045462  1.5594734  1.4977154
 1.3779749  1.5998962  1.8849952  1.6791254  1.1957067  0.760022
 0.4485112  0.21800101 0.19238102 0.3490294  0.58606064 0.7661565
 0.8725888  0.97787285 1.1182137  1.2341808  1.3451167  1.5368824
 1.8246256  2.1069095  2.2962928  2.3687017  2.3241296  2.186192
 2.0235918  1.8744797  1.710032   1.5175997  1.3921493  1.3698368
 1.2275525  0.8474241  0.545366   0.5579689  0.6789906  0.7174882
 0.7422588  1.0410426  1.7864506  1.3744045  1.1408608  1.136567
 1.150076   1.0602802  0.84449995 0.5996631  0.50477606 0.56662744
 0.60225564 0.5343379  0.5843337  0.69734037 0.6912688  0.62528116
 0.60443825 0.5594256  0.29601315 0.4132799  0.572079   0.9281789
 1.2017124  1.2694508  1.1837565  1.2404585  1.2138025  1.3138589
 1.4613835  1.1895148  0.7763438  0.4664773  0.28379068 0.22805545
 0.3311402  0.48373392 0.521431   0.5205563  0.52335423 0.5586201
 0.6226382  0.6949479  0.7803117  0.93433577 1.2003657  1.4951625
 1.7278509  1.8727703  1.9128886  1.8196665  1.6285222  1.4433696
 1.3051671  1.1611149  1.0502361  1.0691451  0.9905299  0.6097632
 0.31535524 0.4195183  0.43085468 0.4036    ]
