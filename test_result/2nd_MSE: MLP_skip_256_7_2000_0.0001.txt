time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.17%, model saved.
Epoch: 0 Train: 4008.67749 Test: 4089.01611
Epoch: 100 Train: 265.33722 Test: 248.33040
Epoch 200: New minimal relative error: 84.98%, model saved.
Epoch: 200 Train: 77.74220 Test: 53.09282
Epoch 300: New minimal relative error: 29.90%, model saved.
Epoch: 300 Train: 26.14190 Test: 30.18865
Epoch 400: New minimal relative error: 27.00%, model saved.
Epoch: 400 Train: 11.72018 Test: 14.63503
Epoch 500: New minimal relative error: 14.38%, model saved.
Epoch: 500 Train: 6.65562 Test: 8.90753
Epoch: 600 Train: 10.19269 Test: 12.61246
Epoch: 700 Train: 15.88041 Test: 17.64398
Epoch: 800 Train: 13.69357 Test: 14.48818
Epoch: 900 Train: 10.79777 Test: 8.49632
Epoch: 1000 Train: 5.86978 Test: 7.89847
Epoch: 1100 Train: 6.43754 Test: 6.71327
Epoch: 1200 Train: 4.27254 Test: 4.76328
Epoch: 1300 Train: 6.72744 Test: 5.72723
Epoch: 1400 Train: 3.39076 Test: 4.00732
Epoch: 1500 Train: 3.55905 Test: 4.01383
Epoch: 1600 Train: 2.84343 Test: 3.21397
Epoch: 1700 Train: 3.58491 Test: 4.67724
Epoch: 1800 Train: 2.12168 Test: 2.57398
Epoch 1900: New minimal relative error: 11.13%, model saved.
Epoch: 1900 Train: 5.10621 Test: 6.49748
Epoch: 2000 Train: 1.64941 Test: 2.36784
Epoch 2100: New minimal relative error: 10.24%, model saved.
Epoch: 2100 Train: 1.98015 Test: 2.01864
Epoch: 2200 Train: 1.76974 Test: 2.16300
Epoch: 2300 Train: 3.34928 Test: 5.15315
Epoch: 2400 Train: 1.69483 Test: 2.11985
Epoch: 2500 Train: 1.13411 Test: 1.55617
Epoch: 2600 Train: 1.88163 Test: 2.91461
Epoch 2700: New minimal relative error: 9.92%, model saved.
Epoch: 2700 Train: 1.49974 Test: 2.20427
Epoch: 2800 Train: 1.03101 Test: 2.61532
Epoch: 2900 Train: 0.89417 Test: 1.83462
Epoch: 3000 Train: 0.63327 Test: 1.02852
Epoch: 3100 Train: 0.61556 Test: 0.98862
Epoch: 3200 Train: 0.60267 Test: 1.02942
Epoch: 3300 Train: 0.74332 Test: 1.14234
Epoch: 3400 Train: 0.50869 Test: 0.85599
Epoch: 3500 Train: 0.51120 Test: 0.84077
Epoch 3600: New minimal relative error: 9.55%, model saved.
Epoch: 3600 Train: 0.49005 Test: 0.80010
Epoch: 3700 Train: 1.26913 Test: 1.94901
Epoch: 3800 Train: 0.66760 Test: 0.94053
Epoch: 3900 Train: 0.47497 Test: 0.78557
Epoch: 4000 Train: 1.13799 Test: 1.93182
Epoch: 4100 Train: 1.39186 Test: 1.48335
Epoch: 4200 Train: 10.26474 Test: 6.26834
Epoch: 4300 Train: 0.37840 Test: 0.67426
Epoch: 4400 Train: 1.63492 Test: 1.92805
Epoch: 4500 Train: 0.47159 Test: 0.84770
Epoch: 4600 Train: 0.57841 Test: 0.73996
Epoch: 4700 Train: 1.11819 Test: 1.19454
Epoch 4800: New minimal relative error: 6.80%, model saved.
Epoch: 4800 Train: 0.36395 Test: 0.66284
Epoch: 4900 Train: 0.36352 Test: 0.60385
Epoch: 5000 Train: 0.45656 Test: 0.68899
Epoch: 5100 Train: 0.29560 Test: 0.55238
Epoch: 5200 Train: 0.27779 Test: 0.53865
Epoch: 5300 Train: 0.32595 Test: 0.61963
Epoch: 5400 Train: 0.39143 Test: 0.64919
Epoch: 5500 Train: 0.25865 Test: 0.49778
Epoch: 5600 Train: 0.31575 Test: 0.93884
Epoch: 5700 Train: 0.24555 Test: 0.48330
Epoch: 5800 Train: 0.24128 Test: 0.48643
Epoch: 5900 Train: 0.53909 Test: 0.76836
Epoch: 6000 Train: 0.23747 Test: 0.50978
Epoch: 6100 Train: 0.98576 Test: 0.82290
Epoch: 6200 Train: 0.22428 Test: 0.44382
Epoch: 6300 Train: 0.21937 Test: 0.44618
Epoch: 6400 Train: 0.38825 Test: 0.54939
Epoch: 6500 Train: 0.22588 Test: 0.44244
Epoch: 6600 Train: 0.20600 Test: 0.43188
Epoch: 6700 Train: 0.21438 Test: 0.44319
Epoch: 6800 Train: 0.23570 Test: 0.43526
Epoch: 6900 Train: 0.23177 Test: 0.44207
Epoch: 7000 Train: 2.36728 Test: 3.12839
Epoch: 7100 Train: 0.18913 Test: 0.40228
Epoch: 7200 Train: 0.19482 Test: 0.40155
Epoch: 7300 Train: 0.18473 Test: 0.39999
Epoch: 7400 Train: 0.57344 Test: 0.73848
Epoch: 7500 Train: 0.17919 Test: 0.38636
Epoch: 7600 Train: 0.23402 Test: 0.53944
Epoch: 7700 Train: 1.24436 Test: 1.64147
Epoch: 7800 Train: 0.17031 Test: 0.37121
Epoch: 7900 Train: 0.17245 Test: 0.37418
Epoch: 8000 Train: 0.17627 Test: 0.38280
Epoch: 8100 Train: 0.20718 Test: 0.39053
Epoch: 8200 Train: 0.16036 Test: 0.35786
Epoch: 8300 Train: 0.28632 Test: 0.40862
Epoch: 8400 Train: 0.15606 Test: 0.35404
Epoch: 8500 Train: 0.15701 Test: 0.35378
Epoch: 8600 Train: 0.18272 Test: 0.37191
Epoch: 8700 Train: 0.14972 Test: 0.34049
Epoch: 8800 Train: 0.14901 Test: 0.34582
Epoch: 8900 Train: 0.18741 Test: 0.36651
Epoch: 9000 Train: 0.14799 Test: 0.34306
Epoch: 9100 Train: 0.14187 Test: 0.33065
Epoch: 9200 Train: 0.14402 Test: 0.33898
Epoch: 9300 Train: 0.14754 Test: 0.33767
Epoch: 9400 Train: 0.13690 Test: 0.32428
Epoch: 9500 Train: 0.16874 Test: 0.36967
Epoch: 9600 Train: 0.13348 Test: 0.31970
Epoch: 9700 Train: 0.21051 Test: 0.33849
Epoch: 9800 Train: 0.13042 Test: 0.31532
Epoch: 9900 Train: 0.13058 Test: 0.32058
Epoch: 9999 Train: 0.13563 Test: 0.31807
Training Loss: tensor(0.1356)
Test Loss: tensor(0.3181)
Learned LE: [ 0.66713953 -0.02304703 -2.1918092 ]
True LE: [ 8.6302507e-01  7.0090261e-03 -1.4542902e+01]
Relative Error: [3.2712207  2.6387625  2.1462588  1.7185516  1.5811095  1.5426229
 1.2311375  0.8084908  0.24203935 0.9149842  1.5481207  2.5231538
 3.5712323  4.5113435  5.71455    6.2177     6.3810096  5.774563
 5.2984595  4.986859   4.5632052  4.2515244  3.6983435  3.2377489
 3.052259   2.7346115  2.7439883  2.7118244  2.6050768  2.5968876
 2.6592207  2.4410415  2.7845511  3.9020114  4.4742665  4.67091
 4.8653264  4.7468734  4.5248094  4.4058557  4.126057   3.6888518
 3.3914843  2.9952762  2.710447   2.8382704  3.0280807  2.512724
 2.3743565  2.1595352  1.9545157  1.9065467  2.1292157  2.1475873
 2.2658572  2.3738444  2.7591274  3.5202835  4.2633123  5.2130895
 4.518293   3.872247   3.367185   2.8424258  2.3430548  1.9829152
 1.7211003  1.6426859  1.6981943  1.4077752  0.96533436 0.33405375
 0.93652076 1.5196688  2.2855582  3.423771   4.6496887  5.786223
 6.3190646  5.6926894  5.4893265  5.112327   4.7908273  4.3135657
 3.9666665  3.5997794  3.164281   2.947453   2.7194164  2.7263584
 2.5469298  2.3204029  2.2951577  2.678711   2.4555323  2.6863017
 3.7889473  4.146491   4.5347986  4.2788563  3.9720345  3.693458
 3.4305227  3.264685   2.9865124  2.5994449  2.2360048  2.075682
 2.1844318  2.248759   2.0363479  1.9141011  1.8494645  1.8285302
 2.0732067  2.0500832  1.9630442  1.9523193  1.9154658  2.23954
 2.8554804  3.6734705  4.636739   3.9672024  3.2740958  2.7879906
 2.4576745  2.1704655  1.9878231  1.8678026  1.9600589  2.0467992
 1.7995555  1.2754222  0.74325085 0.9067204  1.4494272  1.9277642
 3.0310087  4.4879656  5.7695537  5.869934   5.437722   5.4206066
 5.14243    4.7791505  4.135569   3.9310071  3.4668102  2.8519573
 2.5881379  2.55815    2.6998663  2.4874153  2.1300874  1.8870535
 2.0154526  2.369764   2.4133627  3.482797   3.8618789  3.8654034
 3.5904436  3.3868783  3.1476982  2.6815708  2.5608873  2.409167
 2.040088   1.6889076  1.4841536  1.638635   1.7204351  2.1327977
 2.077677   1.923614   1.8582172  2.0624897  1.6330583  1.5372928
 1.5438815  1.4278046  1.5322322  2.1451967  2.967199   3.757536
 3.3377132  2.7306244  2.3130097  2.1470828  2.073874   2.1112506
 2.0063038  1.864562   2.185948   2.221591   1.7401727  0.9879816
 0.8685185  1.3546635  1.8285613  2.3634307  3.8494375  5.3627257
 5.465287   5.26614    5.3990326  5.3370557  4.8753557  4.278172
 4.1121373  3.6894448  3.1437886  2.7334778  2.4838557  2.490113
 2.3885927  2.2122061  1.837794   1.4731493  1.6323913  2.3617241
 2.7992535  3.5404444  2.99972    2.9075618  2.787829   2.6673474
 2.2835512  2.0018053  1.9916246  1.7000986  1.3698611  0.994605
 1.0935321  1.0997306  1.8447521  2.4634802  2.1093504  1.9426979
 1.869548   1.2876767  1.0669305  1.1701573  1.0324042  0.94286054
 1.3546699  2.0382495  2.7808673  3.0862226  2.3650436  1.8914169
 1.7411771  1.8309288  2.087308   2.2808619  2.205346   2.1287794
 2.406373   2.4309583  1.9048548  0.97159374 1.1107924  1.7553803
 2.1610117  2.7304955  4.3255577  5.218589   5.075073   5.3357086
 5.5708895  5.1926455  4.931007   4.6597     4.6268053  4.162661
 3.4652498  2.9576392  2.7495422  2.4590747  2.130613   1.9252281
 1.7080035  1.1439716  1.2632833  2.2791898  3.0286734  2.5711155
 2.0637429  2.089266   2.0193515  2.0495875  1.542565   1.7027313
 1.3842607  1.4095044  0.96612453 0.49773547 0.635168   1.1006382
 1.8486447  2.5073464  2.1030755  1.9846205  1.2456343  0.8421231
 0.7802966  0.8519263  0.56545794 0.6720574  1.1049008  1.7624186
 2.5403786  2.4258149  1.7014142  1.346664   1.4478822  1.7825078
 2.2327974  2.446188   2.3733568  2.208366   2.484604   2.5558007
 2.1659358  1.3984259  1.4125134  2.100631   2.599255   3.1355004
 4.216788   4.959471   5.052471   5.492216   5.8134766  5.504024
 5.549743   5.0930166  4.8678536  4.224918   3.404226   2.984997
 3.0161877  2.5172336  2.046978   1.6196036  1.2664483  1.1164231
 0.88258994 1.600492   2.6624436  2.0335057  1.378702   1.3692685
 1.2829248  1.2171614  0.9058322  1.366772   0.96413875 1.372941
 0.9430968  0.48206863 0.59924835 0.8472335  1.3686417  1.9014543
 2.139736   1.5556239  0.91645586 0.64544106 0.7064806  0.6581216
 0.40125614 0.5481654  0.83177656 1.3702457  2.109194   2.0848856
 1.3854225  1.1337851  1.382421   1.7463367  2.0019548  2.2386374
 2.251899   2.041518   2.052375   2.2839284  2.4019551  2.06722
 1.5282382  1.9877198  2.9186287  3.502385   4.2404175  4.804946
 5.1604176  5.3508816  5.5404153  5.0644603 ]
