time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.23%, model saved.
Epoch: 0 Train: 31187.26562 Test: 4203.67920
Epoch 100: New minimal relative error: 82.03%, model saved.
Epoch: 100 Train: 8089.59619 Test: 1440.47266
Epoch 200: New minimal relative error: 67.68%, model saved.
Epoch: 200 Train: 6348.90283 Test: 1022.81995
Epoch: 300 Train: 6813.87207 Test: 1160.34265
Epoch: 400 Train: 5294.80127 Test: 721.23059
Epoch: 500 Train: 4096.25781 Test: 444.79181
Epoch: 600 Train: 3293.25024 Test: 273.50354
Epoch 700: New minimal relative error: 21.16%, model saved.
Epoch: 700 Train: 1158.77271 Test: 75.76852
Epoch: 800 Train: 422.83670 Test: 24.76493
Epoch: 900 Train: 269.91443 Test: 17.60569
Epoch 1000: New minimal relative error: 13.09%, model saved.
Epoch: 1000 Train: 179.64523 Test: 4.49141
Epoch 1100: New minimal relative error: 9.97%, model saved.
Epoch: 1100 Train: 147.00423 Test: 8.32469
Epoch: 1200 Train: 220.33934 Test: 11.18534
Epoch 1300: New minimal relative error: 5.16%, model saved.
Epoch: 1300 Train: 111.04767 Test: 1.78024
Epoch: 1400 Train: 113.63005 Test: 4.20923
Epoch: 1500 Train: 87.50851 Test: 2.27729
Epoch: 1600 Train: 74.81613 Test: 2.23561
Epoch: 1700 Train: 90.72099 Test: 5.43246
Epoch: 1800 Train: 75.30537 Test: 9.18442
Epoch: 1900 Train: 61.50480 Test: 0.94601
Epoch: 2000 Train: 101.63525 Test: 7.03793
Epoch: 2100 Train: 66.36661 Test: 2.07828
Epoch: 2200 Train: 69.32452 Test: 8.15608
Epoch: 2300 Train: 54.86783 Test: 1.55519
Epoch: 2400 Train: 48.55726 Test: 0.81566
Epoch: 2500 Train: 48.68775 Test: 1.91133
Epoch: 2600 Train: 57.53469 Test: 7.26583
Epoch: 2700 Train: 44.82592 Test: 0.21427
Epoch: 2800 Train: 67.64173 Test: 10.91214
Epoch 2900: New minimal relative error: 2.19%, model saved.
Epoch: 2900 Train: 60.91105 Test: 0.92585
Epoch: 3000 Train: 44.88234 Test: 0.92096
Epoch: 3100 Train: 47.88918 Test: 4.08661
Epoch: 3200 Train: 56.77845 Test: 11.68043
Epoch: 3300 Train: 39.48011 Test: 1.37244
Epoch: 3400 Train: 36.82658 Test: 0.10395
Epoch: 3500 Train: 35.93652 Test: 0.35044
Epoch: 3600 Train: 39.09826 Test: 1.44993
Epoch: 3700 Train: 37.63318 Test: 2.10449
Epoch: 3800 Train: 38.33174 Test: 1.56667
Epoch: 3900 Train: 28.92292 Test: 0.25484
Epoch 4000: New minimal relative error: 1.70%, model saved.
Epoch: 4000 Train: 29.59672 Test: 0.15355
Epoch: 4100 Train: 31.98974 Test: 1.35831
Epoch: 4200 Train: 30.75581 Test: 0.14443
Epoch: 4300 Train: 28.07772 Test: 0.16757
Epoch: 4400 Train: 26.48414 Test: 0.46084
Epoch: 4500 Train: 29.25868 Test: 0.16783
Epoch: 4600 Train: 25.52938 Test: 0.08507
Epoch: 4700 Train: 32.29040 Test: 2.21521
Epoch: 4800 Train: 31.40520 Test: 0.90448
Epoch: 4900 Train: 23.69654 Test: 0.07378
Epoch: 5000 Train: 32.40878 Test: 4.93261
Epoch: 5100 Train: 22.70938 Test: 0.18614
Epoch: 5200 Train: 22.79259 Test: 0.08415
Epoch: 5300 Train: 22.08795 Test: 1.02765
Epoch: 5400 Train: 24.27124 Test: 0.48168
Epoch: 5500 Train: 26.68426 Test: 3.66050
Epoch: 5600 Train: 21.33061 Test: 0.08808
Epoch: 5700 Train: 29.48087 Test: 2.14050
Epoch: 5800 Train: 21.05691 Test: 0.29042
Epoch: 5900 Train: 25.15151 Test: 0.89499
Epoch: 6000 Train: 24.33162 Test: 2.35123
Epoch: 6100 Train: 19.66178 Test: 0.04550
Epoch: 6200 Train: 23.03594 Test: 0.10285
Epoch: 6300 Train: 24.99733 Test: 0.11386
Epoch: 6400 Train: 27.64415 Test: 0.30096
Epoch: 6500 Train: 34.47082 Test: 3.53184
Epoch: 6600 Train: 23.36440 Test: 0.08342
Epoch: 6700 Train: 26.24907 Test: 1.64302
Epoch: 6800 Train: 22.13015 Test: 0.06821
Epoch: 6900 Train: 25.57042 Test: 1.42703
Epoch 7000: New minimal relative error: 1.26%, model saved.
Epoch: 7000 Train: 21.31168 Test: 0.05726
Epoch: 7100 Train: 23.94946 Test: 2.43402
Epoch: 7200 Train: 25.48988 Test: 1.74634
Epoch 7300: New minimal relative error: 0.77%, model saved.
Epoch: 7300 Train: 20.53019 Test: 0.04769
Epoch: 7400 Train: 20.58182 Test: 0.99583
Epoch: 7500 Train: 19.45601 Test: 0.07910
Epoch: 7600 Train: 20.31658 Test: 0.80930
Epoch: 7700 Train: 19.54748 Test: 0.07384
Epoch: 7800 Train: 19.30311 Test: 0.09325
Epoch: 7900 Train: 18.70177 Test: 0.06086
Epoch: 8000 Train: 18.40394 Test: 0.13749
Epoch: 8100 Train: 20.70946 Test: 0.16311
Epoch: 8200 Train: 18.94939 Test: 0.03791
Epoch: 8300 Train: 17.30943 Test: 0.35374
Epoch 8400: New minimal relative error: 0.56%, model saved.
Epoch: 8400 Train: 16.35194 Test: 0.03825
Epoch: 8500 Train: 15.85052 Test: 0.46099
Epoch: 8600 Train: 16.98204 Test: 0.31851
Epoch: 8700 Train: 21.14909 Test: 1.67222
Epoch: 8800 Train: 15.48469 Test: 0.06205
Epoch: 8900 Train: 15.76115 Test: 0.16179
Epoch: 9000 Train: 17.02315 Test: 0.04722
Epoch: 9100 Train: 17.24592 Test: 0.05988
Epoch: 9200 Train: 17.24701 Test: 0.04585
Epoch: 9300 Train: 16.71483 Test: 0.07811
Epoch: 9400 Train: 17.88001 Test: 0.25877
Epoch: 9500 Train: 16.61427 Test: 0.43189
Epoch: 9600 Train: 18.09097 Test: 0.31431
Epoch: 9700 Train: 16.31147 Test: 0.04235
Epoch: 9800 Train: 21.58227 Test: 0.21384
Epoch: 9900 Train: 18.98594 Test: 0.06346
Epoch: 9999 Train: 20.74779 Test: 3.00205
Training Loss: tensor(20.7478)
Test Loss: tensor(3.0020)
Learned LE: [  0.85172933   0.02914714 -14.545229  ]
True LE: [ 8.7800753e-01 -1.4378878e-03 -1.4547396e+01]
Relative Error: [7.009568   6.845641   6.7960143  7.059492   7.3607616  7.595472
 7.6270595  7.4623747  7.536953   7.8349166  8.223192   8.80827
 9.179537   9.150326   8.874967   8.369117   7.359641   6.4080772
 5.485839   4.7175937  4.288967   4.0111175  3.9177477  4.0673594
 4.443139   5.178173   6.2151814  7.0764327  7.6148257  7.6880894
 7.472417   7.361125   7.513544   7.6580524  7.7635074  7.6241164
 7.2754927  6.827867   6.371548   6.3592854  6.579293   7.0727596
 7.791891   8.230369   9.181818   9.077333   8.601974   8.37882
 8.360792   8.589508   7.9475813  7.258003   6.7179623  6.5539927
 6.4739385  6.5692673  6.562507   6.507679   6.155596   6.097467
 6.29038    6.092192   5.7383113  5.6366997  5.598858   5.7221107
 5.9742045  5.8782635  5.9806986  5.8636284  5.838383   5.915784
 6.3271403  6.7698727  7.35791    7.8489566  7.8929024  7.583645
 7.238458   6.2699437  5.437462   4.5269465  4.0114975  3.6391761
 3.351783   3.288422   3.4926078  3.9336362  4.781924   5.7512307
 6.4586124  6.984786   6.6450253  6.4292626  6.3699393  6.274517
 6.5291286  6.339187   6.2094293  5.836632   5.2079864  5.0007496
 4.8941116  5.372839   6.131582   6.3207216  7.27024    8.241083
 7.8688197  7.403533   7.2043204  7.337611   7.3405437  6.521536
 5.784814   5.4149613  5.4301314  5.7795134  5.673004   5.5248537
 5.299424   5.1325393  5.1252956  4.9699125  4.8289475  4.666584
 4.6553564  4.72437    4.6298847  4.470642   4.4311295  4.50376
 4.4935827  4.493065   4.631873   5.0075436  5.3698564  5.978175
 6.52398    6.630511   6.451062   6.0381627  5.32794    4.634998
 3.9522707  3.486769   3.1077151  2.8687048  2.7506943  2.8974104
 3.4004264  4.15715    5.1509013  5.7879105  6.175337   5.8411384
 5.448156   5.300883   5.2828155  5.3193154  5.1736712  4.9607
 4.452792   3.8823745  3.8261862  3.762989   4.331436   4.813304
 5.207132   6.307766   7.1750054  6.826744   6.2761054  6.105675
 6.280837   5.8901896  5.1839323  4.4982324  4.278644   4.6316133
 5.040159   4.7847137  4.516965   4.4669213  4.243424   3.9448452
 3.8801546  3.910294   3.9099739  3.9761815  3.6854346  3.460675
 3.2833853  3.2193632  3.376319   3.3980567  3.425808   3.5593872
 3.8021858  4.1273975  4.7084904  5.173085   5.4548745  5.4286785
 4.901706   4.4409857  4.051273   3.4457731  3.0119748  2.7665567
 2.4489095  2.1642954  2.2667205  2.6792629  3.4272923  4.371451
 5.0309954  5.2760015  5.049716   4.6645684  4.3756237  4.334908
 4.239563   4.0484557  3.6449578  3.2773309  2.8297896  2.8395288
 2.7261755  3.3467152  3.6535912  4.2318606  5.185576   5.868737
 5.9585824  5.2886443  5.0502734  4.919685   4.5729074  3.7994027
 3.3829584  3.4382322  3.9758682  4.183842   3.869221   3.6471174
 3.6367164  3.2493618  3.0917811  3.0824618  3.193819   3.268651
 2.862063   2.722388   2.5713108  2.3991342  2.3593798  2.5049798
 2.8247478  2.8297982  2.753036   2.9434896  3.0780141  3.4839723
 3.736649   4.066175   4.2858543  3.9728706  3.7350404  3.375133
 3.0329943  2.5589144  2.326731   2.0524058  1.6116178  1.5214387
 1.7163002  2.372491   3.2994335  4.0916934  4.533136   4.220503
 3.9291217  3.6056454  3.434676   3.3100736  2.9683578  2.5207787
 2.3136883  1.9057751  1.9109076  1.8466333  2.3772545  2.672819
 3.2546024  4.0582967  4.5988007  5.099481   4.464382   4.071182
 3.734043   3.369205   2.6225483  2.403729   2.6678927  3.0973744
 3.2958958  3.0290973  2.821018   2.6042254  2.4427257  2.493141
 2.456692   2.4959738  2.3344665  1.984625   1.8458439  1.9277748
 1.8746743  1.8484769  2.0808957  2.6109276  2.8018234  2.5840073
 2.4412212  2.3095021  2.3814418  2.4949849  2.586885   2.9744055
 3.109826   3.086338   2.8200178  2.5438552  2.2231145  1.8280294
 1.5955007  1.1365077  0.84957963 0.8245021  1.2286593  2.0943894
 2.897703   3.5180948  3.4960833  3.269138   2.9280531  2.6729863
 2.569062   2.1669426  1.7334651  1.5672485  1.3978517  1.0742692
 1.1271143  1.4865624  1.804409   2.3824441  2.9131494  3.3617861
 3.6876452  3.9146152  3.2577078  2.6843274  2.1560276  1.6538473
 1.468499   1.7752978  2.290546   2.4571311  2.338598   2.0473993
 1.706792   1.6667981  1.7576275  1.8953707  1.7512506  1.5360638
 1.3087015  1.1237527  1.1016717  1.2760539  1.4559542  1.616277
 1.9749682  2.5874043  2.4106064  2.0130305  1.7961684  1.63706
 1.8733516  1.8276628  1.7854168  1.9803329  2.4026866  2.4301152
 2.1637352  1.9305445  1.7170932  1.2020266 ]
