time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 20000
num_train: 3000
num_test: 3000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 2
reg_param: 100.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 254.718063354 Test: 10.509783745
Epoch 0: New minimal relative error: 10.51%, model saved.
Epoch: 200 Train: 19.725877762 Test: 11.165081024
Epoch: 400 Train: 13.513988495 Test: 10.161857605
Epoch 400: New minimal relative error: 10.16%, model saved.
Epoch: 600 Train: 12.877453804 Test: 10.353294373
Epoch: 800 Train: 13.285570145 Test: 10.183989525
Epoch: 1000 Train: 12.780714035 Test: 10.007369995
Epoch 1000: New minimal relative error: 10.01%, model saved.
Epoch: 1200 Train: 12.819722176 Test: 10.084191322
Epoch: 1400 Train: 12.563343048 Test: 10.536776543
Epoch: 1600 Train: 12.717226982 Test: 10.605616570
Epoch: 1800 Train: 12.640695572 Test: 10.650186539
Epoch: 2000 Train: 12.517823219 Test: 10.190662384
Epoch: 2200 Train: 12.947364807 Test: 10.244690895
Epoch: 2400 Train: 14.656921387 Test: 9.917648315
Epoch 2400: New minimal relative error: 9.92%, model saved.
Epoch: 2600 Train: 13.814768791 Test: 10.157404900
Epoch: 2800 Train: 15.506725311 Test: 10.102035522
Epoch: 3000 Train: 15.935005188 Test: 10.531474113
Epoch: 3200 Train: 14.589116096 Test: 10.657811165
Epoch: 3400 Train: 14.691804886 Test: 9.694213867
Epoch 3400: New minimal relative error: 9.69%, model saved.
Epoch: 3600 Train: 14.249129295 Test: 9.451589584
Epoch 3600: New minimal relative error: 9.45%, model saved.
Epoch: 3800 Train: 15.065647125 Test: 9.638742447
Epoch: 4000 Train: 15.221998215 Test: 9.840675354
Epoch: 4200 Train: 15.793392181 Test: 9.932074547
Epoch: 4400 Train: 18.165458679 Test: 10.117928505
Epoch: 4600 Train: 21.608011246 Test: 10.169113159
Epoch: 4800 Train: 20.796421051 Test: 10.211282730
Epoch: 5000 Train: 15.821641922 Test: 9.931975365
Epoch: 5200 Train: 16.070131302 Test: 10.031646729
Epoch: 5400 Train: 16.029661179 Test: 9.857158661
Epoch: 5600 Train: 14.718328476 Test: 10.094811440
Epoch: 5800 Train: 13.993165970 Test: 10.226557732
Epoch: 6000 Train: 14.510442734 Test: 10.373271942
Epoch: 6200 Train: 18.229799271 Test: 10.441395760
Epoch: 6400 Train: 20.748916626 Test: 10.607299805
Epoch: 6600 Train: 18.689748764 Test: 10.386809349
Epoch: 6800 Train: 16.964241028 Test: 10.421450615
Epoch: 7000 Train: 15.670494080 Test: 10.343600273
Epoch: 7200 Train: 15.057182312 Test: 10.363874435
Epoch: 7400 Train: 14.272970200 Test: 10.382436752
Epoch: 7600 Train: 13.828746796 Test: 10.328559875
Epoch: 7800 Train: 12.735326767 Test: 10.317344666
Epoch: 8000 Train: 13.487330437 Test: 10.524457932
Epoch: 8200 Train: 13.665281296 Test: 10.616484642
Epoch: 8400 Train: 14.183730125 Test: 10.617678642
Epoch: 8600 Train: 15.422033310 Test: 10.575703621
Epoch: 8800 Train: 16.708333969 Test: 10.596932411
Epoch: 9000 Train: 18.363422394 Test: 10.613118172
Epoch: 9200 Train: 19.292655945 Test: 10.607976913
Epoch: 9400 Train: 20.364318848 Test: 10.599434853
Epoch: 9600 Train: 21.632560730 Test: 10.616440773
Epoch: 9800 Train: 23.348278046 Test: 10.655468941
Epoch: 10000 Train: 21.174335480 Test: 10.587681770
Epoch: 10200 Train: 18.826194763 Test: 10.391465187
Epoch: 10400 Train: 19.346021652 Test: 10.138950348
Epoch: 10600 Train: 19.484680176 Test: 10.024788857
Epoch: 10800 Train: 19.624561310 Test: 9.942782402
Epoch: 11000 Train: 19.791557312 Test: 9.946607590
Epoch: 11200 Train: 20.802036285 Test: 10.391737938
Epoch: 11400 Train: 20.963876724 Test: 10.446188927
Epoch: 11600 Train: 20.295681000 Test: 10.368228912
Epoch: 11800 Train: 20.286834717 Test: 10.330671310
Epoch: 12000 Train: 22.135501862 Test: 10.273163795
Epoch: 12200 Train: 23.842267990 Test: 10.255620956
Epoch: 12400 Train: 25.672542572 Test: 10.374397278
Epoch: 12600 Train: 27.473173141 Test: 10.394936562
Epoch: 12800 Train: 28.827939987 Test: 10.413427353
Epoch: 13000 Train: 29.754192352 Test: 10.391652107
Epoch: 13200 Train: 30.727600098 Test: 10.366906166
Epoch: 13400 Train: 31.846858978 Test: 10.347804070
Epoch: 13600 Train: 33.431270599 Test: 10.350514412
Epoch: 13800 Train: 34.972351074 Test: 10.365504265
Epoch: 14000 Train: 35.565299988 Test: 10.361829758
Epoch: 14200 Train: 35.413002014 Test: 10.349636078
Epoch: 14400 Train: 33.573753357 Test: 10.352073669
Epoch: 14600 Train: 32.166210175 Test: 10.531395912
Epoch: 14800 Train: 29.215143204 Test: 10.572915077
Epoch: 15000 Train: 27.903945923 Test: 10.600818634
Epoch: 15200 Train: 27.010839462 Test: 10.529708862
Epoch: 15400 Train: 25.761089325 Test: 10.347046852
Epoch: 15600 Train: 25.387836456 Test: 10.218147278
Epoch: 15800 Train: 25.365806580 Test: 10.182562828
Epoch: 16000 Train: 25.546752930 Test: 10.240917206
Epoch: 16200 Train: 25.475395203 Test: 10.312951088
Epoch: 16400 Train: 25.810445786 Test: 10.495783806
Epoch: 16600 Train: 25.808475494 Test: 10.623168945
Epoch: 16800 Train: 25.466564178 Test: 10.755196571
Epoch: 17000 Train: 24.877561569 Test: 10.800540924
Epoch: 17200 Train: 24.056020737 Test: 10.821593285
Epoch: 17400 Train: 24.096527100 Test: 10.806996346
Epoch: 17600 Train: 23.554712296 Test: 10.734129906
Epoch: 17800 Train: 23.635726929 Test: 10.665421486
Epoch: 18000 Train: 23.640554428 Test: 10.624632835
Epoch: 18200 Train: 24.070987701 Test: 10.605415344
Epoch: 18400 Train: 24.428897858 Test: 10.569045067
Epoch: 18600 Train: 24.464698792 Test: 10.548237801
Epoch: 18800 Train: 24.893955231 Test: 10.527926445
Epoch: 19000 Train: 25.550727844 Test: 10.505119324
Epoch: 19200 Train: 26.072883606 Test: 10.506496429
Epoch: 19400 Train: 26.476722717 Test: 10.545094490
Epoch: 19600 Train: 27.109088898 Test: 10.576332092
Epoch: 19800 Train: 27.768970490 Test: 10.573173523
Epoch: 19999 Train: 28.019565582 Test: 10.582670212
Training Loss: tensor(28.0196)
Test Loss: tensor(10.5827)
True Mean x: tensor(3.2361, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.1394, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.1741)
Jacobian term Test Loss: tensor(0.1286)
Learned LE: [ 9.118001  -0.6213539]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
