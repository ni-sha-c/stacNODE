time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 5
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 26.795745850 Test: 23.860115051
Epoch 0: New minimal relative error: 23.86%, model saved.
Epoch: 100 Train: 5.203938007 Test: 5.327967644
Epoch 100: New minimal relative error: 5.33%, model saved.
Epoch: 200 Train: 5.083433628 Test: 5.180274487
Epoch 200: New minimal relative error: 5.18%, model saved.
Epoch: 300 Train: 5.120260239 Test: 5.246752739
Epoch: 400 Train: 5.097967148 Test: 5.217825890
Epoch: 500 Train: 5.127503395 Test: 5.223402023
Epoch: 600 Train: 5.132276535 Test: 5.224118233
Epoch: 700 Train: 5.074414253 Test: 5.196667671
Epoch: 800 Train: 5.192573547 Test: 5.265947342
Epoch: 900 Train: 5.098534584 Test: 5.207403183
Epoch: 1000 Train: 5.100674629 Test: 5.227900505
Epoch: 1100 Train: 5.112503529 Test: 5.262013435
Epoch: 1200 Train: 5.150490284 Test: 5.282268524
Epoch: 1300 Train: 5.129043579 Test: 5.233294487
Epoch: 1400 Train: 5.251110554 Test: 5.326182365
Epoch: 1500 Train: 5.164621353 Test: 5.280690193
Epoch: 1600 Train: 5.166947842 Test: 5.285770416
Epoch: 1700 Train: 5.193404675 Test: 5.286132336
Epoch: 1800 Train: 5.214468002 Test: 5.296553612
Epoch: 1900 Train: 5.161734581 Test: 5.268977165
Epoch: 2000 Train: 5.186734200 Test: 5.303061485
Epoch: 2100 Train: 5.167435646 Test: 5.311621666
Epoch: 2200 Train: 5.157259941 Test: 5.299296379
Epoch: 2300 Train: 5.162253380 Test: 5.310685158
Epoch: 2400 Train: 5.148468971 Test: 5.329528809
Epoch: 2500 Train: 5.162505150 Test: 5.349801540
Epoch: 2600 Train: 5.184052467 Test: 5.368366241
Epoch: 2700 Train: 5.175683975 Test: 5.387629032
Epoch: 2800 Train: 5.195917606 Test: 5.474278450
Epoch: 2900 Train: 5.082651138 Test: 5.355014801
Epoch: 3000 Train: 5.129004478 Test: 5.409426689
Epoch: 3100 Train: 5.159442902 Test: 5.376068115
Epoch: 3200 Train: 5.162430286 Test: 5.413311481
Epoch: 3300 Train: 5.195309639 Test: 5.403607368
Epoch: 3400 Train: 5.187375069 Test: 5.316463470
Epoch: 3500 Train: 5.175622463 Test: 5.320659637
Epoch: 3600 Train: 5.171189785 Test: 5.301495075
Epoch: 3700 Train: 5.161630154 Test: 5.287078857
Epoch: 3800 Train: 5.160839081 Test: 5.298140526
Epoch: 3900 Train: 5.166221619 Test: 5.313251495
Epoch: 4000 Train: 5.215892792 Test: 5.350205421
Epoch: 4100 Train: 5.185711384 Test: 5.321980476
Epoch: 4200 Train: 5.181302547 Test: 5.299480915
Epoch: 4300 Train: 5.183659077 Test: 5.304358006
Epoch: 4400 Train: 5.186773300 Test: 5.300699234
Epoch: 4500 Train: 5.191600800 Test: 5.303910255
Epoch: 4600 Train: 5.200101852 Test: 5.306458473
Epoch: 4700 Train: 5.192394257 Test: 5.299962997
Epoch: 4800 Train: 5.181928635 Test: 5.297258377
Epoch: 4900 Train: 5.175243378 Test: 5.296518803
Epoch: 5000 Train: 5.176895142 Test: 5.304244041
Epoch: 5100 Train: 5.183812141 Test: 5.311816216
Epoch: 5200 Train: 5.186985016 Test: 5.311489105
Epoch: 5300 Train: 5.189764977 Test: 5.313529968
Epoch: 5400 Train: 5.179367065 Test: 5.309645653
Epoch: 5500 Train: 5.168809414 Test: 5.296836853
Epoch: 5600 Train: 5.169806004 Test: 5.309858322
Epoch: 5700 Train: 5.176736832 Test: 5.314406395
Epoch: 5800 Train: 5.182911396 Test: 5.321957111
Epoch: 5900 Train: 5.194113731 Test: 5.344847679
Epoch: 6000 Train: 5.191082954 Test: 5.345214367
Epoch: 6100 Train: 5.205954552 Test: 5.366965294
Epoch: 6200 Train: 5.207246780 Test: 5.349566460
Epoch: 6300 Train: 5.210703850 Test: 5.345571518
Epoch: 6400 Train: 5.204370499 Test: 5.351215363
Epoch: 6500 Train: 5.207204819 Test: 5.374007225
Epoch: 6600 Train: 5.208110809 Test: 5.376903534
Epoch: 6700 Train: 5.210000992 Test: 5.380466461
Epoch: 6800 Train: 5.207277298 Test: 5.362555504
Epoch: 6900 Train: 5.213418961 Test: 5.376939774
Epoch: 7000 Train: 5.214176655 Test: 5.368053436
Epoch: 7100 Train: 5.212699890 Test: 5.385706902
Epoch: 7200 Train: 5.213815212 Test: 5.389994144
Epoch: 7300 Train: 5.212532520 Test: 5.394783974
Epoch: 7400 Train: 5.208832741 Test: 5.400979519
Epoch: 7500 Train: 5.204773903 Test: 5.392005920
Epoch: 7600 Train: 5.205071926 Test: 5.388731003
Epoch: 7700 Train: 5.202233315 Test: 5.369721413
Epoch: 7800 Train: 5.195846558 Test: 5.365364075
Epoch: 7900 Train: 5.197150707 Test: 5.365534782
Epoch: 8000 Train: 5.194425583 Test: 5.357367992
Epoch: 8100 Train: 5.194830418 Test: 5.359020233
Epoch: 8200 Train: 5.197008133 Test: 5.357792377
Epoch: 8300 Train: 5.195000648 Test: 5.353744507
Epoch: 8400 Train: 5.193024635 Test: 5.351612091
Epoch: 8500 Train: 5.192129612 Test: 5.353416920
Epoch: 8600 Train: 5.189981937 Test: 5.355694771
Epoch: 8700 Train: 5.189160347 Test: 5.356760979
Epoch: 8800 Train: 5.193511963 Test: 5.360030174
Epoch: 8900 Train: 5.195307255 Test: 5.363370419
Epoch: 9000 Train: 5.198363304 Test: 5.367011070
Epoch: 9100 Train: 5.200904846 Test: 5.371138573
Epoch: 9200 Train: 5.202622414 Test: 5.373959541
Epoch: 9300 Train: 5.204220772 Test: 5.378003120
Epoch: 9400 Train: 5.206094742 Test: 5.380818367
Epoch: 9500 Train: 5.208448887 Test: 5.378889084
Epoch: 9600 Train: 5.208100319 Test: 5.376550674
Epoch: 9700 Train: 5.212130547 Test: 5.371332169
Epoch: 9800 Train: 5.214086533 Test: 5.365656376
Epoch: 9900 Train: 5.213056087 Test: 5.363358498
Epoch: 9999 Train: 5.214419842 Test: 5.365038395
Training Loss: tensor(5.2144)
Test Loss: tensor(5.3650)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.2056, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0027, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0051)
Jacobian term Test Loss: tensor(0.0053)
Learned LE: [0.6275727 0.593535 ]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
