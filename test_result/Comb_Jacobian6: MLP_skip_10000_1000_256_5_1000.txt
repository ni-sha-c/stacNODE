time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.74%, model saved.
Epoch: 0 Train: 59735.99609 Test: 4019.53760
Epoch: 100 Train: 16192.63965 Test: 1520.56824
Epoch 200: New minimal relative error: 63.54%, model saved.
Epoch: 200 Train: 14521.46094 Test: 1200.35486
Epoch: 300 Train: 14811.77637 Test: 1323.12378
Epoch: 400 Train: 14479.16797 Test: 1240.01416
Epoch 500: New minimal relative error: 61.79%, model saved.
Epoch: 500 Train: 13948.40332 Test: 1174.89392
Epoch: 600 Train: 14167.75781 Test: 1247.10645
Epoch: 700 Train: 14389.68848 Test: 1276.31860
Epoch: 800 Train: 13809.27539 Test: 1178.31824
Epoch: 900 Train: 11532.72656 Test: 900.07794
Epoch: 1000 Train: 10040.45508 Test: 631.91907
Epoch: 1100 Train: 8375.43164 Test: 513.12628
Epoch: 1200 Train: 6720.83545 Test: 336.01352
Epoch: 1300 Train: 5467.74854 Test: 214.83093
Epoch 1400: New minimal relative error: 48.24%, model saved.
Epoch: 1400 Train: 1950.37793 Test: 42.10164
Epoch 1500: New minimal relative error: 29.21%, model saved.
Epoch: 1500 Train: 1298.77881 Test: 21.75926
Epoch 1600: New minimal relative error: 15.00%, model saved.
Epoch: 1600 Train: 1085.29919 Test: 15.63712
Epoch: 1700 Train: 901.35132 Test: 12.32177
Epoch 1800: New minimal relative error: 11.46%, model saved.
Epoch: 1800 Train: 769.18579 Test: 9.65321
Epoch: 1900 Train: 721.21960 Test: 8.86754
Epoch: 2000 Train: 653.96552 Test: 7.40605
Epoch: 2100 Train: 626.34064 Test: 7.28702
Epoch: 2200 Train: 560.03497 Test: 7.72513
Epoch 2300: New minimal relative error: 10.10%, model saved.
Epoch: 2300 Train: 507.84845 Test: 6.75620
Epoch: 2400 Train: 477.98154 Test: 7.68542
Epoch: 2500 Train: 423.51566 Test: 4.03599
Epoch: 2600 Train: 411.61960 Test: 5.56233
Epoch: 2700 Train: 416.90244 Test: 8.66846
Epoch 2800: New minimal relative error: 9.74%, model saved.
Epoch: 2800 Train: 354.55206 Test: 2.81173
Epoch: 2900 Train: 319.73380 Test: 2.88728
Epoch 3000: New minimal relative error: 3.95%, model saved.
Epoch: 3000 Train: 310.88712 Test: 3.20139
Epoch: 3100 Train: 439.51428 Test: 6.44057
Epoch: 3200 Train: 332.66614 Test: 3.70302
Epoch: 3300 Train: 345.96844 Test: 5.25641
Epoch: 3400 Train: 305.32483 Test: 3.06927
Epoch: 3500 Train: 347.90622 Test: 3.65004
Epoch 3600: New minimal relative error: 3.73%, model saved.
Epoch: 3600 Train: 236.60693 Test: 1.78813
Epoch: 3700 Train: 249.95450 Test: 2.28766
Epoch: 3800 Train: 244.52417 Test: 2.70018
Epoch: 3900 Train: 223.60966 Test: 1.72116
Epoch: 4000 Train: 235.44040 Test: 5.88465
Epoch: 4100 Train: 273.54199 Test: 2.55914
Epoch: 4200 Train: 194.03006 Test: 1.83108
Epoch: 4300 Train: 232.02098 Test: 1.97457
Epoch: 4400 Train: 211.98430 Test: 2.48391
Epoch 4500: New minimal relative error: 3.39%, model saved.
Epoch: 4500 Train: 180.67210 Test: 1.24386
Epoch 4600: New minimal relative error: 2.82%, model saved.
Epoch: 4600 Train: 180.03224 Test: 1.27026
Epoch: 4700 Train: 185.89624 Test: 1.67922
Epoch: 4800 Train: 186.50517 Test: 1.59376
Epoch: 4900 Train: 208.06709 Test: 2.95214
Epoch: 5000 Train: 217.44563 Test: 1.64446
Epoch: 5100 Train: 184.99400 Test: 1.37827
Epoch: 5200 Train: 198.36264 Test: 14.67098
Epoch: 5300 Train: 165.64040 Test: 1.16481
Epoch: 5400 Train: 178.94958 Test: 1.40824
Epoch: 5500 Train: 167.03592 Test: 1.45345
Epoch: 5600 Train: 188.62207 Test: 1.76746
Epoch: 5700 Train: 194.29311 Test: 2.45341
Epoch: 5800 Train: 175.56287 Test: 2.21487
Epoch: 5900 Train: 177.28955 Test: 1.46364
Epoch: 6000 Train: 179.31061 Test: 1.68394
Epoch: 6100 Train: 151.39946 Test: 2.02656
Epoch: 6200 Train: 186.70509 Test: 1.51705
Epoch: 6300 Train: 159.48491 Test: 1.18162
Epoch: 6400 Train: 159.72728 Test: 1.55113
Epoch: 6500 Train: 181.05113 Test: 1.80724
Epoch: 6600 Train: 163.66040 Test: 1.38924
Epoch: 6700 Train: 142.01488 Test: 0.98589
Epoch: 6800 Train: 134.50601 Test: 1.28197
Epoch: 6900 Train: 135.00269 Test: 1.06066
Epoch: 7000 Train: 162.63657 Test: 1.71513
Epoch: 7100 Train: 180.48454 Test: 1.62728
Epoch: 7200 Train: 182.63469 Test: 1.88158
Epoch: 7300 Train: 161.79076 Test: 1.45528
Epoch: 7400 Train: 145.12460 Test: 1.03720
Epoch: 7500 Train: 138.57211 Test: 0.93921
Epoch: 7600 Train: 137.00212 Test: 0.84592
Epoch: 7700 Train: 137.90204 Test: 1.04604
Epoch: 7800 Train: 126.26912 Test: 0.78680
Epoch: 7900 Train: 126.70571 Test: 0.81573
Epoch: 8000 Train: 130.46794 Test: 0.89982
Epoch: 8100 Train: 136.83423 Test: 1.24981
Epoch: 8200 Train: 132.31549 Test: 1.03214
Epoch: 8300 Train: 124.72101 Test: 0.86587
Epoch: 8400 Train: 221.28743 Test: 3.93215
Epoch: 8500 Train: 237.22012 Test: 2.73496
Epoch: 8600 Train: 149.38213 Test: 1.10544
Epoch: 8700 Train: 129.44485 Test: 0.72906
Epoch: 8800 Train: 150.40363 Test: 0.90084
Epoch: 8900 Train: 129.32748 Test: 0.96830
Epoch: 9000 Train: 135.52119 Test: 0.84210
Epoch: 9100 Train: 171.85046 Test: 1.44330
Epoch: 9200 Train: 147.33749 Test: 1.13860
Epoch: 9300 Train: 127.20779 Test: 0.94460
Epoch: 9400 Train: 123.72242 Test: 0.98805
Epoch: 9500 Train: 130.45987 Test: 0.97946
Epoch: 9600 Train: 118.82008 Test: 0.71045
Epoch: 9700 Train: 163.44502 Test: 1.46930
Epoch: 9800 Train: 125.50430 Test: 0.93357
Epoch: 9900 Train: 124.43317 Test: 0.86339
Epoch: 9999 Train: 128.51285 Test: 1.11673
Training Loss: tensor(128.5128)
Test Loss: tensor(1.1167)
Learned LE: [ 8.5060352e-01  1.6066449e-03 -1.4540026e+01]
True LE: [ 8.7128180e-01 -6.1653648e-03 -1.4541613e+01]
Relative Error: [2.8672962  3.374144   4.1438828  4.1298203  3.765722   3.3402948
 3.0995145  2.7360692  2.7469084  3.040041   3.3459067  3.557916
 3.6523979  3.6150196  3.3595886  3.156805   2.991098   2.5121148
 1.8528668  1.3235484  0.94349927 0.8526666  0.83073175 0.7783322
 0.94926107 1.1610645  1.4665548  1.5711989  1.6166776  1.6326157
 1.6357737  1.5948247  1.9576598  2.3202207  2.7732682  2.8022425
 2.2093337  1.6976379  1.415159   1.5743878  2.2585516  3.340698
 3.5199277  2.3611124  1.4985795  1.2209067  1.4398713  1.4358459
 1.4418921  1.4468534  1.4465712  1.7150657  1.805732   1.8830723
 2.0457826  2.0279744  1.8077886  1.485476   1.2892908  1.4164972
 1.8747604  2.1467571  2.4357407  2.9089792  3.6195776  3.8441365
 3.5402727  3.1716478  2.9880989  2.6088068  2.4811127  2.7732804
 3.0963278  3.330746   3.385729   3.2677758  3.042378   2.8837829
 2.6304617  2.236214   1.5243846  1.0385247  0.9173317  1.1617143
 1.2454363  1.0443115  0.7976322  0.7834009  0.9835823  1.146041
 1.2463926  1.3189857  1.3582811  1.3505734  1.7676452  2.1059053
 2.47684    2.6770694  2.2242615  1.7293155  1.3740437  1.3293122
 1.825037   2.8516376  3.1899686  2.1287847  1.1787791  0.7729691
 1.0965321  1.2259171  1.4095542  1.4419549  1.3720616  1.4529326
 1.605508   1.6619945  1.8273351  1.8485354  1.648323   1.3583511
 1.0849005  1.092368   1.4882225  1.8010503  1.9972302  2.4279041
 3.0478137  3.5084448  3.362668   3.0331159  2.7715034  2.4163814
 2.2026372  2.5044827  2.862596   3.062307   3.070749   2.919442
 2.6446052  2.3816438  2.1881413  1.8701875  1.2088239  0.7064969
 1.0410047  1.6206675  1.939764   1.819599   1.4417688  1.0619122
 0.87418985 0.8044176  0.8663997  0.95381373 1.041275   1.0546916
 1.4566582  1.7773311  2.116509   2.496243   2.2750638  1.8154867
 1.4565485  1.1869918  1.4030757  2.3142974  2.8305984  2.0696375
 1.0581976  0.34231794 0.614645   0.96745974 1.1637838  1.364972
 1.3465724  1.1576902  1.3265036  1.3610715  1.477205   1.5767164
 1.4454006  1.2247925  0.8790066  0.7339926  1.0411904  1.4378799
 1.5521202  1.9231524  2.4347475  2.9279952  3.224061   2.8258386
 2.5158799  2.3446994  1.9977285  2.2634606  2.6368015  2.7856402
 2.8187058  2.6503687  2.305045   1.9189987  1.625218   1.2829671
 0.9025744  0.5873496  1.2822477  2.073564   2.5284657  2.7716556
 2.4823756  1.9439552  1.5818717  1.2417862  0.85783535 0.735226
 0.7060904  0.7637955  1.1196803  1.3637996  1.7052398  2.2226348
 2.3025422  1.9204004  1.5018877  1.1051894  1.0232005  1.7110543
 2.510227   2.1667578  1.1386311  0.30574954 0.2871514  0.68817663
 0.9166509  1.140095   1.2350174  1.0809978  0.89171624 0.9800916
 1.0567521  1.2047405  1.1954489  1.1034503  0.79246813 0.54838413
 0.7010286  1.0958611  1.1078502  1.3685622  1.8026179  2.34895
 2.989054   2.5974717  2.2916446  2.1932957  1.870702   2.038928
 2.314522   2.5477011  2.582542   2.4469914  2.0718272  1.5442569
 1.1221701  0.6830137  0.43706605 0.7728795  1.5848778  2.5271833
 3.1688695  3.5724335  3.6909063  3.2869723  2.7249594  2.3317537
 1.7951498  1.209585   0.8298819  0.71848494 0.79879826 0.98510563
 1.237358   1.7964791  2.1519291  1.9409182  1.566558   1.2475352
 0.9197303  1.1283735  2.1248856  2.2391198  1.317243   0.49153742
 0.22598763 0.5075812  0.7020415  0.86706084 1.0213869  1.0216148
 0.6764232  0.5149792  0.6020689  0.7674682  0.9476652  0.9359763
 0.75978816 0.5866158  0.6241974  0.9135695  0.84186125 0.82928014
 1.1538243  1.7613355  2.267693   2.432819   2.0948055  1.9892958
 1.8578038  1.7384629  1.9889855  2.31296    2.403485   2.2627554
 1.9263141  1.3337271  0.76225924 0.27786732 0.6633513  1.213756
 2.0661123  2.9446478  3.8073611  4.373049   4.287644   3.9588637
 3.3495057  2.8664083  2.513599   2.2084064  1.759141   1.3176141
 0.95282143 0.767458   0.8040713  1.2107998  1.7274923  1.9045213
 1.6664664  1.466051   1.1929008  0.7677748  1.1346246  1.7610838
 1.554331   0.7246066  0.42099094 0.52658665 0.5809987  0.67548376
 0.7599447  0.8097399  0.8165418  0.3714801  0.3012143  0.5405528
 0.6517569  0.7133288  0.62084746 0.44331956 0.49007246 0.73446065
 0.844091   0.6356714  0.5499511  1.0400563  1.5050591  2.0727248
 1.943437   1.7747319  1.8834723  1.5217497  1.6388274  2.0123615
 2.2762425  2.1887403  1.8471686  1.2821883  0.5391595  0.33292052
 1.1000733  1.8653194  2.6457734  3.563045   3.937535   3.9491653
 3.9330568  3.8024054  3.398674   2.8302197 ]
