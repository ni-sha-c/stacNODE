time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.02%, model saved.
Epoch: 0 Train: 32566.24023 Test: 4300.02051
Epoch 80: New minimal relative error: 87.35%, model saved.
Epoch: 80 Train: 8985.77051 Test: 1586.71411
Epoch: 160 Train: 8669.27637 Test: 1486.73987
Epoch 240: New minimal relative error: 75.71%, model saved.
Epoch: 240 Train: 7992.40967 Test: 1272.41443
Epoch: 320 Train: 8051.02002 Test: 1276.96594
Epoch: 400 Train: 7856.35791 Test: 1181.53760
Epoch: 480 Train: 6805.66309 Test: 900.14850
Epoch: 560 Train: 4840.92627 Test: 503.94626
Epoch: 640 Train: 3485.36938 Test: 278.94153
Epoch: 720 Train: 1778.40186 Test: 96.94707
Epoch: 800 Train: 786.52881 Test: 34.95766
Epoch 880: New minimal relative error: 35.47%, model saved.
Epoch: 880 Train: 488.17972 Test: 12.89626
Epoch: 960 Train: 346.36993 Test: 8.03597
Epoch 1040: New minimal relative error: 18.68%, model saved.
Epoch: 1040 Train: 255.30022 Test: 4.13801
Epoch: 1120 Train: 208.01036 Test: 11.12412
Epoch: 1200 Train: 180.80225 Test: 2.23251
Epoch: 1280 Train: 166.97144 Test: 1.96846
Epoch: 1360 Train: 155.72562 Test: 1.96571
Epoch: 1440 Train: 137.66678 Test: 1.44942
Epoch: 1520 Train: 132.35011 Test: 1.44721
Epoch: 1600 Train: 127.68131 Test: 1.20204
Epoch 1680: New minimal relative error: 8.72%, model saved.
Epoch: 1680 Train: 126.02462 Test: 3.11190
Epoch: 1760 Train: 117.68562 Test: 1.30067
Epoch 1840: New minimal relative error: 8.05%, model saved.
Epoch: 1840 Train: 111.48651 Test: 1.33943
Epoch: 1920 Train: 112.89389 Test: 1.46691
Epoch: 2000 Train: 100.95039 Test: 2.06129
Epoch: 2080 Train: 100.56771 Test: 0.82751
Epoch 2160: New minimal relative error: 7.19%, model saved.
Epoch: 2160 Train: 95.86920 Test: 0.86506
Epoch: 2240 Train: 90.37919 Test: 0.72519
Epoch: 2320 Train: 87.15060 Test: 0.67753
Epoch 2400: New minimal relative error: 4.75%, model saved.
Epoch: 2400 Train: 84.59196 Test: 0.67778
Epoch: 2480 Train: 88.29561 Test: 0.69863
Epoch: 2560 Train: 83.45866 Test: 0.72094
Epoch: 2640 Train: 86.81136 Test: 0.77738
Epoch: 2720 Train: 92.61071 Test: 2.36239
Epoch: 2800 Train: 89.47825 Test: 0.81494
Epoch: 2880 Train: 85.65601 Test: 1.03566
Epoch: 2960 Train: 78.98283 Test: 0.62299
Epoch: 3040 Train: 80.62658 Test: 3.82417
Epoch: 3120 Train: 75.84809 Test: 0.57812
Epoch: 3200 Train: 74.31439 Test: 0.71598
Epoch: 3280 Train: 72.08742 Test: 1.29001
Epoch: 3360 Train: 69.65765 Test: 0.46538
Epoch: 3440 Train: 69.33775 Test: 0.47021
Epoch: 3520 Train: 69.06785 Test: 0.84378
Epoch: 3600 Train: 67.46384 Test: 0.44175
Epoch: 3680 Train: 66.32464 Test: 0.40572
Epoch: 3760 Train: 65.94978 Test: 0.41783
Epoch: 3840 Train: 65.15356 Test: 0.40838
Epoch: 3920 Train: 64.78484 Test: 0.40366
Epoch: 4000 Train: 62.91983 Test: 0.41453
Epoch: 4080 Train: 63.52611 Test: 0.41436
Epoch: 4160 Train: 64.40504 Test: 0.56027
Epoch: 4240 Train: 60.82367 Test: 0.36462
Epoch 4320: New minimal relative error: 3.79%, model saved.
Epoch: 4320 Train: 59.36162 Test: 0.37018
Epoch: 4400 Train: 59.22702 Test: 0.36524
Epoch: 4480 Train: 58.21301 Test: 0.30926
Epoch: 4560 Train: 58.74882 Test: 0.33115
Epoch: 4640 Train: 59.98430 Test: 0.33760
Epoch: 4720 Train: 60.34216 Test: 0.33415
Epoch: 4800 Train: 58.85735 Test: 0.34578
Epoch: 4880 Train: 57.79888 Test: 0.32178
Epoch: 4960 Train: 56.84101 Test: 0.32310
Epoch: 5040 Train: 56.00629 Test: 0.39343
Epoch: 5120 Train: 55.02496 Test: 0.93073
Epoch: 5200 Train: 53.46109 Test: 0.30097
Epoch: 5280 Train: 53.36639 Test: 0.27367
Epoch 5360: New minimal relative error: 2.63%, model saved.
Epoch: 5360 Train: 53.63280 Test: 0.28384
Epoch 5440: New minimal relative error: 2.15%, model saved.
Epoch: 5440 Train: 52.52647 Test: 0.27986
Epoch: 5520 Train: 51.64521 Test: 0.32170
Epoch: 5600 Train: 50.70922 Test: 0.39306
Epoch: 5680 Train: 49.29121 Test: 0.27230
Epoch: 5760 Train: 48.75672 Test: 0.26077
Epoch: 5840 Train: 49.13580 Test: 0.58186
Epoch: 5920 Train: 48.48641 Test: 0.22098
Epoch: 6000 Train: 48.37843 Test: 0.28193
Epoch: 6080 Train: 47.29366 Test: 0.23091
Epoch: 6160 Train: 47.60265 Test: 0.22061
Epoch: 6240 Train: 48.19487 Test: 0.22209
Epoch: 6320 Train: 48.42825 Test: 0.24150
Epoch: 6400 Train: 48.30991 Test: 0.23562
Epoch: 6480 Train: 48.24726 Test: 0.22942
Epoch: 6560 Train: 46.56350 Test: 0.21692
Epoch: 6640 Train: 47.73521 Test: 0.22462
Epoch: 6720 Train: 46.81369 Test: 0.37132
Epoch: 6800 Train: 46.04902 Test: 0.17439
Epoch: 6880 Train: 45.75199 Test: 0.19732
Epoch: 6960 Train: 42.42619 Test: 0.27485
Epoch 7040: New minimal relative error: 1.82%, model saved.
Epoch: 7040 Train: 41.35164 Test: 0.14071
Epoch: 7120 Train: 40.06770 Test: 0.13450
Epoch: 7200 Train: 38.78804 Test: 0.12264
Epoch: 7280 Train: 39.19503 Test: 0.13419
Epoch: 7360 Train: 39.40435 Test: 0.14167
Epoch: 7440 Train: 39.35393 Test: 0.16525
Epoch: 7520 Train: 38.62356 Test: 0.13226
Epoch: 7600 Train: 36.58269 Test: 0.13242
Epoch: 7680 Train: 35.74562 Test: 0.11555
Epoch: 7760 Train: 35.55459 Test: 0.11287
Epoch: 7840 Train: 35.87054 Test: 0.12076
Epoch: 7920 Train: 37.01711 Test: 0.30875
Epoch: 7999 Train: 36.16111 Test: 0.12421
Training Loss: tensor(36.1611)
Test Loss: tensor(0.1242)
Learned LE: [  0.8782299   -0.03767348 -14.520195  ]
True LE: [ 8.4026521e-01  5.7190098e-03 -1.4515159e+01]
Relative Error: [12.58651   11.94354   11.197386  10.838051  10.717551  10.992527
 11.870601  12.66751   13.535911  14.254315  14.026217  14.201848
 14.804092  15.820556  17.22673   18.551577  16.970547  15.87103
 15.242193  15.070161  14.642687  14.278373  13.40013   12.528472
 11.884039  11.558655  11.646773  11.489111  11.050731  10.846533
 10.677506  10.860986  11.065455  11.288151  11.528678  11.705827
 11.771105  11.608801   9.997515   9.114006   8.759417   8.648374
  8.746443   9.164142   9.677407  10.542916  11.213929   9.949617
  9.060809   8.709499   8.525328   8.763557   9.740136  11.30857
 12.531993  14.185032  14.228625  13.479218  12.826486  12.107084
 11.522444  11.106796  10.867761  10.38275    9.865405   9.402447
  9.23365    9.299483  10.226169  11.068672  11.846959  12.093347
 11.804846  11.806774  12.218963  13.068819  14.2859955 15.867085
 15.822893  14.532457  13.724229  13.33531   13.273404  12.808479
 12.643084  11.985128  11.298298  10.780391  10.735151  10.629767
 10.00337    9.749659   9.554661   9.812949  10.05617   10.3121605
 10.4809885 10.788771  10.805924  10.0797415  8.2551365  7.604867
  7.5135393  7.4037423  7.39073    7.746014   8.023645   8.763756
 10.015043   8.6948805  7.6839685  6.9209957  6.4890637  6.5931864
  7.1447587  8.0807705  9.216608  10.788268  12.397363  11.916962
 11.282214  10.609936   9.924557   9.377133   9.109954   8.826114
  8.527975   8.198534   8.031611   7.842838   8.583743   9.577238
 10.272729  10.117879   9.863069   9.637267   9.800831  10.427862
 11.421735  12.763254  14.54423   13.506974  12.457668  11.77531
 11.515243  11.466802  11.156118  11.221103  10.858519  10.3201065
  9.945221   9.8175955  9.225683   8.712488   8.654356   8.67263
  9.141413   9.409337   9.612831   9.851311  10.019049   9.031128
  7.084306   6.3211083  6.004513   6.2094054  6.1819954  6.3919716
  6.552382   7.3870125  8.593411   8.014752   6.90283    5.88095
  5.3529553  5.1805487  5.366306   5.912383   6.407262   7.951343
  9.823698  10.786791  10.502041   9.930892   9.279857   8.534563
  7.776849   7.361539   7.081237   6.917084   7.018802   6.866173
  7.1004333  7.977785   8.882914   8.505482   8.102007   7.782869
  7.6394095  7.931108   8.668118   9.762212  11.279921  12.932521
 11.648655  10.656303  10.060484   9.949065   9.821008   9.710123
 10.005263   9.986354   9.456827   8.573851   8.032652   7.396915
  7.056247   7.006975   7.6739864  8.595387   8.864474   9.039815
  9.306958   8.548561   6.424003   5.380119   4.7683434  4.7090793
  5.074761   5.186607   5.260431   5.741959   6.7017884  7.1935153
  5.9184546  4.8549933  4.196884   4.131682   4.421068   4.963358
  5.0296416  5.310992   6.8553677  8.247125   9.347868   9.437363
  8.913955   8.310286   7.625022   6.910622   6.3381376  5.70657
  5.743175   6.057147   5.983678   6.590239   7.3448734  7.329012
  6.676935   6.2764587  5.8523035  5.725159   6.1670766  6.9767184
  8.116781   9.705103  11.137679   9.937884   9.006515   8.562069
  8.535125   8.468763   8.475918   8.952837   8.868683   7.7482114
  6.9389763  6.432998   5.894712   5.6452565  5.719048   6.525267
  7.5780034  8.41891    8.606812   8.696169   6.2195525  4.9286175
  3.9371948  3.521212   3.7704349  3.95659    4.029743   4.5401015
  5.4079485  6.3551054  5.525204   4.0109334  3.0614417  2.8033793
  3.1518488  3.8570375  4.2443285  4.4431043  4.860475   5.5953665
  6.765899   8.007834   8.709164   8.265137   7.72489    7.066749
  6.379007   5.7844014  5.116884   4.9471965  5.259813   5.5925894
  6.106121   6.3872285  5.8391266  5.080454   4.577573   4.088466
  4.0174117  4.477365   5.363821   6.5637684  8.2112465  9.854731
  8.6510315  7.6123238  7.219921   7.2506914  7.3980002  7.4289503
  7.3310657  7.585741   6.4493804  5.571528   5.10443    4.671623
  4.45947    4.586767   5.442166   6.472471   7.618095   8.277125
  6.8235235  5.0351586  3.7263799  2.8101864  2.6279223  2.9469981
  3.4253075  4.0455484  4.681363   5.4308834  6.2969565  4.5909896
  3.285337   2.4493222  1.888844   2.2181187  3.133817   3.521243
  3.9700131  4.574745   4.881547   5.319562   6.684812   8.232758
  7.9915156  7.5466776  6.949287   6.2811246  5.659271   4.931679
  4.736327   4.6840205  5.227829   5.7331095  5.2676697  4.5779467
  3.7826478  3.1094053  2.5810893  2.617707   3.1413176  3.9147065
  5.0094132  6.524481   8.434963   7.884074   6.788181   6.1662745
  6.0176086  6.4581885  6.0435505  5.7317796]
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.02%, model saved.
Epoch: 0 Train: 32566.24023 Test: 4300.02051
Epoch 80: New minimal relative error: 87.35%, model saved.
Epoch: 80 Train: 8985.77051 Test: 1586.71411
Epoch: 160 Train: 8669.27637 Test: 1486.73987
Epoch 240: New minimal relative error: 75.71%, model saved.
Epoch: 240 Train: 7992.40967 Test: 1272.41443
Epoch: 320 Train: 8051.02002 Test: 1276.96594
Epoch: 400 Train: 7856.35791 Test: 1181.53760
Epoch: 480 Train: 6805.66309 Test: 900.14850
Epoch: 560 Train: 4840.92627 Test: 503.94626
Epoch: 640 Train: 3485.36938 Test: 278.94153
Epoch: 720 Train: 1778.40186 Test: 96.94707
Epoch: 800 Train: 786.52881 Test: 34.95766
Epoch 880: New minimal relative error: 35.47%, model saved.
Epoch: 880 Train: 488.17972 Test: 12.89626
Epoch: 960 Train: 346.36993 Test: 8.03597
Epoch 1040: New minimal relative error: 18.68%, model saved.
Epoch: 1040 Train: 255.30022 Test: 4.13801
Epoch: 1120 Train: 208.01036 Test: 11.12412
Epoch: 1200 Train: 180.80225 Test: 2.23251
Epoch: 1280 Train: 166.97144 Test: 1.96846
Epoch: 1360 Train: 155.72562 Test: 1.96571
Epoch: 1440 Train: 137.66678 Test: 1.44942
Epoch: 1520 Train: 132.35011 Test: 1.44721
Epoch: 1600 Train: 127.68131 Test: 1.20204
Epoch 1680: New minimal relative error: 8.72%, model saved.
Epoch: 1680 Train: 126.02462 Test: 3.11190
Epoch: 1760 Train: 117.68562 Test: 1.30067
Epoch 1840: New minimal relative error: 8.05%, model saved.
Epoch: 1840 Train: 111.48651 Test: 1.33943
Epoch: 1920 Train: 112.89389 Test: 1.46691
Epoch: 2000 Train: 100.95039 Test: 2.06129
Epoch: 2080 Train: 100.56771 Test: 0.82751
Epoch 2160: New minimal relative error: 7.19%, model saved.
Epoch: 2160 Train: 95.86920 Test: 0.86506
Epoch: 2240 Train: 90.37919 Test: 0.72519
Epoch: 2320 Train: 87.15060 Test: 0.67753
Epoch 2400: New minimal relative error: 4.75%, model saved.
Epoch: 2400 Train: 84.59196 Test: 0.67778
Epoch: 2480 Train: 88.29561 Test: 0.69863
Epoch: 2560 Train: 83.45866 Test: 0.72094
Epoch: 2640 Train: 86.81136 Test: 0.77738
Epoch: 2720 Train: 92.61071 Test: 2.36239
Epoch: 2800 Train: 89.47825 Test: 0.81494
Epoch: 2880 Train: 85.65601 Test: 1.03566
Epoch: 2960 Train: 78.98283 Test: 0.62299
Epoch: 3040 Train: 80.62658 Test: 3.82417
Epoch: 3120 Train: 75.84809 Test: 0.57812
Epoch: 3200 Train: 74.31439 Test: 0.71598
Epoch: 3280 Train: 72.08742 Test: 1.29001
Epoch: 3360 Train: 69.65765 Test: 0.46538
Epoch: 3440 Train: 69.33775 Test: 0.47021
Epoch: 3520 Train: 69.06785 Test: 0.84378
Epoch: 3600 Train: 67.46384 Test: 0.44175
Epoch: 3680 Train: 66.32464 Test: 0.40572
Epoch: 3760 Train: 65.94978 Test: 0.41783
Epoch: 3840 Train: 65.15356 Test: 0.40838
Epoch: 3920 Train: 64.78484 Test: 0.40366
Epoch: 4000 Train: 62.91983 Test: 0.41453
Epoch: 4080 Train: 63.52611 Test: 0.41436
Epoch: 4160 Train: 64.40504 Test: 0.56027
Epoch: 4240 Train: 60.82367 Test: 0.36462
Epoch 4320: New minimal relative error: 3.79%, model saved.
Epoch: 4320 Train: 59.36162 Test: 0.37018
Epoch: 4400 Train: 59.22702 Test: 0.36524
Epoch: 4480 Train: 58.21301 Test: 0.30926
Epoch: 4560 Train: 58.74882 Test: 0.33115
Epoch: 4640 Train: 59.98430 Test: 0.33760
Epoch: 4720 Train: 60.34216 Test: 0.33415
Epoch: 4800 Train: 58.85735 Test: 0.34578
Epoch: 4880 Train: 57.79888 Test: 0.32178
Epoch: 4960 Train: 56.84101 Test: 0.32310
Epoch: 5040 Train: 56.00629 Test: 0.39343
Epoch: 5120 Train: 55.02496 Test: 0.93073
Epoch: 5200 Train: 53.46109 Test: 0.30097
Epoch: 5280 Train: 53.36639 Test: 0.27367
Epoch 5360: New minimal relative error: 2.63%, model saved.
Epoch: 5360 Train: 53.63280 Test: 0.28384
Epoch 5440: New minimal relative error: 2.15%, model saved.
Epoch: 5440 Train: 52.52647 Test: 0.27986
Epoch: 5520 Train: 51.64521 Test: 0.32170
Epoch: 5600 Train: 50.70922 Test: 0.39306
Epoch: 5680 Train: 49.29121 Test: 0.27230
Epoch: 5760 Train: 48.75672 Test: 0.26077
Epoch: 5840 Train: 49.13580 Test: 0.58186
Epoch: 5920 Train: 48.48641 Test: 0.22098
Epoch: 6000 Train: 48.37843 Test: 0.28193
Epoch: 6080 Train: 47.29366 Test: 0.23091
Epoch: 6160 Train: 47.60265 Test: 0.22061
Epoch: 6240 Train: 48.19487 Test: 0.22209
Epoch: 6320 Train: 48.42825 Test: 0.24150
Epoch: 6400 Train: 48.30991 Test: 0.23562
Epoch: 6480 Train: 48.24726 Test: 0.22942
Epoch: 6560 Train: 46.56350 Test: 0.21692
Epoch: 6640 Train: 47.73521 Test: 0.22462
Epoch: 6720 Train: 46.81369 Test: 0.37132
Epoch: 6800 Train: 46.04902 Test: 0.17439
Epoch: 6880 Train: 45.75199 Test: 0.19732
Epoch: 6960 Train: 42.42619 Test: 0.27485
Epoch 7040: New minimal relative error: 1.82%, model saved.
Epoch: 7040 Train: 41.35164 Test: 0.14071
Epoch: 7120 Train: 40.06770 Test: 0.13450
Epoch: 7200 Train: 38.78804 Test: 0.12264
Epoch: 7280 Train: 39.19503 Test: 0.13419
Epoch: 7360 Train: 39.40435 Test: 0.14167
Epoch: 7440 Train: 39.35393 Test: 0.16525
Epoch: 7520 Train: 38.62356 Test: 0.13226
Epoch: 7600 Train: 36.58269 Test: 0.13242
Epoch: 7680 Train: 35.74562 Test: 0.11555
Epoch: 7760 Train: 35.55459 Test: 0.11287
Epoch: 7840 Train: 35.87054 Test: 0.12076
Epoch: 7920 Train: 37.01711 Test: 0.30875
Epoch: 7999 Train: 36.16111 Test: 0.12421
Training Loss: tensor(36.1611)
Test Loss: tensor(0.1242)
Learned LE: [ 8.2909167e-01  7.1214824e-03 -1.4492449e+01]
True LE: [ 8.4026521e-01  5.7190098e-03 -1.4515159e+01]
Relative Error: [0.5737847  0.590364   0.74384594 0.7767208  0.91161346 1.0661302
 1.0144298  0.8765851  0.74642545 0.39212528 0.14845444 0.34532285
 0.56672937 0.6147378  0.93126976 1.0739567  1.0479633  1.2514997
 1.4934455  1.6410192  1.5857627  1.2895356  0.8295201  0.7278865
 0.5644614  0.506285   0.5540115  0.50978357 0.514146   0.82223266
 1.0636648  1.2516435  1.4577068  1.7579364  2.0571887  2.2667317
 1.5181186  1.0193309  0.8007801  0.8078943  0.8066785  0.6966371
 0.4291649  0.26015943 0.18687223 0.6502993  0.8976718  1.1656038
 1.6162815  1.7515279  1.5605654  1.5727133  1.5218465  1.0995568
 0.9549397  1.0479918  1.1566544  1.0775591  0.9475677  0.9321348
 0.8350447  0.6684689  0.61371195 0.5691281  0.61513174 0.5596493
 0.52906424 0.6291275  0.95973974 1.0777042  1.017947   0.7795609
 0.40209988 0.13268057 0.1743175  0.17091508 0.5369541  0.73956704
 0.6439496  0.82756364 1.1722802  1.4290922  1.5460995  1.3962474
 1.0781094  0.9260987  0.6916584  0.6355635  0.7244702  0.6592297
 0.50527036 0.50217396 0.6192264  0.7977532  0.8673903  1.204162
 1.5396146  1.9020306  1.9595034  1.3025006  0.7799025  0.67772686
 0.6147456  0.51427686 0.3093555  0.26815847 0.3082442  0.73463535
 0.8408896  1.0318017  1.4479423  1.5861514  1.4933963  1.542052
 1.4506104  1.1882999  1.0231162  1.0989757  1.2344581  1.2174561
 1.0478694  0.8732132  0.8003423  0.8159413  0.7003693  0.7217905
 0.76169974 0.8172841  0.57553875 0.37327254 0.49961832 0.91800076
 1.1364837  1.0451684  0.9909282  0.57441187 0.2126062  0.13821904
 0.21943145 0.5658442  0.6249724  0.46571997 0.8221996  1.1403488
 1.3538134  1.3224779  1.2719015  1.1974443  0.871779   0.82132936
 0.8855886  0.8536758  0.64420587 0.46357125 0.37731075 0.5272496
 0.57090956 0.59781903 1.0884867  1.354233   1.6298532  1.6214182
 1.0714228  0.62945473 0.54282546 0.5050478  0.308856   0.26202163
 0.42649555 0.7515896  0.7694428  0.8248569  1.1746949  1.3177644
 1.3782252  1.4340181  1.3428655  1.1198734  1.0617242  1.1631263
 1.25565    1.2988484  1.1334957  0.95268404 0.83411515 0.6956481
 0.7217749  0.6910898  0.78385633 0.9973081  1.0291096  0.8065274
 0.44012374 0.5858253  0.8915151  0.984381   0.978138   1.0110346
 0.66114503 0.2795086  0.14963949 0.37083206 0.5913663  0.5105901
 0.56537604 0.9801919  1.067242   1.0493305  1.1171746  1.2637446
 1.0872955  0.97737825 1.045602   0.9676039  0.81892514 0.6091534
 0.43644154 0.35437685 0.4792331  0.33926618 0.4839912  1.0477124
 1.3065419  1.2636259  1.2938687  0.9926794  0.5587124  0.5394658
 0.4784916  0.29830533 0.33267388 0.68698096 0.6784159  0.6223887
 0.8339287  1.066905   1.0379429  1.2212415  1.2489974  1.0465058
 1.0471143  1.2454487  1.4357517  1.3921216  1.2829272  1.0631967
 0.8624572  0.7564725  0.5888664  0.4471607  0.5654162  0.8066698
 1.1490029  1.170274   1.0302281  0.499503   0.5969333  0.8919635
 0.7675814  0.75937784 0.90590453 0.770553   0.3892038  0.26573214
 0.53485763 0.6243039  0.45999843 0.63264275 0.91811365 0.95810777
 0.82677275 1.1112858  1.1765165  1.2081643  1.1288059  1.0814847
 0.9130483  0.75990623 0.5603128  0.35451773 0.30941904 0.37077138
 0.17847094 0.47611856 1.0281203  1.2872094  0.9243028  0.92377084
 1.00775    0.6148188  0.5270314  0.5595199  0.33762887 0.5040421
 0.6355623  0.482145   0.61503905 0.81539255 0.74215907 0.8900933
 0.9713832  0.9242197  0.89976025 1.0800282  1.3134912  1.4081901
 1.3449429  1.1560235  0.97868264 0.7943049  0.58192587 0.3808782
 0.28328592 0.31701744 0.62461627 1.0187805  1.1726807  1.0790135
 0.56476885 0.46857312 0.91786486 0.56139916 0.57379454 0.76360697
 0.669968   0.524303   0.44668847 0.64165795 0.6283538  0.47308183
 0.5277753  0.84671044 0.8603425  1.0033616  1.0588589  1.2133738
 1.3945144  1.2684155  1.0696272  0.7829716  0.68514675 0.4902983
 0.31603995 0.23578641 0.3032032  0.18223552 0.4459083  0.9514862
 1.2667999  0.70969564 0.55215836 0.9587114  0.7334059  0.5217562
 0.51423264 0.41615567 0.6633726  0.4919847  0.5155143  0.5568111
 0.78119004 0.7262111  0.95068944 0.97957563 0.82947177 0.84335387
 1.0063486  1.1546735  1.1358572  1.155593   0.9460574  0.7786007
 0.70417404 0.66661304 0.30752775 0.32410774 0.30776274 0.3442146
 0.72828066 0.89361995 0.9273744  0.5898721  0.29797548 0.736925
 0.5645196  0.32482603 0.62744874 0.6351631  0.5551494  0.6294924
 0.7180566  0.60452724 0.39716792 0.30447882 0.64776015 0.7942021
 1.0322322  1.1366915  1.2929192  1.4716644 ]
