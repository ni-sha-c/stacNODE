time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.82%, model saved.
Epoch: 0 Train: 61572.83594 Test: 4000.00757
Epoch: 100 Train: 15561.80762 Test: 1474.19763
Epoch 200: New minimal relative error: 78.29%, model saved.
Epoch: 200 Train: 16299.41992 Test: 1289.13318
Epoch: 300 Train: 15890.39844 Test: 1317.38708
Epoch: 400 Train: 16018.26172 Test: 1136.41736
Epoch: 500 Train: 14123.69922 Test: 1890.07300
Epoch: 600 Train: 14142.15527 Test: 1272.21704
Epoch: 700 Train: 14527.08984 Test: 1167.42566
Epoch 800: New minimal relative error: 67.65%, model saved.
Epoch: 800 Train: 13789.92188 Test: 1254.32874
Epoch: 900 Train: 12029.83594 Test: 945.24329
Epoch 1000: New minimal relative error: 67.17%, model saved.
Epoch: 1000 Train: 12425.34082 Test: 995.33734
Epoch: 1100 Train: 11632.83594 Test: 836.45673
Epoch: 1200 Train: 9923.95508 Test: 736.93433
Epoch: 1300 Train: 7856.78564 Test: 487.31021
Epoch: 1400 Train: 6670.26855 Test: 306.19302
Epoch: 1500 Train: 4873.04688 Test: 207.43166
Epoch: 1600 Train: 4349.81348 Test: 736.45428
Epoch 1700: New minimal relative error: 16.23%, model saved.
Epoch: 1700 Train: 1335.58582 Test: 23.16077
Epoch 1800: New minimal relative error: 15.38%, model saved.
Epoch: 1800 Train: 1044.81006 Test: 14.59130
Epoch: 1900 Train: 913.41229 Test: 9.96206
Epoch: 2000 Train: 883.88776 Test: 11.91798
Epoch: 2100 Train: 723.55927 Test: 8.76088
Epoch: 2200 Train: 802.73517 Test: 163.27106
Epoch 2300: New minimal relative error: 12.82%, model saved.
Epoch: 2300 Train: 543.17975 Test: 4.93470
Epoch: 2400 Train: 475.27591 Test: 3.41865
Epoch 2500: New minimal relative error: 9.65%, model saved.
Epoch: 2500 Train: 452.07318 Test: 8.54083
Epoch: 2600 Train: 495.40656 Test: 10.13104
Epoch: 2700 Train: 391.49484 Test: 4.57527
Epoch: 2800 Train: 392.97018 Test: 3.47410
Epoch: 2900 Train: 336.54715 Test: 3.19700
Epoch: 3000 Train: 286.55319 Test: 1.76098
Epoch: 3100 Train: 254.61617 Test: 19.56608
Epoch: 3200 Train: 204.41519 Test: 0.81085
Epoch: 3300 Train: 199.87874 Test: 0.97000
Epoch: 3400 Train: 189.68710 Test: 0.90542
Epoch: 3500 Train: 214.03214 Test: 20.02210
Epoch 3600: New minimal relative error: 6.41%, model saved.
Epoch: 3600 Train: 182.37251 Test: 1.18531
Epoch: 3700 Train: 173.12387 Test: 0.81138
Epoch: 3800 Train: 165.28508 Test: 0.67170
Epoch: 3900 Train: 155.28790 Test: 0.66594
Epoch: 4000 Train: 156.23909 Test: 2.00827
Epoch: 4100 Train: 153.10464 Test: 0.67583
Epoch: 4200 Train: 144.54366 Test: 0.61866
Epoch 4300: New minimal relative error: 4.99%, model saved.
Epoch: 4300 Train: 134.87143 Test: 0.58783
Epoch: 4400 Train: 125.09311 Test: 0.99475
Epoch: 4500 Train: 122.86687 Test: 0.54097
Epoch: 4600 Train: 109.21872 Test: 1.32464
Epoch: 4700 Train: 174.18425 Test: 10.65625
Epoch: 4800 Train: 110.40938 Test: 0.43207
Epoch: 4900 Train: 108.85315 Test: 0.42187
Epoch: 5000 Train: 102.62891 Test: 0.35798
Epoch: 5100 Train: 111.94893 Test: 0.51240
Epoch: 5200 Train: 131.28635 Test: 0.80545
Epoch: 5300 Train: 113.03504 Test: 0.56388
Epoch 5400: New minimal relative error: 4.53%, model saved.
Epoch: 5400 Train: 107.32990 Test: 0.66598
Epoch: 5500 Train: 123.93790 Test: 1.99211
Epoch: 5600 Train: 126.25311 Test: 0.95021
Epoch: 5700 Train: 109.72396 Test: 0.53017
Epoch: 5800 Train: 103.16809 Test: 0.44890
Epoch: 5900 Train: 98.46851 Test: 0.39985
Epoch: 6000 Train: 99.63238 Test: 0.52216
Epoch: 6100 Train: 96.53378 Test: 1.46087
Epoch: 6200 Train: 119.78313 Test: 1.18203
Epoch: 6300 Train: 101.69295 Test: 0.77197
Epoch: 6400 Train: 115.99717 Test: 0.90638
Epoch: 6500 Train: 108.84567 Test: 0.86878
Epoch: 6600 Train: 89.29128 Test: 0.40034
Epoch: 6700 Train: 70.81351 Test: 0.18234
Epoch: 6800 Train: 75.09525 Test: 0.36022
Epoch: 6900 Train: 76.43071 Test: 0.35800
Epoch: 7000 Train: 77.55862 Test: 0.30496
Epoch: 7100 Train: 67.32608 Test: 0.45140
Epoch: 7200 Train: 66.38583 Test: 0.22741
Epoch: 7300 Train: 66.35501 Test: 0.25866
Epoch: 7400 Train: 64.22948 Test: 0.25159
Epoch 7500: New minimal relative error: 4.20%, model saved.
Epoch: 7500 Train: 69.18035 Test: 0.51137
Epoch 7600: New minimal relative error: 3.96%, model saved.
Epoch: 7600 Train: 70.17754 Test: 0.27873
Epoch: 7700 Train: 65.10530 Test: 0.23091
Epoch: 7800 Train: 61.37035 Test: 0.44282
Epoch: 7900 Train: 63.66980 Test: 0.20705
Epoch: 8000 Train: 65.36617 Test: 0.26409
Epoch: 8100 Train: 61.82697 Test: 0.23095
Epoch: 8200 Train: 58.55124 Test: 0.19374
Epoch: 8300 Train: 56.24726 Test: 0.37317
Epoch: 8400 Train: 53.25679 Test: 0.88076
Epoch: 8500 Train: 46.91518 Test: 0.09681
Epoch: 8600 Train: 46.16190 Test: 0.09886
Epoch 8700: New minimal relative error: 3.82%, model saved.
Epoch: 8700 Train: 49.67572 Test: 0.16430
Epoch: 8800 Train: 58.70977 Test: 0.21927
Epoch: 8900 Train: 59.74493 Test: 0.17098
Epoch: 9000 Train: 57.95315 Test: 0.24421
Epoch: 9100 Train: 58.12091 Test: 0.27904
Epoch: 9200 Train: 77.96617 Test: 0.68141
Epoch: 9300 Train: 62.90152 Test: 0.33613
Epoch: 9400 Train: 65.62782 Test: 0.32139
Epoch: 9500 Train: 56.79580 Test: 0.21812
Epoch: 9600 Train: 52.95255 Test: 0.19129
Epoch: 9700 Train: 49.42171 Test: 0.15601
Epoch: 9800 Train: 52.86110 Test: 0.23713
Epoch: 9900 Train: 65.07081 Test: 0.38338
Epoch: 9999 Train: 54.14326 Test: 0.30170
Training Loss: tensor(54.1433)
Test Loss: tensor(0.3017)
Learned LE: [ 8.6800510e-01 -7.9061749e-04 -1.4544571e+01]
True LE: [ 8.7599558e-01 -1.8137411e-04 -1.4552431e+01]
Relative Error: [2.433163   1.460178   0.9830079  1.5753541  2.0814152  2.4954636
 2.8558192  3.0892768  3.236302   3.1879947  3.213574   3.1541739
 2.728168   2.3435225  1.9472976  1.6496764  1.7996986  2.4885702
 2.553765   2.399816   2.2376256  1.800403   1.369517   0.8659707
 0.47471225 0.40830877 0.3747206  0.71324825 0.9741895  1.0185797
 0.87913543 0.83870006 0.74661285 0.74274194 1.0033128  1.1011456
 1.2080256  1.2383281  1.2770096  1.4129215  1.382309   1.3240459
 1.347319   1.3564407  1.2377082  1.0087098  0.8913999  0.87018424
 0.8600937  0.7609832  0.6292661  0.81602794 1.0789609  1.2000095
 1.1373364  1.0464002  1.3709066  2.2169235  2.0944018  1.810803
 1.738633   1.7995778  2.063348   1.391553   1.1600763  1.8974361
 2.539464   2.9985235  3.3664603  3.6593273  3.7902923  3.7312489
 3.5780723  3.5399952  3.1911206  2.7003088  2.296019   1.7563419
 1.285283   1.3494284  2.1566021  2.1982398  1.8710203  1.982172
 1.4998609  1.113958   0.6979937  0.6488004  0.5143993  0.70599353
 0.9728172  1.0227791  0.8877896  0.8826468  0.7544789  0.5846518
 0.7453816  0.81280607 0.87660366 0.87790656 0.80200285 0.85657865
 0.9817597  0.96934336 0.92757016 0.9793     1.0390506  0.96147263
 0.8628849  0.9445753  0.98268616 0.7457497  0.5511302  0.62616587
 1.0676708  1.2840341  1.3165371  1.1438649  1.0896374  1.502079
 2.0065541  1.8308809  1.7194946  1.7030964  1.7711604  1.4079933
 1.38671    2.1273482  2.806114   3.4249384  3.8005624  4.03468
 4.2221966  4.220249   4.0604563  3.8874164  3.5938838  3.1774416
 2.7399037  2.2121758  1.56892    0.8031196  0.9554611  1.7251047
 1.7150959  1.5903931  1.6888285  1.3062273  0.9460423  0.8556242
 0.7460645  0.7161844  0.92217046 1.0362831  0.95820177 0.8985745
 0.72497755 0.519825   0.6172113  0.6780029  0.6796275  0.7383332
 0.5967099  0.44882    0.4257332  0.5398339  0.58766884 0.6021379
 0.62592655 0.6990368  0.694936   0.8416109  1.0581392  0.9595878
 0.53720844 0.44935918 0.79706025 1.2264066  1.4323199  1.3583862
 1.09053    1.1017399  1.5743505  1.8245213  1.7229146  1.6944641
 1.6850584  1.6078036  1.3293865  2.1461372  3.0319796  3.6242592
 4.0735383  4.326235   4.4424543  4.5587826  4.456739   4.2948375
 4.077131   3.6620219  3.1973474  2.7573195  2.1633775  1.3929646
 0.5646001  0.5233921  1.4689081  1.3817507  1.3145444  1.5099113
 1.3778199  1.1455237  1.1151458  0.9193695  1.0074242  1.0232393
 0.9442699  0.8766234  0.7765464  0.43033454 0.40029246 0.5164527
 0.6149417  0.7870567  0.8898901  0.72799665 0.6122907  0.5903626
 0.62886196 0.61580336 0.5861951  0.5868325  0.60526973 0.60441405
 0.88289994 1.0210257  0.87676144 0.51635075 0.48073313 0.778264
 1.2893273  1.458066   1.2797397  1.0357381  1.082147   1.531078
 1.7454975  1.7913895  1.7799832  1.714861   1.380093   1.8811045
 2.826041   3.7454777  4.1971507  4.4651737  4.595603   4.6100497
 4.677029   4.5245347  4.3850045  4.1166496  3.7422214  3.252717
 2.7206042  2.0990167  1.4862331  0.87907165 0.15317099 1.242618
 1.1347507  1.2228587  1.6316673  1.5490762  1.4159423  1.3359267
 1.2505169  1.2672054  1.1889318  1.0157616  0.8099515  0.48409992
 0.33587858 0.29787767 0.44965404 0.73640245 0.90887916 1.0309036
 1.0335826  0.94584554 0.9081603  0.95937335 1.0823976  1.0153339
 0.9021813  0.73723805 0.6763371  0.9195705  0.85190886 0.72016054
 0.5629881  0.41897497 0.7694482  1.1743329  1.4297581  1.25128
 0.984546   0.8722327  1.2987857  1.6573538  1.828801   1.7727858
 1.7218801  1.4820855  2.2211487  3.2578123  4.0617146  4.5511174
 4.6522336  4.68109    4.5725355  4.600928   4.4573298  4.3210073
 4.080748   3.7841775  3.3225377  2.795457   2.2191486  1.73573
 1.3433735  0.45432895 0.8732228  1.1202277  1.2102938  1.7231092
 1.7038147  1.634346   1.4432532  1.3353045  1.4068483  1.2588391
 1.1527908  0.882614   0.5668288  0.40119347 0.495497   0.6894668
 0.8298968  0.9389894  1.0481852  1.2349287  1.2967104  1.0339786
 0.89785075 0.8882707  1.0737365  0.91719246 0.80301315 0.6259702
 0.75400615 0.6945653  0.5184615  0.46410322 0.43053734 0.6471156
 0.9297184  1.2977672  1.3292933  1.134969   0.81608117 0.8741175
 1.3343862  1.7133391  1.7259493  1.6178876  1.4024714  2.2281058
 3.3204317  4.1148524  4.618861   4.727865   4.6302996  4.4298453
 4.35033    4.218206   4.1064816  3.951972   3.7062232  3.3744297
 2.9312758  2.4135299  2.0152557  1.6997764  1.0334176  0.21191968
 1.2083071  1.2205583  1.6036795  1.9191132 ]
