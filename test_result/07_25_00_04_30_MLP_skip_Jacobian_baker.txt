time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 3000
num_train: 2000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 6
reg_param: 50.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 63.104347229 Test: 10.614480972
Epoch 0: New minimal relative error: 10.61%, model saved.
Epoch: 30 Train: 9.988954544 Test: 3.435937166
Epoch 30: New minimal relative error: 3.44%, model saved.
Epoch: 60 Train: 7.633537769 Test: 4.472991943
Epoch: 90 Train: 7.491391182 Test: 4.259881973
Epoch: 120 Train: 7.623734474 Test: 4.104556561
Epoch: 150 Train: 7.402598381 Test: 3.961178303
Epoch: 180 Train: 7.974898338 Test: 3.512339592
Epoch: 210 Train: 7.212564945 Test: 3.883525610
Epoch: 240 Train: 6.769792557 Test: 4.248289108
Epoch: 270 Train: 6.515050411 Test: 4.093630314
Epoch: 300 Train: 6.231921673 Test: 4.033582211
Epoch: 330 Train: 5.799889565 Test: 4.312286377
Epoch: 360 Train: 5.750322342 Test: 4.456821918
Epoch: 390 Train: 5.906954765 Test: 4.505185604
Epoch: 420 Train: 5.708132744 Test: 4.449435711
Epoch: 450 Train: 5.693734169 Test: 4.555940628
Epoch: 480 Train: 6.132467270 Test: 4.404551983
Epoch: 510 Train: 5.957729340 Test: 4.384997368
Epoch: 540 Train: 6.147077084 Test: 4.262044430
Epoch: 570 Train: 5.995340347 Test: 4.329716206
Epoch: 600 Train: 6.010498047 Test: 4.320327759
Epoch: 630 Train: 5.913352013 Test: 4.458054543
Epoch: 660 Train: 5.714550972 Test: 4.494379997
Epoch: 690 Train: 5.916988850 Test: 4.517776966
Epoch: 720 Train: 6.430809975 Test: 4.157156944
Epoch: 750 Train: 6.280080795 Test: 4.216540813
Epoch: 780 Train: 6.055902958 Test: 4.355913162
Epoch: 810 Train: 5.950037003 Test: 4.364394188
Epoch: 840 Train: 5.890022278 Test: 4.394854069
Epoch: 870 Train: 5.798621178 Test: 4.491651535
Epoch: 900 Train: 5.789878845 Test: 4.540628910
Epoch: 930 Train: 6.321538925 Test: 3.938071489
Epoch: 960 Train: 6.095795155 Test: 4.312466621
Epoch: 990 Train: 5.906331062 Test: 4.381619453
Epoch: 1020 Train: 5.854226589 Test: 4.403985023
Epoch: 1050 Train: 5.795144558 Test: 4.441006660
Epoch: 1080 Train: 5.771132469 Test: 4.452038765
Epoch: 1110 Train: 5.767678738 Test: 4.518845558
Epoch: 1140 Train: 5.778295517 Test: 4.573094368
Epoch: 1170 Train: 5.878679752 Test: 4.391121864
Epoch: 1200 Train: 5.877948761 Test: 4.458314896
Epoch: 1230 Train: 5.846096516 Test: 4.477102757
Epoch: 1260 Train: 5.772165298 Test: 4.485867500
Epoch: 1290 Train: 5.747578621 Test: 4.479909897
Epoch: 1320 Train: 5.712618351 Test: 4.505996704
Epoch: 1350 Train: 5.662264824 Test: 4.486135483
Epoch: 1380 Train: 5.653210640 Test: 4.505477428
Epoch: 1410 Train: 5.737853050 Test: 4.428387642
Epoch: 1440 Train: 5.768971443 Test: 4.446235657
Epoch: 1470 Train: 5.733930111 Test: 4.438554764
Epoch: 1500 Train: 5.703749180 Test: 4.448532581
Epoch: 1530 Train: 5.667468071 Test: 4.461744785
Epoch: 1560 Train: 5.651070118 Test: 4.459952831
Epoch: 1590 Train: 5.637624741 Test: 4.430817604
Epoch: 1620 Train: 5.619167328 Test: 4.473085880
Epoch: 1650 Train: 5.624255180 Test: 4.489266396
Epoch: 1680 Train: 5.646458149 Test: 4.494044781
Epoch: 1710 Train: 5.623937607 Test: 4.494295597
Epoch: 1740 Train: 5.654095173 Test: 4.482987881
Epoch: 1770 Train: 5.657427311 Test: 4.422783852
Epoch: 1800 Train: 5.669513702 Test: 4.460808754
Epoch: 1830 Train: 5.674790382 Test: 4.481983662
Epoch: 1860 Train: 5.655347347 Test: 4.470347881
Epoch: 1890 Train: 5.652629852 Test: 4.421249866
Epoch: 1920 Train: 5.646541595 Test: 4.506936550
Epoch: 1950 Train: 5.647118568 Test: 4.518426418
Epoch: 1980 Train: 5.650010109 Test: 4.534142971
Epoch: 2010 Train: 5.654099941 Test: 4.544144154
Epoch: 2040 Train: 5.657073021 Test: 4.557480335
Epoch: 2070 Train: 5.658980846 Test: 4.567018032
Epoch: 2100 Train: 5.666445732 Test: 4.572435379
Epoch: 2130 Train: 5.663643837 Test: 4.587383747
Epoch: 2160 Train: 5.653870583 Test: 4.568351269
Epoch: 2190 Train: 5.653787136 Test: 4.578242779
Epoch: 2220 Train: 5.647635460 Test: 4.593235970
Epoch: 2250 Train: 5.642823696 Test: 4.597048759
Epoch: 2280 Train: 5.636528969 Test: 4.605027676
Epoch: 2310 Train: 5.640793800 Test: 4.605349064
Epoch: 2340 Train: 5.646561146 Test: 4.618999958
Epoch: 2370 Train: 5.646077156 Test: 4.621107578
Epoch: 2400 Train: 5.651799679 Test: 4.613904953
Epoch: 2430 Train: 5.656847000 Test: 4.628791809
Epoch: 2460 Train: 5.666581154 Test: 4.631562233
Epoch: 2490 Train: 5.683337212 Test: 4.635892391
Epoch: 2520 Train: 5.684533119 Test: 4.642451286
Epoch: 2550 Train: 5.680694103 Test: 4.656240463
Epoch: 2580 Train: 5.683239460 Test: 4.635091782
Epoch: 2610 Train: 5.676120281 Test: 4.630140305
Epoch: 2640 Train: 5.669767380 Test: 4.623373032
Epoch: 2670 Train: 5.675844669 Test: 4.627523422
Epoch: 2700 Train: 5.678738117 Test: 4.605829239
Epoch: 2730 Train: 5.629440784 Test: 4.618564606
Epoch: 2760 Train: 5.638437748 Test: 4.625855446
Epoch: 2790 Train: 5.672904968 Test: 4.612175465
Epoch: 2820 Train: 5.661146164 Test: 4.611941338
Epoch: 2850 Train: 5.659142971 Test: 4.613302708
Epoch: 2880 Train: 5.674010754 Test: 4.613580227
Epoch: 2910 Train: 5.654940128 Test: 4.611754417
Epoch: 2940 Train: 5.637885094 Test: 4.609562874
Epoch: 2970 Train: 5.625247002 Test: 4.612483501
Epoch: 2999 Train: 5.615542412 Test: 4.613531590
Training Loss: tensor(5.6155)
Test Loss: tensor(4.6135)
True Mean x: tensor(2.9991, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(4.5335e+08, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3920, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(2.0869e+18, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0202)
Jacobian term Test Loss: tensor(0.0003)
Learned LE: [1.5190883  0.47169048]
True LE: tensor([ 0.6932, -0.7437], dtype=torch.float64)
