time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.00%, model saved.
Epoch: 0 Train: 32085.40430 Test: 3535.82959
Epoch 100: New minimal relative error: 89.15%, model saved.
Epoch: 100 Train: 8645.06055 Test: 1200.23779
Epoch: 200 Train: 7567.32764 Test: 1033.93579
Epoch 300: New minimal relative error: 71.98%, model saved.
Epoch: 300 Train: 7132.04150 Test: 893.29218
Epoch: 400 Train: 5735.66162 Test: 695.53607
Epoch: 500 Train: 3476.59668 Test: 304.09454
Epoch: 600 Train: 1898.09924 Test: 112.94265
Epoch 700: New minimal relative error: 37.31%, model saved.
Epoch: 700 Train: 951.08081 Test: 71.12521
Epoch 800: New minimal relative error: 13.81%, model saved.
Epoch: 800 Train: 469.28067 Test: 15.88511
Epoch: 900 Train: 337.92252 Test: 10.01978
Epoch: 1000 Train: 265.30984 Test: 14.09269
Epoch: 1100 Train: 270.05240 Test: 52.00249
Epoch: 1200 Train: 208.74432 Test: 6.64285
Epoch: 1300 Train: 226.39081 Test: 35.16371
Epoch 1400: New minimal relative error: 9.35%, model saved.
Epoch: 1400 Train: 172.23772 Test: 4.28325
Epoch: 1500 Train: 162.07822 Test: 26.10915
Epoch 1600: New minimal relative error: 5.36%, model saved.
Epoch: 1600 Train: 131.38129 Test: 2.47298
Epoch: 1700 Train: 133.67621 Test: 7.64752
Epoch: 1800 Train: 130.41219 Test: 6.39002
Epoch: 1900 Train: 117.77835 Test: 6.35684
Epoch: 2000 Train: 140.45773 Test: 8.94198
Epoch: 2100 Train: 118.96417 Test: 19.62307
Epoch 2200: New minimal relative error: 5.34%, model saved.
Epoch: 2200 Train: 95.24653 Test: 1.77646
Epoch: 2300 Train: 88.92140 Test: 1.55191
Epoch: 2400 Train: 84.42439 Test: 1.44623
Epoch: 2500 Train: 90.06756 Test: 2.00705
Epoch: 2600 Train: 80.35222 Test: 1.41157
Epoch 2700: New minimal relative error: 3.73%, model saved.
Epoch: 2700 Train: 81.96748 Test: 1.50223
Epoch: 2800 Train: 80.99557 Test: 1.49023
Epoch: 2900 Train: 79.59524 Test: 1.33754
Epoch: 3000 Train: 82.14034 Test: 1.55122
Epoch: 3100 Train: 77.33183 Test: 2.52964
Epoch: 3200 Train: 71.63511 Test: 2.01726
Epoch: 3300 Train: 67.65015 Test: 1.52448
Epoch: 3400 Train: 68.28550 Test: 1.42059
Epoch: 3500 Train: 66.51395 Test: 1.07425
Epoch 3600: New minimal relative error: 3.68%, model saved.
Epoch: 3600 Train: 65.82355 Test: 1.21709
Epoch: 3700 Train: 62.95019 Test: 0.97497
Epoch: 3800 Train: 61.30891 Test: 0.95599
Epoch: 3900 Train: 63.68334 Test: 5.66462
Epoch: 4000 Train: 59.63717 Test: 1.77083
Epoch: 4100 Train: 56.01836 Test: 0.89593
Epoch: 4200 Train: 55.35947 Test: 1.48646
Epoch: 4300 Train: 55.93092 Test: 0.94774
Epoch: 4400 Train: 60.57412 Test: 3.36530
Epoch: 4500 Train: 52.21982 Test: 0.64105
Epoch 4600: New minimal relative error: 2.41%, model saved.
Epoch: 4600 Train: 54.08011 Test: 0.64798
Epoch: 4700 Train: 52.30467 Test: 3.17434
Epoch: 4800 Train: 48.05529 Test: 0.63773
Epoch: 4900 Train: 51.05489 Test: 0.89003
Epoch: 5000 Train: 47.42141 Test: 0.58580
Epoch: 5100 Train: 48.88863 Test: 1.20380
Epoch: 5200 Train: 54.48719 Test: 1.41697
Epoch 5300: New minimal relative error: 1.60%, model saved.
Epoch: 5300 Train: 46.42363 Test: 0.59671
Epoch: 5400 Train: 48.62362 Test: 1.43696
Epoch: 5500 Train: 47.72277 Test: 0.93254
Epoch: 5600 Train: 46.11447 Test: 0.69025
Epoch: 5700 Train: 44.79254 Test: 0.54626
Epoch: 5800 Train: 44.16550 Test: 0.79979
Epoch: 5900 Train: 49.11356 Test: 5.90634
Epoch: 6000 Train: 42.22983 Test: 0.52509
Epoch: 6100 Train: 42.29774 Test: 0.53176
Epoch: 6200 Train: 40.49221 Test: 0.48720
Epoch: 6300 Train: 41.98449 Test: 1.15860
Epoch: 6400 Train: 43.25872 Test: 0.65222
Epoch: 6500 Train: 46.71896 Test: 1.52341
Epoch: 6600 Train: 42.66880 Test: 0.60019
Epoch: 6700 Train: 43.27211 Test: 1.01164
Epoch: 6800 Train: 39.88226 Test: 0.47237
Epoch: 6900 Train: 39.54082 Test: 0.48126
Epoch: 7000 Train: 42.06169 Test: 1.68400
Epoch: 7100 Train: 37.88923 Test: 0.56491
Epoch: 7200 Train: 36.83125 Test: 0.38964
Epoch: 7300 Train: 37.54036 Test: 0.41481
Epoch: 7400 Train: 37.89203 Test: 0.49705
Epoch: 7500 Train: 37.22299 Test: 0.99329
Epoch: 7600 Train: 35.77985 Test: 0.42729
Epoch: 7700 Train: 35.98823 Test: 0.49912
Epoch: 7800 Train: 35.16165 Test: 0.43176
Epoch: 7900 Train: 34.43860 Test: 0.40989
Epoch: 8000 Train: 34.58007 Test: 0.49459
Epoch: 8100 Train: 33.53496 Test: 0.40238
Epoch: 8200 Train: 33.23083 Test: 0.50713
Epoch: 8300 Train: 32.20275 Test: 0.44710
Epoch: 8400 Train: 31.87241 Test: 0.51232
Epoch: 8500 Train: 31.78905 Test: 0.33031
Epoch: 8600 Train: 31.11616 Test: 0.34794
Epoch: 8700 Train: 30.71782 Test: 0.29208
Epoch: 8800 Train: 31.03623 Test: 0.29611
Epoch: 8900 Train: 30.66448 Test: 0.27742
Epoch: 9000 Train: 29.69681 Test: 0.39509
Epoch: 9100 Train: 29.69658 Test: 0.64241
Epoch: 9200 Train: 28.88326 Test: 0.30662
Epoch: 9300 Train: 27.95584 Test: 0.26937
Epoch: 9400 Train: 28.11704 Test: 0.27945
Epoch: 9500 Train: 27.72468 Test: 0.25797
Epoch: 9600 Train: 27.15018 Test: 0.24497
Epoch: 9700 Train: 27.18503 Test: 0.26146
Epoch: 9800 Train: 27.90589 Test: 0.26431
Epoch: 9900 Train: 27.75720 Test: 0.27897
Epoch: 9999 Train: 28.22777 Test: 0.27706
Training Loss: tensor(28.2278)
Test Loss: tensor(0.2771)
Learned LE: [  0.9418905   -0.06669644 -14.548387  ]
True LE: [ 8.7361157e-01 -1.4430428e-03 -1.4543825e+01]
Relative Error: [0.64980537 0.43876156 0.22408643 0.23220947 0.3469839  0.5824706
 0.53456414 0.7205681  0.8527428  0.8257316  0.71376884 0.4046874
 0.35946006 0.47254124 0.5227323  0.3337423  0.84651524 1.3979821
 1.5791032  1.5288093  1.2942511  1.3630921  1.3132832  1.4712526
 1.589282   1.5373244  1.3446109  1.0002747  0.96422446 0.7230778
 0.46292806 0.24955478 0.5441903  0.5819367  0.9689083  1.2826852
 1.355916   1.3992702  1.6959971  2.0058105  1.9712381  1.947958
 1.784923   1.9897621  2.088787   2.2280817  2.0851812  1.2404214
 0.6834706  0.7142459  0.87581724 0.9003736  0.7681597  0.7174529
 0.7060908  0.90235865 1.0482708  0.8801028  0.4973425  0.40086907
 0.32986107 0.441633   0.64451414 0.857288   0.7759311  0.67258424
 0.591996   0.72484285 0.9612315  1.278702   1.1660901  1.2161777
 1.3056481  1.3283639  0.90615714 0.5587299  0.5795765  0.52150166
 0.37913033 0.2762333  0.5950259  1.0650058  1.2784834  1.1988255
 1.0485989  1.2503958  1.26725    1.3605065  1.4888605  1.4526441
 1.1260674  1.155942   0.8739791  0.36760497 0.24012649 0.4348119
 0.6096565  0.7273395  1.0263535  1.3374867  1.3328917  1.3601357
 1.6694858  1.7460641  1.6131583  1.5954651  1.5486151  1.7960312
 1.7383432  1.7952011  1.662662   0.91429424 0.73840976 0.77186346
 0.72741807 0.7340094  0.84600776 0.82173395 0.7892393  0.9001273
 0.79503804 0.59895694 0.3866724  0.24365184 0.5145191  0.8134162
 1.0958258  1.08432    0.98692596 0.94986165 0.819859   0.95420426
 1.2890557  1.7427561  1.8605355  2.0262387  2.038237   1.8240701
 1.3256761  1.0516038  0.8787727  0.53459    0.2955956  0.2061382
 0.2910407  0.77060455 1.0248268  0.9443558  0.8735528  1.1876377
 1.0420563  1.1847655  1.4171509  1.3740479  1.2491015  1.1506577
 0.65349966 0.3786402  0.17665446 0.43962985 0.7467136  0.8834578
 0.76144785 0.74918157 0.54524255 0.6628828  1.0495495  1.4148797
 1.3932179  1.4251615  1.2857597  1.5037862  1.2273278  1.3901838
 1.4326847  0.8599232  0.8475612  0.59367555 0.5382174  0.7974578
 0.8318828  0.7924903  0.8981663  0.78152686 0.63317066 0.44384742
 0.22899206 0.48800084 0.8992528  1.0833211  1.0582124  1.0163776
 1.0532794  1.1411264  1.1624513  1.2248001  1.3162684  1.5419896
 1.6886877  1.806224   2.095033   2.0967345  1.7459685  1.5102602
 0.94113034 0.6173487  0.39297426 0.1689862  0.23287083 0.4826558
 0.92689943 0.9237207  0.76881194 0.9263375  0.8140233  0.88092536
 1.3215187  1.4621106  1.299586   1.0980557  0.64102495 0.32890627
 0.24942434 0.26794288 0.60583365 0.3944653  0.2669397  0.49335238
 0.40696543 0.4073845  0.51088053 0.5235319  0.9034789  1.3173442
 1.0300815  0.990282   0.7931661  0.8964029  1.4723959  1.0145875
 0.6739406  0.59272766 0.49580577 0.80200714 0.83322984 0.7439171
 0.8333956  0.799467   0.7250444  0.548918   0.5508271  0.86320275
 0.8930837  1.1274157  1.1625531  1.0440791  1.1064538  0.95733404
 0.6376442  0.37106112 0.21971206 0.4312193  0.8590995  1.0152988
 0.99988717 0.88887376 0.88931006 1.3823017  1.3674427  0.8068924
 0.78306    0.54906636 0.31335804 0.2862121  0.38302445 0.6548798
 0.67185503 0.67803353 0.6953341  0.4779234  0.67025167 1.1203061
 1.0537964  1.180132   0.8554849  0.31397635 0.34517688 0.35090888
 0.20364425 0.25746104 0.3833933  0.2777423  0.50498617 0.5237248
 0.47895885 0.41307867 0.37619615 0.4440473  0.93581694 0.7074812
 0.5306246  0.3051622  0.7034922  1.330844   0.58402914 0.6077559
 0.7049453  0.5585656  0.6013012  0.6219703  0.6640017  0.8515912
 0.9084547  0.76242167 0.9221746  1.0339241  1.0170197  1.0319341
 1.1688026  0.8875084  0.43447992 0.2726315  0.42039096 0.632002
 0.6018617  0.530304   0.5195192  0.48471397 0.30397508 0.349873
 0.3231811  0.3402798  0.66735286 0.8023636  0.9704495  0.8597767
 1.0027896  0.75799286 0.26956764 0.15991665 0.30756974 0.3539892
 0.38879293 0.33578014 0.35472012 0.31312868 0.58181494 0.5867078
 0.7046041  0.64877236 0.27393386 0.17917463 0.44526383 0.38034773
 0.38285565 0.3800398  0.41167    0.24780287 0.34869477 0.38534528
 0.5279449  0.6660218  0.49539298 0.27370587 0.46257573 0.20135815
 0.05247028 0.29614782 0.88727224 0.48017833 0.47896984 0.5861215
 0.2833574  0.3763736  0.46614158 0.5294266  0.9251457  0.92455363
 1.0222058  1.2670736  0.9649149  0.8322686  0.93197393 0.99105155
 0.60653436 0.36492887 0.41489086 0.54425925 0.5786218  0.38229892
 0.5167368  0.47179693 0.44894847 0.56523764 0.3929717  0.30311662
 0.4346238  0.33949152 0.1933285  0.4456041 ]
