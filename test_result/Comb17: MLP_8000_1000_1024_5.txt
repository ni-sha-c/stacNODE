time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.28%, model saved.
Epoch: 0 Train: 3637.75513 Test: 4353.07910
Epoch 80: New minimal relative error: 61.61%, model saved.
Epoch: 80 Train: 26.34044 Test: 29.94978
Epoch: 160 Train: 6.64226 Test: 5.06233
Epoch 240: New minimal relative error: 35.17%, model saved.
Epoch: 240 Train: 6.17908 Test: 9.73084
Epoch 320: New minimal relative error: 19.41%, model saved.
Epoch: 320 Train: 3.47107 Test: 1.58350
Epoch 400: New minimal relative error: 15.31%, model saved.
Epoch: 400 Train: 1.14924 Test: 1.69499
Epoch 480: New minimal relative error: 14.80%, model saved.
Epoch: 480 Train: 27.01114 Test: 18.89190
Epoch: 560 Train: 4.20768 Test: 10.23885
Epoch: 640 Train: 0.20492 Test: 0.12410
Epoch: 720 Train: 0.45630 Test: 0.38271
Epoch: 800 Train: 1.91391 Test: 2.46226
Epoch: 880 Train: 0.88721 Test: 1.11240
Epoch: 960 Train: 5.48664 Test: 4.23335
Epoch: 1040 Train: 1.48133 Test: 3.07063
Epoch: 1120 Train: 0.23327 Test: 0.17685
Epoch: 1200 Train: 3.16217 Test: 2.93066
Epoch: 1280 Train: 6.95218 Test: 7.61075
Epoch 1360: New minimal relative error: 7.53%, model saved.
Epoch: 1360 Train: 0.24529 Test: 0.30171
Epoch: 1440 Train: 2.13479 Test: 1.58513
Epoch: 1520 Train: 0.23149 Test: 0.35784
Epoch: 1600 Train: 0.12607 Test: 0.16957
Epoch: 1680 Train: 2.10731 Test: 2.32664
Epoch: 1760 Train: 0.72650 Test: 0.67517
Epoch: 1840 Train: 1.62270 Test: 1.75432
Epoch: 1920 Train: 0.80708 Test: 0.48559
Epoch: 2000 Train: 2.05612 Test: 2.76583
Epoch: 2080 Train: 0.97064 Test: 1.05759
Epoch: 2160 Train: 0.19015 Test: 0.19071
Epoch: 2240 Train: 0.26372 Test: 0.32412
Epoch: 2320 Train: 1.13185 Test: 1.61248
Epoch: 2400 Train: 1.89539 Test: 1.58767
Epoch: 2480 Train: 0.17870 Test: 0.28198
Epoch: 2560 Train: 2.15320 Test: 3.02062
Epoch: 2640 Train: 0.04654 Test: 0.05144
Epoch: 2720 Train: 0.08802 Test: 0.08790
Epoch: 2800 Train: 0.24917 Test: 0.32626
Epoch: 2880 Train: 0.74448 Test: 0.67888
Epoch: 2960 Train: 0.81718 Test: 1.11683
Epoch: 3040 Train: 0.37872 Test: 0.38850
Epoch: 3120 Train: 0.25335 Test: 0.29024
Epoch: 3200 Train: 0.07833 Test: 0.09867
Epoch: 3280 Train: 1.33715 Test: 1.46495
Epoch: 3360 Train: 0.66407 Test: 1.13274
Epoch: 3440 Train: 0.04367 Test: 0.04417
Epoch 3520: New minimal relative error: 3.88%, model saved.
Epoch: 3520 Train: 0.42238 Test: 0.44582
Epoch: 3600 Train: 0.43978 Test: 0.54732
Epoch: 3680 Train: 0.88748 Test: 1.06958
Epoch: 3760 Train: 0.05196 Test: 0.04622
Epoch: 3840 Train: 0.31575 Test: 0.29877
Epoch: 3920 Train: 0.05176 Test: 0.06241
Epoch: 4000 Train: 0.02123 Test: 0.02627
Epoch: 4080 Train: 0.19780 Test: 0.23449
Epoch: 4160 Train: 0.54884 Test: 0.54892
Epoch: 4240 Train: 0.54513 Test: 0.65637
Epoch: 4320 Train: 0.25286 Test: 0.15906
Epoch: 4400 Train: 0.56942 Test: 0.48462
Epoch: 4480 Train: 0.25954 Test: 0.23317
Epoch: 4560 Train: 0.14085 Test: 0.18518
Epoch: 4640 Train: 0.31124 Test: 0.39221
Epoch: 4720 Train: 0.16362 Test: 0.21691
Epoch: 4800 Train: 0.25237 Test: 0.26696
Epoch: 4880 Train: 0.27229 Test: 0.35932
Epoch: 4960 Train: 0.01343 Test: 0.01679
Epoch: 5040 Train: 0.05961 Test: 0.06477
Epoch: 5120 Train: 0.25462 Test: 0.30692
Epoch: 5200 Train: 0.23409 Test: 0.33377
Epoch: 5280 Train: 0.37794 Test: 0.42628
Epoch: 5360 Train: 0.43700 Test: 0.65432
Epoch: 5440 Train: 0.02995 Test: 0.04006
Epoch: 5520 Train: 0.01705 Test: 0.01908
Epoch: 5600 Train: 0.15528 Test: 0.16753
Epoch: 5680 Train: 0.03017 Test: 0.03196
Epoch: 5760 Train: 0.07024 Test: 0.06824
Epoch: 5840 Train: 0.18043 Test: 0.23245
Epoch: 5920 Train: 0.20863 Test: 0.27002
Epoch: 6000 Train: 0.13541 Test: 0.16756
Epoch: 6080 Train: 0.28086 Test: 0.36960
Epoch: 6160 Train: 0.62883 Test: 0.60768
Epoch: 6240 Train: 0.12409 Test: 0.15382
Epoch: 6320 Train: 0.02011 Test: 0.02827
Epoch: 6400 Train: 0.01521 Test: 0.01651
Epoch: 6480 Train: 0.02083 Test: 0.02385
Epoch: 6560 Train: 0.61157 Test: 0.83427
Epoch: 6640 Train: 0.06458 Test: 0.08812
Epoch: 6720 Train: 0.01103 Test: 0.01494
Epoch: 6800 Train: 0.00882 Test: 0.01158
Epoch: 6880 Train: 0.01735 Test: 0.02252
Epoch: 6960 Train: 0.01249 Test: 0.01694
Epoch: 7040 Train: 0.16891 Test: 0.21862
Epoch: 7120 Train: 0.03775 Test: 0.02754
Epoch: 7200 Train: 0.11357 Test: 0.11926
Epoch: 7280 Train: 0.08713 Test: 0.05625
Epoch: 7360 Train: 0.02239 Test: 0.02682
Epoch: 7440 Train: 0.01282 Test: 0.01556
Epoch: 7520 Train: 0.01230 Test: 0.01722
Epoch: 7600 Train: 0.02225 Test: 0.02910
Epoch: 7680 Train: 0.02452 Test: 0.02594
Epoch: 7760 Train: 0.03546 Test: 0.04210
Epoch: 7840 Train: 0.12784 Test: 0.14109
Epoch: 7920 Train: 0.00902 Test: 0.01143
Epoch: 7999 Train: 0.25255 Test: 0.28437
Training Loss: tensor(0.2525)
Test Loss: tensor(0.2844)
Learned LE: [ 0.84066755 -0.02155614 -4.8008876 ]
True LE: [ 8.5769463e-01  8.1792064e-03 -1.4537376e+01]
Relative Error: [ 2.1875434   2.07327     1.935773    1.8766656   1.9884994   2.2513375
  2.5526917   2.7832084   2.8780797   2.8138583   2.5927444   2.2308376
  1.7745144   1.4262929   1.700553    2.6927469   4.0532513   5.580552
  7.162437    8.705149   10.117476   11.321575   12.270028   12.957086
 13.420993   13.737209   13.996402   14.266133   14.568324   14.88996
 15.199171   15.4457035  15.567559   15.51505    15.275182   14.87608
 14.364023   13.772517   13.109605   12.371949   11.562292   10.701833
  9.832314    8.9900055   8.183331    7.4085565   6.665431    5.9555125
  5.281039    4.6470146   4.062143    3.536986    3.0812335   2.7014263
  2.3996196   2.17412     2.0217967   1.9399139   1.9233625   1.9582659
  2.0155942   2.0533347   2.0314353   1.9383364   1.8192669   1.7799656
  1.9127171   2.188391    2.4876924   2.7021973   2.7730544   2.68466
  2.4457464   2.0730557   1.5998366   1.2012498   1.4456315   2.4418032
  3.7876189   5.287955    6.838093    8.344875    9.710411   10.8454685
 11.693983   12.251501   12.568323   12.7383795  12.8690405  13.036948
 13.262791   13.530734   13.811082   14.05677    14.198735   14.17116
 13.9467535  13.552711   13.048725   12.484221   11.871927   11.202159
 10.471482    9.693284    8.9024725   8.135672    7.403016    6.7010326
  6.028726    5.385306    4.7714586   4.191983    3.6560004   3.1745918
  2.7572784   2.410218    2.1350737   1.9301931   1.7929465   1.7217574
  1.714406    1.7606467   1.8346068   1.8944204   1.896685    1.8240756
  1.715791    1.6756316   1.8009473   2.0666137   2.3510768   2.5442367
  2.5905976   2.481965    2.2339935   1.8652277   1.3963187   0.96435034
  1.1812698   2.1683393   3.4778666   4.9313455   6.435545    7.9017334
  9.227663   10.310955   11.080445   11.525785   11.706835   11.736775
 11.740423   11.803863   11.946143   12.146438   12.381561   12.616995
 12.785389   12.804768   12.624078   12.255361   11.765884   11.228038
 10.667751   10.069973    9.422747    8.732544    8.025285    7.336285
  6.679221    6.051674    5.451264    4.8745575   4.3206105   3.7942584
  3.3056386   2.86628     2.485498    2.1685112   1.9167672   1.7292136
  1.6038424   1.5398511   1.536812    1.5880502   1.6721195   1.7497963
  1.7757473   1.7251792   1.6246547   1.5675008   1.6593251   1.8941982
  2.1542747   2.3241768   2.3476174   2.2229574   1.9738978   1.6215487
  1.1769559   0.7300885   0.9146851   1.8692223   3.1135843   4.494345
  5.932242    7.3475523   8.639602    9.695647   10.422545   10.791225
 10.862272   10.767001   10.648886   10.606029   10.658495   10.776334
 10.940171   11.137848   11.319599   11.396898   11.290943   10.980714
 10.521042   10.00562     9.4913435   8.96664     8.405428    7.807883
  7.1903706   6.5825267   6.0026603   5.4509373   4.923351    4.4137297
  3.9192915   3.4448926   3.0018601   2.6031327   2.2576966   1.9693394
  1.738758    1.5656916   1.449515    1.390048    1.3873717   1.4374185
  1.5239161   1.6129206   1.6604664   1.6352332   1.5466448   1.4671463
  1.505221    1.6889004   1.9165434   2.064375    2.0685618   1.9310961
  1.6851131   1.357086    0.95227194  0.5124243   0.65959954  1.5480255
  2.691124    3.9671535   5.310337    6.6541553   7.908561    8.9593935
  9.691421   10.040792   10.049516    9.85823     9.630209    9.480078
  9.439926    9.46923     9.537573    9.656607    9.813205    9.9323635
  9.917385    9.707144    9.314081    8.82652     8.34306     7.8846216
  7.410365    6.906844    6.38541     5.864069    5.363616    4.888842
  4.435157    3.993659    3.5591006   3.1358075   2.7366993   2.3769825
  2.0662987   1.8065326   1.5963541   1.4357028   1.3260821   1.2686421
  1.2635399   1.307564    1.3889209   1.4807779   1.5445062   1.5462868
  1.4794271   1.387096    1.3642782   1.4772313   1.663172    1.7932823
  1.7861083   1.6388882   1.3947958   1.09109     0.73359334  0.32784805
  0.44774166  1.2233016   2.220975    3.3548675   4.5669484   5.8058205
  6.9998846   8.049029    8.828774    9.232951    9.255086    9.020695
  8.708767    8.453882    8.316335    8.264023    8.234411    8.238835
  8.314547    8.426305    8.480741    8.391411    8.11932     7.6995015
  7.2365904   6.819483    6.4285083   6.0180626   5.5948143   5.1681194
  4.7507424   4.3536944   3.9748952   3.6029036   3.2297087   2.8583481
  2.5020237   2.1789954   1.902003    1.6721      1.4840431   1.3355824
  1.2299223   1.1714586   1.1615746   1.1969771   1.2681229   1.3546288
  1.4254767   1.4502494   1.4133923   1.3297045   1.2614323   1.2926329
  1.4231855   1.5412205   1.5392488   1.3930327   1.1482677   0.8603919
  0.5492771   0.22357388  0.3463663   0.93916726  1.737385    2.6871312
  3.7284467   4.8186827   5.9081016   6.9235196   7.7573833   8.278637
  8.411536    8.221565    7.880583    7.542485  ]
