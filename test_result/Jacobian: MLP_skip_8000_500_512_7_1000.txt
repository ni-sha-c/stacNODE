time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 101.98%, model saved.
Epoch: 0 Train: 59697.64453 Test: 3858.52002
Epoch: 80 Train: 16052.57227 Test: 1625.88208
Epoch: 160 Train: 14703.68359 Test: 1540.78406
Epoch: 240 Train: 13556.52441 Test: 1576.31079
Epoch: 320 Train: 12454.45215 Test: 2230.28687
Epoch 400: New minimal relative error: 71.37%, model saved.
Epoch: 400 Train: 14807.36035 Test: 1246.33569
Epoch: 480 Train: 12927.92773 Test: 1155.39868
Epoch: 560 Train: 12596.42969 Test: 1205.05884
Epoch: 640 Train: 14769.79492 Test: 1326.40405
Epoch: 720 Train: 11713.53223 Test: 1447.61035
Epoch: 800 Train: 15222.24512 Test: 1513.25476
Epoch: 880 Train: 11663.27930 Test: 1107.25671
Epoch: 960 Train: 13223.26758 Test: 1128.98376
Epoch 1040: New minimal relative error: 57.65%, model saved.
Epoch: 1040 Train: 11973.39941 Test: 1036.26941
Epoch: 1120 Train: 13441.12402 Test: 1051.12195
Epoch: 1200 Train: 12468.66211 Test: 978.70203
Epoch: 1280 Train: 10735.21289 Test: 843.11145
Epoch 1360: New minimal relative error: 56.17%, model saved.
Epoch: 1360 Train: 10189.85059 Test: 825.46234
Epoch: 1440 Train: 10590.13770 Test: 1300.15601
Epoch: 1520 Train: 9362.06348 Test: 617.70117
Epoch: 1600 Train: 8945.62988 Test: 621.24579
Epoch: 1680 Train: 8039.90283 Test: 450.76456
Epoch: 1760 Train: 7160.18457 Test: 364.15201
Epoch: 1840 Train: 6402.36035 Test: 305.22849
Epoch: 1920 Train: 4943.82129 Test: 188.58818
Epoch: 2000 Train: 2406.02344 Test: 93.10735
Epoch: 2080 Train: 1216.43091 Test: 39.48112
Epoch 2160: New minimal relative error: 22.29%, model saved.
Epoch: 2160 Train: 796.29303 Test: 19.08060
Epoch: 2240 Train: 635.78259 Test: 15.75481
Epoch: 2320 Train: 433.36285 Test: 12.48306
Epoch 2400: New minimal relative error: 14.71%, model saved.
Epoch: 2400 Train: 390.46997 Test: 7.10343
Epoch: 2480 Train: 349.00998 Test: 4.40195
Epoch 2560: New minimal relative error: 7.14%, model saved.
Epoch: 2560 Train: 317.81601 Test: 3.78180
Epoch: 2640 Train: 289.78082 Test: 7.59982
Epoch: 2720 Train: 273.18182 Test: 4.31527
Epoch: 2800 Train: 266.38385 Test: 3.47911
Epoch: 2880 Train: 239.43883 Test: 2.70051
Epoch: 2960 Train: 225.73706 Test: 2.40170
Epoch 3040: New minimal relative error: 7.14%, model saved.
Epoch: 3040 Train: 241.31680 Test: 4.72891
Epoch: 3120 Train: 277.28848 Test: 7.23140
Epoch: 3200 Train: 259.80939 Test: 6.04741
Epoch: 3280 Train: 221.81924 Test: 2.85225
Epoch: 3360 Train: 223.04715 Test: 2.37875
Epoch: 3440 Train: 236.15981 Test: 5.41221
Epoch 3520: New minimal relative error: 3.46%, model saved.
Epoch: 3520 Train: 192.50749 Test: 1.69987
Epoch 3600: New minimal relative error: 3.10%, model saved.
Epoch: 3600 Train: 185.89943 Test: 1.58831
Epoch: 3680 Train: 176.23199 Test: 1.72558
Epoch: 3760 Train: 169.92970 Test: 2.16738
Epoch: 3840 Train: 161.51512 Test: 1.55605
Epoch: 3920 Train: 143.98476 Test: 1.32696
Epoch: 4000 Train: 138.26324 Test: 1.35236
Epoch: 4080 Train: 127.07024 Test: 1.43801
Epoch: 4160 Train: 130.03149 Test: 2.39740
Epoch: 4240 Train: 117.44934 Test: 0.95765
Epoch: 4320 Train: 113.77375 Test: 1.01768
Epoch: 4400 Train: 120.40031 Test: 1.16698
Epoch: 4480 Train: 122.33498 Test: 4.46747
Epoch: 4560 Train: 122.99661 Test: 1.13944
Epoch: 4640 Train: 126.87502 Test: 1.15526
Epoch: 4720 Train: 114.04701 Test: 1.16560
Epoch: 4800 Train: 121.45501 Test: 1.41179
Epoch: 4880 Train: 181.03554 Test: 3.33663
Epoch: 4960 Train: 133.31018 Test: 1.16928
Epoch: 5040 Train: 112.28755 Test: 1.30247
Epoch: 5120 Train: 100.47406 Test: 1.20188
Epoch: 5200 Train: 103.97527 Test: 1.56761
Epoch: 5280 Train: 101.28185 Test: 0.80061
Epoch: 5360 Train: 107.24385 Test: 1.24316
Epoch: 5440 Train: 122.03197 Test: 1.85001
Epoch: 5520 Train: 114.70633 Test: 2.11405
Epoch: 5600 Train: 96.69443 Test: 0.63505
Epoch: 5680 Train: 94.90854 Test: 2.24724
Epoch: 5760 Train: 88.78376 Test: 0.55529
Epoch: 5840 Train: 102.39599 Test: 1.99040
Epoch: 5920 Train: 86.84123 Test: 0.75293
Epoch: 6000 Train: 88.88607 Test: 0.66281
Epoch: 6080 Train: 103.50337 Test: 2.03172
Epoch: 6160 Train: 84.08211 Test: 0.60981
Epoch: 6240 Train: 84.04579 Test: 0.67229
Epoch: 6320 Train: 87.67821 Test: 0.83074
Epoch 6400: New minimal relative error: 2.84%, model saved.
Epoch: 6400 Train: 89.04481 Test: 0.78284
Epoch: 6480 Train: 82.72622 Test: 0.50495
Epoch: 6560 Train: 83.50977 Test: 0.62341
Epoch: 6640 Train: 88.69857 Test: 0.81876
Epoch: 6720 Train: 94.80827 Test: 0.95065
Epoch: 6800 Train: 101.04460 Test: 0.86231
Epoch: 6880 Train: 89.22176 Test: 0.88738
Epoch: 6960 Train: 93.56113 Test: 0.86889
Epoch: 7040 Train: 85.02824 Test: 0.86228
Epoch: 7120 Train: 84.98286 Test: 0.76824
Epoch: 7200 Train: 73.80697 Test: 0.82856
Epoch: 7280 Train: 68.30170 Test: 0.67083
Epoch: 7360 Train: 65.26501 Test: 0.36729
Epoch: 7440 Train: 64.55289 Test: 1.44248
Epoch: 7520 Train: 58.36753 Test: 0.32072
Epoch: 7600 Train: 58.70705 Test: 0.42692
Epoch: 7680 Train: 56.97551 Test: 0.41297
Epoch: 7760 Train: 64.14780 Test: 0.50793
Epoch: 7840 Train: 59.81464 Test: 0.39660
Epoch: 7920 Train: 91.28487 Test: 3.65546
Epoch: 7999 Train: 62.88900 Test: 0.41783
Training Loss: tensor(62.8890)
Test Loss: tensor(0.4178)
Learned LE: [  0.8177871    0.06432705 -14.554127  ]
True LE: [ 8.7830925e-01 -4.4778632e-03 -1.4551398e+01]
Relative Error: [1.3973938  1.6492608  1.9158583  2.0749211  2.0820527  2.0577557
 2.0120652  1.8452024  1.7091429  1.6597271  1.7406821  1.8835624
 1.9474394  1.9001888  1.8682327  1.9022616  1.902967   1.8786732
 2.1060956  2.2247224  2.2203119  2.1857123  1.9259728  1.8095759
 1.6317481  1.4074795  1.3330611  1.5023018  1.7193934  1.9968506
 1.851012   1.7032511  1.5401341  1.5092213  1.5427958  1.5517638
 1.6224135  1.716982   1.4771395  1.1051979  0.9402172  0.97126764
 1.2775831  1.6372479  2.1681893  2.7766333  3.1180482  2.7891984
 2.4401073  2.2457502  2.1162992  2.1614816  2.1930048  1.9198451
 1.5644885  1.2354928  1.0522674  0.8203483  0.63237107 0.36523968
 0.5096816  0.82340604 1.2745906  1.5103855  1.6768022  1.7980442
 1.8503629  1.729805   1.6556048  1.6594305  1.5419688  1.3997858
 1.4465668  1.5443572  1.5451082  1.7082171  1.6585233  1.5859615
 1.5846041  1.6285522  1.8035985  2.002633   1.9955953  1.9990662
 1.9677579  1.727135   1.6961783  1.3194729  1.1364915  1.1969943
 1.4415258  1.7861675  1.8407958  1.6848123  1.506327   1.4423499
 1.4893059  1.4738715  1.5053483  1.707223   1.6419967  1.1884146
 0.8017279  0.78080887 0.9762241  1.4401029  1.738544   2.348111
 2.809719   2.914625   2.4271464  2.1303086  2.1563132  2.125756
 2.1902537  2.012586   1.7481056  1.4644125  1.1737301  0.8147698
 0.70538515 0.49093845 0.6139488  0.9148179  1.1769272  1.3466564
 1.406004   1.4657074  1.5259346  1.4841897  1.3521569  1.2403642
 1.3014625  1.2220814  1.1381894  1.1588858  1.220618   1.171687
 1.327729   1.3044006  1.2747049  1.2525451  1.4373705  1.5738554
 1.8655624  1.8065317  1.7711014  1.7792751  1.5785661  1.3495111
 1.0239248  0.8950404  0.99765015 1.3100457  1.6890874  1.6807128
 1.4356687  1.3473704  1.3657227  1.3639592  1.479345   1.4699955
 1.6564852  1.5334892  1.0240037  0.7544455  0.75777996 1.0428921
 1.4247739  1.9754003  2.434582   2.6513753  2.6707344  2.3795795
 2.1191597  2.040344   2.0693715  2.0334666  1.8080754  1.6366028
 1.4180235  1.1453154  0.7484075  0.5951959  0.5771391  0.8869832
 1.0311278  1.2819837  1.2951255  1.1930934  1.1801462  1.2001815
 1.1207443  1.039146   1.025724   1.1234617  1.1439112  1.1217281
 1.0953002  1.1136985  1.0153521  1.198062   1.1352699  1.0478725
 1.0976431  1.2184017  1.4144121  1.6920971  1.5224812  1.4751654
 1.5207876  1.3698946  1.0787308  0.78580946 0.7576522  0.92497385
 1.2976147  1.4694071  1.3896161  1.2396673  1.28446    1.2976947
 1.2725904  1.3390764  1.3998157  1.6674988  1.5188347  0.98787916
 0.83728087 0.8353969  1.1418568  1.4486731  1.9849565  2.2887964
 2.3779895  2.501678   2.2971914  2.0691662  1.9797679  1.8541614
 1.7981305  1.686487   1.55832    1.3818223  1.0163577  0.6975064
 0.5847866  0.7750599  0.9927289  1.0536182  1.1908593  1.239742
 1.1786574  1.0208442  1.057982   1.1336294  1.1304216  1.0762986
 1.1114391  1.2503132  1.3184106  1.1966556  1.1572394  1.0931404
 1.1035684  0.9593707  0.9070555  0.9268729  1.0545137  1.2184209
 1.3777372  1.250488   1.1168345  1.1935483  1.1621444  0.8702854
 0.6554439  0.613294   0.90526474 1.2376902  1.3270636  1.085223
 1.0891606  1.2262902  1.3438195  1.2294359  1.14427    1.2054806
 1.4518459  1.4120456  0.93073153 0.79697585 0.93862957 1.2926553
 1.5025691  1.8556956  2.003513   2.1461778  2.218844   2.1276128
 1.9268926  1.8197947  1.6484821  1.5602157  1.5268289  1.416315
 1.2029442  0.9311418  0.71566796 0.5929145  0.88348526 0.97677207
 0.92824435 1.1228145  1.1792319  1.1753708  1.188131   1.3168925
 1.2670635  1.1186781  1.2161065  1.1889684  1.2755358  1.3742164
 1.3913187  1.3067583  1.2552204  1.1493101  1.0430279  0.841898
 0.7404735  0.79532725 0.89839214 1.0152802  1.0139201  0.8708714
 0.8528403  0.95983845 0.8318536  0.67804736 0.48868972 0.6989841
 1.0770953  1.2057168  1.0024836  0.93459606 1.0123539  1.2666342
 1.2679137  0.9710758  1.0082366  0.91467273 0.893039   0.7361566
 0.48499373 0.56405514 0.822887   1.2663728  1.6917179  1.7048998
 1.7742332  1.9053432  1.8357041  1.7107553  1.5744485  1.5362656
 1.3294399  1.2314211  1.126175   0.9365898  0.86376685 0.6319079
 0.47761613 0.6124389  0.8086809  0.80615526 0.83296204 1.1311247
 1.0658121  1.1808429  1.2276027  1.3067625  1.2026726  1.1783936
 1.2436018  1.2089723  1.3692731  1.5073293  1.4124378  1.4220188
 1.3095689  1.1435643  1.0984209  0.88757014 0.7684662  0.58369726
 0.6115886  0.74977183 0.681036   0.63839   ]
