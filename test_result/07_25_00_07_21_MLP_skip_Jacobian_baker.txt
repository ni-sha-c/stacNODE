time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 4000
num_train: 2000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 7
reg_param: 50.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 58.356365204 Test: 11.848759651
Epoch 0: New minimal relative error: 11.85%, model saved.
Epoch: 40 Train: 8.283626556 Test: 3.907902956
Epoch 40: New minimal relative error: 3.91%, model saved.
Epoch: 80 Train: 7.890223026 Test: 4.437008858
Epoch: 120 Train: 7.688323975 Test: 4.204070091
Epoch: 160 Train: 7.794892311 Test: 3.893792629
Epoch 160: New minimal relative error: 3.89%, model saved.
Epoch: 200 Train: 6.683519363 Test: 4.164894104
Epoch: 240 Train: 6.381532192 Test: 4.351975441
Epoch: 280 Train: 6.068566322 Test: 4.483208656
Epoch: 320 Train: 5.898857117 Test: 4.510931015
Epoch: 360 Train: 5.929477692 Test: 4.452854156
Epoch: 400 Train: 5.774255753 Test: 4.555535793
Epoch: 440 Train: 5.809167862 Test: 4.509555817
Epoch: 480 Train: 5.701716423 Test: 4.618697643
Epoch: 520 Train: 6.241034508 Test: 4.353449821
Epoch: 560 Train: 5.891664505 Test: 4.428384304
Epoch: 600 Train: 5.903870583 Test: 4.421693802
Epoch: 640 Train: 6.067018509 Test: 4.431781769
Epoch: 680 Train: 5.921405315 Test: 4.541030884
Epoch: 720 Train: 6.122877121 Test: 4.595678329
Epoch: 760 Train: 6.025263786 Test: 4.528593063
Epoch: 800 Train: 5.986458778 Test: 4.559198380
Epoch: 840 Train: 5.952882767 Test: 4.595472813
Epoch: 880 Train: 5.917160988 Test: 4.616809845
Epoch: 920 Train: 5.917353153 Test: 4.612084866
Epoch: 960 Train: 5.932154655 Test: 4.611260891
Epoch: 1000 Train: 5.897238731 Test: 4.599807739
Epoch: 1040 Train: 5.861081600 Test: 4.631142139
Epoch: 1080 Train: 5.819176674 Test: 4.644891739
Epoch: 1120 Train: 5.786489964 Test: 4.638798237
Epoch: 1160 Train: 5.780201912 Test: 4.625005722
Epoch: 1200 Train: 5.718142509 Test: 4.619204521
Epoch: 1240 Train: 5.737837791 Test: 4.600268364
Epoch: 1280 Train: 5.690582752 Test: 4.625119209
Epoch: 1320 Train: 5.662768364 Test: 4.578294754
Epoch: 1360 Train: 5.681690216 Test: 4.637870312
Epoch: 1400 Train: 5.708939075 Test: 4.751136303
Epoch: 1440 Train: 5.715435028 Test: 4.666090012
Epoch: 1480 Train: 5.760072708 Test: 4.639680862
Epoch: 1520 Train: 5.762124062 Test: 4.746126652
Epoch: 1560 Train: 5.717018127 Test: 4.583293438
Epoch: 1600 Train: 5.708996773 Test: 4.586642742
Epoch: 1640 Train: 5.653088093 Test: 4.635774136
Epoch: 1680 Train: 5.686126709 Test: 4.608254910
Epoch: 1720 Train: 5.712971210 Test: 4.631645203
Epoch: 1760 Train: 5.732740402 Test: 4.611792564
Epoch: 1800 Train: 5.728383064 Test: 4.604359150
Epoch: 1840 Train: 5.660079002 Test: 4.553996563
Epoch: 1880 Train: 5.628187180 Test: 4.544335365
Epoch: 1920 Train: 5.644707680 Test: 4.573627949
Epoch: 1960 Train: 5.611206055 Test: 4.566946507
Epoch: 2000 Train: 5.593225956 Test: 4.561942577
Epoch: 2040 Train: 5.592735291 Test: 4.573321819
Epoch: 2080 Train: 5.582250595 Test: 4.582396030
Epoch: 2120 Train: 5.615992546 Test: 4.670568466
Epoch: 2160 Train: 5.603380203 Test: 4.585991383
Epoch: 2200 Train: 5.612209797 Test: 4.600021839
Epoch: 2240 Train: 5.699593544 Test: 4.604670048
Epoch: 2280 Train: 5.745576382 Test: 4.574107647
Epoch: 2320 Train: 5.729979038 Test: 4.532285213
Epoch: 2360 Train: 5.689284325 Test: 4.566109657
Epoch: 2400 Train: 5.610692024 Test: 4.554621220
Epoch: 2440 Train: 5.719907284 Test: 4.425468922
Epoch: 2480 Train: 5.604706764 Test: 4.544723034
Epoch: 2520 Train: 5.569808483 Test: 4.565821171
Epoch: 2560 Train: 5.547287464 Test: 4.533449650
Epoch: 2600 Train: 5.581599712 Test: 4.568986416
Epoch: 2640 Train: 5.586112022 Test: 4.539548397
Epoch: 2680 Train: 5.586334705 Test: 4.548833370
Epoch: 2720 Train: 5.571818352 Test: 4.555418015
Epoch: 2760 Train: 5.560953140 Test: 4.543419361
Epoch: 2800 Train: 5.560787201 Test: 4.538870811
Epoch: 2840 Train: 5.560725212 Test: 4.536446095
Epoch: 2880 Train: 5.567567825 Test: 4.545808315
Epoch: 2920 Train: 5.569147110 Test: 4.520213127
Epoch: 2960 Train: 5.570078373 Test: 4.522774696
Epoch: 3000 Train: 5.573238850 Test: 4.514798641
Epoch: 3040 Train: 5.575571537 Test: 4.509802818
Epoch: 3080 Train: 5.571849346 Test: 4.508743286
Epoch: 3120 Train: 5.580674648 Test: 4.509504795
Epoch: 3160 Train: 5.569555283 Test: 4.522611618
Epoch: 3200 Train: 5.566930771 Test: 4.542174816
Epoch: 3240 Train: 6.010747910 Test: 4.474014759
Epoch: 3280 Train: 5.617917061 Test: 4.543524265
Epoch: 3320 Train: 5.621644974 Test: 4.501519203
Epoch: 3360 Train: 5.638924122 Test: 4.488770008
Epoch: 3400 Train: 5.639700890 Test: 4.511789799
Epoch: 3440 Train: 5.637485027 Test: 4.522770405
Epoch: 3480 Train: 5.624743462 Test: 4.524426460
Epoch: 3520 Train: 5.624059677 Test: 4.533079624
Epoch: 3560 Train: 5.627756119 Test: 4.531315804
Epoch: 3600 Train: 5.630620003 Test: 4.550384521
Epoch: 3640 Train: 5.628434181 Test: 4.556705952
Epoch: 3680 Train: 5.630808353 Test: 4.561461449
Epoch: 3720 Train: 5.632871628 Test: 4.565043449
Epoch: 3760 Train: 5.627854347 Test: 4.568028927
Epoch: 3800 Train: 5.627335548 Test: 4.572644234
Epoch: 3840 Train: 5.614101410 Test: 4.579643726
Epoch: 3880 Train: 5.606603622 Test: 4.583060741
Epoch: 3920 Train: 5.597953796 Test: 4.583390236
Epoch: 3960 Train: 5.588369846 Test: 4.587513924
Epoch: 3999 Train: 5.579443455 Test: 4.589707375
Training Loss: tensor(5.5794)
Test Loss: tensor(4.5897)
True Mean x: tensor(2.9991, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(1.7028e+10, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3920, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(3.6167e+21, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0200)
Jacobian term Test Loss: tensor(0.0003)
Learned LE: [1.688506  0.4260574]
True LE: tensor([ 0.6932, -0.7437], dtype=torch.float64)
