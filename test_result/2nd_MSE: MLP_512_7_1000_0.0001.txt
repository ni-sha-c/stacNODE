time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.99%, model saved.
Epoch: 0 Train: 3516.56030 Test: 3956.78711
Epoch 100: New minimal relative error: 78.81%, model saved.
Epoch: 100 Train: 60.14557 Test: 66.45651
Epoch 200: New minimal relative error: 53.31%, model saved.
Epoch: 200 Train: 6.69614 Test: 9.72036
Epoch: 300 Train: 12.00153 Test: 12.15493
Epoch 400: New minimal relative error: 22.73%, model saved.
Epoch: 400 Train: 4.16174 Test: 1.98177
Epoch: 500 Train: 9.36120 Test: 9.48968
Epoch: 600 Train: 4.25099 Test: 3.05786
Epoch: 700 Train: 13.38526 Test: 8.29710
Epoch: 800 Train: 2.26217 Test: 2.14252
Epoch: 900 Train: 1.17995 Test: 2.07445
Epoch: 1000 Train: 1.30106 Test: 0.66130
Epoch: 1100 Train: 0.48696 Test: 0.36613
Epoch: 1200 Train: 2.09812 Test: 2.94430
Epoch: 1300 Train: 2.14726 Test: 1.85580
Epoch: 1400 Train: 0.57184 Test: 1.14725
Epoch: 1500 Train: 0.11370 Test: 0.24257
Epoch 1600: New minimal relative error: 12.36%, model saved.
Epoch: 1600 Train: 0.44342 Test: 0.66386
Epoch: 1700 Train: 0.48647 Test: 1.14253
Epoch: 1800 Train: 2.09902 Test: 1.72854
Epoch: 1900 Train: 0.24363 Test: 0.45762
Epoch: 2000 Train: 3.01819 Test: 3.44805
Epoch: 2100 Train: 0.42174 Test: 0.67148
Epoch: 2200 Train: 0.73598 Test: 0.88146
Epoch: 2300 Train: 2.08984 Test: 1.92530
Epoch: 2400 Train: 1.38968 Test: 2.04623
Epoch: 2500 Train: 0.08101 Test: 0.17726
Epoch: 2600 Train: 2.41800 Test: 2.28969
Epoch: 2700 Train: 0.47127 Test: 0.66997
Epoch: 2800 Train: 1.95843 Test: 1.91283
Epoch: 2900 Train: 0.14678 Test: 0.19061
Epoch: 3000 Train: 0.23106 Test: 0.17490
Epoch: 3100 Train: 0.92459 Test: 1.13663
Epoch: 3200 Train: 1.88109 Test: 1.56572
Epoch: 3300 Train: 0.08758 Test: 0.10253
Epoch: 3400 Train: 0.21677 Test: 0.29356
Epoch: 3500 Train: 0.28157 Test: 0.36757
Epoch: 3600 Train: 0.82837 Test: 1.10737
Epoch: 3700 Train: 0.54062 Test: 0.77207
Epoch: 3800 Train: 1.21390 Test: 1.54525
Epoch: 3900 Train: 0.35003 Test: 0.28248
Epoch: 4000 Train: 0.79084 Test: 0.95813
Epoch: 4100 Train: 0.32365 Test: 0.49719
Epoch: 4200 Train: 0.63824 Test: 0.73677
Epoch: 4300 Train: 0.05692 Test: 0.09414
Epoch: 4400 Train: 0.38161 Test: 0.42055
Epoch: 4500 Train: 0.04576 Test: 0.16117
Epoch: 4600 Train: 1.16064 Test: 1.25830
Epoch: 4700 Train: 0.23200 Test: 0.45152
Epoch: 4800 Train: 2.39858 Test: 2.69356
Epoch: 4900 Train: 0.07309 Test: 0.13204
Epoch: 5000 Train: 1.84326 Test: 1.85671
Epoch: 5100 Train: 0.21245 Test: 0.28762
Epoch: 5200 Train: 0.71268 Test: 1.06645
Epoch: 5300 Train: 1.72986 Test: 2.06804
Epoch: 5400 Train: 0.16717 Test: 0.23920
Epoch: 5500 Train: 0.45808 Test: 0.56206
Epoch: 5600 Train: 0.95754 Test: 1.22333
Epoch: 5700 Train: 0.77803 Test: 0.54798
Epoch: 5800 Train: 0.30070 Test: 0.36668
Epoch: 5900 Train: 0.13087 Test: 0.22720
Epoch: 6000 Train: 0.44598 Test: 0.54141
Epoch: 6100 Train: 0.24380 Test: 0.27576
Epoch: 6200 Train: 0.01720 Test: 0.05420
Epoch: 6300 Train: 0.42558 Test: 0.53688
Epoch: 6400 Train: 0.31902 Test: 0.33652
Epoch: 6500 Train: 0.64459 Test: 0.84796
Epoch: 6600 Train: 0.07451 Test: 0.05696
Epoch: 6700 Train: 0.05206 Test: 0.10384
Epoch: 6800 Train: 0.18869 Test: 0.29048
Epoch: 6900 Train: 0.10548 Test: 0.18485
Epoch: 7000 Train: 0.07312 Test: 0.07629
Epoch: 7100 Train: 0.22432 Test: 0.33783
Epoch: 7200 Train: 0.81249 Test: 1.00944
Epoch: 7300 Train: 0.12543 Test: 0.19226
Epoch: 7400 Train: 0.48083 Test: 0.49965
Epoch: 7500 Train: 0.04782 Test: 0.05702
Epoch: 7600 Train: 0.00802 Test: 0.04398
Epoch: 7700 Train: 0.02303 Test: 0.06225
Epoch: 7800 Train: 0.46742 Test: 0.62226
Epoch: 7900 Train: 0.29845 Test: 0.20702
Epoch: 8000 Train: 0.04203 Test: 0.07911
Epoch: 8100 Train: 0.11245 Test: 0.19091
Epoch: 8200 Train: 0.08664 Test: 0.12163
Epoch: 8300 Train: 0.02343 Test: 0.06106
Epoch: 8400 Train: 0.16410 Test: 0.12006
Epoch: 8500 Train: 0.01107 Test: 0.05108
Epoch: 8600 Train: 0.17420 Test: 0.27824
Epoch: 8700 Train: 0.25572 Test: 0.32051
Epoch: 8800 Train: 0.08278 Test: 0.14462
Epoch: 8900 Train: 0.02345 Test: 0.06029
Epoch: 9000 Train: 0.04596 Test: 0.08240
Epoch: 9100 Train: 0.18312 Test: 0.19222
Epoch: 9200 Train: 0.22644 Test: 0.34564
Epoch: 9300 Train: 0.00839 Test: 0.04167
Epoch: 9400 Train: 0.05667 Test: 0.11231
Epoch: 9500 Train: 0.02479 Test: 0.05282
Epoch: 9600 Train: 0.05761 Test: 0.09502
Epoch: 9700 Train: 0.02851 Test: 0.08021
Epoch: 9800 Train: 0.06762 Test: 0.07553
Epoch: 9900 Train: 0.01013 Test: 0.04240
Epoch: 9999 Train: 0.01349 Test: 0.04701
Training Loss: tensor(0.0135)
Test Loss: tensor(0.0470)
Learned LE: [ 0.9151608 -0.0323069 -4.184412 ]
True LE: [ 8.5736662e-01  5.0238832e-03 -1.4546778e+01]
Relative Error: [ 5.411059    4.8012333   4.2572284   3.7743766   3.3435564   2.9515722
  2.5830514   2.2253895   1.8740621   1.5366027   1.2336764   0.9959166
  0.85343885  0.8149412   0.85660356  0.9461838   1.0666578   1.2206767
  1.4286169   1.7208494   2.122434    2.6432557   3.2867312   4.0254383
  4.73415     5.42238     6.0534363   6.5826454   7.015571    7.3918223
  7.764946    8.183895    8.675322    9.232522    9.819111   10.38426
 10.880121   11.276135   11.566669   11.770256   11.920069   12.049369
 12.183214   12.3427     12.544237   12.780826   13.028627   13.287161
 13.547362   13.747482   13.807629   13.660695   13.268471   12.646088
 11.863908   11.017625   10.211542    9.180255    8.119309    7.217466
  6.4290075   5.725573    5.091097    4.5215483   4.015508    3.5686433
  3.1727638   2.8148835   2.4791412   2.1517305   1.8287629   1.522227
  1.260782    1.0804642   1.000355    1.0019568   1.046368    1.106167
  1.1735399   1.2559507   1.376431    1.5725883   1.8816856   2.3211896
  2.8915434   3.5871136   4.2952595   4.972563    5.5891905   6.101175
  6.5182967   6.886241    7.2646713   7.707791    8.241626    8.850912
  9.4866705  10.0870495  10.597466   10.984392   11.242057   11.393719
 11.48424    11.56103    11.655428   11.780639   11.946189   12.15151
 12.3696995  12.590939   12.824707   13.023125   13.101757   12.985918
 12.626537   12.029763   11.266055   10.436242    9.655668    8.6825285
  7.666029    6.81348     6.0754547   5.419319    4.827383    4.2961464
  3.824272    3.4082096   3.0414524   2.7130702   2.407112    2.108326
  1.8120954   1.5330585   1.3061028   1.1699785   1.1333072   1.1606908
  1.2064197   1.245732    1.2756834   1.3056724   1.3554239   1.4610741
  1.6689669   2.011125    2.4936428   3.116621    3.8148675   4.481437
  5.0881357   5.5903425   5.999041    6.362898    6.746998    7.213328
  7.7889247   8.448678    9.129732    9.759686   10.279342   10.654935
 10.880447   10.978651   10.99998    11.00845    11.0528     11.146918
 11.282851   11.455258   11.645383   11.828588   12.023991   12.215887
 12.321467   12.255674   11.95559    11.410533   10.683873    9.881306
  9.124255    8.24505     7.2677927   6.461522    5.7737293   5.166124
  4.6175528   4.123813    3.6830583   3.2922313   2.94785     2.6426797
  2.3623242   2.0900688   1.8186804   1.5623621   1.359112    1.2502424
  1.2396284   1.2830919   1.3293976   1.3532985   1.3550595   1.3470701
  1.3457364   1.3776485   1.4869887   1.7198285   2.0984066   2.6245494
  3.2846313   3.9428473   4.544547    5.046435    5.456336    5.8213625
  6.210217    6.6950836   7.3073916   8.013097    8.734429    9.387979
  9.910919   10.271738   10.4680805  10.520366   10.473659   10.398976
 10.369466   10.419596   10.534069   10.682252   10.848115   11.004322
 11.153666   11.32227    11.452285   11.449977   11.235599   10.774832
 10.111738    9.350761    8.6119375   7.86096     6.920678    6.154718
  5.5156565   4.9583364   4.45543     4.00069     3.5907261   3.221707
  2.8932526   2.6036677   2.34307     2.0945778   1.8469884   1.6100463
  1.4200343   1.3204441   1.3182303   1.3683376   1.4146779   1.4274508
  1.4068648   1.3692054   1.3318819   1.3116674   1.3385034   1.4602479
  1.7191164   2.1322625   2.7035584   3.351387    3.9508026   4.4617286
  4.8844748   5.259182    5.6515102   6.1444697   6.78137     7.524302
  8.281669    8.957557    9.482454    9.825607    9.992726   10.009271
  9.91064     9.754786    9.62733     9.598081    9.675412    9.81344
  9.965789   10.113398   10.232093   10.355492   10.487773   10.547256
 10.437595   10.096576    9.535385    8.840048    8.128891    7.4964337
  6.625124    5.8861623   5.289676    4.783508    4.330004    3.9186583
  3.543438    3.1976118   2.8815727   2.6000278   2.3510034   2.1214552
  1.8970056   1.6795393   1.4974796   1.392454    1.3799057   1.4241059
  1.4680161   1.4743602   1.4376559   1.3752514   1.3096207   1.255346
  1.2253267   1.25077     1.3825346   1.6631159   2.1069615   2.7027917
  3.2999258   3.8245504   4.2705207   4.667529    5.0657253   5.551419
  6.189138    6.951987    7.740654    8.445411    8.98464     9.320122
  9.459727    9.441995    9.308588    9.097328    8.872086    8.726149
  8.71604     8.822847    8.979256    9.132285    9.261546    9.349481
  9.446468    9.535962    9.528441    9.331654    8.917968    8.331584
  7.676173    7.079053    6.389622    5.650755    5.0820684   4.622944
  4.221927    3.8597374   3.5282202   3.2146976   2.9151757   2.6378798
  2.3911846   2.1717403   1.9667616   1.7707878   1.5995536   1.4850512
  1.4481153   1.4706917   1.5048693   1.5079372   1.4638636   1.3834295
  1.2920157   1.2110385   1.1478461   1.1093667   1.1279714   1.2608188
  1.5538918   2.0142803   2.5929987   3.1252928 ]
