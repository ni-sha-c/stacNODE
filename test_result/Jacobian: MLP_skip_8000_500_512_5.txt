time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.12%, model saved.
Epoch: 0 Train: 171574.28125 Test: 4165.76758
Epoch: 80 Train: 44533.81641 Test: 1951.17188
Epoch: 160 Train: 40057.04297 Test: 1766.67114
Epoch: 240 Train: 42196.63281 Test: 1922.52222
Epoch 320: New minimal relative error: 63.18%, model saved.
Epoch: 320 Train: 40696.70703 Test: 1404.86230
Epoch: 400 Train: 36494.56641 Test: 1315.09961
Epoch: 480 Train: 37460.03125 Test: 2193.77002
Epoch: 560 Train: 41790.15625 Test: 1483.53369
Epoch: 640 Train: 38325.53906 Test: 1354.03711
Epoch: 720 Train: 42265.57031 Test: 1516.43335
Epoch: 800 Train: 40666.28125 Test: 1996.09692
Epoch: 880 Train: 39674.55078 Test: 1381.93713
Epoch: 960 Train: 40226.18359 Test: 1679.05090
Epoch 1040: New minimal relative error: 62.71%, model saved.
Epoch: 1040 Train: 40500.71875 Test: 1499.36523
Epoch: 1120 Train: 36025.64844 Test: 1255.25671
Epoch: 1200 Train: 42042.71094 Test: 1492.02319
Epoch: 1280 Train: 39649.99609 Test: 1560.19751
Epoch: 1360 Train: 38870.23047 Test: 1361.85852
Epoch: 1440 Train: 38468.36328 Test: 1393.64551
Epoch 1520: New minimal relative error: 56.73%, model saved.
Epoch: 1520 Train: 38137.14453 Test: 1380.30383
Epoch: 1600 Train: 37868.15625 Test: 1349.47913
Epoch: 1680 Train: 37297.03516 Test: 1290.88123
Epoch: 1760 Train: 35679.44141 Test: 1267.38477
Epoch: 1840 Train: 37496.91797 Test: 1235.50647
Epoch: 1920 Train: 35314.51172 Test: 1229.97083
Epoch: 2000 Train: 34815.93359 Test: 1216.74426
Epoch: 2080 Train: 34206.00781 Test: 1177.82275
Epoch: 2160 Train: 34011.77734 Test: 1236.59705
Epoch: 2240 Train: 34805.96484 Test: 1198.00000
Epoch: 2320 Train: 33165.63281 Test: 1087.29932
Epoch: 2400 Train: 32555.57227 Test: 1084.96313
Epoch: 2480 Train: 32293.19922 Test: 1246.17468
Epoch: 2560 Train: 33332.20312 Test: 1116.56116
Epoch: 2640 Train: 30877.38086 Test: 1028.10425
Epoch: 2720 Train: 32025.08203 Test: 988.47314
Epoch: 2800 Train: 30754.02344 Test: 983.16113
Epoch: 2880 Train: 30070.57812 Test: 958.30304
Epoch: 2960 Train: 29162.99805 Test: 872.12445
Epoch: 3040 Train: 28595.27148 Test: 854.40405
Epoch: 3120 Train: 28377.99023 Test: 856.06561
Epoch: 3200 Train: 28940.59961 Test: 823.28302
Epoch: 3280 Train: 27676.55469 Test: 827.17114
Epoch: 3360 Train: 27567.58789 Test: 802.24426
Epoch: 3440 Train: 27432.21875 Test: 780.87836
Epoch: 3520 Train: 26520.16992 Test: 721.60913
Epoch: 3600 Train: 25209.65625 Test: 636.02985
Epoch: 3680 Train: 25400.07422 Test: 626.79352
Epoch: 3760 Train: 25352.96094 Test: 655.91217
Epoch: 3840 Train: 24351.76953 Test: 624.84509
Epoch: 3920 Train: 23426.11133 Test: 536.36176
Epoch: 4000 Train: 22561.20117 Test: 508.19498
Epoch: 4080 Train: 22274.22266 Test: 515.12262
Epoch: 4160 Train: 21381.27539 Test: 468.82602
Epoch: 4240 Train: 19895.08008 Test: 396.48022
Epoch: 4320 Train: 18337.97461 Test: 341.24414
Epoch: 4400 Train: 17186.11328 Test: 299.14429
Epoch: 4480 Train: 15922.29102 Test: 232.78227
Epoch: 4560 Train: 13885.85156 Test: 177.63145
Epoch: 4640 Train: 11950.04199 Test: 126.78605
Epoch: 4720 Train: 7599.59766 Test: 82.54316
Epoch: 4800 Train: 6118.17529 Test: 54.69112
Epoch 4880: New minimal relative error: 35.28%, model saved.
Epoch: 4880 Train: 5006.90771 Test: 44.71749
Epoch 4960: New minimal relative error: 20.70%, model saved.
Epoch: 4960 Train: 4326.16357 Test: 26.85513
Epoch: 5040 Train: 4035.41992 Test: 23.10096
Epoch: 5120 Train: 4271.73633 Test: 27.58414
Epoch 5200: New minimal relative error: 18.01%, model saved.
Epoch: 5200 Train: 4241.75830 Test: 30.00346
Epoch: 5280 Train: 4188.09326 Test: 46.87460
Epoch 5360: New minimal relative error: 15.16%, model saved.
Epoch: 5360 Train: 3605.75391 Test: 21.57103
Epoch: 5440 Train: 4192.41357 Test: 26.49052
Epoch: 5520 Train: 4073.03809 Test: 56.27440
Epoch 5600: New minimal relative error: 13.69%, model saved.
Epoch: 5600 Train: 3863.76196 Test: 24.56229
Epoch: 5680 Train: 3872.60449 Test: 50.40526
Epoch: 5760 Train: 3954.22876 Test: 54.75644
Epoch: 5840 Train: 3785.19678 Test: 31.43385
Epoch: 5920 Train: 3928.08350 Test: 39.15827
Epoch: 6000 Train: 3687.83423 Test: 37.23843
Epoch: 6080 Train: 3497.83521 Test: 15.27524
Epoch: 6160 Train: 3499.67993 Test: 29.98675
Epoch: 6240 Train: 3452.23022 Test: 46.32782
Epoch: 6320 Train: 3541.33276 Test: 44.41791
Epoch: 6400 Train: 3120.24536 Test: 18.70427
Epoch: 6480 Train: 2923.47754 Test: 15.19630
Epoch: 6560 Train: 2888.54810 Test: 12.40273
Epoch: 6640 Train: 3543.99243 Test: 69.82047
Epoch: 6720 Train: 2992.11426 Test: 17.40315
Epoch: 6800 Train: 2765.97559 Test: 15.39116
Epoch: 6880 Train: 3010.61914 Test: 21.06490
Epoch: 6960 Train: 3026.04785 Test: 29.44067
Epoch: 7040 Train: 2954.14111 Test: 19.97218
Epoch: 7120 Train: 2752.27954 Test: 12.27507
Epoch: 7200 Train: 2626.09302 Test: 10.72399
Epoch: 7280 Train: 2434.44751 Test: 10.41897
Epoch 7360: New minimal relative error: 11.01%, model saved.
Epoch: 7360 Train: 2307.00513 Test: 8.13512
Epoch: 7440 Train: 2190.86279 Test: 8.59080
Epoch 7520: New minimal relative error: 7.90%, model saved.
Epoch: 7520 Train: 2162.40552 Test: 7.57765
Epoch: 7600 Train: 2305.07642 Test: 12.57502
Epoch: 7680 Train: 2116.26440 Test: 16.91268
Epoch: 7760 Train: 2262.91968 Test: 10.63497
Epoch: 7840 Train: 2385.14893 Test: 54.21413
Epoch: 7920 Train: 2022.93335 Test: 29.25678
Epoch: 7999 Train: 1880.45850 Test: 11.40523
Training Loss: tensor(1880.4585)
Test Loss: tensor(11.4052)
Learned LE: [  0.9844726   -0.17813179 -14.49643   ]
True LE: [ 8.688041e-01 -3.602521e-04 -1.454392e+01]
Relative Error: [7.1260767  7.458536   7.498987   7.7287493  7.823457   7.5354724
 6.993657   5.8060255  4.3514957  3.105573   2.175581   1.6905686
 1.634565   1.6737385  1.7108872  1.7388562  1.7858006  1.7647349
 1.6622298  1.4794431  1.2658724  1.4091848  2.1859782  2.7900023
 2.4170215  2.4439118  2.601663   2.752009   2.629657   2.300527
 1.8910084  1.5120466  1.791645   2.8701887  4.2069626  4.340161
 4.344558   4.271151   4.377213   5.2938385  6.1667776  5.894452
 5.0222783  4.4449177  4.0879884  3.719543   3.5756574  3.7402906
 4.0899963  4.7164555  3.9065995  3.3935997  3.4399793  3.8450487
 4.285478   4.6283693  4.8205585  4.7903004  4.7720904  5.0967774
 5.3804517  5.8020344  6.516343   7.204276   7.451141   7.9322004
 8.028588   7.9695396  7.3836455  6.413764   4.8653326  3.4046297
 2.3605063  1.6426392  1.2827903  1.1073905  1.0450699  1.0179526
 1.2212851  1.3819813  1.5575442  1.5061394  1.2587092  1.0962757
 1.5639235  2.6554844  2.338383   2.223414   2.4047322  2.6104932
 2.7699056  2.7388556  2.3838613  2.1410468  2.0902808  2.9749124
 2.658654   3.2122028  3.231348   3.2125375  3.0416129  3.3856153
 4.4994683  5.9870524  5.687819   4.9783845  4.4840293  3.9005897
 3.6251094  3.6173496  3.8650267  4.3951893  4.245774   3.4567814
 3.3121402  3.5809784  3.988635   4.3929205  4.6736608  4.789505
 4.724585   4.6136823  4.98626    5.3291736  5.8908076  6.785993
 7.4556136  7.907694   8.462961   8.543037   8.287044   7.366649
 6.084807   4.52697    3.696403   2.7577355  2.1670406  1.627022
 1.1461803  0.45464736 0.4896187  1.0397377  1.5422776  1.7795278
 1.7741147  1.5635582  1.4878056  1.9915333  2.4763134  2.150942
 2.1998086  2.4682474  2.7029812  2.8871958  3.02307    2.728041
 2.7366335  2.2150853  1.3156073  2.5656638  3.062339   3.1888041
 3.0936086  2.9819117  3.041975   3.456819   4.506041   5.718801
 5.0753117  4.2979126  3.8240285  3.573154   3.6461937  4.0200915
 4.625902   3.5715454  3.1630988  3.3561013  3.796702   4.22503
 4.51999    4.7127333  4.727865   4.604226   4.4880137  5.0120735
 5.4998727  6.269545   7.2908583  7.9852557  8.773697   9.301235
 9.337556   8.889059   7.8317246  6.408743   5.4472265  4.6029844
 3.8830194  3.1430726  2.4340615  1.5691277  0.71885043 0.7044927
 1.3378544  1.8740379  2.0223715  2.0928812  2.1777592  2.4464278
 3.0446424  2.7194905  2.29442    2.2770185  2.6057334  2.8843079
 3.1258757  3.3855762  3.3288262  2.3186295  0.11630613 1.9768178
 3.4519746  3.8119967  3.7483587  3.7882187  3.4276135  2.3330667
 2.0598314  3.2243643  5.4671783  5.0158577  4.2653146  3.7199254
 3.5189965  3.6321006  4.0237803  3.9852903  3.0875206  3.0974362
 3.5843625  4.109915   4.4948506  4.7131453  4.778738   4.761853
 4.671391   4.5631948  5.250863   5.9848094  6.9531603  8.078161
 8.860481   9.922684   9.988391   9.363614   8.415632   7.170264
 5.927453   5.9158573  5.8902545  5.3047004  4.425023   3.5052133
 2.292181   1.4040502  1.0205163  1.5149263  1.9709376  2.3430014
 2.581029   2.725909   3.095526   3.6615584  3.001239   2.494121
 2.5298226  2.8034348  3.14488    3.4735155  3.8439968  3.3715057
 1.1400517  1.5043167  3.2273638  4.3364844  4.736646   4.6571126
 4.736429   3.610719   2.1338143  1.0421598  2.0488522  4.158722
 5.119531   4.221461   3.6533706  3.4245834  3.5218039  3.8869777
 3.3293047  2.7702856  3.0682333  3.6732972  4.2090917  4.566384
 4.763913   4.826482   4.891996   4.892309   4.941783   5.5728846
 6.612755   7.727537   9.031276   8.910403   8.956995   8.84171
 8.194836   7.2549195  5.935145   4.7606077  4.576384   4.4898076
 4.6780386  5.199111   4.5560336  3.2915885  1.9868368  1.4971715
 1.8553549  2.414004   2.655349   2.7243083  2.3659253  2.4737668
 3.2063112  2.7548423  2.1694705  2.1665041  2.6741755  3.4477777
 3.876588   4.35231    2.7680464  1.5293114  2.580107   4.1055975
 5.104221   5.539705   5.482869   5.510528   4.1908646  2.7484033
 1.4845634  0.88196766 2.5865233  4.9922547  4.402037   3.789467
 3.4763258  3.442086   3.75961    3.0758615  2.6319008  3.0047052
 3.640663   4.2159586  4.58109    4.7551756  4.845853   4.91152
 5.1446986  5.4877663  5.7863073  7.160721   7.8737216  7.753683
 7.7367916  7.8182945  8.011476   7.5135365  6.625235   5.236747
 4.1385317  3.577967   3.2538273  3.1074517  3.4692574  3.9226418
 4.474232   3.2168221  2.8514888  3.0175946  3.0960908  3.1178043
 2.4651842  1.9780712  1.909602   2.6274729 ]
