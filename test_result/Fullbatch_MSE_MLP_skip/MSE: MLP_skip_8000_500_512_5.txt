time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 102.12%, model saved.
Epoch: 0 Train: 3905.63525 Test: 4440.30371
Epoch 80: New minimal relative error: 83.55%, model saved.
Epoch: 80 Train: 104.18778 Test: 101.53239
Epoch 160: New minimal relative error: 44.78%, model saved.
Epoch: 160 Train: 14.17655 Test: 12.81010
Epoch 240: New minimal relative error: 22.55%, model saved.
Epoch: 240 Train: 12.76940 Test: 39.06226
Epoch: 320 Train: 6.54819 Test: 9.71398
Epoch 400: New minimal relative error: 16.56%, model saved.
Epoch: 400 Train: 4.62077 Test: 5.44754
Epoch 480: New minimal relative error: 12.77%, model saved.
Epoch: 480 Train: 3.87204 Test: 4.12994
Epoch: 560 Train: 25.01171 Test: 23.43151
Epoch: 640 Train: 3.05678 Test: 2.97963
Epoch: 720 Train: 12.74091 Test: 10.87530
Epoch: 800 Train: 7.03978 Test: 8.81903
Epoch: 880 Train: 13.93618 Test: 19.27389
Epoch: 960 Train: 1.66150 Test: 1.87500
Epoch 1040: New minimal relative error: 12.38%, model saved.
Epoch: 1040 Train: 2.09739 Test: 1.16826
Epoch: 1120 Train: 3.16247 Test: 3.03958
Epoch: 1200 Train: 4.30902 Test: 5.55487
Epoch: 1280 Train: 3.32102 Test: 2.96418
Epoch: 1360 Train: 4.67861 Test: 5.12048
Epoch 1440: New minimal relative error: 10.76%, model saved.
Epoch: 1440 Train: 0.44696 Test: 0.40447
Epoch: 1520 Train: 2.52821 Test: 3.06977
Epoch 1600: New minimal relative error: 9.79%, model saved.
Epoch: 1600 Train: 1.72183 Test: 2.04952
Epoch: 1680 Train: 0.74909 Test: 0.76902
Epoch 1760: New minimal relative error: 9.63%, model saved.
Epoch: 1760 Train: 2.69286 Test: 3.30968
Epoch: 1840 Train: 1.27132 Test: 1.18402
Epoch: 1920 Train: 1.90296 Test: 2.07831
Epoch: 2000 Train: 2.55010 Test: 2.65844
Epoch: 2080 Train: 0.81573 Test: 0.76179
Epoch: 2160 Train: 0.32031 Test: 0.41942
Epoch: 2240 Train: 0.92736 Test: 0.82773
Epoch: 2320 Train: 0.22422 Test: 0.21683
Epoch: 2400 Train: 2.15463 Test: 2.61739
Epoch: 2480 Train: 4.58259 Test: 5.36044
Epoch 2560: New minimal relative error: 8.09%, model saved.
Epoch: 2560 Train: 0.33822 Test: 0.38702
Epoch: 2640 Train: 0.22257 Test: 0.21208
Epoch: 2720 Train: 0.41490 Test: 0.40644
Epoch: 2800 Train: 0.78016 Test: 0.88272
Epoch: 2880 Train: 0.53477 Test: 0.61868
Epoch: 2960 Train: 0.46284 Test: 0.44951
Epoch: 3040 Train: 0.27397 Test: 0.37164
Epoch: 3120 Train: 0.21865 Test: 0.24572
Epoch: 3200 Train: 6.00735 Test: 6.42973
Epoch: 3280 Train: 0.41073 Test: 0.49744
Epoch: 3360 Train: 1.01660 Test: 1.09280
Epoch: 3440 Train: 0.22419 Test: 0.21059
Epoch: 3520 Train: 0.30469 Test: 0.34783
Epoch 3600: New minimal relative error: 6.87%, model saved.
Epoch: 3600 Train: 0.21829 Test: 0.19380
Epoch: 3680 Train: 0.92313 Test: 1.34512
Epoch: 3760 Train: 0.53389 Test: 0.58044
Epoch: 3840 Train: 0.23841 Test: 0.22668
Epoch: 3920 Train: 0.12060 Test: 0.12731
Epoch: 4000 Train: 0.55602 Test: 0.62004
Epoch: 4080 Train: 1.29901 Test: 1.69104
Epoch: 4160 Train: 0.76488 Test: 0.76896
Epoch: 4240 Train: 0.28204 Test: 0.38264
Epoch: 4320 Train: 0.10244 Test: 0.15640
Epoch: 4400 Train: 0.53205 Test: 0.33634
Epoch: 4480 Train: 0.12138 Test: 0.31203
Epoch 4560: New minimal relative error: 6.49%, model saved.
Epoch: 4560 Train: 0.07498 Test: 0.08275
Epoch: 4640 Train: 0.12018 Test: 0.13175
Epoch: 4720 Train: 0.69947 Test: 0.72763
Epoch: 4800 Train: 0.76866 Test: 0.86057
Epoch: 4880 Train: 0.44951 Test: 0.56679
Epoch 4960: New minimal relative error: 6.34%, model saved.
Epoch: 4960 Train: 0.07543 Test: 0.08127
Epoch: 5040 Train: 0.53564 Test: 0.62552
Epoch: 5120 Train: 1.88144 Test: 1.81296
Epoch: 5200 Train: 0.21954 Test: 0.28146
Epoch: 5280 Train: 0.29727 Test: 0.33434
Epoch: 5360 Train: 0.21788 Test: 0.24605
Epoch: 5440 Train: 0.09165 Test: 0.08444
Epoch: 5520 Train: 0.13726 Test: 0.16460
Epoch: 5600 Train: 0.27689 Test: 0.29813
Epoch: 5680 Train: 0.25501 Test: 0.27223
Epoch: 5760 Train: 0.08226 Test: 0.12006
Epoch: 5840 Train: 0.05696 Test: 0.06816
Epoch: 5920 Train: 0.06408 Test: 0.07126
Epoch: 6000 Train: 0.47274 Test: 0.53942
Epoch: 6080 Train: 0.12039 Test: 0.10689
Epoch: 6160 Train: 0.07553 Test: 0.09204
Epoch: 6240 Train: 0.06243 Test: 0.06608
Epoch: 6320 Train: 0.06942 Test: 0.10397
Epoch: 6400 Train: 0.05841 Test: 0.07125
Epoch: 6480 Train: 0.05909 Test: 0.06491
Epoch: 6560 Train: 0.13741 Test: 0.15881
Epoch 6640: New minimal relative error: 5.60%, model saved.
Epoch: 6640 Train: 0.04849 Test: 0.05837
Epoch: 6720 Train: 0.21783 Test: 0.28719
Epoch: 6800 Train: 0.13219 Test: 0.15989
Epoch: 6880 Train: 0.31468 Test: 0.34364
Epoch: 6960 Train: 0.04896 Test: 0.05881
Epoch: 7040 Train: 0.08620 Test: 0.10990
Epoch: 7120 Train: 0.04662 Test: 0.05756
Epoch: 7200 Train: 0.08076 Test: 0.09737
Epoch: 7280 Train: 0.05486 Test: 0.06591
Epoch: 7360 Train: 0.05944 Test: 0.07025
Epoch: 7440 Train: 0.24106 Test: 0.26418
Epoch: 7520 Train: 0.21043 Test: 0.23235
Epoch: 7600 Train: 0.20496 Test: 0.23524
Epoch: 7680 Train: 0.09502 Test: 0.10135
Epoch: 7760 Train: 0.10573 Test: 0.11673
Epoch: 7840 Train: 0.12134 Test: 0.13077
Epoch: 7920 Train: 0.05562 Test: 0.06608
Epoch: 7999 Train: 0.05357 Test: 0.06930
Training Loss: tensor(0.0536)
Test Loss: tensor(0.0693)
Learned LE: [ 0.8195806  -0.02585708 -3.4578123 ]
True LE: [ 8.4999371e-01  5.7357858e-04 -1.4529313e+01]
Relative Error: [6.5657988  5.910241   4.93329    3.9531312  3.2509007  2.562352
 2.1394544  1.9272592  2.0530484  2.3588607  2.749923   3.2455564
 3.6322403  3.6234546  3.5633478  3.6910324  3.7189026  3.7565484
 3.7957742  3.8097825  3.6121767  3.306993   3.1120455  3.10747
 3.0816782  3.129628   3.2730353  3.3680487  3.612436   3.7110393
 3.4472976  3.141769   3.0944648  2.747709   2.698185   2.555191
 2.4939613  2.5856729  2.3625524  2.6848085  3.1114428  3.234672
 3.1664498  3.2150307  3.4799027  3.7288628  4.245374   4.8479786
 5.2912235  5.628409   5.740234   5.8592997  6.0116906  6.082795
 6.1179256  6.1293974  6.2156086  6.278128   6.2755933  6.1853137
 6.170709   6.1427846  6.000579   5.4811993  4.5689197  3.567471
 2.8409152  2.2344844  1.9468635  1.9455639  2.2567835  2.6471908
 3.110192   3.477437   3.8081367  3.7671294  3.6432967  3.661117
 3.6954865  3.6764297  3.6962218  3.7150674  3.777466   3.610289
 3.3080142  3.1705208  3.1060817  3.125704   3.1752062  3.1923168
 3.3598304  3.6772795  3.5330672  3.1544573  3.0277452  2.599854
 2.5076418  2.4191265  2.366813   2.4290633  1.9644402  2.284331
 2.7020578  2.6704152  2.6663182  2.7092981  2.8883514  3.2538314
 3.8802142  4.3927665  4.825205   5.0730443  5.1766424  5.3372803
 5.517443   5.650669   5.7611337  5.842558   5.8867393  6.0007424
 5.9814606  5.8056183  5.612536   5.506831   5.407381   5.0378447
 4.2791076  3.2987282  2.5481088  1.995453   1.9091799  2.1176026
 2.5590785  2.9720433  3.3618276  3.78834    3.9706244  3.8059466
 3.66996    3.6299276  3.4718688  3.392582   3.3988912  3.4523416
 3.6885917  3.853112   3.6680405  3.2967124  3.1885202  3.182051
 3.2071245  3.126498   3.1858776  3.5594475  3.6087506  3.2332685
 2.9512672  2.4666982  2.309686   2.3392413  2.2187324  2.1975307
 1.5236462  1.7633313  2.1807463  2.1137588  2.1801977  2.2506967
 2.375781   2.8127806  3.4178336  3.8894854  4.2846212  4.524962
 4.634363   4.8509502  5.0575595  5.196715   5.2779174  5.322861
 5.3513074  5.5033092  5.552759   5.377731   5.1745367  4.9960804
 4.8176417  4.580021   4.047228   3.180441   2.3782675  1.9330337
 2.093054   2.3621356  2.839965   3.244955   3.656743   4.001806
 3.991921   3.8195584  3.7348242  3.6197062  3.5099175  3.3361685
 3.2639544  3.3947043  3.52664    3.6749034  3.8254154  3.6336403
 3.3972487  3.2202408  3.1907678  3.1300447  3.1135328  3.4006557
 3.5375514  3.3305812  2.9835563  2.3931806  2.1006432  2.1411364
 2.0846934  1.922656   1.1241664  1.2016337  1.6000996  1.5978074
 1.727774   1.756776   1.9180106  2.343369   2.8367612  3.329636
 3.7370284  3.9969847  4.129989   4.4019094  4.5903587  4.6472664
 4.7405114  4.8625965  4.920204   4.9108353  4.9354033  4.925448
 4.781024   4.5466366  4.3536553  4.1690507  3.8341136  3.186417
 2.3553433  2.0379727  2.36545    2.644239   2.9416533  3.2865875
 3.8492658  4.2001905  4.085392   3.8870866  3.9320714  3.7111535
 3.5030603  3.3663232  3.409305   3.491734   3.5756125  3.5828109
 3.6523838  3.8354836  3.725985   3.4989386  3.195259   3.0895464
 3.0395668  3.1824782  3.4707494  3.422141   3.0800211  2.514456
 2.0472474  1.9448498  1.9435416  1.6779327  0.83916986 0.62119734
 0.94280744 0.9919574  1.2281158  1.2510638  1.4594258  1.8260839
 2.2456455  2.7326708  3.1948042  3.4728556  3.6622887  3.9603848
 4.1509295  4.2235584  4.386292   4.499632   4.549843   4.4301
 4.389961   4.3498454  4.3507767  4.204357   3.9446492  3.717341
 3.547246   3.204433   2.4595966  2.1374834  2.2494164  2.1955466
 2.4245706  2.8684556  3.3368394  3.6916482  3.9331133  3.774209
 4.0223236  3.7822592  3.3972416  3.1409633  3.1136727  3.250206
 3.5037134  3.6981008  3.732751   3.7311933  3.896753   3.9221983
 3.50603    3.0676837  2.9213722  3.022501   3.2853193  3.4494288
 3.3211193  2.7026904  2.1421673  1.8482562  1.7277313  1.5573949
 0.6863533  0.18592618 0.35962752 0.36850768 0.65380275 0.7899605
 1.0801136  1.3554723  1.6656494  2.188904   2.6143112  2.937026
 3.1368632  3.4401734  3.7001019  3.86361    4.096291   4.309115
 4.3649793  4.063131   3.8881109  3.8632796  3.8265579  3.812953
 3.6793947  3.4146254  3.1771715  3.1205068  2.7101219  1.9682199
 1.6784313  1.9243784  2.1080098  2.4798646  2.9636562  3.1489916
 3.309742   3.2976582  3.5547078  3.8615758  3.4481015  3.1235437
 2.9973283  3.1074376  3.253734   3.2868814  3.6258712  3.8677135
 3.821077   3.9697464  3.9739416  3.4108288 ]
