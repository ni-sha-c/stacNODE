time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.59%, model saved.
Epoch: 0 Train: 3441.28931 Test: 4051.57300
Epoch 80: New minimal relative error: 95.06%, model saved.
Epoch: 80 Train: 317.63040 Test: 277.63556
Epoch: 160 Train: 198.17853 Test: 266.72919
Epoch 240: New minimal relative error: 32.79%, model saved.
Epoch: 240 Train: 10.25967 Test: 7.87213
Epoch: 320 Train: 11.00997 Test: 15.80727
Epoch 400: New minimal relative error: 25.87%, model saved.
Epoch: 400 Train: 10.60079 Test: 11.48784
Epoch 480: New minimal relative error: 24.93%, model saved.
Epoch: 480 Train: 20.98074 Test: 36.46568
Epoch 560: New minimal relative error: 22.38%, model saved.
Epoch: 560 Train: 14.98666 Test: 21.21080
Epoch: 640 Train: 18.58748 Test: 5.52539
Epoch 720: New minimal relative error: 13.78%, model saved.
Epoch: 720 Train: 10.87526 Test: 4.94751
Epoch: 800 Train: 9.13504 Test: 11.12893
Epoch 880: New minimal relative error: 12.20%, model saved.
Epoch: 880 Train: 4.78032 Test: 5.07968
Epoch: 960 Train: 4.95175 Test: 2.84226
Epoch: 1040 Train: 2.47752 Test: 1.97818
Epoch: 1120 Train: 10.26548 Test: 9.65365
Epoch: 1200 Train: 2.47542 Test: 1.76465
Epoch: 1280 Train: 2.60540 Test: 2.22777
Epoch 1360: New minimal relative error: 10.70%, model saved.
Epoch: 1360 Train: 1.89100 Test: 2.41762
Epoch: 1440 Train: 9.71690 Test: 6.68386
Epoch: 1520 Train: 6.87211 Test: 8.63085
Epoch: 1600 Train: 2.66765 Test: 3.18580
Epoch: 1680 Train: 14.38808 Test: 14.97884
Epoch: 1760 Train: 1.46382 Test: 2.46042
Epoch: 1840 Train: 3.43721 Test: 7.53247
Epoch: 1920 Train: 1.70994 Test: 2.41098
Epoch: 2000 Train: 0.43310 Test: 0.48923
Epoch 2080: New minimal relative error: 6.37%, model saved.
Epoch: 2080 Train: 0.32571 Test: 0.37824
Epoch: 2160 Train: 0.38967 Test: 0.42510
Epoch: 2240 Train: 3.89435 Test: 4.80255
Epoch: 2320 Train: 4.01390 Test: 6.87005
Epoch 2400: New minimal relative error: 4.50%, model saved.
Epoch: 2400 Train: 0.24146 Test: 0.28646
Epoch: 2480 Train: 0.31460 Test: 0.37784
Epoch: 2560 Train: 0.93592 Test: 0.89929
Epoch: 2640 Train: 2.50816 Test: 2.20083
Epoch: 2720 Train: 0.45607 Test: 0.60361
Epoch: 2800 Train: 0.17765 Test: 0.22793
Epoch: 2880 Train: 0.59702 Test: 0.58964
Epoch: 2960 Train: 2.27786 Test: 2.20724
Epoch: 3040 Train: 0.26752 Test: 0.40699
Epoch: 3120 Train: 0.16125 Test: 0.20960
Epoch: 3200 Train: 0.33656 Test: 0.36519
Epoch: 3280 Train: 0.12977 Test: 0.18658
Epoch: 3360 Train: 0.16676 Test: 0.20493
Epoch: 3440 Train: 0.68362 Test: 0.99940
Epoch: 3520 Train: 0.24812 Test: 0.34937
Epoch: 3600 Train: 0.15704 Test: 0.21014
Epoch: 3680 Train: 0.67568 Test: 0.64548
Epoch: 3760 Train: 3.28392 Test: 4.27615
Epoch: 3840 Train: 0.10055 Test: 0.14367
Epoch: 3920 Train: 0.24501 Test: 0.30011
Epoch: 4000 Train: 0.11717 Test: 0.22312
Epoch: 4080 Train: 1.90290 Test: 2.58273
Epoch: 4160 Train: 0.16330 Test: 0.23214
Epoch: 4240 Train: 0.13512 Test: 0.16453
Epoch: 4320 Train: 5.73411 Test: 5.38047
Epoch: 4400 Train: 0.08378 Test: 0.12568
Epoch: 4480 Train: 0.11793 Test: 0.16482
Epoch: 4560 Train: 0.08931 Test: 0.11647
Epoch 4640: New minimal relative error: 2.50%, model saved.
Epoch: 4640 Train: 0.11387 Test: 0.16097
Epoch: 4720 Train: 0.20651 Test: 0.36352
Epoch: 4800 Train: 0.07271 Test: 0.11331
Epoch: 4880 Train: 0.10657 Test: 0.15166
Epoch: 4960 Train: 0.14676 Test: 0.15426
Epoch: 5040 Train: 0.26281 Test: 0.35425
Epoch: 5120 Train: 0.20004 Test: 0.29755
Epoch: 5200 Train: 0.07613 Test: 0.12225
Epoch: 5280 Train: 1.52222 Test: 1.19502
Epoch: 5360 Train: 3.32989 Test: 3.94318
Epoch: 5440 Train: 0.09594 Test: 0.11227
Epoch: 5520 Train: 2.94462 Test: 3.67022
Epoch: 5600 Train: 0.07017 Test: 0.11415
Epoch: 5680 Train: 0.07797 Test: 0.13160
Epoch: 5760 Train: 0.10784 Test: 0.12924
Epoch: 5840 Train: 0.07354 Test: 0.10926
Epoch: 5920 Train: 0.09393 Test: 0.13770
Epoch: 6000 Train: 0.06311 Test: 0.11235
Epoch: 6080 Train: 0.17993 Test: 0.25038
Epoch: 6160 Train: 0.17432 Test: 0.25805
Epoch: 6240 Train: 0.10368 Test: 0.14320
Epoch: 6320 Train: 0.07167 Test: 0.11173
Epoch: 6400 Train: 0.06361 Test: 0.09076
Epoch: 6480 Train: 0.15741 Test: 0.11698
Epoch: 6560 Train: 0.16391 Test: 0.20765
Epoch: 6640 Train: 0.05089 Test: 0.08805
Epoch: 6720 Train: 0.09425 Test: 0.13496
Epoch: 6800 Train: 0.04773 Test: 0.08497
Epoch: 6880 Train: 0.09621 Test: 0.11877
Epoch: 6960 Train: 0.05036 Test: 0.08104
Epoch: 7040 Train: 0.48847 Test: 0.46613
Epoch: 7120 Train: 0.04452 Test: 0.07922
Epoch: 7200 Train: 0.07181 Test: 0.11116
Epoch: 7280 Train: 0.06734 Test: 0.10438
Epoch: 7360 Train: 0.04501 Test: 0.07846
Epoch: 7440 Train: 0.06594 Test: 0.08221
Epoch: 7520 Train: 0.86474 Test: 0.88139
Epoch: 7600 Train: 0.04870 Test: 0.08099
Epoch: 7680 Train: 0.10387 Test: 0.13097
Epoch 7760: New minimal relative error: 2.24%, model saved.
Epoch: 7760 Train: 0.06326 Test: 0.10719
Epoch: 7840 Train: 0.04062 Test: 0.07370
Epoch: 7920 Train: 0.12006 Test: 0.12892
Epoch: 7999 Train: 0.07265 Test: 0.10331
Training Loss: tensor(0.0727)
Test Loss: tensor(0.1033)
Learned LE: [ 0.7393607   0.01662851 -4.0945506 ]
True LE: [ 8.4139097e-01 -2.2739246e-03 -1.4516838e+01]
Relative Error: [1.2347864  1.110912   0.83936423 0.6137489  0.65739    0.97200435
 1.2591153  1.6809341  2.4793627  2.0982313  2.1474297  1.4846417
 0.94212365 1.2383505  1.1487778  0.93682194 0.8057917  0.66103274
 0.6836345  0.46117055 0.6263654  0.5962535  0.5649558  0.48732343
 0.7396303  0.7178537  0.81538785 0.874723   0.7724789  0.89797604
 0.9449204  0.8440256  0.78850865 0.9970947  1.35023    1.2404671
 0.821713   0.7692051  0.5022619  0.40775943 0.6092638  0.7652918
 0.85655886 0.72663605 0.7376854  0.5876063  0.4759811  0.44434106
 0.4551819  0.6166602  0.77491486 0.79817325 0.8202371  0.91774195
 0.8569694  0.5772178  0.43768045 0.46782774 0.60111105 0.33394888
 0.4705402  0.9330339  1.1970733  1.2654145  0.8786206  0.43699312
 0.4965121  0.83188164 1.1073285  1.669753   2.3713942  1.8975921
 1.9067254  1.3714833  0.89794    0.9172922  1.0570247  0.96287566
 0.83023643 0.67441875 0.83609074 0.6073991  0.456506   0.5954811
 0.5207446  0.31314212 0.5615397  0.6287544  0.57432544 0.6303003
 0.6845171  0.72614443 0.92176294 0.8550897  0.9175297  0.91597
 1.231926   1.3046812  0.70093733 0.47721025 0.6026583  0.45849538
 0.72923136 0.857577   0.93050504 0.8763225  0.83117336 0.8092028
 0.79174256 0.5490108  0.4701035  0.4686945  0.5475971  0.6513804
 0.7056408  0.83143294 0.8408005  0.725124   0.55882317 0.53958553
 0.44097716 0.3111764  0.23307139 0.6934487  1.11417    1.2105803
 0.9707314  0.5584573  0.2766414  0.5222768  0.8609826  1.4367893
 2.1526964  1.7808862  1.5511129  1.359424   0.91093683 0.8032401
 1.0181506  1.019088   0.7403284  0.57432    0.6358939  0.68681705
 0.4578386  0.34379426 0.32814494 0.15939268 0.29860687 0.6588518
 0.7197951  0.59501404 0.52233624 0.6966839  0.6688399  0.95179516
 0.98861074 1.0049343  1.07601    1.2661332  0.771814   0.27281135
 0.66719043 0.49972147 0.75011134 0.9006344  0.9484682  1.0342621
 0.9894344  0.8311768  0.91847426 0.74815947 0.6318732  0.5580801
 0.4260666  0.55335397 0.66493666 0.77859086 0.84196806 0.94591516
 0.5324437  0.5746705  0.41882807 0.3488906  0.09263021 0.29500657
 0.84424984 1.0614904  0.9592813  0.58868515 0.49357888 0.20958284
 0.704165   1.1127137  1.7259876  1.657224   1.4339205  1.2923678
 0.9292462  0.871352   0.75045216 0.8839463  0.6416756  0.40796053
 0.45437193 0.51276356 0.62675595 0.51485467 0.48880577 0.37394753
 0.31205997 0.5357698  0.782786   0.76179296 0.5692588  0.68352723
 0.69933784 0.7295171  0.7095702  0.83720416 0.9317838  0.9522723
 1.0224161  0.5763587  0.60516906 0.52737355 0.7094887  0.8751018
 1.023375   1.0861871  0.93812245 0.85519916 0.9087044  0.89825046
 0.7692984  0.67269516 0.6802849  0.47319275 0.44738552 0.579132
 0.6229924  0.74591434 0.5873512  0.40718937 0.43597612 0.36218804
 0.16796501 0.19154805 0.47912118 0.8678603  0.89333886 0.74499285
 0.3853394  0.453709   0.49748313 0.8542983  1.2770777  1.3331375
 1.1819249  1.471073   0.86724794 1.0725007  0.59066385 0.6132875
 0.6038262  0.39438018 0.33096093 0.31251678 0.44204336 0.77012426
 0.79585576 0.8385825  0.77523535 0.5406394  0.5975847  0.8292226
 0.57127017 0.6460593  0.6367675  0.59029955 0.6520626  0.6642667
 0.6947224  0.6750587  0.7954203  0.81340146 0.4989732  0.6918174
 0.7051219  0.78296417 1.0758822  1.0048034  0.9407065  0.6693423
 0.6470793  0.7537426  0.82550955 0.69962925 0.6343962  0.47282094
 0.31946972 0.29858437 0.51217026 0.5012617  0.67911464 0.40136492
 0.3302276  0.3574098  0.33051634 0.24828038 0.30777627 0.55613834
 0.7610393  0.7037638  0.5226865  0.29076776 0.3769852  0.9989376
 0.8848985  1.144919   0.9505577  1.1556183  1.0726317  0.87282574
 0.6467961  0.4796437  0.55554986 0.6089612  0.40026218 0.29952002
 0.5001578  0.77078474 0.79738367 0.693932   0.7106581  0.60278726
 0.65717924 0.6969107  0.6876279  0.56566477 0.68082696 0.7191837
 0.45340887 0.5837546  0.70154786 0.657976   0.47754714 0.6202474
 0.90040267 0.56696177 0.5780282  0.77901524 0.8896228  0.97316885
 0.6869637  0.45490068 0.46400777 0.30008408 0.5318296  0.4438382
 0.11407532 0.27362072 0.34525305 0.33508933 0.20842536 0.32833752
 0.4062978  0.41106334 0.25783828 0.35082614 0.37325206 0.36865786
 0.34311643 0.35279715 0.52077794 0.63115495 0.55275005 0.41472125
 0.45836103 0.30561158 1.2997459  0.84876716 0.82097423 0.718374
 1.1169142  0.8964229  0.830949   0.50562716 0.544033   0.45411038
 0.5021867  0.3486322  0.43170285 0.67979765 0.84970146 0.86755717
 0.5886573  0.56196725 0.41295713 0.58670443]
