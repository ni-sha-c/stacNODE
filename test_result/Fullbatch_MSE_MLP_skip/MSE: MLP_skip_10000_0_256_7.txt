time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.09%, model saved.
Epoch: 0 Train: 3649.93311 Test: 4381.20459
Epoch: 100 Train: 364.28540 Test: 357.23831
Epoch 200: New minimal relative error: 32.52%, model saved.
Epoch: 200 Train: 54.85179 Test: 49.30049
Epoch: 300 Train: 26.16960 Test: 34.34915
Epoch: 400 Train: 15.10294 Test: 12.29085
Epoch: 500 Train: 19.80490 Test: 21.19682
Epoch: 600 Train: 9.73181 Test: 7.23540
Epoch: 700 Train: 9.62514 Test: 9.95037
Epoch: 800 Train: 6.51052 Test: 5.45195
Epoch: 900 Train: 7.72621 Test: 7.10493
Epoch 1000: New minimal relative error: 27.16%, model saved.
Epoch: 1000 Train: 7.49739 Test: 7.54341
Epoch 1100: New minimal relative error: 26.04%, model saved.
Epoch: 1100 Train: 8.34143 Test: 5.69871
Epoch: 1200 Train: 14.50843 Test: 17.51611
Epoch: 1300 Train: 21.52934 Test: 24.36593
Epoch 1400: New minimal relative error: 19.05%, model saved.
Epoch: 1400 Train: 6.89759 Test: 7.05908
Epoch: 1500 Train: 4.28074 Test: 4.24607
Epoch: 1600 Train: 3.55210 Test: 2.51197
Epoch: 1700 Train: 4.73762 Test: 3.43694
Epoch 1800: New minimal relative error: 8.62%, model saved.
Epoch: 1800 Train: 2.88832 Test: 1.91016
Epoch: 1900 Train: 4.39681 Test: 2.25617
Epoch: 2000 Train: 2.79995 Test: 2.07682
Epoch: 2100 Train: 6.12738 Test: 5.07972
Epoch: 2200 Train: 4.00516 Test: 4.11050
Epoch: 2300 Train: 2.63555 Test: 1.43424
Epoch: 2400 Train: 1.91677 Test: 1.18195
Epoch 2500: New minimal relative error: 7.28%, model saved.
Epoch: 2500 Train: 1.80622 Test: 1.21902
Epoch: 2600 Train: 2.17240 Test: 1.27697
Epoch: 2700 Train: 1.89951 Test: 1.09564
Epoch: 2800 Train: 2.72654 Test: 1.82062
Epoch: 2900 Train: 8.61689 Test: 8.98221
Epoch: 3000 Train: 1.35926 Test: 0.85593
Epoch: 3100 Train: 0.98193 Test: 0.54870
Epoch: 3200 Train: 1.41735 Test: 1.28618
Epoch: 3300 Train: 1.10312 Test: 1.34207
Epoch: 3400 Train: 0.87763 Test: 0.47514
Epoch: 3500 Train: 2.22504 Test: 1.45968
Epoch: 3600 Train: 0.71401 Test: 0.46883
Epoch: 3700 Train: 8.68408 Test: 4.38428
Epoch: 3800 Train: 0.64219 Test: 0.36507
Epoch: 3900 Train: 0.52910 Test: 0.31815
Epoch: 4000 Train: 8.20992 Test: 4.58775
Epoch: 4100 Train: 4.99259 Test: 4.13611
Epoch: 4200 Train: 0.62948 Test: 0.44941
Epoch: 4300 Train: 4.58991 Test: 4.54414
Epoch: 4400 Train: 0.47105 Test: 0.35199
Epoch: 4500 Train: 3.28873 Test: 3.10239
Epoch: 4600 Train: 0.44758 Test: 0.36934
Epoch: 4700 Train: 2.26698 Test: 2.73498
Epoch: 4800 Train: 1.65554 Test: 1.07885
Epoch: 4900 Train: 1.02556 Test: 0.73692
Epoch: 5000 Train: 0.30594 Test: 0.22160
Epoch: 5100 Train: 0.30069 Test: 0.21683
Epoch: 5200 Train: 0.44415 Test: 0.27509
Epoch 5300: New minimal relative error: 6.13%, model saved.
Epoch: 5300 Train: 0.44611 Test: 0.36183
Epoch: 5400 Train: 0.58030 Test: 0.73485
Epoch: 5500 Train: 0.30238 Test: 0.28455
Epoch 5600: New minimal relative error: 4.45%, model saved.
Epoch: 5600 Train: 0.26655 Test: 0.22335
Epoch 5700: New minimal relative error: 3.47%, model saved.
Epoch: 5700 Train: 0.24021 Test: 0.18979
Epoch 5800: New minimal relative error: 3.46%, model saved.
Epoch: 5800 Train: 0.22704 Test: 0.18365
Epoch: 5900 Train: 0.24376 Test: 0.21834
Epoch 6000: New minimal relative error: 3.07%, model saved.
Epoch: 6000 Train: 0.21563 Test: 0.17543
Epoch: 6100 Train: 0.21456 Test: 0.17551
Epoch: 6200 Train: 0.20769 Test: 0.19251
Epoch: 6300 Train: 0.20272 Test: 0.17321
Epoch: 6400 Train: 0.22308 Test: 0.19431
Epoch: 6500 Train: 0.74437 Test: 0.51121
Epoch: 6600 Train: 0.21390 Test: 0.17921
Epoch: 6700 Train: 0.25839 Test: 0.16863
Epoch: 6800 Train: 2.53181 Test: 2.84424
Epoch: 6900 Train: 0.17786 Test: 0.17882
Epoch: 7000 Train: 2.99346 Test: 2.36579
Epoch: 7100 Train: 0.16286 Test: 0.14802
Epoch: 7200 Train: 0.17211 Test: 0.15382
Epoch: 7300 Train: 0.16159 Test: 0.15627
Epoch: 7400 Train: 0.20994 Test: 0.19615
Epoch: 7500 Train: 0.18132 Test: 0.19059
Epoch 7600: New minimal relative error: 2.80%, model saved.
Epoch: 7600 Train: 0.14953 Test: 0.14057
Epoch: 7700 Train: 0.14725 Test: 0.13827
Epoch: 7800 Train: 0.17082 Test: 0.15405
Epoch: 7900 Train: 1.32849 Test: 1.77987
Epoch: 8000 Train: 0.14187 Test: 0.13858
Epoch 8100: New minimal relative error: 2.72%, model saved.
Epoch: 8100 Train: 0.13720 Test: 0.13205
Epoch: 8200 Train: 0.13980 Test: 0.14111
Epoch: 8300 Train: 0.13899 Test: 0.14033
Epoch: 8400 Train: 0.13143 Test: 0.12885
Epoch: 8500 Train: 0.13287 Test: 0.13486
Epoch: 8600 Train: 0.13016 Test: 0.12831
Epoch: 8700 Train: 0.12654 Test: 0.12507
Epoch: 8800 Train: 0.18707 Test: 0.18872
Epoch: 8900 Train: 0.12478 Test: 0.12654
Epoch: 9000 Train: 0.12211 Test: 0.12196
Epoch: 9100 Train: 0.12709 Test: 0.12742
Epoch: 9200 Train: 0.25062 Test: 0.29943
Epoch: 9300 Train: 0.11707 Test: 0.11856
Epoch: 9400 Train: 0.11781 Test: 0.12001
Epoch: 9500 Train: 0.56212 Test: 0.73437
Epoch: 9600 Train: 0.11306 Test: 0.11556
Epoch: 9700 Train: 0.12612 Test: 0.13539
Epoch: 9800 Train: 0.11611 Test: 0.12373
Epoch: 9900 Train: 0.10941 Test: 0.11254
Epoch: 9999 Train: 0.11863 Test: 0.12907
Training Loss: tensor(0.1186)
Test Loss: tensor(0.1291)
Learned LE: [ 0.82953554  0.01302196 -3.499373  ]
True LE: [ 8.8010246e-01  1.3725531e-03 -1.4553171e+01]
Relative Error: [3.1796858  3.041413   2.841192   2.631252   2.377541   2.1960402
 2.152838   2.4175837  3.0345762  3.8392837  4.5782385  5.0465713
 5.8804083  6.704767   6.9532566  7.2693615  7.473073   6.8038387
 5.860014   4.8457885  4.6602583  5.0296583  5.3934584  5.683469
 5.7727966  5.9724684  6.2510786  6.639445   6.272537   5.6294055
 4.8374305  3.9248514  2.9861817  2.318042   2.6086     3.0795228
 3.0465722  3.0792847  3.262551   3.7257683  4.048208   3.9235187
 3.8860478  3.746679   3.399916   3.2374957  3.184559   3.1467836
 3.1804507  2.9474561  2.8404353  2.8661103  2.9815474  2.9115167
 2.892664   2.811769   2.7263956  2.6306238  2.5457754  2.571791
 2.6724443  2.7759438  2.6705015  2.5304077  2.2714815  2.0088785
 1.7632861  1.6906545  1.6041133  1.9204261  2.5266788  3.2646108
 4.035021   4.415902   5.784269   6.3874736  6.780787   7.049667
 7.1350565  6.226334   5.2786374  4.332625   4.236771   4.537326
 4.822197   4.8339124  4.859129   4.9818096  5.3388906  5.8264375
 5.5135856  4.9284244  4.1711392  3.265979   2.500855   1.9932466
 2.3549867  2.7817345  2.5853226  2.6222157  2.8356812  3.1485214
 3.3362932  3.3762343  3.223078   3.0211704  2.7271223  2.646612
 2.5836833  2.5402498  2.5622916  2.38373    2.392502   2.5009146
 2.5231376  2.4882715  2.4793518  2.403689   2.304728   2.1990027
 2.0735571  2.0609365  2.0881205  2.222377   2.2186375  1.9810196
 1.7063274  1.529632   1.3501983  1.3544949  1.2497499  1.5108856
 2.0598876  2.7382534  3.417411   4.457257   5.8272657  6.118854
 6.6914916  6.9595394  6.7271414  5.7566576  4.81487    3.9129593
 3.8611162  4.081039   4.1694717  4.0203786  3.9698222  4.070439
 4.421502   4.8770356  4.6720753  4.2091002  3.4942052  2.6073923
 1.999908   1.772682   2.2158773  2.4199448  2.175805   2.20072
 2.4277456  2.6737816  2.6751382  2.79552    2.6011088  2.371893
 2.142721   2.1063197  1.9817024  1.9684225  1.9925816  1.9372797
 2.017757   2.1422126  2.1215308  2.1413996  2.129728   2.0526521
 1.9532164  1.8254164  1.668825   1.6315917  1.6082352  1.6558135
 1.6835967  1.4758354  1.3045715  1.2794935  1.2870228  1.2940412
 1.2100357  1.3527968  1.8088249  2.3945394  2.9446568  4.2451034
 5.387159   5.762599   6.0532594  6.597352   6.308006   5.210013
 4.3166     3.4806497  3.52359    3.648677   3.4676049  3.252277
 3.1030705  3.2072458  3.4719853  3.8982697  3.8096933  3.4265466
 2.7774203  1.9533502  1.5073324  1.6816094  2.1897814  2.1596868
 1.8541509  1.8313328  2.0502388  2.2024093  2.1135187  2.2130723
 2.0354373  1.827535   1.6318092  1.5889273  1.465707   1.4938544
 1.5605043  1.546291   1.7079082  1.7536275  1.7802848  1.860768
 1.8254218  1.7594275  1.6732225  1.5487095  1.3800162  1.2893267
 1.2084153  1.0842341  1.1876657  1.1566292  1.1445541  1.2974055
 1.4059389  1.4300543  1.403637   1.593507   1.7398514  2.004874
 2.3379529  4.0475073  4.900063   4.8072886  4.951403   5.6479726
 5.7552648  4.772112   3.6353421  2.981158   3.123915   3.252115
 2.8840833  2.568767   2.341224   2.3633177  2.535841   2.9350042
 3.0401812  2.757      2.1687784  1.5295795  1.1964141  1.6176068
 2.1972697  1.9606991  1.6162521  1.5411532  1.7450849  1.8153648
 1.7888714  1.8786354  1.7150681  1.436511   1.174784   1.0797372
 1.0035262  1.1069381  1.2104592  1.2731783  1.4111071  1.434564
 1.5053283  1.6026605  1.5813477  1.547544   1.5057873  1.4026244
 1.20901    1.0659868  0.9540942  0.83148956 0.8715039  1.0396909
 1.1953087  1.5053713  1.6781021  1.7638696  1.6999251  1.5457394
 1.7195586  1.770411   1.921989   3.9841812  4.4041743  4.081718
 3.9660845  4.693538   4.5073376  3.720538   2.8770125  2.757959
 2.7554173  2.778612   2.389404   1.9487216  1.6794873  1.5709592
 1.6852912  2.004036   2.3282995  2.0746617  1.8664073  1.3363758
 0.948347   1.5245667  2.189991   1.8188105  1.2497692  1.24358
 1.530936   1.5339494  1.4896011  1.5613184  1.4905809  1.272979
 0.94944584 0.6749699  0.6234622  0.7569558  0.8987711  1.0324935
 1.1266792  1.1612078  1.2955945  1.3930806  1.4085532  1.4270546
 1.4318094  1.3664947  1.2171977  1.1008917  1.1447508  0.98456097
 0.8849288  1.070535   1.367476   1.7863085  1.967052   2.172206
 1.557611   1.4801054  1.7459899  1.8888103  1.769253   3.452966
 3.6693547  3.4530447  3.2677233  3.5802848  3.3043578  2.5789742
 2.0891604  2.0848684  2.5964553  2.3415647  1.8998675  1.4714285
 1.0928737  0.93389434 0.96242213 1.1373117 ]
