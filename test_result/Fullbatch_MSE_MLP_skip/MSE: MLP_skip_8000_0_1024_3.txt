time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 109.90%, model saved.
Epoch: 0 Train: 4169.17383 Test: 3796.13184
Epoch 80: New minimal relative error: 23.25%, model saved.
Epoch: 80 Train: 57.17834 Test: 67.05698
Epoch: 160 Train: 13.71215 Test: 21.12870
Epoch 240: New minimal relative error: 22.19%, model saved.
Epoch: 240 Train: 9.72695 Test: 21.25944
Epoch: 320 Train: 7.34359 Test: 11.87262
Epoch 400: New minimal relative error: 9.53%, model saved.
Epoch: 400 Train: 4.92587 Test: 8.31311
Epoch: 480 Train: 17.84173 Test: 24.06995
Epoch: 560 Train: 29.28340 Test: 42.46215
Epoch: 640 Train: 9.02146 Test: 14.00779
Epoch: 720 Train: 9.68664 Test: 12.30263
Epoch: 800 Train: 7.88992 Test: 11.94191
Epoch: 880 Train: 4.93844 Test: 6.15103
Epoch: 960 Train: 10.44309 Test: 11.25949
Epoch: 1040 Train: 7.52234 Test: 9.09647
Epoch: 1120 Train: 11.98890 Test: 8.90886
Epoch: 1200 Train: 1.00113 Test: 2.49735
Epoch 1280: New minimal relative error: 6.09%, model saved.
Epoch: 1280 Train: 0.95846 Test: 2.91810
Epoch: 1360 Train: 2.91682 Test: 5.38953
Epoch: 1440 Train: 4.65945 Test: 5.20227
Epoch: 1520 Train: 0.96571 Test: 2.03497
Epoch: 1600 Train: 0.80240 Test: 2.16857
Epoch: 1680 Train: 4.54538 Test: 5.61569
Epoch: 1760 Train: 0.72194 Test: 1.68945
Epoch: 1840 Train: 1.53479 Test: 2.84743
Epoch: 1920 Train: 1.41201 Test: 1.94425
Epoch: 2000 Train: 4.47205 Test: 5.47032
Epoch: 2080 Train: 1.11133 Test: 1.84377
Epoch: 2160 Train: 1.27954 Test: 2.88966
Epoch: 2240 Train: 0.46960 Test: 1.51347
Epoch: 2320 Train: 0.91789 Test: 2.16731
Epoch: 2400 Train: 0.69724 Test: 1.50812
Epoch: 2480 Train: 1.23174 Test: 1.83698
Epoch: 2560 Train: 0.53753 Test: 1.20894
Epoch: 2640 Train: 0.27239 Test: 1.05650
Epoch: 2720 Train: 0.35385 Test: 1.09582
Epoch: 2800 Train: 0.53838 Test: 1.12952
Epoch 2880: New minimal relative error: 3.10%, model saved.
Epoch: 2880 Train: 0.20302 Test: 0.98029
Epoch: 2960 Train: 0.44830 Test: 1.48302
Epoch: 3040 Train: 0.19788 Test: 1.12601
Epoch: 3120 Train: 0.60784 Test: 1.01460
Epoch: 3200 Train: 1.85618 Test: 2.07383
Epoch: 3280 Train: 0.54105 Test: 1.36077
Epoch: 3360 Train: 0.90594 Test: 1.77782
Epoch: 3440 Train: 0.91944 Test: 1.24243
Epoch: 3520 Train: 0.15622 Test: 0.86522
Epoch: 3600 Train: 0.26234 Test: 0.98283
Epoch: 3680 Train: 0.37980 Test: 0.97545
Epoch: 3760 Train: 0.26387 Test: 0.83917
Epoch: 3840 Train: 0.39626 Test: 1.14001
Epoch: 3920 Train: 0.63607 Test: 1.66719
Epoch: 4000 Train: 0.65951 Test: 1.56239
Epoch: 4080 Train: 0.28451 Test: 1.06206
Epoch: 4160 Train: 0.27604 Test: 0.93857
Epoch: 4240 Train: 0.11533 Test: 0.73473
Epoch: 4320 Train: 0.20980 Test: 0.89614
Epoch: 4400 Train: 0.11443 Test: 0.86149
Epoch: 4480 Train: 0.32615 Test: 0.85955
Epoch: 4560 Train: 1.06405 Test: 1.77431
Epoch: 4640 Train: 0.24359 Test: 0.74288
Epoch: 4720 Train: 0.31748 Test: 0.83929
Epoch: 4800 Train: 0.28022 Test: 0.85345
Epoch: 4880 Train: 0.16657 Test: 0.78629
Epoch: 4960 Train: 0.19782 Test: 0.72027
Epoch: 5040 Train: 0.11510 Test: 0.66623
Epoch: 5120 Train: 0.38847 Test: 1.06251
Epoch: 5200 Train: 0.18891 Test: 0.76984
Epoch: 5280 Train: 0.51468 Test: 0.89193
Epoch: 5360 Train: 0.81570 Test: 1.17996
Epoch: 5440 Train: 0.07839 Test: 0.61225
Epoch: 5520 Train: 0.09822 Test: 0.67311
Epoch: 5600 Train: 0.07701 Test: 0.61617
Epoch: 5680 Train: 0.07668 Test: 0.59571
Epoch: 5760 Train: 0.07656 Test: 0.60000
Epoch: 5840 Train: 0.78767 Test: 1.10166
Epoch: 5920 Train: 0.08351 Test: 0.63341
Epoch: 6000 Train: 0.07448 Test: 0.58261
Epoch 6080: New minimal relative error: 2.52%, model saved.
Epoch: 6080 Train: 0.12743 Test: 0.67612
Epoch: 6160 Train: 0.34674 Test: 0.88543
Epoch: 6240 Train: 0.06603 Test: 0.58320
Epoch: 6320 Train: 0.07058 Test: 0.56536
Epoch: 6400 Train: 0.06449 Test: 0.58277
Epoch: 6480 Train: 0.09027 Test: 0.55041
Epoch: 6560 Train: 0.19505 Test: 0.75257
Epoch: 6640 Train: 0.07348 Test: 0.54902
Epoch: 6720 Train: 0.07752 Test: 0.62086
Epoch: 6800 Train: 0.05927 Test: 0.55584
Epoch: 6880 Train: 0.05788 Test: 0.55338
Epoch: 6960 Train: 0.06185 Test: 0.56959
Epoch: 7040 Train: 0.06037 Test: 0.53964
Epoch: 7120 Train: 0.53729 Test: 1.19237
Epoch: 7200 Train: 0.05491 Test: 0.54205
Epoch: 7280 Train: 0.06525 Test: 0.55623
Epoch: 7360 Train: 0.10861 Test: 0.54472
Epoch: 7440 Train: 0.06355 Test: 0.57131
Epoch: 7520 Train: 0.05297 Test: 0.53356
Epoch: 7600 Train: 0.06585 Test: 0.59022
Epoch: 7680 Train: 0.08998 Test: 0.62339
Epoch: 7760 Train: 0.22761 Test: 0.82472
Epoch: 7840 Train: 0.04938 Test: 0.51822
Epoch: 7920 Train: 0.05119 Test: 0.53361
Epoch: 7999 Train: 0.05738 Test: 0.54497
Training Loss: tensor(0.0574)
Test Loss: tensor(0.5450)
Learned LE: [ 0.83568513  0.04461043 -3.9142053 ]
True LE: [ 8.7259281e-01  4.2990739e-03 -1.4551294e+01]
Relative Error: [0.86688316 0.7753511  0.6997568  0.79438317 1.1966754  1.6764853
 2.0143106  2.2021554  2.4101586  2.7297163  3.0708687  2.870675
 1.962056   1.2927216  0.4416829  1.1855351  2.0030386  1.8923419
 1.2544163  0.7779125  0.41448766 0.2379059  0.17367429 0.09313846
 0.23632985 0.42470336 0.43858066 0.58375925 0.7972628  0.9086991
 1.0626671  1.0641794  0.9925306  0.95196766 1.1879066  1.1982044
 1.2464865  1.0907505  0.94635963 0.9994103  1.3225358  1.3942562
 1.2964013  1.3902503  1.3764378  1.4493523  1.5514897  1.2164085
 0.8893414  0.8759277  1.1276429  1.1781762  1.1615685  1.1442076
 1.2361935  1.4319702  1.6112211  1.655024   1.4694942  1.3861841
 1.1888777  0.9638257  0.724596   0.5850698  0.55570275 0.5164404
 0.8433372  1.4160848  1.9952877  2.3073661  2.5088484  2.6741297
 2.9320672  2.528396   1.9002233  0.3255397  1.0613189  1.5442256
 1.7923928  1.0750778  0.33462873 0.4312267  0.67476994 0.76111764
 0.62509036 0.5144059  0.49051774 0.60551745 0.8545344  0.92169327
 0.99475336 1.1121259  1.1248771  1.2055795  1.1244985  1.1255772
 1.3690997  1.4807959  1.4691142  1.233394   1.1577083  1.2611746
 1.4854199  1.480508   1.2714454  1.3152019  1.381631   1.3423109
 1.4349202  1.276846   0.89286363 0.84249663 1.0878425  1.138617
 1.2223645  1.2589867  1.4408399  1.6778301  1.7132722  1.5724967
 1.4057344  1.2299314  1.0953879  0.8760966  0.67616177 0.48901895
 0.43627214 0.36865744 0.6099598  1.1056751  1.7010182  2.2695546
 2.6352534  2.718247   2.6511624  2.1917882  1.0714282  1.003674
 1.6707908  1.7910782  1.504367   0.35911348 0.74838364 1.1004431
 1.4225363  1.4278823  1.1988068  0.99256814 0.92982453 0.79991996
 0.95970196 1.1371377  1.1871308  1.1881722  1.2815201  1.1107324
 1.0838408  1.0535557  1.3105911  1.4246538  1.6312158  1.5096577
 1.3658215  1.3578947  1.6840647  1.6300548  1.3570831  1.3590934
 1.4654962  1.2807697  1.3520823  1.1951883  0.93983114 0.9155436
 1.0759134  1.1475298  1.2939676  1.3304123  1.530835   1.6503252
 1.5994589  1.4002006  1.2322447  1.0707511  0.98811156 0.8810225
 0.6909678  0.49461374 0.46077743 0.3782679  0.38734114 0.7765011
 1.5427239  2.0680242  2.5649254  2.8017237  2.4294772  1.8522583
 0.41468307 1.8886963  1.9265549  1.7192116  1.0379729  0.5531547
 1.1769329  1.5997088  1.9050767  1.8951609  1.5470996  1.3032389
 1.2259648  1.0490589  0.91563094 1.031239   1.2566482  1.3813906
 1.205458   1.1557149  1.0477234  1.0165467  1.1249019  1.2747955
 1.4621298  1.5831908  1.3792458  1.4463036  1.6579425  1.7680628
 1.5789764  1.4699743  1.5266964  1.2963654  1.2329953  1.0790879
 0.93885887 0.956065   1.0486337  1.1413745  1.190247   1.2577938
 1.4135656  1.4644793  1.4088999  1.3143743  1.1458104  0.94038343
 0.80265456 0.7378615  0.6843888  0.56827486 0.45658368 0.5206585
 0.5326355  0.6605327  1.225575   1.9844645  2.3835557  2.6533113
 2.3461506  1.2500083  1.3538649  2.4116263  2.2811863  1.5793496
 0.7424292  0.6658218  1.2690797  1.784527   2.2366264  2.10834
 1.8229465  1.4787151  1.3225466  1.208016   1.1283096  1.0258045
 1.1547728  1.290674   1.2891016  1.1591942  1.1565474  1.0742937
 0.9190006  1.0432625  1.2235115  1.3908318  1.342952   1.3368973
 1.5243566  1.6396221  1.6788114  1.6457653  1.5642153  1.3939823
 1.1413842  1.0500382  0.8884976  0.8872713  1.0279224  1.0814906
 1.1572156  1.20058    1.2269182  1.2530255  1.2054211  1.2053475
 1.132158   0.94189143 0.7512461  0.6408938  0.589531   0.57344747
 0.5611419  0.53294456 0.64154154 0.73384464 1.0951451  1.7982321
 2.3729968  2.3904374  2.2374015  1.1078985  1.902955   2.8198438
 2.683717   1.6962543  0.69052625 0.6652353  1.24294    1.8060526
 2.077929   2.1186805  1.8931296  1.5740043  1.3774655  1.314361
 1.2458391  1.2387884  1.2188803  1.2207118  1.109574   1.0814607
 1.0551546  1.0395721  1.0342064  0.8347978  1.0341657  1.3942037
 1.3086793  1.2254293  1.3320827  1.457323   1.4832503  1.7387437
 1.6271204  1.4983416  1.132748   0.9676243  0.7968507  0.8699582
 0.87707293 0.9289198  1.0480479  1.0609739  1.0939418  1.0977144
 1.1297959  1.072774   1.0205368  0.8696604  0.7340633  0.5904283
 0.53009474 0.5124989  0.5974847  0.58080536 0.5455135  0.7358472
 0.7484134  1.5230999  2.253648   2.529912   2.2146013  1.2804617
 1.70538    3.251869   2.9495444  2.0854242  1.0708672  0.49844575
 1.0981102  1.6410645  1.9246403  1.8130726  1.9267645  1.7299626
 1.436734   1.3596648  1.2920318  1.3197618 ]
