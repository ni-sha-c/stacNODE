time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 104.36%, model saved.
Epoch: 0 Train: 3774.91650 Test: 3927.06226
Epoch: 100 Train: 154.01736 Test: 191.93808
Epoch 200: New minimal relative error: 18.08%, model saved.
Epoch: 200 Train: 22.99123 Test: 34.18140
Epoch: 300 Train: 48.27499 Test: 105.83128
Epoch 400: New minimal relative error: 13.55%, model saved.
Epoch: 400 Train: 6.41093 Test: 12.02235
Epoch 500: New minimal relative error: 12.91%, model saved.
Epoch: 500 Train: 5.33490 Test: 10.37681
Epoch: 600 Train: 10.66663 Test: 18.68136
Epoch: 700 Train: 14.34915 Test: 12.30582
Epoch: 800 Train: 5.49095 Test: 8.76756
Epoch: 900 Train: 3.70065 Test: 7.35440
Epoch: 1000 Train: 17.32949 Test: 30.83278
Epoch: 1100 Train: 2.92651 Test: 6.20779
Epoch 1200: New minimal relative error: 11.08%, model saved.
Epoch: 1200 Train: 2.71522 Test: 5.91939
Epoch: 1300 Train: 11.76269 Test: 12.68531
Epoch: 1400 Train: 11.97020 Test: 14.75008
Epoch: 1500 Train: 1.98185 Test: 4.73674
Epoch: 1600 Train: 3.55188 Test: 4.73690
Epoch: 1700 Train: 3.37602 Test: 5.71133
Epoch: 1800 Train: 1.69887 Test: 4.05818
Epoch: 1900 Train: 2.44020 Test: 6.40534
Epoch: 2000 Train: 1.46998 Test: 3.83885
Epoch: 2100 Train: 1.26325 Test: 3.35290
Epoch 2200: New minimal relative error: 10.14%, model saved.
Epoch: 2200 Train: 2.05937 Test: 3.20632
Epoch: 2300 Train: 1.23555 Test: 3.37617
Epoch: 2400 Train: 1.01682 Test: 2.87031
Epoch: 2500 Train: 4.46780 Test: 6.77140
Epoch: 2600 Train: 0.67968 Test: 2.47883
Epoch: 2700 Train: 0.86084 Test: 2.57733
Epoch 2800: New minimal relative error: 7.27%, model saved.
Epoch: 2800 Train: 0.60363 Test: 2.26016
Epoch: 2900 Train: 0.55888 Test: 2.15136
Epoch: 3000 Train: 0.51312 Test: 2.05677
Epoch: 3100 Train: 0.49935 Test: 2.03227
Epoch: 3200 Train: 3.22758 Test: 4.98500
Epoch: 3300 Train: 1.21107 Test: 2.33584
Epoch: 3400 Train: 0.40470 Test: 1.77690
Epoch: 3500 Train: 0.38197 Test: 1.75812
Epoch: 3600 Train: 0.39182 Test: 1.75157
Epoch: 3700 Train: 0.43511 Test: 1.72643
Epoch: 3800 Train: 0.33922 Test: 1.65290
Epoch: 3900 Train: 0.35826 Test: 1.62572
Epoch: 4000 Train: 1.15614 Test: 2.55054
Epoch: 4100 Train: 5.73702 Test: 5.76490
Epoch: 4200 Train: 0.32062 Test: 1.52188
Epoch: 4300 Train: 2.94670 Test: 3.78257
Epoch: 4400 Train: 5.91321 Test: 6.99032
Epoch: 4500 Train: 0.27103 Test: 1.44479
Epoch: 4600 Train: 0.55506 Test: 1.64581
Epoch: 4700 Train: 0.44093 Test: 1.61129
Epoch: 4800 Train: 0.28533 Test: 1.47813
Epoch: 4900 Train: 0.37412 Test: 1.53562
Epoch: 5000 Train: 0.28295 Test: 1.40672
Epoch: 5100 Train: 0.41523 Test: 1.62866
Epoch: 5200 Train: 1.68365 Test: 2.51564
Epoch: 5300 Train: 0.64874 Test: 1.75618
Epoch: 5400 Train: 0.24050 Test: 1.32200
Epoch: 5500 Train: 0.21423 Test: 1.27161
Epoch: 5600 Train: 0.21018 Test: 1.25037
Epoch: 5700 Train: 0.20475 Test: 1.26542
Epoch: 5800 Train: 0.24971 Test: 1.27443
Epoch: 5900 Train: 0.89082 Test: 1.88130
Epoch: 6000 Train: 0.21971 Test: 1.26096
Epoch: 6100 Train: 0.23376 Test: 1.25315
Epoch: 6200 Train: 1.15130 Test: 2.08873
Epoch: 6300 Train: 0.28496 Test: 1.21782
Epoch: 6400 Train: 0.17946 Test: 1.15056
Epoch: 6500 Train: 0.17822 Test: 1.15331
Epoch: 6600 Train: 0.17903 Test: 1.17988
Epoch: 6700 Train: 0.17214 Test: 1.13201
Epoch: 6800 Train: 0.17273 Test: 1.12418
Epoch: 6900 Train: 0.16745 Test: 1.10653
Epoch: 7000 Train: 0.17193 Test: 1.09966
Epoch: 7100 Train: 0.16771 Test: 1.09868
Epoch: 7200 Train: 0.16102 Test: 1.09028
Epoch: 7300 Train: 0.21329 Test: 1.09490
Epoch: 7400 Train: 0.15710 Test: 1.06216
Epoch 7500: New minimal relative error: 5.96%, model saved.
Epoch: 7500 Train: 0.15296 Test: 1.05950
Epoch: 7600 Train: 0.23189 Test: 1.16841
Epoch: 7700 Train: 0.22204 Test: 1.11016
Epoch: 7800 Train: 0.17166 Test: 1.06044
Epoch: 7900 Train: 0.15183 Test: 1.03660
Epoch: 8000 Train: 0.22715 Test: 1.10511
Epoch: 8100 Train: 0.14148 Test: 1.00746
Epoch: 8200 Train: 0.14351 Test: 1.01350
Epoch: 8300 Train: 0.26184 Test: 1.15892
Epoch: 8400 Train: 0.14446 Test: 1.01089
Epoch: 8500 Train: 0.73474 Test: 1.54308
Epoch: 8600 Train: 0.14814 Test: 0.99595
Epoch: 8700 Train: 0.13279 Test: 0.96472
Epoch: 8800 Train: 0.13141 Test: 0.97299
Epoch: 8900 Train: 0.12972 Test: 0.96770
Epoch: 9000 Train: 0.15487 Test: 1.00746
Epoch: 9100 Train: 0.24608 Test: 1.05121
Epoch: 9200 Train: 0.15205 Test: 0.97499
Epoch: 9300 Train: 0.17203 Test: 0.98701
Epoch: 9400 Train: 0.13757 Test: 0.96151
Epoch: 9500 Train: 0.18193 Test: 1.00872
Epoch: 9600 Train: 0.18737 Test: 1.00905
Epoch: 9700 Train: 0.17270 Test: 0.98226
Epoch: 9800 Train: 0.11833 Test: 0.91319
Epoch: 9900 Train: 0.11714 Test: 0.91178
Epoch: 9999 Train: 0.12125 Test: 0.91412
Training Loss: tensor(0.1213)
Test Loss: tensor(0.9141)
Learned LE: [ 0.81098276  0.03441862 -3.482532  ]
True LE: [ 8.7843573e-01 -1.5060285e-03 -1.4549833e+01]
Relative Error: [2.871377   2.7835164  2.5118022  2.1605942  1.8555664  1.6772115
 2.3734992  3.2989569  3.0200503  2.8891711  2.5767577  2.3000379
 2.2311957  1.3812518  1.7136858  2.4050865  3.1748593  3.6939268
 3.8042161  3.8220232  3.71509    2.5683587  2.0506752  1.7481763
 1.1092685  1.2647835  1.6166065  1.4253283  1.1860852  1.1506268
 1.2448242  1.449824   1.8386815  2.2691646  2.0229142  1.8887844
 1.9837692  1.7552441  1.7077432  1.6648859  1.8048396  2.2058046
 3.0165656  2.280212   1.0736411  0.90529287 0.7799599  0.6672441
 0.4718764  0.60952246 1.3134612  2.0597327  2.6504116  2.2709537
 2.0996306  1.7104777  1.5409318  1.4037259  1.0012797  1.150707
 1.6910063  2.0908184  2.3681376  2.5644724  2.5833576  2.2217407
 1.8369591  1.7461131  1.7248524  2.247204   2.980079   2.6947029
 2.6471386  2.3229795  2.1243672  1.9156374  1.0492835  1.5151087
 2.1073728  2.7529185  2.8331625  3.0244427  3.1088889  2.4405842
 1.7765555  1.4223256  1.7121915  1.742511   2.1280572  2.37826
 2.2605922  1.732908   1.4757808  1.4003768  1.5985932  1.9577603
 2.1720397  2.2874365  1.9234442  1.674772   1.320937   1.2855289
 1.1767694  1.3844788  1.9104961  2.8480062  2.2258449  0.9005843
 0.6961582  0.61235654 0.64866924 0.65719366 0.72172165 1.5163413
 1.9530269  2.3200665  1.8344916  1.5154289  1.2585934  1.1618698
 1.1509762  0.73703843 0.9459269  1.3812323  1.7249359  1.966587
 2.1068373  2.0799172  1.6699669  1.3370346  1.4017338  1.7441305
 2.3399477  2.7999234  2.5722754  2.3297045  2.172694   1.9345545
 1.7676845  0.8647063  1.1888697  1.8862493  2.0557148  2.0518239
 2.2251744  2.5493507  1.980104   1.6930252  2.354812   3.273507
 2.715617   2.3831325  2.8606317  3.0663743  2.7098598  2.1569104
 1.7353015  1.6907123  1.9408945  2.169428   2.0385368  2.0282376
 1.6472998  1.0540687  0.8813139  0.93447316 0.8448461  1.2734199
 2.2098634  2.282055   1.0829846  0.5331642  0.44483185 0.82777077
 0.83883566 0.7091408  1.3226192  1.6804805  1.9715014  1.5918732
 1.1770971  0.9799545  0.8887796  1.3216091  0.76654667 0.50700116
 0.85893303 1.2719891  1.4930727  1.5364771  1.4840639  1.3394196
 1.0965095  1.0330923  1.4954572  2.1589642  2.9205317  2.6647136
 2.4127657  2.0809493  1.7232332  1.7544463  1.1398641  0.6406837
 1.496625   1.6865445  1.4461459  1.6295086  2.1239202  2.0656471
 2.3673801  3.2657373  3.2897923  2.8378613  2.4614127  2.4326618
 2.5608273  1.9649854  1.7027909  1.9038366  1.8143542  1.6169001
 1.5889425  1.5398327  1.4970281  1.5765892  1.4896647  0.6200538
 0.6491071  0.9762222  0.7322597  1.0948328  2.372007   1.2409236
 0.6736234  0.19740093 0.6060963  0.90250504 0.4962099  0.99594605
 1.2571353  1.7715387  1.4448572  1.018651   0.73143786 0.6424368
 1.0058584  1.1075835  0.4161411  0.58302927 1.0010985  1.2086533
 1.091228   0.8003049  0.57610655 0.5027072  0.6762016  1.0928307
 1.7771788  2.3173053  2.436576   2.210725   1.9807694  1.6431974
 1.1812792  1.0869453  0.79418194 0.20478421 0.78089565 1.0846988
 1.2113543  1.7980155  2.1994631  2.6311548  3.3536758  3.2904239
 3.0304065  2.6346016  2.150889   1.7239047  1.5886333  1.2371806
 1.0271438  1.2910777  1.4628415  1.2091016  0.74694765 0.90699595
 0.94042706 0.9989     1.2426641  0.5878435  0.7232093  1.339833
 1.1701337  0.7580184  1.9691635  0.9560044  0.27566737 0.5388963
 0.59950787 0.4939004  0.6514419  0.81288135 1.2567657  1.4485672
 1.065383   0.47915637 0.4278778  0.50630856 1.193648   0.7972573
 0.22388673 0.7372838  0.9546111  0.91676575 0.724516   0.53782755
 0.47101033 0.6873147  1.0685848  1.1762754  1.4345124  1.8390585
 1.5969918  1.5070606  1.7040716  1.5561852  1.3718408  0.77453226
 0.8945354  0.65905136 0.22557317 0.5417418  0.9907663  2.0037868
 2.4233387  2.9092815  3.6178892  2.8931658  2.9551456  2.4826338
 1.8860478  1.3209313  1.3683203  0.9403485  0.53268385 0.4939195
 0.5043445  0.700512   0.8225726  0.3135438  0.45120683 0.3125262
 0.54753155 0.9054864  0.59453136 0.8943714  1.1387173  0.6003655
 1.5908775  0.92744344 0.73699474 0.38684493 0.39368752 0.1396121
 0.57804644 0.6746642  0.9982176  1.1913307  0.8550008  0.47752103
 0.2395815  0.57860535 1.1341321  0.9553194  0.16883193 0.46245718
 0.6215701  0.65102756 0.33421564 0.13899864 0.2670798  0.5215344
 0.5019202  0.63180673 0.93645704 1.3631907  1.4700347  1.004772
 0.8881568  1.1207486  1.2789875  1.1011842  0.710599   1.1145768
 0.7312942  0.7895897  1.3482869  1.6490912 ]
