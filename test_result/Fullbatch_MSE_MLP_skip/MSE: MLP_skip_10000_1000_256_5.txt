time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 103.18%, model saved.
Epoch: 0 Train: 3915.93433 Test: 3951.98584
Epoch 100: New minimal relative error: 64.86%, model saved.
Epoch: 100 Train: 83.50704 Test: 89.02293
Epoch 200: New minimal relative error: 37.18%, model saved.
Epoch: 200 Train: 27.56957 Test: 25.51652
Epoch 300: New minimal relative error: 17.59%, model saved.
Epoch: 300 Train: 9.02163 Test: 10.20253
Epoch: 400 Train: 9.31335 Test: 10.77506
Epoch: 500 Train: 8.56012 Test: 8.45874
Epoch: 600 Train: 7.66589 Test: 8.12079
Epoch: 700 Train: 4.96966 Test: 5.36461
Epoch 800: New minimal relative error: 10.20%, model saved.
Epoch: 800 Train: 4.70752 Test: 5.30341
Epoch: 900 Train: 6.57092 Test: 6.63916
Epoch: 1000 Train: 5.47050 Test: 6.47990
Epoch: 1100 Train: 7.74429 Test: 7.97283
Epoch: 1200 Train: 5.59754 Test: 6.79409
Epoch: 1300 Train: 4.96681 Test: 10.08553
Epoch: 1400 Train: 4.87871 Test: 5.43672
Epoch: 1500 Train: 3.18653 Test: 3.01781
Epoch: 1600 Train: 2.19632 Test: 2.39802
Epoch 1700: New minimal relative error: 10.11%, model saved.
Epoch: 1700 Train: 3.74932 Test: 3.28673
Epoch: 1800 Train: 2.74788 Test: 3.32094
Epoch: 1900 Train: 2.26963 Test: 2.24344
Epoch: 2000 Train: 1.81930 Test: 1.76342
Epoch: 2100 Train: 2.13180 Test: 2.33269
Epoch: 2200 Train: 1.66444 Test: 1.85728
Epoch: 2300 Train: 2.83087 Test: 2.54270
Epoch: 2400 Train: 2.92688 Test: 2.33599
Epoch: 2500 Train: 1.26850 Test: 1.33278
Epoch: 2600 Train: 1.79280 Test: 1.90432
Epoch: 2700 Train: 2.25079 Test: 2.58051
Epoch: 2800 Train: 1.31016 Test: 1.19860
Epoch: 2900 Train: 2.70017 Test: 2.65223
Epoch: 3000 Train: 4.59321 Test: 4.58709
Epoch: 3100 Train: 1.21952 Test: 1.11130
Epoch: 3200 Train: 3.34593 Test: 4.17725
Epoch: 3300 Train: 3.50485 Test: 3.81563
Epoch: 3400 Train: 0.98734 Test: 0.88083
Epoch: 3500 Train: 1.38275 Test: 1.07954
Epoch: 3600 Train: 0.64427 Test: 0.71904
Epoch: 3700 Train: 3.84901 Test: 4.33497
Epoch: 3800 Train: 3.49692 Test: 2.25900
Epoch: 3900 Train: 0.98963 Test: 0.98578
Epoch: 4000 Train: 0.46868 Test: 0.53851
Epoch 4100: New minimal relative error: 6.19%, model saved.
Epoch: 4100 Train: 0.38316 Test: 0.45086
Epoch: 4200 Train: 0.36937 Test: 0.43701
Epoch: 4300 Train: 0.50805 Test: 0.61894
Epoch: 4400 Train: 1.15295 Test: 1.43713
Epoch: 4500 Train: 1.20287 Test: 1.24527
Epoch: 4600 Train: 0.50607 Test: 0.61461
Epoch: 4700 Train: 0.80829 Test: 1.01857
Epoch: 4800 Train: 0.90193 Test: 1.14202
Epoch: 4900 Train: 0.42852 Test: 0.52424
Epoch: 5000 Train: 0.55803 Test: 0.61181
Epoch: 5100 Train: 0.31001 Test: 0.43937
Epoch: 5200 Train: 0.24206 Test: 0.34190
Epoch: 5300 Train: 0.23617 Test: 0.32026
Epoch: 5400 Train: 0.25295 Test: 0.34714
Epoch: 5500 Train: 0.27739 Test: 0.34878
Epoch: 5600 Train: 0.21024 Test: 0.29057
Epoch: 5700 Train: 0.25296 Test: 0.34609
Epoch: 5800 Train: 0.24110 Test: 0.32443
Epoch: 5900 Train: 0.33541 Test: 0.48580
Epoch: 6000 Train: 1.77629 Test: 1.38319
Epoch: 6100 Train: 1.38916 Test: 1.37264
Epoch: 6200 Train: 0.67564 Test: 0.99619
Epoch: 6300 Train: 0.35372 Test: 0.53954
Epoch: 6400 Train: 0.20401 Test: 0.31099
Epoch: 6500 Train: 0.28229 Test: 0.37362
Epoch: 6600 Train: 0.34032 Test: 0.34201
Epoch: 6700 Train: 0.87996 Test: 0.71662
Epoch: 6800 Train: 0.53446 Test: 0.73974
Epoch: 6900 Train: 0.15914 Test: 0.23809
Epoch: 7000 Train: 0.15570 Test: 0.23187
Epoch: 7100 Train: 0.15199 Test: 0.22837
Epoch: 7200 Train: 0.93839 Test: 0.97151
Epoch: 7300 Train: 0.18363 Test: 0.29116
Epoch: 7400 Train: 0.17500 Test: 0.25916
Epoch: 7500 Train: 0.15832 Test: 0.22468
Epoch: 7600 Train: 0.15819 Test: 0.27357
Epoch: 7700 Train: 0.25380 Test: 0.43824
Epoch: 7800 Train: 0.13494 Test: 0.20591
Epoch: 7900 Train: 0.13331 Test: 0.20710
Epoch: 8000 Train: 0.21753 Test: 0.34993
Epoch: 8100 Train: 0.13594 Test: 0.21140
Epoch: 8200 Train: 0.12739 Test: 0.20095
Epoch: 8300 Train: 0.13454 Test: 0.20169
Epoch: 8400 Train: 0.14290 Test: 0.21428
Epoch: 8500 Train: 0.20293 Test: 0.25372
Epoch: 8600 Train: 0.14205 Test: 0.23097
Epoch: 8700 Train: 0.25680 Test: 0.31279
Epoch: 8800 Train: 0.22138 Test: 0.32755
Epoch: 8900 Train: 0.11556 Test: 0.18567
Epoch: 9000 Train: 0.11573 Test: 0.18560
Epoch: 9100 Train: 0.14345 Test: 0.20127
Epoch: 9200 Train: 0.11831 Test: 0.18512
Epoch: 9300 Train: 0.13920 Test: 0.21671
Epoch: 9400 Train: 0.10891 Test: 0.17857
Epoch: 9500 Train: 0.74483 Test: 0.74643
Epoch: 9600 Train: 0.10644 Test: 0.17539
Epoch: 9700 Train: 0.10685 Test: 0.17510
Epoch: 9800 Train: 0.13754 Test: 0.20841
Epoch: 9900 Train: 0.10577 Test: 0.17549
Epoch: 9999 Train: 0.10187 Test: 0.17008
Training Loss: tensor(0.1019)
Test Loss: tensor(0.1701)
Learned LE: [ 0.66416025  0.03064199 -2.8097584 ]
True LE: [ 8.7035030e-01  4.0023681e-03 -1.4553672e+01]
Relative Error: [15.552928  15.749405  15.195354  14.506478  13.715631  12.8717375
 12.012458  11.077525  10.539407  10.182026   9.819393   9.452992
  9.139999   8.937369   8.726155   8.892916   9.599996  10.614753
 11.849535  13.311098  14.843652  16.538815  18.41713   20.525852
 21.550053  22.5272    23.52602   23.836355  23.27596   22.444305
 21.553871  20.553225  19.612072  18.497322  17.426588  16.435919
 15.823409  15.24383   14.757784  14.379142  14.372902  14.855316
 15.516165  15.710224  15.077712  14.616124  14.32873   14.206073
 14.228428  14.3587265 14.565253  14.249182  13.297345  12.1984825
 11.2713    11.2377205 11.427952  12.009721  12.346875  12.587291
 13.197624  13.884981  14.415865  14.343866  13.859512  13.243495
 12.512155  11.787182  10.953389  10.028306   9.5596075  9.212026
  8.7947855  8.352812   7.949028   7.7489285  7.627508   7.828734
  8.60057    9.569643  10.771514  12.221027  13.813211  15.295956
 17.078428  18.861462  19.746567  20.770012  21.790531  21.714003
 21.133207  20.399134  19.589891  18.652782  17.74509   16.664766
 15.688499  14.772193  14.165594  13.596704  13.074395  12.752238
 13.040902  13.54744   14.245944  14.590582  13.9839325 13.558311
 13.312764  13.233548  13.315368  13.480939  13.621523  13.350551
 12.361474  11.249529  10.196742   9.890264  10.039743  10.675257
 11.083839  11.410797  11.944264  12.734763  13.299547  13.048551
 12.646857  12.111674  11.513929  10.84335   10.040432   9.164954
  8.7476635  8.368532   7.8920603  7.363315   7.0157347  6.859649
  6.7675314  6.876604   7.5797567  8.492311   9.652049  11.08233
 12.786688  14.306138  15.867776  17.06464   17.983438  19.055859
 20.048838  19.674814  19.047049  18.445194  17.777172  16.886448
 16.018429  15.102887  14.092334  13.231893  12.633464  12.0262165
 11.466493  11.4345045 11.838371  12.389528  13.013981  13.465622
 12.973243  12.593046  12.431746  12.454006  12.470236  12.535532
 12.636799  12.315507  11.439126  10.383555   9.2978325  8.658825
  8.795752   9.360424   9.853077  10.270217  10.723374  11.60321
 11.999429  11.846951  11.54758   11.101874  10.649507  10.040725
  9.278597   8.443572   8.073345   7.6687307  7.133761   6.671323
  6.368629   6.1566978  5.999821   5.9834495  6.536734   7.379083
  8.485115   9.884155  11.573031  13.231629  14.867832  15.445262
 16.223242  17.244198  18.063112  17.72315   17.1334    16.587872
 16.019592  15.251231  14.460718  13.626444  12.640103  11.825686
 11.217779  10.532311  10.119191  10.350635  10.738669  11.274859
 11.839333  12.2843075 12.02248   11.721752  11.667044  11.618265
 11.63164   11.727684  11.562254  11.3812895 10.519688   9.646318
  8.510946   7.5532517  7.6473913  8.195854   8.641553   9.147696
  9.678326  10.493694  10.772675  10.733948  10.549612  10.266323
  9.909642   9.377571   8.673013   7.873831   7.5531654  7.135161
  6.6255903  6.276133   5.931493   5.582985   5.4266715  5.329835
  5.701984   6.3471637  7.315369   8.656581  10.303136  12.073034
 13.686034  13.972929  14.476553  15.439076  16.09015   15.795873
 15.332727  14.859529  14.4103365 13.718925  13.070464  12.294941
 11.34311   10.544466   9.871751   9.204085   9.077557   9.325567
  9.674447  10.194576  10.719564  11.203467  11.253199  11.005018
 10.857463  10.757839  10.784769  10.730183  10.567665  10.488441
  9.681889   8.882257   7.877979   6.7848706  6.4959035  7.0558605
  7.7170057  8.025977   8.666729   9.351045   9.590573   9.689538
  9.6363125  9.527057   9.284266   8.852398   8.228272   7.4532986
  7.180809   6.76639    6.449689   6.229259   5.9886     5.784758
  5.4633155  5.107407   5.244723   5.748465   6.466916   7.4451184
  8.989176  10.867884  12.275091  12.471715  12.831829  13.614595
 14.158305  13.934705  13.597327  13.256274  12.887819  12.324938
 11.770879  11.098332  10.195618   9.391225   8.637213   8.196113
  8.141197   8.321547   8.623532   9.122043   9.667197  10.2098
 10.56351   10.293742  10.000236   9.871845   9.9354     9.770313
  9.632352   9.584906   8.914681   8.119323   7.374192   6.228252
  5.477123   5.8654943  6.777085   7.2182765  7.6245866  8.149349
  8.434736   8.687192   8.78385    8.861262   8.759656   8.452005
  7.9281406  7.1886263  6.952841   6.792558   6.773371   6.780786
  6.6572895  6.372856   5.929863   5.3174515  4.9314284  5.1437087
  5.7032194  6.567385   7.7447147  9.456079  10.78081   10.975585
 11.297071  11.890388  12.296519  12.156123 ]
