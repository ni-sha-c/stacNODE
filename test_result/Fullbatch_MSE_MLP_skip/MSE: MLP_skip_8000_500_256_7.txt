time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.76%, model saved.
Epoch: 0 Train: 3689.85352 Test: 4280.48633
Epoch 80: New minimal relative error: 78.38%, model saved.
Epoch: 80 Train: 618.22778 Test: 641.94440
Epoch 160: New minimal relative error: 54.22%, model saved.
Epoch: 160 Train: 129.72757 Test: 136.43330
Epoch 240: New minimal relative error: 26.82%, model saved.
Epoch: 240 Train: 32.41542 Test: 40.93548
Epoch 320: New minimal relative error: 15.21%, model saved.
Epoch: 320 Train: 17.08581 Test: 19.51244
Epoch: 400 Train: 16.95357 Test: 25.80793
Epoch: 480 Train: 7.81629 Test: 9.30607
Epoch: 560 Train: 48.33151 Test: 24.77357
Epoch: 640 Train: 5.85527 Test: 6.53555
Epoch: 720 Train: 9.57350 Test: 6.28009
Epoch: 800 Train: 9.28154 Test: 9.99353
Epoch: 880 Train: 6.14719 Test: 6.20867
Epoch: 960 Train: 4.12268 Test: 4.15763
Epoch: 1040 Train: 3.17600 Test: 3.42561
Epoch 1120: New minimal relative error: 10.09%, model saved.
Epoch: 1120 Train: 2.96446 Test: 3.15412
Epoch: 1200 Train: 7.16624 Test: 8.62136
Epoch: 1280 Train: 3.80586 Test: 4.11709
Epoch: 1360 Train: 3.01078 Test: 2.93788
Epoch: 1440 Train: 2.51084 Test: 2.59373
Epoch: 1520 Train: 2.37246 Test: 2.49287
Epoch: 1600 Train: 14.91755 Test: 20.97579
Epoch: 1680 Train: 2.21054 Test: 2.67061
Epoch: 1760 Train: 1.86138 Test: 1.90854
Epoch: 1840 Train: 1.76641 Test: 1.78999
Epoch: 1920 Train: 1.55943 Test: 1.61394
Epoch: 2000 Train: 1.52055 Test: 1.54495
Epoch 2080: New minimal relative error: 7.28%, model saved.
Epoch: 2080 Train: 1.39288 Test: 1.43111
Epoch: 2160 Train: 1.46151 Test: 1.53516
Epoch: 2240 Train: 1.58868 Test: 1.79304
Epoch: 2320 Train: 7.34252 Test: 7.22049
Epoch: 2400 Train: 1.40205 Test: 1.52435
Epoch: 2480 Train: 1.39053 Test: 1.27639
Epoch: 2560 Train: 1.20538 Test: 1.25640
Epoch: 2640 Train: 1.48624 Test: 1.69038
Epoch: 2720 Train: 6.90799 Test: 5.84192
Epoch: 2800 Train: 1.79767 Test: 2.10237
Epoch: 2880 Train: 6.67821 Test: 4.04266
Epoch: 2960 Train: 2.30233 Test: 2.80403
Epoch: 3040 Train: 1.00958 Test: 1.01970
Epoch: 3120 Train: 0.86596 Test: 0.77829
Epoch: 3200 Train: 0.96103 Test: 1.00416
Epoch: 3280 Train: 0.75957 Test: 0.74962
Epoch: 3360 Train: 0.76710 Test: 0.67565
Epoch: 3440 Train: 1.14473 Test: 0.92080
Epoch: 3520 Train: 0.72945 Test: 0.64294
Epoch: 3600 Train: 0.83874 Test: 0.79262
Epoch: 3680 Train: 0.85383 Test: 0.82105
Epoch: 3760 Train: 2.19349 Test: 2.25882
Epoch: 3840 Train: 2.18483 Test: 2.90415
Epoch: 3920 Train: 4.52385 Test: 4.96910
Epoch: 4000 Train: 0.55246 Test: 0.52220
Epoch 4080: New minimal relative error: 6.00%, model saved.
Epoch: 4080 Train: 0.57953 Test: 0.55234
Epoch: 4160 Train: 0.55909 Test: 0.54553
Epoch: 4240 Train: 1.17850 Test: 1.51150
Epoch: 4320 Train: 0.51091 Test: 0.48734
Epoch: 4400 Train: 0.55881 Test: 0.55340
Epoch: 4480 Train: 1.70277 Test: 1.60169
Epoch: 4560 Train: 1.98275 Test: 2.59651
Epoch: 4640 Train: 0.46200 Test: 0.45378
Epoch: 4720 Train: 0.46320 Test: 0.47723
Epoch: 4800 Train: 1.90902 Test: 1.25774
Epoch: 4880 Train: 0.65974 Test: 0.76158
Epoch: 4960 Train: 0.42559 Test: 0.42483
Epoch: 5040 Train: 0.42085 Test: 0.44876
Epoch: 5120 Train: 0.42509 Test: 0.42948
Epoch: 5200 Train: 0.72425 Test: 0.71191
Epoch: 5280 Train: 2.07517 Test: 2.45947
Epoch: 5360 Train: 0.38956 Test: 0.40319
Epoch: 5440 Train: 0.38456 Test: 0.40788
Epoch: 5520 Train: 0.87840 Test: 1.13689
Epoch: 5600 Train: 0.36885 Test: 0.38544
Epoch 5680: New minimal relative error: 5.36%, model saved.
Epoch: 5680 Train: 0.36306 Test: 0.38579
Epoch: 5760 Train: 0.39509 Test: 0.41726
Epoch: 5840 Train: 0.35638 Test: 0.38619
Epoch: 5920 Train: 0.34880 Test: 0.37383
Epoch: 6000 Train: 0.35710 Test: 0.38594
Epoch: 6080 Train: 0.45887 Test: 0.51860
Epoch: 6160 Train: 0.32671 Test: 0.35106
Epoch: 6240 Train: 0.38559 Test: 0.42845
Epoch: 6320 Train: 0.31568 Test: 0.34304
Epoch: 6400 Train: 0.31335 Test: 0.34156
Epoch: 6480 Train: 0.30418 Test: 0.33746
Epoch: 6560 Train: 1.21186 Test: 1.12600
Epoch: 6640 Train: 0.29379 Test: 0.32688
Epoch: 6720 Train: 0.31523 Test: 0.37016
Epoch: 6800 Train: 0.28513 Test: 0.32225
Epoch: 6880 Train: 0.29972 Test: 0.33743
Epoch: 6960 Train: 0.27467 Test: 0.31385
Epoch: 7040 Train: 0.31147 Test: 0.34395
Epoch: 7120 Train: 0.40340 Test: 0.47750
Epoch: 7200 Train: 0.26256 Test: 0.30689
Epoch: 7280 Train: 0.25947 Test: 0.30293
Epoch: 7360 Train: 0.27729 Test: 0.32942
Epoch 7440: New minimal relative error: 5.30%, model saved.
Epoch: 7440 Train: 0.24979 Test: 0.29574
Epoch: 7520 Train: 1.33091 Test: 1.72240
Epoch: 7600 Train: 0.24304 Test: 0.29130
Epoch: 7680 Train: 0.76006 Test: 0.89291
Epoch: 7760 Train: 0.23896 Test: 0.29227
Epoch: 7840 Train: 0.23314 Test: 0.28268
Epoch: 7920 Train: 0.22907 Test: 0.27926
Epoch: 7999 Train: 0.22599 Test: 0.27764
Training Loss: tensor(0.2260)
Test Loss: tensor(0.2776)
Learned LE: [ 0.60905266  0.08772478 -2.4038796 ]
True LE: [ 8.8525283e-01  1.4157579e-03 -1.4565527e+01]
Relative Error: [10.425924  10.799025  10.494669   9.749824   8.910218   8.264164
  7.9106     7.6877303  7.6337347  7.3360934  7.179726   7.150744
  6.5497108  5.3566775  3.6880703  2.4213045  1.5549703  1.2956017
  1.0451068  1.3745357  3.0289197  4.472246   5.409636   6.1156964
  6.5246162  6.8534517  6.816285   6.3137302  5.849885   5.3255906
  4.7599187  4.185269   3.8209188  3.4340267  3.180233   3.2962046
  3.6507719  4.634079   5.878434   7.2635136  8.4728155 10.167209
 12.089778  14.07522   14.843136  16.13904   16.418346  16.4023
 15.660141  14.663574  13.938797  14.028302  13.857667  13.4671955
 12.991588  12.247244  11.499477  10.722289   9.920342   9.500096
  9.151767   8.900638   8.850011   9.037171   9.621861   9.0259285
  8.357074   7.634787   7.1408896  6.896648   6.8370285  6.6010647
  6.490779   6.5511084  5.9914227  4.6322055  3.0702193  1.9355363
  1.6442925  1.5585287  1.2441678  1.9086391  3.884387   5.3392425
  6.165617   6.695377   7.056508   7.305964   7.355556   6.6459484
  6.2018704  5.689201   4.9700813  4.281321   3.6852553  3.388751
  3.0937128  3.1639197  3.584965   4.2914686  5.3737793  6.5945783
  7.761795   9.323146  11.327887  13.591276  14.297149  15.190965
 15.295487  15.185661  14.486533  13.609282  13.202212  13.287197
 13.127628  12.830898  12.24167   11.528446  10.775926   9.92348
  9.099295   8.570928   8.089142   7.6979527  7.444323   7.52421
  7.9885015  8.25307    7.73523    7.016647   6.354609   6.058978
  5.987097   5.831697   5.7462225  5.882037   5.4265814  3.956366
  2.511582   1.8847361  2.097075   1.8404096  1.4617491  2.308131
  4.2993164  6.038116   6.7791996  7.182388   7.4691544  7.561261
  7.533864   6.981097   6.317275   5.5828223  4.9568563  4.3441424
  3.7386491  3.3202503  3.1170838  2.937214   3.3342626  3.955973
  4.8612685  5.829141   7.0132732  8.442712  10.429577  12.908904
 13.8913    14.170699  14.216633  14.048722  13.46862   12.762892
 12.557467  12.5912    12.592859  12.222071  11.658974  10.979061
 10.16364    9.231957   8.371927   7.7132425  7.0907245  6.5490894
  6.2858605  6.4162073  6.8138757  7.334932   7.1981535  6.577959
  5.7898016  5.1746564  5.0564375  5.030621   4.934688   5.137034
  4.8384132  3.1980782  2.1910152  2.0993109  2.6986418  2.1657186
  1.7324898  2.5686347  4.530572   6.3550124  7.2873445  7.4578977
  7.5012794  7.4973097  7.4597487  7.126494   6.097645   5.210053
  4.7577653  4.296903   3.9064088  3.3943632  3.1509619  2.966042
  2.9254432  3.4165785  4.225688   5.0084553  6.295951   7.49944
  9.359845  11.88806   13.460244  13.454102  13.280495  12.971873
 12.570015  12.119571  11.99364   12.201773  12.073419  11.764882
 11.212289  10.552786   9.676686   8.678315   7.8012614  7.032702
  6.3342147  5.8727217  5.5315185  5.559585   5.8875027  6.4118237
  6.733547   6.3802257  5.5777473  4.8081355  4.2848086  4.078448
  4.0581455  4.2596993  4.0688334  2.5291445  2.1508534  2.419887
  3.313395   2.6085432  2.0612087  2.6688933  4.592302   6.334942
  7.4875355  7.482017   7.441534   7.4201117  7.345145   7.037718
  5.8903613  4.9898443  4.38507    4.1315784  3.84798    3.6056023
  3.2315896  2.961148   2.6586542  2.9327977  3.5030377  4.2620687
  5.391544   6.511391   8.153039  10.6048155 13.05459   12.821124
 12.3583555 11.953891  11.1496935 10.717473  10.606958  10.952106
 11.234197  11.291842  10.880568  10.244095   9.3449955  8.3946495
  7.472385   6.5972857  5.967265   5.3918014  5.0267534  4.974681
  5.1275992  5.5572243  6.10331    5.915871   5.525928   4.65067
  3.8824599  3.543889   3.2362652  3.2638617  3.1312392  1.9592053
  2.1015031  2.8096125  3.636076   3.2010343  2.5123968  2.616042
  4.426017   6.0538826  7.232278   7.4528704  7.357175   7.3119326
  7.2072597  6.9636183  5.860769   4.982074   4.300529   3.834061
  3.6908321  3.615658   3.4678142  3.004921   2.7347498  2.477809
  2.7709327  3.549552   4.357872   5.647434   6.897918   9.08153
 11.768533  12.349432  11.422132  10.499141   9.689009   9.204869
  9.154708   9.464845   9.757129  10.012449  10.242137   9.9547205
  9.289334   8.3208685  7.352048   6.5583696  5.8491006  5.220487
  4.7358327  4.435203   4.3521824  4.596759   5.1473     5.626719
  5.037795   4.549855   3.7262173  3.0375907  2.7194023  2.5622773
  2.1496255  1.6187687  2.167554   3.2306633  3.9840505  3.9610772
  3.1464942  2.4222486  4.058155   5.6330895  6.6874967  7.179198
  7.294985   7.268381   7.0805893  6.7070713]
