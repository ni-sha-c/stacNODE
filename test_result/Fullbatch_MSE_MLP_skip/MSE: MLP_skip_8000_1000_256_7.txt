time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 110.84%, model saved.
Epoch: 0 Train: 3816.37183 Test: 4149.00732
Epoch: 80 Train: 608.95215 Test: 553.97650
Epoch 160: New minimal relative error: 98.03%, model saved.
Epoch: 160 Train: 159.79713 Test: 219.49399
Epoch 240: New minimal relative error: 44.14%, model saved.
Epoch: 240 Train: 133.77432 Test: 53.49748
Epoch 320: New minimal relative error: 26.46%, model saved.
Epoch: 320 Train: 56.57820 Test: 32.29607
Epoch: 400 Train: 52.70072 Test: 38.37008
Epoch: 480 Train: 75.12253 Test: 67.37159
Epoch: 560 Train: 14.64945 Test: 18.00917
Epoch: 640 Train: 8.45603 Test: 8.54907
Epoch 720: New minimal relative error: 22.85%, model saved.
Epoch: 720 Train: 6.25728 Test: 7.27087
Epoch: 800 Train: 9.38975 Test: 11.03854
Epoch 880: New minimal relative error: 18.10%, model saved.
Epoch: 880 Train: 7.92079 Test: 8.96800
Epoch 960: New minimal relative error: 12.79%, model saved.
Epoch: 960 Train: 10.37001 Test: 8.78938
Epoch: 1040 Train: 5.87283 Test: 5.48206
Epoch: 1120 Train: 5.58571 Test: 5.63282
Epoch: 1200 Train: 11.32002 Test: 11.02857
Epoch: 1280 Train: 4.36877 Test: 4.71109
Epoch: 1360 Train: 15.93500 Test: 20.13076
Epoch: 1440 Train: 7.68632 Test: 7.67742
Epoch 1520: New minimal relative error: 9.82%, model saved.
Epoch: 1520 Train: 4.19697 Test: 3.03144
Epoch: 1600 Train: 3.71444 Test: 3.71088
Epoch: 1680 Train: 4.98537 Test: 5.08522
Epoch: 1760 Train: 4.38877 Test: 4.32594
Epoch: 1840 Train: 7.27320 Test: 7.59664
Epoch: 1920 Train: 6.15987 Test: 5.74083
Epoch: 2000 Train: 3.05619 Test: 3.24076
Epoch: 2080 Train: 3.31180 Test: 3.58970
Epoch: 2160 Train: 2.61925 Test: 2.22685
Epoch: 2240 Train: 1.73731 Test: 1.61638
Epoch 2320: New minimal relative error: 9.69%, model saved.
Epoch: 2320 Train: 1.67774 Test: 1.55278
Epoch: 2400 Train: 3.00407 Test: 3.24570
Epoch: 2480 Train: 6.29963 Test: 4.78382
Epoch: 2560 Train: 1.85639 Test: 1.99845
Epoch: 2640 Train: 1.74270 Test: 1.44559
Epoch: 2720 Train: 1.95062 Test: 2.20917
Epoch: 2800 Train: 3.77806 Test: 2.94246
Epoch: 2880 Train: 1.73560 Test: 1.66659
Epoch: 2960 Train: 3.03043 Test: 2.30881
Epoch: 3040 Train: 2.04809 Test: 2.13318
Epoch: 3120 Train: 0.98771 Test: 0.85715
Epoch 3200: New minimal relative error: 6.69%, model saved.
Epoch: 3200 Train: 0.97266 Test: 0.86673
Epoch: 3280 Train: 0.91831 Test: 0.89287
Epoch: 3360 Train: 1.68007 Test: 1.28235
Epoch 3440: New minimal relative error: 4.70%, model saved.
Epoch: 3440 Train: 0.77785 Test: 0.66991
Epoch: 3520 Train: 0.80470 Test: 0.67628
Epoch: 3600 Train: 3.05127 Test: 2.96817
Epoch: 3680 Train: 8.60856 Test: 10.47460
Epoch: 3760 Train: 0.67406 Test: 0.57759
Epoch: 3840 Train: 0.84435 Test: 0.73654
Epoch: 3920 Train: 2.76952 Test: 2.76740
Epoch: 4000 Train: 1.93100 Test: 2.45894
Epoch: 4080 Train: 0.58830 Test: 0.50313
Epoch: 4160 Train: 0.62364 Test: 0.52636
Epoch: 4240 Train: 0.69285 Test: 0.49644
Epoch: 4320 Train: 2.13055 Test: 2.80565
Epoch: 4400 Train: 0.51754 Test: 0.43997
Epoch: 4480 Train: 0.51788 Test: 0.47663
Epoch: 4560 Train: 0.56408 Test: 0.53417
Epoch: 4640 Train: 0.47863 Test: 0.40663
Epoch: 4720 Train: 0.59864 Test: 0.83176
Epoch: 4800 Train: 0.45797 Test: 0.39070
Epoch: 4880 Train: 0.44510 Test: 0.38134
Epoch: 4960 Train: 0.79170 Test: 0.69938
Epoch: 5040 Train: 0.43435 Test: 0.36482
Epoch: 5120 Train: 3.05791 Test: 3.20861
Epoch: 5200 Train: 0.40609 Test: 0.34546
Epoch: 5280 Train: 0.39941 Test: 0.34031
Epoch: 5360 Train: 0.41576 Test: 0.33924
Epoch: 5440 Train: 0.40128 Test: 0.35638
Epoch: 5520 Train: 0.44407 Test: 0.37194
Epoch: 5600 Train: 1.85738 Test: 1.27279
Epoch: 5680 Train: 0.46709 Test: 0.46329
Epoch: 5760 Train: 1.27503 Test: 1.60663
Epoch: 5840 Train: 0.34504 Test: 0.29539
Epoch: 5920 Train: 0.50031 Test: 0.59049
Epoch: 6000 Train: 0.33444 Test: 0.28784
Epoch: 6080 Train: 0.33954 Test: 0.29938
Epoch: 6160 Train: 0.31611 Test: 0.27315
Epoch: 6240 Train: 0.31508 Test: 0.26794
Epoch: 6320 Train: 0.32164 Test: 0.26243
Epoch: 6400 Train: 0.29995 Test: 0.25921
Epoch: 6480 Train: 0.35503 Test: 0.30238
Epoch: 6560 Train: 0.75092 Test: 0.48718
Epoch: 6640 Train: 0.43525 Test: 0.47177
Epoch: 6720 Train: 1.52007 Test: 1.36480
Epoch: 6800 Train: 0.26257 Test: 0.22202
Epoch: 6880 Train: 0.26753 Test: 0.22407
Epoch: 6960 Train: 0.88045 Test: 1.06406
Epoch: 7040 Train: 0.24630 Test: 0.20846
Epoch: 7120 Train: 0.28706 Test: 0.25782
Epoch: 7200 Train: 0.24785 Test: 0.21651
Epoch: 7280 Train: 0.23522 Test: 0.20136
Epoch: 7360 Train: 0.24445 Test: 0.21791
Epoch: 7440 Train: 0.22656 Test: 0.19453
Epoch: 7520 Train: 0.22716 Test: 0.19527
Epoch: 7600 Train: 0.22774 Test: 0.19076
Epoch: 7680 Train: 0.25695 Test: 0.23440
Epoch: 7760 Train: 0.21481 Test: 0.19460
Epoch 7840: New minimal relative error: 4.17%, model saved.
Epoch: 7840 Train: 0.21417 Test: 0.18744
Epoch: 7920 Train: 0.20831 Test: 0.17978
Epoch: 7999 Train: 0.55933 Test: 0.48876
Training Loss: tensor(0.5593)
Test Loss: tensor(0.4888)
Learned LE: [ 0.857229   -0.02435287 -3.7344356 ]
True LE: [ 8.7642789e-01 -5.7792652e-04 -1.4546682e+01]
Relative Error: [ 3.8587158   5.3145843   6.9722204   8.722073   10.490078   12.220231
 13.765095   14.743116   15.357982   15.731721   15.785444   15.3939085
 14.561627   13.547436   12.672846   11.761537   10.875617   10.088452
  9.424569    8.894927    8.582104    8.0307665   7.056554    6.143198
  5.34496     4.806413    4.516875    4.4038954   4.501982    4.7551003
  5.017302    5.3542356   5.9460573   6.654782    6.4816465   6.0741262
  5.695538    5.135503    4.3027735   3.4688673   2.980237    3.0908556
  3.752107    4.45641     4.3248158   4.673902    4.8774524   4.993572
  5.141378    5.5192327   5.784569    5.797307    5.607804    5.188571
  4.610803    4.0585713   3.5720575   3.209304    2.8983319   2.5712757
  2.3298898   2.5075078   3.400833    4.8493934   6.400445    7.9466276
  9.65827    11.490888   13.145198   14.345251   14.939252   15.229015
 15.236476   14.865028   14.272705   13.250096   12.288047   11.305854
 10.35029     9.491148    8.750756    8.221378    7.912478    7.039687
  6.1164165   5.2717733   4.481947    3.9501944   3.6683836   3.525591
  3.6091664   3.9064329   4.163082    4.5083346   5.0307775   5.7784986
  6.3865314   5.81183     5.440367    4.842267    3.9856608   3.0770824
  2.660807    3.114011    4.005904    4.494389    4.459851    4.857507
  5.036151    4.941042    5.1442075   5.6605926   5.9778833   6.0006227
  5.811093    5.407757    4.85078     4.3508396   3.9169698   3.576563
  3.2474537   2.8642745   2.4911366   2.383074    2.8099425   3.7705307
  5.0357637   6.549455    8.2357855   9.979831   11.625567   13.247969
 13.947002   14.36695    14.460012   14.167687   13.733973   12.862009
 12.006331   10.939151    9.8946705   8.959257    8.144144    7.5803466
  7.1295977   6.297203    5.300638    4.4327807   3.6001399   3.0601323
  2.8129332   2.7769732   2.8063657   2.9134018   3.2877874   3.7302847
  4.204276    4.9621506   5.7438617   5.661486    5.281572    4.6705666
  3.7726495   2.7519307   2.409601    3.1426926   4.299904    4.562328
  4.7243857   4.9866686   5.121693    4.7798676   5.3117714   5.9452577
  6.303382    6.327241    5.910758    5.280874    4.782448    4.386252
  4.276084    4.0151534   3.6455762   3.2587197   2.8734107   2.571826
  2.6221876   2.6943474   3.783381    5.2308197   6.7685547   8.3048115
  9.774401   11.3181     12.929121   13.410953   13.484168   13.329739
 13.052824   12.472439   11.522955   10.608492    9.539182    8.478926
  7.5990267   7.025006    6.3892574   5.5741234   4.5306964   3.599304
  2.7542305   2.2122638   1.9823532   2.0139325   2.1081243   2.125548
  2.350735    2.9224772   3.5322506   4.1418924   4.9849977   5.5253916
  5.2775655   4.5763073   3.6682408   2.5367997   2.1363332   3.1079347
  4.371937    4.711348    4.881778    5.0867424   5.096097    4.829822
  5.609378    6.3312845   6.476025    5.99709     5.380211    4.740393
  4.1699967   3.7702131   3.6981084   3.755935    3.9653773   3.7522607
  3.3950999   3.055298    2.8149667   2.4549625   2.6884058   3.8647485
  5.2799163   6.592691    8.019057    9.510938   11.07715    12.51431
 12.56282    12.439065   12.319144   11.98935    11.185061   10.161419
  9.172272    8.118578    7.123244    6.6204844   5.6353726   4.9020915
  3.7938256   2.85565     1.8687425   1.2863629   1.0867206   1.2466178
  1.5058522   1.4269469   1.6601602   2.0783682   2.825002    3.4701445
  4.1585674   4.8064322   5.2373924   4.5432916   3.4969053   2.334835
  1.7487291   2.8983521   4.293354    4.8258038   5.013801    4.7902555
  4.5846725   4.6728964   5.81269     6.3952355   6.145274    5.630452
  4.9508786   4.3178687   3.6244378   3.2226038   3.1585686   3.1690662
  3.28845     3.6270964   3.92537     3.5767548   3.286824    2.7601902
  2.2626119   2.579118    3.6810205   4.923787    6.306205    7.7628255
  9.166644   10.547049   11.63984    11.540215   11.480058   11.467786
 10.787037    9.881056    8.428166    7.1327963   6.1817045   5.814892
  5.1506853   4.4689727   3.5208588   2.26749     1.2571597   0.9265719
  0.95072633  1.0744045   1.2150769   1.124645    0.9129202   1.4916954
  2.0120294   2.7698321   3.4077837   4.1211295   4.4476013   4.538818
  3.465497    2.2243094   1.3690821   2.5143654   4.1045914   4.897684
  4.4136114   4.172204    3.9011781   4.276787    5.189996    5.376827
  5.5529523   5.3810163   4.70563     4.014784    3.190322    2.817481
  2.6591516   2.54543     2.5744824   2.8580995   3.3727746   4.0509286
  3.870278    3.6460838   2.3007      2.0263076   2.262258    3.337358
  4.5705295   5.8299975   7.1564207   8.405862    9.708841   10.579105
 10.444596   10.167723    9.740669    8.62544     7.3408456   5.8472257
  5.03996     5.1313934   5.2248244   4.5290947   3.5639431   2.2872462
  1.3455516   1.2496678   1.6891407   1.7084924 ]
