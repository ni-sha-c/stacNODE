time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 105.11%, model saved.
Epoch: 0 Train: 3495.44019 Test: 3701.97217
Epoch: 100 Train: 59.61512 Test: 65.06300
Epoch 200: New minimal relative error: 43.54%, model saved.
Epoch: 200 Train: 18.39154 Test: 33.16591
Epoch 300: New minimal relative error: 28.54%, model saved.
Epoch: 300 Train: 34.65293 Test: 27.33602
Epoch 400: New minimal relative error: 14.72%, model saved.
Epoch: 400 Train: 6.00814 Test: 6.33964
Epoch: 500 Train: 11.44731 Test: 16.57363
Epoch: 600 Train: 15.22581 Test: 20.48330
Epoch: 700 Train: 3.26719 Test: 3.66655
Epoch 800: New minimal relative error: 12.25%, model saved.
Epoch: 800 Train: 2.51284 Test: 2.95703
Epoch: 900 Train: 14.00355 Test: 11.41398
Epoch: 1000 Train: 4.32037 Test: 4.71029
Epoch: 1100 Train: 4.20867 Test: 4.97619
Epoch: 1200 Train: 6.51745 Test: 9.52099
Epoch 1300: New minimal relative error: 10.95%, model saved.
Epoch: 1300 Train: 1.51980 Test: 2.15394
Epoch 1400: New minimal relative error: 7.95%, model saved.
Epoch: 1400 Train: 1.85896 Test: 1.61417
Epoch: 1500 Train: 3.98748 Test: 4.50630
Epoch: 1600 Train: 4.87948 Test: 6.77948
Epoch: 1700 Train: 0.66522 Test: 0.92074
Epoch: 1800 Train: 2.84908 Test: 3.56281
Epoch: 1900 Train: 2.72484 Test: 2.48208
Epoch 2000: New minimal relative error: 7.76%, model saved.
Epoch: 2000 Train: 1.13296 Test: 1.05045
Epoch: 2100 Train: 2.09586 Test: 1.95281
Epoch: 2200 Train: 0.46118 Test: 0.45864
Epoch 2300: New minimal relative error: 5.31%, model saved.
Epoch: 2300 Train: 0.51643 Test: 0.68555
Epoch: 2400 Train: 2.54137 Test: 3.76499
Epoch: 2500 Train: 0.73198 Test: 0.90582
Epoch: 2600 Train: 1.41897 Test: 1.11314
Epoch: 2700 Train: 2.73306 Test: 2.18568
Epoch 2800: New minimal relative error: 4.88%, model saved.
Epoch: 2800 Train: 0.36041 Test: 0.60948
Epoch: 2900 Train: 0.56175 Test: 0.69530
Epoch: 3000 Train: 0.63562 Test: 0.67785
Epoch: 3100 Train: 1.55892 Test: 2.20073
Epoch: 3200 Train: 2.24074 Test: 2.97403
Epoch: 3300 Train: 0.37912 Test: 0.51777
Epoch: 3400 Train: 0.35161 Test: 0.46637
Epoch: 3500 Train: 0.26173 Test: 0.31536
Epoch: 3600 Train: 0.50449 Test: 0.55222
Epoch: 3700 Train: 0.90950 Test: 1.17181
Epoch: 3800 Train: 0.56380 Test: 0.74262
Epoch: 3900 Train: 1.78626 Test: 2.39648
Epoch: 4000 Train: 0.37801 Test: 0.48019
Epoch: 4100 Train: 0.19901 Test: 0.25775
Epoch: 4200 Train: 0.36820 Test: 0.52304
Epoch: 4300 Train: 0.60334 Test: 1.00267
Epoch: 4400 Train: 0.31439 Test: 0.35678
Epoch: 4500 Train: 1.23023 Test: 0.99954
Epoch: 4600 Train: 0.32333 Test: 0.41322
Epoch: 4700 Train: 0.90897 Test: 0.33552
Epoch: 4800 Train: 0.35256 Test: 0.47172
Epoch: 4900 Train: 0.27735 Test: 0.40101
Epoch: 5000 Train: 0.16932 Test: 0.23423
Epoch 5100: New minimal relative error: 3.53%, model saved.
Epoch: 5100 Train: 0.12170 Test: 0.19001
Epoch: 5200 Train: 0.11893 Test: 0.18772
Epoch 5300: New minimal relative error: 2.43%, model saved.
Epoch: 5300 Train: 0.16337 Test: 0.20172
Epoch: 5400 Train: 0.17093 Test: 0.29737
Epoch: 5500 Train: 0.61284 Test: 0.50856
Epoch: 5600 Train: 0.46057 Test: 0.44995
Epoch: 5700 Train: 0.14855 Test: 0.20035
Epoch: 5800 Train: 0.19076 Test: 0.27694
Epoch: 5900 Train: 0.71200 Test: 1.00093
Epoch: 6000 Train: 2.31563 Test: 3.41536
Epoch: 6100 Train: 0.13038 Test: 0.22497
Epoch: 6200 Train: 0.25650 Test: 0.30213
Epoch: 6300 Train: 0.16089 Test: 0.22680
Epoch: 6400 Train: 1.28093 Test: 1.67073
Epoch: 6500 Train: 1.61996 Test: 1.00500
Epoch: 6600 Train: 1.34232 Test: 1.58249
Epoch: 6700 Train: 0.40379 Test: 0.26670
Epoch: 6800 Train: 0.13734 Test: 0.23307
Epoch: 6900 Train: 0.09798 Test: 0.15916
Epoch: 7000 Train: 0.17413 Test: 0.23373
Epoch: 7100 Train: 0.07716 Test: 0.14287
Epoch: 7200 Train: 0.06485 Test: 0.13004
Epoch: 7300 Train: 0.12544 Test: 0.19892
Epoch: 7400 Train: 0.58821 Test: 0.58469
Epoch: 7500 Train: 0.84232 Test: 0.81785
Epoch: 7600 Train: 0.14534 Test: 0.23344
Epoch: 7700 Train: 0.08241 Test: 0.13424
Epoch: 7800 Train: 0.26422 Test: 0.28662
Epoch: 7900 Train: 0.32650 Test: 0.41705
Epoch: 8000 Train: 0.06405 Test: 0.13418
Epoch: 8100 Train: 0.06124 Test: 0.12160
Epoch: 8200 Train: 0.05848 Test: 0.12240
Epoch: 8300 Train: 0.14604 Test: 0.19918
Epoch: 8400 Train: 0.09445 Test: 0.14388
Epoch: 8500 Train: 0.69936 Test: 0.81237
Epoch: 8600 Train: 0.26293 Test: 0.38428
Epoch: 8700 Train: 0.06380 Test: 0.14368
Epoch: 8800 Train: 0.05537 Test: 0.11647
Epoch: 8900 Train: 0.18535 Test: 0.27289
Epoch: 9000 Train: 0.20784 Test: 0.31158
Epoch: 9100 Train: 0.06756 Test: 0.11755
Epoch: 9200 Train: 0.04707 Test: 0.10659
Epoch: 9300 Train: 0.21890 Test: 0.35291
Epoch: 9400 Train: 0.09180 Test: 0.13787
Epoch: 9500 Train: 0.05212 Test: 0.11591
Epoch: 9600 Train: 0.15502 Test: 0.17553
Epoch 9700: New minimal relative error: 2.03%, model saved.
Epoch: 9700 Train: 0.04913 Test: 0.10773
Epoch: 9800 Train: 0.07160 Test: 0.13410
Epoch: 9900 Train: 0.04750 Test: 0.10921
Epoch: 9999 Train: 0.04498 Test: 0.10057
Training Loss: tensor(0.0450)
Test Loss: tensor(0.1006)
Learned LE: [ 8.2359260e-01 -7.9312676e-04 -3.3131416e+00]
True LE: [ 8.6308604e-01  5.9107742e-03 -1.4539301e+01]
Relative Error: [0.9541217  0.9404813  0.9559512  0.9803609  0.91453016 0.76629084
 0.6289455  0.6463102  0.7280036  0.9612683  1.3110461  1.6316642
 2.0239148  2.671666   3.2272744  3.7174928  3.9584413  4.1260004
 3.7875943  3.2236655  3.1444647  3.201285   3.0999732  2.8333442
 2.505559   2.7277975  3.4008708  3.9880931  4.2100215  4.118605
 3.9334555  3.7945404  3.8172257  3.8532252  3.71233    3.4955547
 3.075717   2.7050865  2.3492162  2.0126185  1.6393101  1.2682086
 0.6788662  0.6122136  1.3675051  2.1049674  2.3917627  2.3855302
 2.2677343  2.316479   2.4268787  2.3449652  2.4093275  2.459847
 2.2011538  1.6667292  1.2292376  1.2162572  0.9973695  0.8538084
 0.7084227  0.6644518  0.7425745  0.80949134 0.83816916 0.8047581
 0.747111   0.59647906 0.39027658 0.39511183 0.5032191  0.77739286
 1.0044994  1.2793005  1.7788719  2.476759   2.9761267  3.4041371
 3.3993044  3.413544   3.363749   2.8971736  2.8231041  3.126014
 3.0122242  2.6724713  2.4326122  2.6069975  3.625118   4.2786455
 4.419242   4.349801   4.344316   4.1163726  4.1034503  4.1358056
 4.021401   3.8912103  3.5300832  3.2577395  2.8219912  2.4023683
 2.005676   1.4921477  1.0934227  0.5726198  0.9967405  1.58938
 2.0119357  2.0807834  1.9453881  1.8268116  1.9654866  1.8044941
 1.7830505  1.9861298  2.1921275  1.8113806  1.274622   1.0948057
 0.73987114 0.49372655 0.3749764  0.41770577 0.5485472  0.7026616
 0.7193049  0.7609328  0.7005704  0.5931789  0.38149607 0.19300325
 0.33065873 0.52768487 0.76566166 1.0331807  1.437132   2.1600356
 2.7406151  2.988604   2.8939984  2.8515573  2.8312523  2.626809
 2.595023   2.9407837  2.6694508  2.2172005  2.0410495  1.9665613
 2.4584236  3.4241438  4.109259   4.575652   4.738175   4.220951
 3.9919827  4.74702    4.271342   3.5117493  3.5288322  3.290673
 2.9980159  2.6941936  2.3498046  2.0396476  1.6362509  0.7982966
 0.7269278  1.0963501  1.5881797  1.7126391  1.6394132  1.4355999
 1.2812515  1.1066282  1.0828109  1.3104303  1.7333429  1.8242002
 1.4645283  1.0570979  0.59009147 0.14224423 0.22939079 0.39849168
 0.4133867  0.5702137  0.6940155  0.84639144 0.8126017  0.6577991
 0.4952972  0.26192376 0.29469344 0.3356545  0.50906956 0.7877431
 1.0228261  1.5500587  2.2955754  2.6379666  2.4633079  2.363193
 2.3307848  2.377448   2.3976908  2.4393544  2.349179   1.8217629
 1.6381966  1.1656395  1.3965905  2.3290362  3.06226    3.6431365
 4.15915    5.899509   5.51083    5.18896    5.2060165  5.242168
 4.567666   3.127699   2.968969   2.6389053  2.2563484  2.2652655
 2.107673   1.4265649  0.7202903  0.8119885  1.2259597  1.3539087
 1.2837274  1.1329185  0.81693995 0.5897977  0.629627   0.71529627
 1.1302623  1.3838891  1.6352654  1.3642713  0.75875485 0.1866759
 0.32922107 0.6259517  0.58521974 0.5375052  0.7111671  0.83287644
 0.9409605  0.81569004 0.63432217 0.47083917 0.4377125  0.37446848
 0.27347964 0.46445224 0.6837027  0.931664   1.609086   2.1611354
 2.0896316  1.8581771  1.8802477  1.9460285  2.2013607  2.114092
 1.967077   1.5834467  1.2479258  0.8305882  0.8770501  1.4975426
 2.450455   3.1764503  4.9024777  5.853115   6.4218     6.5254593
 5.999803   5.898296   5.8279114  5.0366855  3.383178   2.6357484
 2.1620646  1.8668052  1.9651836  2.0028467  1.3093611  0.65258443
 0.7672212  0.9724397  0.8919649  0.81231695 0.71727026 0.35236585
 0.45605186 0.46897247 0.5203285  0.8717495  1.1446542  1.2026211
 1.0404199  0.50073445 0.2976309  0.5073197  0.7354997  0.7298714
 0.72619075 0.71765214 0.79270416 0.8488314  0.7643428  0.5690829
 0.49991956 0.4657358  0.31409165 0.13185728 0.3875311  0.6118179
 1.0286185  1.6970886  1.7509851  1.4402779  1.353187   1.4948171
 1.7889211  1.9144582  1.7862096  1.2524099  1.0495789  0.83048975
 0.6196319  1.0058085  1.6204292  3.197264   5.504304   6.342967
 6.3732357  6.9368043  7.420315   6.628535   6.3812895  5.849916
 5.1712117  3.7619934  2.4265833  1.6791277  1.4487917  1.4837433
 1.9150821  1.285247   0.6232603  0.604424   0.6880218  0.49891192
 0.52817965 0.56484544 0.39181703 0.48683053 0.21977156 0.3999692
 0.6450247  0.7480256  0.9619589  0.63117325 0.35068196 0.26805726
 0.40654886 0.6785895  0.8218092  0.7663785  0.59743816 0.6756313
 0.6620083  0.5939653  0.5721985  0.4131943  0.43720463 0.24433033
 0.12148027 0.29253644 0.5140983  1.0665883  1.4519747  1.3183901
 0.9346853  0.9784379  1.2382487  1.5564808  1.720684   1.3775679
 0.81063944 0.7982719  0.6887428  0.73597324]
