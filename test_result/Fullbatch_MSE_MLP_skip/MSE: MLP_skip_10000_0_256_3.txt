time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.81%, model saved.
Epoch: 0 Train: 3916.82300 Test: 4163.06445
Epoch: 100 Train: 198.27921 Test: 191.76250
Epoch 200: New minimal relative error: 29.18%, model saved.
Epoch: 200 Train: 27.70693 Test: 24.10766
Epoch 300: New minimal relative error: 22.19%, model saved.
Epoch: 300 Train: 11.64117 Test: 10.78561
Epoch: 400 Train: 7.88011 Test: 7.26705
Epoch 500: New minimal relative error: 20.90%, model saved.
Epoch: 500 Train: 6.91511 Test: 6.78220
Epoch 600: New minimal relative error: 10.34%, model saved.
Epoch: 600 Train: 5.07567 Test: 4.65553
Epoch 700: New minimal relative error: 9.92%, model saved.
Epoch: 700 Train: 4.35874 Test: 3.99125
Epoch: 800 Train: 5.64444 Test: 6.31375
Epoch: 900 Train: 3.26846 Test: 3.91409
Epoch 1000: New minimal relative error: 9.67%, model saved.
Epoch: 1000 Train: 2.77178 Test: 2.56349
Epoch: 1100 Train: 2.43289 Test: 2.24738
Epoch: 1200 Train: 2.37265 Test: 2.31201
Epoch: 1300 Train: 1.89155 Test: 1.73453
Epoch: 1400 Train: 1.71188 Test: 1.56665
Epoch: 1500 Train: 1.57034 Test: 1.42987
Epoch: 1600 Train: 1.94857 Test: 1.59705
Epoch: 1700 Train: 1.35128 Test: 1.48743
Epoch: 1800 Train: 2.10490 Test: 1.82041
Epoch: 1900 Train: 1.30362 Test: 1.26523
Epoch: 2000 Train: 2.00457 Test: 1.59621
Epoch: 2100 Train: 1.04081 Test: 0.96363
Epoch: 2200 Train: 1.07588 Test: 1.00418
Epoch: 2300 Train: 1.32227 Test: 1.14781
Epoch: 2400 Train: 0.94962 Test: 0.83752
Epoch 2500: New minimal relative error: 6.76%, model saved.
Epoch: 2500 Train: 0.83747 Test: 0.77352
Epoch: 2600 Train: 1.43127 Test: 1.44980
Epoch: 2700 Train: 0.96414 Test: 0.92480
Epoch: 2800 Train: 2.45906 Test: 2.21809
Epoch: 2900 Train: 2.51718 Test: 2.79945
Epoch: 3000 Train: 0.63093 Test: 0.59566
Epoch: 3100 Train: 0.60017 Test: 0.56440
Epoch: 3200 Train: 0.59044 Test: 0.54819
Epoch: 3300 Train: 2.75065 Test: 1.52767
Epoch: 3400 Train: 0.53338 Test: 0.51188
Epoch: 3500 Train: 1.97351 Test: 1.20518
Epoch: 3600 Train: 0.49506 Test: 0.48189
Epoch: 3700 Train: 0.64911 Test: 0.58395
Epoch 3800: New minimal relative error: 6.55%, model saved.
Epoch: 3800 Train: 0.47101 Test: 0.47591
Epoch: 3900 Train: 0.66892 Test: 0.61410
Epoch: 4000 Train: 1.09367 Test: 1.32631
Epoch: 4100 Train: 0.44974 Test: 0.45898
Epoch: 4200 Train: 0.40449 Test: 0.41172
Epoch: 4300 Train: 0.39655 Test: 0.40901
Epoch: 4400 Train: 0.41650 Test: 0.43467
Epoch: 4500 Train: 0.41867 Test: 0.43358
Epoch: 4600 Train: 0.35914 Test: 0.37497
Epoch: 4700 Train: 0.35819 Test: 0.37500
Epoch: 4800 Train: 0.36236 Test: 0.37234
Epoch: 4900 Train: 0.68643 Test: 0.52282
Epoch: 5000 Train: 0.32110 Test: 0.34638
Epoch: 5100 Train: 0.31805 Test: 0.34424
Epoch: 5200 Train: 0.30823 Test: 0.35238
Epoch: 5300 Train: 0.29730 Test: 0.32859
Epoch: 5400 Train: 0.29100 Test: 0.32562
Epoch: 5500 Train: 0.31864 Test: 0.35289
Epoch 5600: New minimal relative error: 6.24%, model saved.
Epoch: 5600 Train: 0.27649 Test: 0.31128
Epoch: 5700 Train: 0.27036 Test: 0.30912
Epoch: 5800 Train: 0.26766 Test: 0.30347
Epoch: 5900 Train: 0.25836 Test: 0.29730
Epoch: 6000 Train: 0.31415 Test: 0.34120
Epoch: 6100 Train: 0.25071 Test: 0.28948
Epoch: 6200 Train: 0.24231 Test: 0.28718
Epoch: 6300 Train: 0.90282 Test: 1.03710
Epoch: 6400 Train: 0.23235 Test: 0.27725
Epoch: 6500 Train: 0.22879 Test: 0.27530
Epoch: 6600 Train: 0.22377 Test: 0.27016
Epoch 6700: New minimal relative error: 3.60%, model saved.
Epoch: 6700 Train: 0.21922 Test: 0.26688
Epoch: 6800 Train: 0.21604 Test: 0.26791
Epoch: 6900 Train: 0.21134 Test: 0.26039
Epoch: 7000 Train: 0.21520 Test: 0.25973
Epoch: 7100 Train: 0.20420 Test: 0.25392
Epoch: 7200 Train: 0.22085 Test: 0.31810
Epoch: 7300 Train: 0.19717 Test: 0.24851
Epoch: 7400 Train: 0.23995 Test: 0.28945
Epoch: 7500 Train: 0.39786 Test: 0.45561
Epoch: 7600 Train: 0.18754 Test: 0.23984
Epoch: 7700 Train: 0.18515 Test: 0.23707
Epoch: 7800 Train: 0.18276 Test: 0.23509
Epoch: 7900 Train: 0.17901 Test: 0.23294
Epoch: 8000 Train: 0.17945 Test: 0.23582
Epoch: 8100 Train: 0.17633 Test: 0.23359
Epoch: 8200 Train: 0.17101 Test: 0.22424
Epoch: 8300 Train: 0.21694 Test: 0.27195
Epoch: 8400 Train: 0.16650 Test: 0.21906
Epoch: 8500 Train: 0.16385 Test: 0.21702
Epoch: 8600 Train: 0.18075 Test: 0.23889
Epoch: 8700 Train: 0.16547 Test: 0.22220
Epoch: 8800 Train: 0.16306 Test: 0.21280
Epoch: 8900 Train: 0.15525 Test: 0.20893
Epoch: 9000 Train: 0.15355 Test: 0.20985
Epoch: 9100 Train: 0.15620 Test: 0.20958
Epoch: 9200 Train: 0.15266 Test: 0.20813
Epoch: 9300 Train: 0.27143 Test: 0.31049
Epoch: 9400 Train: 0.14997 Test: 0.20793
Epoch: 9500 Train: 0.14923 Test: 0.20108
Epoch: 9600 Train: 0.14274 Test: 0.19654
Epoch: 9700 Train: 0.14106 Test: 0.19440
Epoch: 9800 Train: 0.17955 Test: 0.25069
Epoch: 9900 Train: 0.13792 Test: 0.19155
Epoch: 9999 Train: 0.13641 Test: 0.19018
Training Loss: tensor(0.1364)
Test Loss: tensor(0.1902)
Learned LE: [ 0.8042656   0.02703442 -3.8805313 ]
True LE: [  0.84903103   0.01549262 -14.539311  ]
Relative Error: [ 8.481842   8.410135   8.456426   8.642097   8.919795   9.266075
  9.684303  10.18009   10.756573  11.410511  12.12795   12.859659
 13.54817   14.004311  13.928053  13.756559  13.452798  13.047636
 12.342697  11.417108  10.4567585 10.363066  11.07515   11.643963
 12.060909  12.545652  13.39373   14.138942  14.739811  15.234298
 15.658717  16.05405   16.463032  16.91407   17.467607  17.701015
 17.784012  17.991213  17.45281   16.873152  16.323357  15.854146
 15.475197  15.040079  14.506403  13.885628  13.124159  12.304161
 11.556753  10.592504   9.632243   8.997337   8.50768    8.399343
  8.363804   8.360672   8.324564   8.393774   8.60344    8.760257
  8.544706   8.242146   8.102915   8.086908   8.160617   8.334877
  8.614882   8.956245   9.367559   9.859165  10.438615  11.105791
 11.848463  12.601711  13.35112   13.785818  13.780315  13.683145
 13.20413   12.108255  11.342715  10.5148945  9.630598   9.4812
 10.2432    10.8427    11.340094  12.016329  12.773792  13.467532
 14.009709  14.434436  14.780617  15.091568  15.414828  15.800013
 16.279741  16.685177  16.70598   16.868135  16.471394  15.892467
 15.36537   14.946896  14.41915   14.025497  13.545699  12.988502
 12.28818   11.61233   10.889485  10.005755   9.099822   8.519212
  8.079      8.034992   7.9982347  8.0190935  8.028862   8.150755
  8.467677   8.38946    8.105773   7.8497677  7.7542076  7.790338
  7.8900204  8.050811   8.315949   8.641234   9.034959   9.512964
 10.088012  10.763711  11.529839  12.312607  13.124861  13.567524
 13.641798  13.633664  12.770457  11.106647  10.434282   9.713498
  8.96067    8.664403   9.480059  10.113298  10.869083  11.509001
 12.188302  12.835014  13.317379  13.671407  13.934752  14.154948
 14.3830185 14.673456  15.082704  15.640749  15.6167    15.729743
 15.488461  14.913837  14.449088  14.05001   13.471084  13.046505
 12.63698   12.152115  11.573818  11.122754  10.385097   9.563585
  8.7058325  8.158947   7.7822595  7.6958804  7.6445975  7.654395
  7.7873416  8.069867   8.345902   8.067057   7.7281103  7.4898577
  7.399592   7.4395475  7.5992427  7.795564   8.020396   8.317902
  8.682148   9.135564   9.696613  10.374003  11.160312  11.977548
 12.831696  13.336879  13.4991455 13.59268   12.218566  10.510532
  9.610713   9.001149   8.383683   8.115478   8.776544   9.532358
 10.4191    11.044504  11.636102  12.23877   12.661264  12.947456
 13.122982  13.252753  13.377828  13.562578  13.86919   14.359684
 14.517119  14.574972  14.5062475 13.962036  13.535613  13.161415
 12.617218  12.104441  11.774958  11.441437  11.052369  10.622108
  9.9707155  9.1944895  8.398272   7.8470364  7.5362816  7.415146
  7.292717   7.3948054  7.710616   8.058141   8.029195   7.7702985
  7.3937254  7.15788    7.066842   7.092911   7.225251   7.463211
  7.7326035  7.987369   8.308796   8.723502   9.257866   9.926922
 10.727162  11.580704  12.485549  13.079187  13.336931  13.228874
 11.700047   9.99958    8.874183   8.369498   7.875911   7.662401
  8.119177   9.099679  10.003155  10.625752  11.145331  11.767062
 12.167579  12.388148  12.4720545 12.469222  12.434674  12.472068
 12.661748  13.0441265 13.413982  13.403652  13.526665  13.017999
 12.622288  12.276976  11.787954  11.194209  11.018155  10.844872
 10.608045  10.202697   9.629456   8.90732    8.1455555  7.5896425
  7.31778    7.1467586  7.096463   7.2437816  7.6219068  8.053841
  7.7394643  7.460653   7.0883284  6.856896   6.755118   6.763524
  6.864006   7.0554047  7.348633   7.6584435  7.919505   8.278343
  8.768428   9.41384   10.217296  11.104408  12.066734  12.791483
 13.136305  12.786415  11.264359   9.654596   8.40598    7.8067913
  7.4264536  7.263629   7.4687824  8.683412   9.620688  10.30792
 10.787826  11.376131  11.725326  11.874814  11.8715    11.766534
 11.617234  11.48752   11.47172   11.724787  12.252015  12.221132
 12.396478  12.058529  11.706754  11.391415  10.979075  10.4567375
 10.350478  10.289242  10.130842   9.839523   9.351728   8.312482
  7.4112034  6.8563366  6.7115917  6.791846   7.0038223  7.126379
  7.5046635  7.839437   7.4719415  7.1665177  6.804707   6.585325
  6.468983   6.456265   6.5240026  6.665508   6.890922   7.2401605
  7.528514   7.809391   8.231219   8.83012    9.618658  10.529908
 11.552206  12.453489  12.872521  12.29656   10.897758   9.40681
  8.162188   7.297413   7.0192227  6.927934   7.066276   8.272329
  9.319675  10.078171  10.505306  11.082299 ]
