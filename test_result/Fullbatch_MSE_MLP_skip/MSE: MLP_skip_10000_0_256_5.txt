time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.55%, model saved.
Epoch: 0 Train: 3524.19800 Test: 4205.09766
Epoch: 100 Train: 184.08769 Test: 169.08922
Epoch 200: New minimal relative error: 14.36%, model saved.
Epoch: 200 Train: 19.83290 Test: 19.53571
Epoch: 300 Train: 9.42215 Test: 8.35840
Epoch: 400 Train: 22.14231 Test: 22.28621
Epoch 500: New minimal relative error: 11.39%, model saved.
Epoch: 500 Train: 6.98817 Test: 6.80429
Epoch: 600 Train: 6.88368 Test: 9.04997
Epoch 700: New minimal relative error: 8.04%, model saved.
Epoch: 700 Train: 4.36007 Test: 3.94575
Epoch: 800 Train: 10.74905 Test: 10.79698
Epoch: 900 Train: 8.09739 Test: 9.29446
Epoch: 1000 Train: 4.37727 Test: 3.41622
Epoch: 1100 Train: 5.69777 Test: 6.11255
Epoch: 1200 Train: 12.03369 Test: 12.80867
Epoch: 1300 Train: 2.35763 Test: 3.42722
Epoch: 1400 Train: 2.49163 Test: 2.66954
Epoch 1500: New minimal relative error: 5.96%, model saved.
Epoch: 1500 Train: 1.44296 Test: 1.80400
Epoch: 1600 Train: 9.31057 Test: 8.29092
Epoch: 1700 Train: 7.62720 Test: 8.68711
Epoch: 1800 Train: 0.89699 Test: 1.05498
Epoch: 1900 Train: 3.31947 Test: 3.28213
Epoch: 2000 Train: 2.58548 Test: 2.29568
Epoch: 2100 Train: 2.12166 Test: 1.94483
Epoch: 2200 Train: 2.90443 Test: 2.96717
Epoch: 2300 Train: 0.84131 Test: 1.34856
Epoch: 2400 Train: 0.93085 Test: 1.12267
Epoch: 2500 Train: 0.76651 Test: 0.92068
Epoch: 2600 Train: 4.09897 Test: 4.06391
Epoch: 2700 Train: 0.58654 Test: 0.60246
Epoch: 2800 Train: 0.92297 Test: 0.92879
Epoch: 2900 Train: 1.90904 Test: 1.81752
Epoch: 3000 Train: 0.77757 Test: 0.89995
Epoch: 3100 Train: 0.45412 Test: 0.50357
Epoch: 3200 Train: 0.94914 Test: 1.08531
Epoch 3300: New minimal relative error: 5.32%, model saved.
Epoch: 3300 Train: 0.41583 Test: 0.49524
Epoch 3400: New minimal relative error: 4.47%, model saved.
Epoch: 3400 Train: 0.32053 Test: 0.35989
Epoch: 3500 Train: 0.88647 Test: 0.81273
Epoch: 3600 Train: 0.47789 Test: 0.48085
Epoch: 3700 Train: 1.18277 Test: 1.06686
Epoch: 3800 Train: 0.90632 Test: 1.01030
Epoch: 3900 Train: 0.42046 Test: 0.38254
Epoch: 4000 Train: 0.57474 Test: 0.76282
Epoch: 4100 Train: 0.34716 Test: 0.35702
Epoch: 4200 Train: 2.24515 Test: 1.81197
Epoch: 4300 Train: 0.46986 Test: 0.57938
Epoch: 4400 Train: 0.29199 Test: 0.30696
Epoch 4500: New minimal relative error: 4.33%, model saved.
Epoch: 4500 Train: 0.23439 Test: 0.27244
Epoch: 4600 Train: 1.91512 Test: 1.69819
Epoch: 4700 Train: 0.27328 Test: 0.27867
Epoch: 4800 Train: 0.49541 Test: 0.56664
Epoch: 4900 Train: 0.72117 Test: 0.79063
Epoch: 5000 Train: 1.40835 Test: 1.63079
Epoch: 5100 Train: 0.31576 Test: 0.34895
Epoch: 5200 Train: 0.24857 Test: 0.26251
Epoch: 5300 Train: 0.27799 Test: 0.31626
Epoch: 5400 Train: 0.29804 Test: 0.31555
Epoch: 5500 Train: 0.31317 Test: 0.34334
Epoch: 5600 Train: 0.23185 Test: 0.24012
Epoch: 5700 Train: 0.55853 Test: 0.32899
Epoch: 5800 Train: 0.72519 Test: 0.92247
Epoch 5900: New minimal relative error: 4.22%, model saved.
Epoch: 5900 Train: 0.23764 Test: 0.23102
Epoch: 6000 Train: 0.19674 Test: 0.24905
Epoch: 6100 Train: 0.45662 Test: 0.46149
Epoch: 6200 Train: 0.21795 Test: 0.25816
Epoch: 6300 Train: 0.14039 Test: 0.15852
Epoch: 6400 Train: 0.30948 Test: 0.33338
Epoch: 6500 Train: 0.14737 Test: 0.16247
Epoch: 6600 Train: 0.16489 Test: 0.21055
Epoch: 6700 Train: 0.39573 Test: 0.41666
Epoch: 6800 Train: 0.26295 Test: 0.21530
Epoch: 6900 Train: 0.16212 Test: 0.17177
Epoch 7000: New minimal relative error: 2.52%, model saved.
Epoch: 7000 Train: 0.21516 Test: 0.27891
Epoch: 7100 Train: 0.22940 Test: 0.23436
Epoch: 7200 Train: 0.26980 Test: 0.29323
Epoch: 7300 Train: 0.21005 Test: 0.22000
Epoch: 7400 Train: 0.33875 Test: 0.30990
Epoch: 7500 Train: 0.15597 Test: 0.17468
Epoch: 7600 Train: 0.18401 Test: 0.21560
Epoch: 7700 Train: 0.37941 Test: 0.34298
Epoch: 7800 Train: 0.17151 Test: 0.20116
Epoch: 7900 Train: 0.14343 Test: 0.17759
Epoch: 8000 Train: 0.14873 Test: 0.18928
Epoch: 8100 Train: 0.14778 Test: 0.15833
Epoch: 8200 Train: 0.18622 Test: 0.22889
Epoch: 8300 Train: 0.24826 Test: 0.32461
Epoch: 8400 Train: 0.15614 Test: 0.17482
Epoch: 8500 Train: 0.11234 Test: 0.15285
Epoch: 8600 Train: 0.18435 Test: 0.19994
Epoch: 8700 Train: 0.11507 Test: 0.15366
Epoch: 8800 Train: 0.20582 Test: 0.22988
Epoch: 8900 Train: 0.09782 Test: 0.11467
Epoch: 9000 Train: 0.11247 Test: 0.12731
Epoch: 9100 Train: 0.11000 Test: 0.11783
Epoch: 9200 Train: 0.12025 Test: 0.14833
Epoch: 9300 Train: 0.12106 Test: 0.13039
Epoch: 9400 Train: 0.14541 Test: 0.17497
Epoch: 9500 Train: 0.14290 Test: 0.17580
Epoch: 9600 Train: 0.12260 Test: 0.14005
Epoch: 9700 Train: 0.11885 Test: 0.15131
Epoch: 9800 Train: 0.13831 Test: 0.15841
Epoch: 9900 Train: 0.11468 Test: 0.14637
Epoch: 9999 Train: 0.12990 Test: 0.12885
Training Loss: tensor(0.1299)
Test Loss: tensor(0.1288)
Learned LE: [ 8.4696937e-01 -8.2835810e-05 -3.3021894e+00]
True LE: [ 8.6763519e-01 -1.9339676e-03 -1.4540325e+01]
Relative Error: [3.5917025  3.4577549  3.290414   3.088787   2.9606655  3.0406115
 3.130385   3.2256906  3.391923   3.734744   4.31817    4.8477383
 5.128621   5.023936   4.688958   4.4246035  3.7880938  3.3748727
 2.9401     2.5967617  2.926433   3.5180879  4.3070326  5.2393637
 6.403364   7.0027695  7.4932413  7.77357    7.7265525  7.951536
 7.49741    6.5554986  5.7121162  4.707245   3.835384   3.4567382
 3.2754753  2.867485   2.6787486  2.7754703  2.589972   2.0928054
 1.6336157  0.9840909  0.69930786 0.9220716  0.8127569  0.3687298
 1.1262636  1.704037   2.6687613  3.558059   3.6976836  3.3724976
 2.9456024  2.5034075  2.4612508  2.5840094  2.7406263  2.8672433
 2.9336514  2.9860868  3.07364    3.1692665  3.00532    2.7930033
 2.5003123  2.352572   2.4373863  2.5755937  2.68504    2.939453
 3.3680785  4.259991   4.5778036  4.689475   4.4384675  4.31469
 3.693813   3.0201335  2.701095   2.3224626  2.5407073  3.2175198
 4.1118426  4.1546397  4.5653     5.3441925  5.943774   6.3101454
 6.5153728  7.1089864  7.6326184  7.216628   6.426257   5.3366413
 4.228359   3.1633816  2.831413   2.817408   2.4873583  2.2692811
 2.452114   2.2630215  1.5979925  1.0527625  0.6864406  0.81459785
 1.0126193  0.5475682  0.3286848  1.1385784  1.8028585  2.6694765
 2.9436142  2.792818   2.409794   1.8017325  1.6560259  1.900363
 2.0186663  2.1844847  2.3174512  2.4239504  2.495956   2.6097763
 2.679433   2.6387725  2.3882797  2.050605   1.84283    1.91188
 2.042158   2.1964736  2.5910587  3.3264573  4.12853    4.2870936
 4.1754303  4.10264    3.9491568  3.0544806  2.3769152  1.9977126
 2.2585533  2.7305446  3.2608528  3.0164492  2.8226917  3.7002728
 4.631289   5.1645927  5.3479095  5.297144   5.7360263  6.2685266
 6.8780212  6.0546274  4.9582453  3.7165165  2.6928177  2.2238011
 2.3175755  2.1879427  1.907038   2.1747527  1.921062   1.2029735
 0.7309561  1.0100001  1.1588023  1.3551967  1.0692586  0.74624753
 1.1038233  1.9298887  2.2797308  2.37909    2.1675267  1.5505939
 1.1604729  1.4471804  1.5464     1.6657721  1.7811066  1.8981757
 1.9848516  2.045204   2.0865667  2.1056266  2.1711855  2.135721
 1.8341769  1.5438789  1.6868668  1.6764213  1.6957352  2.1370513
 2.8371568  3.5652351  3.6763537  3.5821762  3.5941     3.5335996
 2.6409922  1.7822692  1.5498137  2.3975062  2.4753966  2.3483903
 2.089399   2.2445135  3.2626145  4.189688   4.6043983  4.331391
 4.401816   4.8972783  5.386946   5.8267174  6.1488504  5.04328
 3.6652248  2.3011527  1.6364783  1.7798235  1.9216694  1.6694326
 1.7757652  1.572506   0.9087312  0.773823   1.3175136  1.5852411
 1.8365028  1.8104831  1.3977312  1.494883   1.9525428  2.2239344
 2.3049364  2.0787559  1.51186    1.3573998  1.4892439  1.3398241
 1.3536434  1.4426252  1.539572   1.5778346  1.5787559  1.511648
 1.4975114  1.5362865  1.6060647  1.5075029  1.2554827  1.4720911
 1.4976026  1.4770128  1.7763863  2.3554668  2.8392327  3.2441254
 3.0265     2.8422904  2.9827504  2.4198413  1.3517499  1.276427
 1.654422   2.14026    1.8439369  1.7358079  2.2272432  2.9896
 4.0022364  4.043229   4.114529   4.070504   4.07827    4.1659746
 4.491172   4.808638   5.097943   4.042958   2.4722826  1.385616
 1.2505469  1.5341688  1.4846431  1.4101223  1.3621948  0.81979024
 0.8048112  1.5523483  1.9442842  2.311333   2.530511   2.0543227
 2.1463969  2.3237352  2.6047707  2.8270888  2.4398909  1.8658502
 1.4749627  1.2771646  1.0923669  1.1451193  1.2255698  1.2629342
 1.2324347  1.2019851  1.1397504  1.1488633  1.2187489  1.0864661
 0.81803066 0.66186416 0.4195028  0.90422    1.2270106  1.3616908
 1.7960364  2.3133903  2.6536608  2.7959566  2.4154143  2.2830753
 2.261324   1.2027845  1.0119311  1.0359924  1.7873377  2.1986716
 1.5720744  2.1793869  2.885692   3.2213748  3.1775692  3.334656
 3.2121902  3.0986564  2.9171054  2.8477836  2.9413447  3.2371454
 3.430777   2.9849784  2.2577903  1.3365971  0.91403025 1.2494715
 1.0868623  1.1706891  0.8977067  1.0072672  1.7760283  2.0836987
 2.5598674  2.7283833  2.1047654  1.829337   2.0059876  2.5977702
 2.8454137  3.0008333  2.3993893  1.7636815  1.2011046  0.7933784
 0.7915495  0.97181034 1.1121205  1.0849484  0.9405232  0.9085495
 1.0532832  0.9585866  0.7129466  0.5374329  0.82139355 1.0424422
 0.8102612  0.7464847  1.1722499  1.3521875  1.3822823  1.6185384
 1.9791404  2.3791509  1.855879   1.6955994  1.3676697  0.9882841
 0.9422955  1.1011224  2.347154   2.4121606 ]
