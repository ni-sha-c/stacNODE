time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.93%, model saved.
Epoch: 0 Train: 3906.70068 Test: 4164.12988
Epoch 100: New minimal relative error: 51.91%, model saved.
Epoch: 100 Train: 61.60259 Test: 104.44668
Epoch 200: New minimal relative error: 38.64%, model saved.
Epoch: 200 Train: 22.26073 Test: 21.63268
Epoch 300: New minimal relative error: 17.90%, model saved.
Epoch: 300 Train: 13.61261 Test: 9.14347
Epoch: 400 Train: 57.72340 Test: 46.81574
Epoch 500: New minimal relative error: 15.26%, model saved.
Epoch: 500 Train: 2.70548 Test: 5.00868
Epoch 600: New minimal relative error: 12.73%, model saved.
Epoch: 600 Train: 2.53968 Test: 3.12914
Epoch: 700 Train: 1.66323 Test: 2.16848
Epoch: 800 Train: 1.63952 Test: 2.13062
Epoch: 900 Train: 3.68040 Test: 4.17395
Epoch: 1000 Train: 6.45763 Test: 8.71688
Epoch: 1100 Train: 6.43865 Test: 6.06049
Epoch: 1200 Train: 6.67697 Test: 3.71773
Epoch: 1300 Train: 1.15517 Test: 2.59149
Epoch: 1400 Train: 3.40155 Test: 4.70815
Epoch: 1500 Train: 2.55088 Test: 2.36084
Epoch 1600: New minimal relative error: 10.40%, model saved.
Epoch: 1600 Train: 0.70980 Test: 0.63006
Epoch: 1700 Train: 3.64008 Test: 4.49187
Epoch 1800: New minimal relative error: 5.35%, model saved.
Epoch: 1800 Train: 0.62065 Test: 0.43251
Epoch: 1900 Train: 1.28840 Test: 1.32509
Epoch: 2000 Train: 1.19288 Test: 1.21468
Epoch: 2100 Train: 4.86325 Test: 5.33556
Epoch: 2200 Train: 2.01157 Test: 1.93689
Epoch: 2300 Train: 3.37981 Test: 3.56861
Epoch: 2400 Train: 0.24902 Test: 0.22628
Epoch: 2500 Train: 2.37803 Test: 1.81147
Epoch: 2600 Train: 0.79329 Test: 0.89726
Epoch: 2700 Train: 0.33234 Test: 0.42089
Epoch: 2800 Train: 0.35813 Test: 0.39539
Epoch: 2900 Train: 1.61298 Test: 1.89763
Epoch: 3000 Train: 2.86023 Test: 3.80992
Epoch: 3100 Train: 0.37805 Test: 0.49247
Epoch: 3200 Train: 3.10198 Test: 3.24578
Epoch: 3300 Train: 3.35183 Test: 3.61479
Epoch: 3400 Train: 2.19180 Test: 2.58790
Epoch: 3500 Train: 0.40399 Test: 0.42954
Epoch: 3600 Train: 0.68056 Test: 1.14530
Epoch: 3700 Train: 0.35875 Test: 0.47695
Epoch: 3800 Train: 0.64217 Test: 0.36623
Epoch 3900: New minimal relative error: 4.39%, model saved.
Epoch: 3900 Train: 0.08801 Test: 0.09826
Epoch: 4000 Train: 0.44525 Test: 0.16556
Epoch: 4100 Train: 0.80342 Test: 0.91072
Epoch: 4200 Train: 1.60538 Test: 1.42983
Epoch: 4300 Train: 0.50475 Test: 0.45605
Epoch: 4400 Train: 0.29160 Test: 0.43262
Epoch: 4500 Train: 0.81772 Test: 0.61531
Epoch: 4600 Train: 0.17544 Test: 0.17661
Epoch: 4700 Train: 0.33263 Test: 0.38666
Epoch: 4800 Train: 1.80968 Test: 2.32773
Epoch: 4900 Train: 0.05636 Test: 0.05679
Epoch: 5000 Train: 1.08478 Test: 1.26840
Epoch: 5100 Train: 0.41917 Test: 0.43950
Epoch: 5200 Train: 0.67133 Test: 0.45539
Epoch: 5300 Train: 0.15512 Test: 0.14828
Epoch: 5400 Train: 0.11969 Test: 0.12298
Epoch: 5500 Train: 0.44034 Test: 0.34230
Epoch: 5600 Train: 0.30754 Test: 0.23174
Epoch: 5700 Train: 0.65195 Test: 0.79372
Epoch: 5800 Train: 0.32238 Test: 0.43799
Epoch: 5900 Train: 0.14404 Test: 0.20328
Epoch: 6000 Train: 0.07836 Test: 0.09575
Epoch: 6100 Train: 0.63453 Test: 0.51711
Epoch: 6200 Train: 0.22966 Test: 0.28997
Epoch: 6300 Train: 0.42123 Test: 0.43405
Epoch: 6400 Train: 0.04254 Test: 0.04246
Epoch: 6500 Train: 0.05979 Test: 0.06262
Epoch: 6600 Train: 0.04108 Test: 0.04757
Epoch: 6700 Train: 0.16024 Test: 0.17657
Epoch: 6800 Train: 1.16605 Test: 1.17820
Epoch: 6900 Train: 1.12823 Test: 1.14371
Epoch: 7000 Train: 0.13755 Test: 0.14406
Epoch: 7100 Train: 0.29473 Test: 0.30248
Epoch: 7200 Train: 0.19526 Test: 0.25403
Epoch: 7300 Train: 0.10605 Test: 0.09817
Epoch: 7400 Train: 0.98834 Test: 0.89052
Epoch: 7500 Train: 0.08147 Test: 0.09741
Epoch: 7600 Train: 0.10211 Test: 0.10164
Epoch: 7700 Train: 0.15254 Test: 0.16130
Epoch: 7800 Train: 0.28177 Test: 0.21866
Epoch: 7900 Train: 0.06981 Test: 0.09333
Epoch: 8000 Train: 0.54884 Test: 0.35427
Epoch 8100: New minimal relative error: 3.77%, model saved.
Epoch: 8100 Train: 0.06069 Test: 0.06201
Epoch: 8200 Train: 0.26748 Test: 0.20961
Epoch: 8300 Train: 0.25730 Test: 0.33981
Epoch: 8400 Train: 0.03722 Test: 0.04375
Epoch: 8500 Train: 0.22712 Test: 0.22867
Epoch: 8600 Train: 0.21071 Test: 0.24735
Epoch: 8700 Train: 0.47479 Test: 0.61676
Epoch: 8800 Train: 0.03249 Test: 0.03598
Epoch: 8900 Train: 0.07059 Test: 0.09483
Epoch: 9000 Train: 0.02790 Test: 0.03161
Epoch: 9100 Train: 0.02553 Test: 0.03282
Epoch: 9200 Train: 0.08195 Test: 0.07241
Epoch: 9300 Train: 0.15843 Test: 0.17608
Epoch: 9400 Train: 0.16620 Test: 0.14280
Epoch: 9500 Train: 0.05390 Test: 0.04629
Epoch: 9600 Train: 0.09715 Test: 0.10746
Epoch: 9700 Train: 0.11054 Test: 0.14037
Epoch: 9800 Train: 0.13897 Test: 0.19132
Epoch: 9900 Train: 0.02337 Test: 0.03261
Epoch: 9999 Train: 0.02704 Test: 0.03694
Training Loss: tensor(0.0270)
Test Loss: tensor(0.0369)
Learned LE: [ 0.90101945 -0.03006636 -4.343065  ]
True LE: [ 8.6006659e-01 -1.9051536e-03 -1.4530845e+01]
Relative Error: [1.1581109  1.0059208  1.3467082  1.9366179  2.1940265  2.236181
 2.302853   1.9403255  1.6938543  1.4077061  1.2302933  1.4661703
 2.0054984  2.488437   2.9537516  3.1469648  3.2477896  3.0801105
 3.0521736  2.9943054  2.8569043  3.2262466  3.5823572  3.7287161
 3.8614805  3.8822272  3.8788598  4.004575   4.0360503  3.924755
 3.851692   3.7702277  3.8161848  4.0067663  4.4745383  4.803986
 4.9822264  5.0281982  4.949991   4.8682714  5.0282774  5.59225
 6.0925097  5.7663274  5.276475   4.7275066  4.341699   4.021417
 3.6322348  3.3207462  3.029802   2.730893   2.3993726  2.2436273
 2.1549134  2.0192652  1.9694477  1.9899521  2.0659904  1.7502838
 1.6400235  1.3361789  0.9574925  0.8380362  1.1103976  1.6553651
 1.9563874  1.769956   1.8119924  1.6547313  1.4622277  1.11826
 0.8955161  1.108231   1.5728089  2.1781588  2.64185    2.9386487
 3.125829   2.879968   2.9601967  2.7471209  2.6880102  3.087493
 3.42996    3.6227558  3.694374   3.61386    3.6588821  3.760104
 3.787158   3.714474   3.5860577  3.3757746  3.263867   3.49021
 3.7796464  4.1280265  4.362693   4.4426336  4.32865    4.1781363
 4.3638988  4.892448   5.3686867  5.0922666  4.6525846  4.129564
 3.763889   3.4889176  3.0979104  2.8501647  2.6114419  2.327453
 1.9920168  1.7946198  1.7691517  1.7423576  1.7619799  1.7523683
 1.7529839  1.572679   1.4855344  1.2693688  0.9530089  0.7928643
 0.97027665 1.4009942  1.6856039  1.4745374  1.2942358  1.4322561
 1.2643555  0.9487235  0.6254057  0.748587   1.1448264  1.8028034
 2.299855   2.8052745  2.8580792  2.8284082  2.7194457  2.5267186
 2.6727886  3.0182583  3.2544234  3.447666   3.4469054  3.3418663
 3.4056797  3.4254742  3.3630483  3.286849   3.2524266  3.0519853
 2.883215   2.972678   3.2060142  3.4765038  3.736866   3.8684747
 3.6912293  3.5089288  3.72071    4.229771   4.6914573  4.4655423
 4.0311294  3.5743396  3.2754693  3.0148883  2.6544168  2.455622
 2.2949371  2.0486777  1.8150604  1.4980459  1.3975922  1.3807384
 1.3762105  1.3675182  1.4301207  1.4652083  1.3380492  1.2124493
 0.94213504 0.91091675 0.9349164  1.1492321  1.381371   1.1844331
 0.95666397 1.0656009  1.1211329  0.75824356 0.41994277 0.5123084
 0.7870578  1.4528683  1.9612782  2.6536481  2.6917963  2.7094004
 2.6404667  2.4576018  2.5453434  2.8253999  3.003272   3.0827384
 3.1267364  3.1739864  3.0259955  2.9313347  2.8829682  2.7027895
 2.7324433  2.6916318  2.5992908  2.5932517  2.7057261  2.852719
 3.1274974  3.289637   3.0789852  2.950031   3.202129   3.566515
 4.015305   3.8816533  3.4440722  3.0713832  2.8117115  2.5631473
 2.3190982  2.2045634  2.1653934  1.9661112  1.7950541  1.4218249
 1.2104177  1.1300681  1.082146   0.9778337  0.9581711  1.2131017
 1.2982547  1.0974371  0.9406507  0.9012941  0.99869317 0.8902953
 1.0123717  0.8922156  0.58152187 0.50524443 0.8058692  0.61583424
 0.24689573 0.40267572 0.51504207 1.1347884  1.6918225  2.4536483
 2.561479   2.4601295  2.5132945  2.310974   2.319582   2.396152
 2.5881476  2.753904   2.7297618  2.7654595  2.6389785  2.6015384
 2.3923237  2.2203958  2.1227472  2.1212819  2.1937516  2.2892535
 2.2048318  2.2679734  2.5985365  2.7428336  2.591021   2.3784783
 2.6485958  2.924305   3.3613677  3.3556833  3.019007   2.6135654
 2.373356   2.1247349  2.0580115  1.972983   2.0198128  2.0296323
 1.7499033  1.4453379  1.1133654  0.96431106 0.94781816 0.80185103
 0.73934335 0.7333343  0.99492055 0.9947029  0.85238695 0.7871456
 0.79469246 0.7387451  0.73395383 0.7472062  0.32243553 0.0699988
 0.4566062  0.5967397  0.1646283  0.36776006 0.43565103 0.80867624
 1.314473   2.056717   2.5249162  2.3462822  2.2624514  2.1081674
 1.9882421  1.984745   1.9826303  2.0801392  2.2637844  2.4832313
 2.1641557  2.1336012  1.8896964  1.8255522  1.717148   1.6020639
 1.728579   1.9068228  1.9205136  1.717175   1.8561429  2.2604556
 2.2822998  1.974164   2.0892756  2.3846846  2.5938778  2.8147995
 2.6299179  2.165995   1.9977875  1.7538915  1.7155173  1.8541585
 1.9325101  1.8875831  1.7015097  1.4837687  1.151519   0.86584634
 0.87268746 0.80158466 0.6050055  0.5051483  0.48995906 0.69817656
 0.76285595 0.7098972  0.6284992  0.4713776  0.48976773 0.49031284
 0.37891725 0.2674856  0.3961386  0.31554118 0.16157441 0.28950387
 0.4079049  0.5622075  0.9154888  1.6280321  2.335382   2.1951663
 1.688537   1.6912026  1.5531785  1.5402392  1.4985952  1.649075
 1.6461635  1.9653764  1.9243783  1.7784687 ]
