time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.62%, model saved.
Epoch: 0 Train: 3462.88403 Test: 4270.12012
Epoch 80: New minimal relative error: 39.73%, model saved.
Epoch: 80 Train: 91.67377 Test: 93.93359
Epoch: 160 Train: 25.23913 Test: 28.76019
Epoch 240: New minimal relative error: 25.93%, model saved.
Epoch: 240 Train: 24.97519 Test: 25.39947
Epoch 320: New minimal relative error: 17.95%, model saved.
Epoch: 320 Train: 9.31769 Test: 16.35793
Epoch: 400 Train: 51.22396 Test: 37.05882
Epoch: 480 Train: 4.32597 Test: 3.61261
Epoch 560: New minimal relative error: 17.56%, model saved.
Epoch: 560 Train: 8.64615 Test: 9.01980
Epoch 640: New minimal relative error: 13.06%, model saved.
Epoch: 640 Train: 2.48260 Test: 2.06834
Epoch: 720 Train: 14.03903 Test: 16.67752
Epoch: 800 Train: 11.72778 Test: 9.48574
Epoch: 880 Train: 4.12327 Test: 5.31591
Epoch 960: New minimal relative error: 8.23%, model saved.
Epoch: 960 Train: 1.14058 Test: 0.98123
Epoch: 1040 Train: 6.95658 Test: 7.74436
Epoch: 1120 Train: 3.46479 Test: 3.17102
Epoch: 1200 Train: 2.46963 Test: 2.21459
Epoch: 1280 Train: 4.54711 Test: 2.16284
Epoch: 1360 Train: 1.06440 Test: 0.99501
Epoch: 1440 Train: 4.55267 Test: 6.28069
Epoch: 1520 Train: 7.81688 Test: 11.33339
Epoch: 1600 Train: 4.64970 Test: 6.31780
Epoch: 1680 Train: 1.97080 Test: 1.14700
Epoch: 1760 Train: 4.65151 Test: 7.10913
Epoch 1840: New minimal relative error: 7.63%, model saved.
Epoch: 1840 Train: 1.99791 Test: 2.99132
Epoch 1920: New minimal relative error: 3.75%, model saved.
Epoch: 1920 Train: 0.26621 Test: 0.25515
Epoch: 2000 Train: 0.55515 Test: 0.45529
Epoch: 2080 Train: 1.55251 Test: 1.70883
Epoch: 2160 Train: 0.84629 Test: 1.04318
Epoch: 2240 Train: 0.54031 Test: 0.39534
Epoch: 2320 Train: 0.57514 Test: 0.41594
Epoch: 2400 Train: 0.46725 Test: 0.45228
Epoch: 2480 Train: 0.29410 Test: 0.26322
Epoch: 2560 Train: 0.34971 Test: 0.20035
Epoch: 2640 Train: 0.43276 Test: 0.45193
Epoch: 2720 Train: 0.19735 Test: 0.20188
Epoch: 2800 Train: 0.63662 Test: 0.54907
Epoch: 2880 Train: 1.50503 Test: 2.17924
Epoch: 2960 Train: 2.77078 Test: 3.33079
Epoch: 3040 Train: 0.32659 Test: 0.48990
Epoch: 3120 Train: 0.76485 Test: 0.73709
Epoch: 3200 Train: 0.49597 Test: 0.44111
Epoch: 3280 Train: 0.59328 Test: 0.68545
Epoch: 3360 Train: 3.64813 Test: 3.74259
Epoch: 3440 Train: 1.48178 Test: 1.40413
Epoch: 3520 Train: 1.61218 Test: 2.01235
Epoch: 3600 Train: 0.15426 Test: 0.15709
Epoch: 3680 Train: 0.32408 Test: 0.34864
Epoch: 3760 Train: 0.28149 Test: 0.33953
Epoch: 3840 Train: 0.30640 Test: 0.27775
Epoch: 3920 Train: 0.34468 Test: 0.33942
Epoch: 4000 Train: 0.49415 Test: 0.77914
Epoch: 4080 Train: 1.91748 Test: 1.83977
Epoch: 4160 Train: 3.21592 Test: 1.89525
Epoch: 4240 Train: 0.87662 Test: 1.27781
Epoch 4320: New minimal relative error: 2.18%, model saved.
Epoch: 4320 Train: 0.13215 Test: 0.16108
Epoch: 4400 Train: 0.24443 Test: 0.16177
Epoch: 4480 Train: 0.94635 Test: 1.12373
Epoch: 4560 Train: 0.08692 Test: 0.10784
Epoch: 4640 Train: 0.14448 Test: 0.14217
Epoch: 4720 Train: 0.19497 Test: 0.19724
Epoch: 4800 Train: 0.49670 Test: 0.50673
Epoch: 4880 Train: 0.15235 Test: 0.17350
Epoch: 4960 Train: 0.16829 Test: 0.18869
Epoch: 5040 Train: 0.30721 Test: 0.33399
Epoch: 5120 Train: 0.20464 Test: 0.19211
Epoch: 5200 Train: 0.13129 Test: 0.12488
Epoch: 5280 Train: 0.17618 Test: 0.14692
Epoch: 5360 Train: 0.20566 Test: 0.24652
Epoch: 5440 Train: 0.69369 Test: 0.68686
Epoch: 5520 Train: 0.70201 Test: 0.76433
Epoch: 5600 Train: 0.24463 Test: 0.33491
Epoch: 5680 Train: 0.48814 Test: 0.38303
Epoch: 5760 Train: 0.55896 Test: 0.51197
Epoch: 5840 Train: 0.09080 Test: 0.12212
Epoch: 5920 Train: 0.14783 Test: 0.13650
Epoch: 6000 Train: 0.15808 Test: 0.16871
Epoch: 6080 Train: 0.08604 Test: 0.14737
Epoch: 6160 Train: 0.17946 Test: 0.17264
Epoch: 6240 Train: 0.39591 Test: 0.45635
Epoch: 6320 Train: 0.09866 Test: 0.12947
Epoch: 6400 Train: 0.12419 Test: 0.18118
Epoch: 6480 Train: 0.08250 Test: 0.10317
Epoch: 6560 Train: 0.11692 Test: 0.11794
Epoch: 6640 Train: 0.09813 Test: 0.14529
Epoch: 6720 Train: 0.19746 Test: 0.31347
Epoch: 6800 Train: 0.04947 Test: 0.07801
Epoch: 6880 Train: 0.07668 Test: 0.10608
Epoch: 6960 Train: 0.10745 Test: 0.13555
Epoch: 7040 Train: 0.21691 Test: 0.32499
Epoch: 7120 Train: 0.06762 Test: 0.09427
Epoch: 7200 Train: 0.08569 Test: 0.12046
Epoch: 7280 Train: 0.05411 Test: 0.07939
Epoch: 7360 Train: 0.04620 Test: 0.07476
Epoch: 7440 Train: 0.04255 Test: 0.07004
Epoch: 7520 Train: 0.11014 Test: 0.16961
Epoch 7600: New minimal relative error: 1.96%, model saved.
Epoch: 7600 Train: 0.07716 Test: 0.10061
Epoch: 7680 Train: 0.08621 Test: 0.11937
Epoch: 7760 Train: 0.07907 Test: 0.11470
Epoch: 7840 Train: 0.04633 Test: 0.07785
Epoch: 7920 Train: 0.07007 Test: 0.10346
Epoch: 7999 Train: 0.12696 Test: 0.15673
Training Loss: tensor(0.1270)
Test Loss: tensor(0.1567)
Learned LE: [ 0.8249069  0.0224521 -3.5487566]
True LE: [ 8.7814349e-01  2.1854439e-03 -1.4558379e+01]
Relative Error: [3.4469995  3.3654351  3.0838318  2.8590186  2.732558   2.8318803
 2.908707   3.0658777  3.1880155  3.3383408  3.466034   3.4889863
 3.6162133  4.156036   4.8521357  5.0404778  5.2592664  5.9355946
 6.2020555  6.497382   6.5663652  6.3047795  6.330551   6.694167
 6.971807   7.4062614  7.7789187  8.0062065  7.987609   8.242907
 8.438459   8.488573   8.525145   8.631112   8.359221   7.8704634
 7.7568216  7.690015   7.4277744  7.1171393  6.873491   7.044293
 6.8983817  6.916249   6.731511   6.431403   5.9841113  5.3017216
 4.516637   3.981388   3.619698   3.2666676  3.0347807  2.9050179
 2.8385236  2.7711618  2.710561   2.6832407  2.7100508  2.8944857
 2.9574535  3.0311303  2.9852872  2.8695502  2.7546334  2.718311
 2.5770223  2.5560255  2.5742245  2.6764636  2.7946792  2.9468944
 3.0383475  3.0797877  3.0913975  3.6347442  4.3542686  4.467204
 4.8118463  5.1936746  5.6280484  6.07385    6.138381   6.0076714
 6.1484194  6.559214   6.7641215  6.850943   7.126256   7.345116
 7.3850355  7.5218973  7.729011   7.9506054  7.908433   7.685048
 7.819251   7.286301   7.0056973  6.9193945  6.658885   6.36649
 6.0830803  5.974268   5.991807   5.974407   6.0266623  5.9008975
 5.554121   4.870521   4.050428   3.4920335  3.2077723  2.940195
 2.7592657  2.672126   2.590262   2.4812322  2.3599904  2.3143654
 2.2583294  2.434666   2.4987948  2.5247467  2.497735   2.4359293
 2.3189871  2.3016424  2.3516946  2.3297231  2.2662973  2.3039348
 2.4727058  2.517379   2.6324813  2.6516879  2.638919   2.962619
 3.6984353  3.9344878  4.3412743  4.6520395  5.02699    5.5884304
 5.473056   5.4328513  5.781322   6.241805   6.617881   6.7419076
 6.881433   6.8312078  6.754638   6.7026405  7.0788884  7.240409
 7.348062   7.1036205  6.8348064  6.7579412  6.325465   6.176929
 5.8127646  5.627552   5.504194   5.095617   4.989726   5.0762115
 5.228525   5.2671003  5.1134586  4.39858    3.6718926  2.9962244
 2.637638   2.5365944  2.3335903  2.2657166  2.2408795  2.2285407
 2.0726638  1.9737577  1.903354   1.9631999  2.0797455  2.042231
 1.9829203  1.9986087  1.9293579  1.8380454  2.0165956  2.0847597
 2.095073   1.989775   2.074682   2.273202   2.2851138  2.2755613
 2.2386122  2.3167753  2.8574815  3.3396409  3.7577293  4.1586537
 4.3778534  4.5754323  4.5548167  4.8426394  5.299799   5.7467475
 6.1644006  6.361952   6.471452   6.628443   6.496147   6.136768
 6.1216173  6.515303   6.614289   6.7202406  6.282723   5.8902545
 5.6145954  5.371832   5.0683517  4.8772016  4.8857994  4.6033287
 4.0335274  4.1588984  4.2780714  4.5421233  4.5205092  3.969156
 3.2865982  2.617511   2.2601304  2.0686183  1.9068252  1.7416004
 1.7166561  1.8296378  1.8344976  1.6987423  1.5586563  1.408602
 1.5497705  1.6486284  1.564971   1.5255815  1.543358   1.4676013
 1.5131419  1.7799329  1.8222747  1.7899609  1.7865976  1.8541214
 2.0136786  1.9349446  1.9133124  1.9512819  2.1957202  2.5653079
 3.1466684  3.6555858  3.5413141  3.699653   3.6837854  3.9953065
 4.5769067  4.994806   5.416977   5.7334476  5.840691   6.0337176
 6.078862   5.971434   5.6845045  5.6092935  5.732255   5.959549
 6.0554023  5.416362   4.868903   4.5475464  4.5068474  4.0982594
 4.284721   4.120074   3.6165714  3.1988277  3.3064592  3.534139
 3.7718296  3.6569402  3.0259502  2.3481417  2.0142305  1.8416892
 1.62501    1.3616235  1.222847   1.3015217  1.3766026  1.3101418
 1.2026504  0.9938215  0.94193786 1.1459023  1.2377915  1.1512592
 1.1185105  1.0685103  1.1446108  1.269087   1.480889   1.5487558
 1.5341797  1.5647771  1.5939702  1.6741176  1.613714   1.6101096
 1.7700493  1.8223978  2.484837   2.920771   2.886503   2.9046326
 2.9968596  2.8279815  3.554999   4.174803   4.67269    5.036905
 5.233086   5.1304016  5.126532   5.265311   5.24618    5.244072
 4.946686   4.947708   5.137456   5.306092   4.5450535  4.065494
 3.534646   3.4950109  3.3386462  3.5799396  3.2975461  2.6936572
 2.4286733  2.6347742  2.5927804  2.93251    2.7707691  2.338659
 1.8448625  1.7521443  1.76392    1.5250877  1.0384206  0.81365955
 0.8637067  0.8438158  0.749365   0.78418106 0.60474175 0.6670905
 0.77366287 0.8565003  0.85077655 0.78077304 0.7687796  0.87724674
 0.90339005 1.112904   1.2294004  1.3192886  1.2873954  1.20142
 1.228717   1.3227377  1.3660737  1.4989847  1.5474283  2.026348
 2.2978122  2.3111475  2.330316   2.2617888  2.3247776  3.080052
 3.8903277  4.3844123  4.571719   4.6115456 ]
