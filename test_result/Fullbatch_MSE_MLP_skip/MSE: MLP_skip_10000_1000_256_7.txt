time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.83%, model saved.
Epoch: 0 Train: 4047.67334 Test: 4002.54736
Epoch 100: New minimal relative error: 71.07%, model saved.
Epoch: 100 Train: 243.61374 Test: 233.62450
Epoch: 200 Train: 38.51031 Test: 42.69206
Epoch 300: New minimal relative error: 59.44%, model saved.
Epoch: 300 Train: 41.61579 Test: 27.53516
Epoch 400: New minimal relative error: 22.47%, model saved.
Epoch: 400 Train: 19.01349 Test: 17.29327
Epoch 500: New minimal relative error: 21.64%, model saved.
Epoch: 500 Train: 6.63654 Test: 7.60574
Epoch: 600 Train: 36.59221 Test: 12.26752
Epoch 700: New minimal relative error: 18.91%, model saved.
Epoch: 700 Train: 15.75405 Test: 12.03040
Epoch: 800 Train: 8.09773 Test: 9.53018
Epoch 900: New minimal relative error: 12.36%, model saved.
Epoch: 900 Train: 10.28334 Test: 9.34721
Epoch: 1000 Train: 7.63165 Test: 10.50228
Epoch: 1100 Train: 4.91030 Test: 6.38452
Epoch: 1200 Train: 19.33529 Test: 12.81463
Epoch: 1300 Train: 2.35753 Test: 2.49374
Epoch: 1400 Train: 4.68223 Test: 4.05160
Epoch: 1500 Train: 7.68304 Test: 7.21763
Epoch 1600: New minimal relative error: 11.92%, model saved.
Epoch: 1600 Train: 7.46230 Test: 3.76839
Epoch: 1700 Train: 4.75452 Test: 4.45043
Epoch: 1800 Train: 1.15854 Test: 1.43968
Epoch: 1900 Train: 8.46568 Test: 5.58574
Epoch: 2000 Train: 4.08225 Test: 5.32541
Epoch 2100: New minimal relative error: 8.62%, model saved.
Epoch: 2100 Train: 0.91210 Test: 1.07948
Epoch: 2200 Train: 1.67330 Test: 1.69603
Epoch: 2300 Train: 1.65348 Test: 1.88588
Epoch: 2400 Train: 4.97748 Test: 4.18424
Epoch: 2500 Train: 1.69244 Test: 1.66123
Epoch: 2600 Train: 0.87399 Test: 1.08353
Epoch: 2700 Train: 1.26937 Test: 1.61405
Epoch: 2800 Train: 1.34130 Test: 2.47456
Epoch: 2900 Train: 1.77947 Test: 2.40937
Epoch: 3000 Train: 0.74683 Test: 0.88436
Epoch: 3100 Train: 3.59788 Test: 3.14559
Epoch: 3200 Train: 0.55042 Test: 0.66151
Epoch: 3300 Train: 0.65529 Test: 0.92495
Epoch: 3400 Train: 0.52953 Test: 0.57231
Epoch: 3500 Train: 0.67132 Test: 1.30227
Epoch: 3600 Train: 0.82273 Test: 1.01882
Epoch: 3700 Train: 1.39728 Test: 1.28990
Epoch: 3800 Train: 0.49427 Test: 0.51740
Epoch: 3900 Train: 0.54497 Test: 0.76138
Epoch: 4000 Train: 0.90823 Test: 1.12650
Epoch: 4100 Train: 0.86974 Test: 0.89431
Epoch: 4200 Train: 2.40668 Test: 3.37933
Epoch: 4300 Train: 1.24356 Test: 1.18826
Epoch: 4400 Train: 3.15007 Test: 4.41711
Epoch: 4500 Train: 0.38413 Test: 0.42068
Epoch: 4600 Train: 2.95155 Test: 3.10530
Epoch: 4700 Train: 0.70340 Test: 0.82259
Epoch 4800: New minimal relative error: 7.84%, model saved.
Epoch: 4800 Train: 1.01921 Test: 1.36299
Epoch: 4900 Train: 2.99518 Test: 2.21470
Epoch: 5000 Train: 0.26034 Test: 0.33602
Epoch: 5100 Train: 0.30934 Test: 0.40315
Epoch: 5200 Train: 0.24370 Test: 0.43020
Epoch: 5300 Train: 0.91866 Test: 1.17533
Epoch: 5400 Train: 0.28174 Test: 0.34909
Epoch: 5500 Train: 0.23005 Test: 0.31231
Epoch: 5600 Train: 0.85735 Test: 0.91936
Epoch: 5700 Train: 0.28880 Test: 0.35261
Epoch: 5800 Train: 0.42416 Test: 0.40493
Epoch: 5900 Train: 0.27358 Test: 0.31237
Epoch: 6000 Train: 1.07494 Test: 1.26559
Epoch: 6100 Train: 0.39749 Test: 0.50181
Epoch: 6200 Train: 0.44019 Test: 0.50871
Epoch: 6300 Train: 0.34911 Test: 0.39458
Epoch: 6400 Train: 0.47351 Test: 0.44860
Epoch: 6500 Train: 0.17919 Test: 0.22976
Epoch: 6600 Train: 0.19513 Test: 0.26395
Epoch 6700: New minimal relative error: 7.84%, model saved.
Epoch: 6700 Train: 0.32264 Test: 0.32972
Epoch: 6800 Train: 0.61373 Test: 0.70844
Epoch: 6900 Train: 0.24913 Test: 0.30739
Epoch: 7000 Train: 0.75066 Test: 0.98584
Epoch: 7100 Train: 0.21730 Test: 0.24940
Epoch: 7200 Train: 0.17678 Test: 0.23166
Epoch: 7300 Train: 0.30541 Test: 0.42996
Epoch: 7400 Train: 0.17024 Test: 0.21429
Epoch: 7500 Train: 0.58760 Test: 0.57602
Epoch: 7600 Train: 0.44035 Test: 0.40733
Epoch: 7700 Train: 0.83482 Test: 0.89931
Epoch: 7800 Train: 0.46294 Test: 0.57688
Epoch: 7900 Train: 0.13357 Test: 0.17909
Epoch: 8000 Train: 0.13377 Test: 0.18244
Epoch: 8100 Train: 0.13181 Test: 0.18091
Epoch: 8200 Train: 0.13504 Test: 0.17531
Epoch: 8300 Train: 0.14544 Test: 0.20000
Epoch: 8400 Train: 0.12544 Test: 0.16835
Epoch: 8500 Train: 0.12401 Test: 0.16850
Epoch: 8600 Train: 0.12387 Test: 0.16923
Epoch: 8700 Train: 0.12213 Test: 0.16252
Epoch: 8800 Train: 0.12350 Test: 0.16970
Epoch: 8900 Train: 0.25847 Test: 0.35125
Epoch: 9000 Train: 0.11902 Test: 0.15764
Epoch: 9100 Train: 0.14934 Test: 0.16818
Epoch: 9200 Train: 0.69519 Test: 0.84142
Epoch: 9300 Train: 0.16027 Test: 0.21240
Epoch: 9400 Train: 0.17326 Test: 0.21715
Epoch: 9500 Train: 0.22315 Test: 0.19164
Epoch: 9600 Train: 0.12973 Test: 0.16217
Epoch: 9700 Train: 0.11573 Test: 0.14916
Epoch: 9800 Train: 0.14248 Test: 0.18136
Epoch: 9900 Train: 0.11078 Test: 0.14390
Epoch: 9999 Train: 0.10435 Test: 0.14284
Training Loss: tensor(0.1043)
Test Loss: tensor(0.1428)
Learned LE: [ 0.78539777 -0.00645722 -2.969759  ]
True LE: [ 8.79427612e-01  1.01155415e-02 -1.45609007e+01]
Relative Error: [ 2.3576326   2.8555286   3.46544     4.109348    4.8201838   5.6374917
  6.426463    7.2211995   8.02427     8.833105    9.771921   10.775097
 11.399171   11.727015   12.004074   12.174651   12.086253   11.748642
 11.47947    11.26861    11.132511   11.167303   11.483829   11.87118
 12.202071   12.185577   11.846874   11.531298   11.036167   10.697149
 10.405955   10.138361    9.886636    9.653636    9.4580345   9.339355
  9.3790045   9.469119    9.517494    9.153138    8.707089    8.396257
  7.8776054   7.408856    6.564682    5.756684    4.9984674   4.3168836
  3.8079495   3.3185315   2.7468457   2.345359    2.1025147   1.950552
  1.9474964   2.0824206   2.38522     2.768088    2.697375    2.4086776
  1.9471768   1.7993921   2.0881367   2.6042864   3.2636974   3.876919
  4.688637    5.514755    6.327127    7.108473    7.864388    8.622313
  9.475703   10.40141    10.990656   11.2759285  11.515542   11.634841
 11.455677   11.169251   10.943904   10.739188   10.686022   10.783166
 11.134102   11.515757   11.909795   11.859726   11.553667   11.263672
 10.770489   10.462947   10.181479    9.911416    9.642359    9.368051
  9.130981    8.9672365   9.003299    9.048339    9.006083    8.643328
  8.183782    7.826817    7.3417954   6.573517    5.75837     5.019706
  4.343966    3.7236657   3.1346135   2.5523481   2.0969343   1.7829173
  1.7135853   1.5557586   1.6057978   1.8591043   2.105917    2.4260643
  2.5092726   2.245448    1.9382186   1.6463507   1.8602513   2.4164836
  3.0657995   3.872107    4.6781516   5.4800987   6.275647    7.044752
  7.7638974   8.45967     9.186198   10.035924   10.581959   10.84547
 11.046153   11.065996   10.855115   10.655024   10.430594   10.270734
 10.262188   10.476387   10.773548   11.191674   11.626727   11.560008
 11.29017    10.989257   10.554493   10.268056    9.9985285   9.704341
  9.400781    9.105902    8.833395    8.644964    8.649937    8.660178
  8.568997    8.132927    7.635306    7.3190002   6.6467657   5.790169
  4.994519    4.3062763   3.7144516   3.0045269   2.4038754   1.9060018
  1.6087356   1.5764108   1.4131954   1.3418707   1.5461326   1.6554108
  1.7841631   2.095726    2.4141405   2.1109164   1.8525317   1.5361261
  1.721953    2.2684593   3.0744374   3.997573    4.7633147   5.518296
  6.271083    6.997981    7.707274    8.335905    8.954702    9.649443
 10.183138   10.42681    10.598073   10.503724   10.301714   10.137849
  9.939537    9.841117    9.92048    10.199603   10.482242   10.927986
 11.35424    11.284364   11.019619   10.745744   10.373911   10.055267
  9.717734    9.407257    9.115986    8.833006    8.554392    8.367721
  8.304535    8.242179    8.165535    7.6639123   7.106544    6.7899294
  6.062376    5.192259    4.3558335   3.6552038   2.9746542   2.305101
  1.8025737   1.4631567   1.4248302   1.5782584   1.336642    1.3660518
  1.5412192   1.3526105   1.4848592   1.8730618   2.339177    1.9690847
  1.6509473   1.5066928   1.6935405   2.2475886   3.1941996   4.138561
  4.908528    5.6035748   6.298036    6.983223    7.634572    8.23951
  8.790111    9.281142    9.776305   10.026175   10.114096    9.965059
  9.798881    9.631923    9.489148    9.461294    9.62446     9.949807
 10.239354   10.68066    11.106693   11.022071   10.771763   10.489293
 10.089787    9.724152    9.410802    9.127796    8.848152    8.558668
  8.278303    8.101867    7.9179244   7.81864     7.6988754   7.2380996
  6.7398267   6.233395    5.5037117   4.6448326   3.8516939   3.0490172
  2.2584188   1.7737033   1.3773557   1.3512833   1.5908146   1.6265161
  1.4184736   1.417796    1.3487781   1.0828701   1.2961589   1.7046824
  2.1728756   1.8485357   1.4953505   1.6349905   1.7432202   2.4274426
  3.3291016   4.2638535   5.0539026   5.7012067   6.3290944   6.954208
  7.5358553   8.094326    8.533689    8.86201     9.351274    9.626915
  9.628959    9.454325    9.311839    9.157762    9.099733    9.1353655
  9.340679    9.681171   10.007368   10.401654   10.862123   10.764926
 10.481262   10.146956    9.732425    9.415125    9.1499      8.895667
  8.624278    8.324228    8.058433    7.771255    7.5394826   7.407508
  7.406673    7.0283875   6.3766546   5.680542    4.940167    4.115607
  3.3746116   2.4033244   1.7470943   1.4531968   1.3363371   1.5352118
  1.9004513   1.5254413   1.1972612   1.0535536   0.9247242   0.90895164
  1.1980034   1.5796258   1.9785073   1.7043451   1.4947158   1.7321646
  1.9289287   2.6573093   3.4810576   4.345745    5.169026    5.819275
  6.368927    6.9170713   7.4467106   7.9481235   8.278724    8.455073
  8.900674    9.164526    9.153269    8.984926    8.858878    8.715934
  8.708928    8.823074    9.062305    9.442438    9.770157   10.110002
 10.579601   10.432653   10.112001    9.761015  ]
