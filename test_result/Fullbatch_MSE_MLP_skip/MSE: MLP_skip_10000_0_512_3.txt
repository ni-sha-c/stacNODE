time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.17%, model saved.
Epoch: 0 Train: 3557.16089 Test: 4011.63452
Epoch 100: New minimal relative error: 37.34%, model saved.
Epoch: 100 Train: 74.77126 Test: 67.15013
Epoch 200: New minimal relative error: 12.43%, model saved.
Epoch: 200 Train: 14.91499 Test: 12.05394
Epoch 300: New minimal relative error: 9.53%, model saved.
Epoch: 300 Train: 8.03909 Test: 6.70620
Epoch: 400 Train: 5.57211 Test: 5.44475
Epoch: 500 Train: 4.64649 Test: 4.19968
Epoch: 600 Train: 5.09864 Test: 6.30040
Epoch: 700 Train: 2.33804 Test: 1.99312
Epoch: 800 Train: 1.89338 Test: 1.61451
Epoch: 900 Train: 1.57745 Test: 1.38013
Epoch 1000: New minimal relative error: 7.60%, model saved.
Epoch: 1000 Train: 2.65157 Test: 1.51270
Epoch: 1100 Train: 1.26710 Test: 0.96814
Epoch: 1200 Train: 1.28140 Test: 1.12991
Epoch: 1300 Train: 2.49959 Test: 2.37899
Epoch: 1400 Train: 0.82387 Test: 0.76834
Epoch: 1500 Train: 1.39424 Test: 1.20804
Epoch 1600: New minimal relative error: 3.80%, model saved.
Epoch: 1600 Train: 0.56193 Test: 0.49741
Epoch: 1700 Train: 1.71948 Test: 1.85889
Epoch: 1800 Train: 0.76911 Test: 0.67803
Epoch: 1900 Train: 0.57453 Test: 0.48966
Epoch: 2000 Train: 1.04280 Test: 0.79588
Epoch: 2100 Train: 2.06789 Test: 2.26207
Epoch: 2200 Train: 1.48087 Test: 1.43560
Epoch: 2300 Train: 0.50863 Test: 0.44033
Epoch: 2400 Train: 0.43609 Test: 0.30760
Epoch: 2500 Train: 0.70996 Test: 0.71982
Epoch: 2600 Train: 0.81518 Test: 0.73897
Epoch: 2700 Train: 1.01141 Test: 0.97670
Epoch: 2800 Train: 0.58390 Test: 0.46941
Epoch: 2900 Train: 0.57250 Test: 0.37718
Epoch: 3000 Train: 0.46222 Test: 0.48485
Epoch: 3100 Train: 0.44247 Test: 0.48450
Epoch: 3200 Train: 0.48798 Test: 0.32735
Epoch: 3300 Train: 0.23353 Test: 0.19438
Epoch: 3400 Train: 0.18268 Test: 0.15154
Epoch: 3500 Train: 1.34151 Test: 1.14608
Epoch 3600: New minimal relative error: 3.14%, model saved.
Epoch: 3600 Train: 0.24011 Test: 0.21886
Epoch: 3700 Train: 0.25018 Test: 0.27285
Epoch: 3800 Train: 0.36189 Test: 0.35015
Epoch: 3900 Train: 0.19692 Test: 0.18333
Epoch: 4000 Train: 0.20252 Test: 0.19311
Epoch: 4100 Train: 0.23536 Test: 0.19461
Epoch: 4200 Train: 0.20466 Test: 0.19403
Epoch: 4300 Train: 0.17969 Test: 0.13186
Epoch: 4400 Train: 1.58814 Test: 1.72059
Epoch: 4500 Train: 0.30919 Test: 0.23567
Epoch: 4600 Train: 0.14841 Test: 0.14540
Epoch: 4700 Train: 0.44702 Test: 0.17825
Epoch: 4800 Train: 0.46352 Test: 0.23359
Epoch: 4900 Train: 0.17289 Test: 0.15896
Epoch: 5000 Train: 0.43825 Test: 0.40026
Epoch: 5100 Train: 0.41118 Test: 0.38311
Epoch: 5200 Train: 0.69929 Test: 0.61795
Epoch: 5300 Train: 0.17969 Test: 0.17747
Epoch: 5400 Train: 0.19960 Test: 0.20578
Epoch: 5500 Train: 0.10789 Test: 0.11297
Epoch: 5600 Train: 0.29093 Test: 0.35968
Epoch: 5700 Train: 0.10611 Test: 0.09940
Epoch: 5800 Train: 0.27012 Test: 0.32079
Epoch: 5900 Train: 0.10069 Test: 0.10497
Epoch: 6000 Train: 0.12039 Test: 0.10706
Epoch: 6100 Train: 0.10021 Test: 0.10621
Epoch: 6200 Train: 0.16628 Test: 0.13202
Epoch: 6300 Train: 0.13962 Test: 0.14432
Epoch: 6400 Train: 0.09969 Test: 0.09612
Epoch: 6500 Train: 0.15217 Test: 0.14932
Epoch: 6600 Train: 0.16824 Test: 0.17835
Epoch: 6700 Train: 0.07556 Test: 0.08041
Epoch: 6800 Train: 0.08370 Test: 0.09078
Epoch: 6900 Train: 0.10676 Test: 0.12604
Epoch: 7000 Train: 0.15989 Test: 0.18248
Epoch: 7100 Train: 0.09953 Test: 0.09338
Epoch: 7200 Train: 0.18916 Test: 0.17491
Epoch: 7300 Train: 0.07384 Test: 0.08438
Epoch: 7400 Train: 0.16128 Test: 0.19175
Epoch 7500: New minimal relative error: 2.32%, model saved.
Epoch: 7500 Train: 0.08965 Test: 0.09242
Epoch: 7600 Train: 0.11134 Test: 0.10084
Epoch 7700: New minimal relative error: 2.21%, model saved.
Epoch: 7700 Train: 0.08185 Test: 0.08076
Epoch: 7800 Train: 0.07015 Test: 0.07859
Epoch: 7900 Train: 0.06476 Test: 0.07331
Epoch: 8000 Train: 0.06571 Test: 0.07914
Epoch: 8100 Train: 0.06139 Test: 0.07162
Epoch: 8200 Train: 0.06302 Test: 0.07533
Epoch: 8300 Train: 0.06299 Test: 0.07207
Epoch: 8400 Train: 0.06205 Test: 0.07602
Epoch: 8500 Train: 0.05853 Test: 0.07047
Epoch: 8600 Train: 0.06367 Test: 0.07760
Epoch: 8700 Train: 0.05928 Test: 0.06977
Epoch: 8800 Train: 0.07359 Test: 0.08327
Epoch: 8900 Train: 0.07091 Test: 0.07434
Epoch 9000: New minimal relative error: 1.93%, model saved.
Epoch: 9000 Train: 0.05576 Test: 0.06766
Epoch: 9100 Train: 0.05603 Test: 0.07080
Epoch: 9200 Train: 0.17098 Test: 0.20577
Epoch: 9300 Train: 0.07953 Test: 0.09906
Epoch: 9400 Train: 0.05358 Test: 0.06767
Epoch: 9500 Train: 0.06954 Test: 0.08458
Epoch: 9600 Train: 0.05888 Test: 0.07489
Epoch: 9700 Train: 0.05135 Test: 0.06648
Epoch: 9800 Train: 0.05040 Test: 0.06459
Epoch 9900: New minimal relative error: 1.13%, model saved.
Epoch: 9900 Train: 0.05024 Test: 0.06550
Epoch: 9999 Train: 0.09177 Test: 0.13034
Training Loss: tensor(0.0918)
Test Loss: tensor(0.1303)
Learned LE: [ 0.88976294  0.00535596 -4.864683  ]
True LE: [ 8.7376022e-01  2.6722022e-03 -1.4557505e+01]
Relative Error: [1.758495   1.8618636  2.273543   2.701849   3.1089323  4.191539
 5.078572   5.5209184  6.0738344  6.463793   6.4520793  6.0364017
 4.1516466  2.6595185  1.8294226  2.0741048  2.7999723  2.6062825
 2.1007702  1.9177983  2.1145902  2.082172   2.1733823  2.5105672
 3.1224144  3.9678714  4.8410206  5.7398624  6.047121   5.306654
 4.6937594  4.5254416  4.0613117  3.7519495  3.7770896  4.030305
 3.7676558  2.5289507  1.9423859  1.9422051  2.1035714  2.7505453
 2.631944   2.339945   1.4800575  0.74637    0.18243241 0.51054883
 0.91137725 1.2800381  1.6632199  1.8473142  1.3136504  1.0084214
 1.0117062  1.2782516  1.5504102  1.4954386  1.4086678  1.5253475
 1.5560884  1.539879   1.4443796  1.4100479  1.5308144  1.7365376
 2.023822   2.47604    3.6643586  4.4337983  4.9357595  5.269549
 5.585537   5.185295   4.2002444  2.7202616  1.6608564  1.8699567
 1.9137423  1.6986206  1.2272122  0.9025143  0.951161   1.0555837
 1.2101775  1.3985553  1.714982   2.4676425  3.4059114  4.202313
 4.8794613  5.0156546  4.8790407  4.4212537  3.968058   3.7035868
 3.5332334  3.5728362  3.5063756  2.8929653  1.7614653  1.52935
 1.6108328  2.049876   2.361037   2.3070703  1.6618477  0.89106494
 0.2364061  0.44309586 0.61273634 0.812959   0.9534091  1.2650845
 0.94510883 0.3589241  0.3567827  0.6612847  1.108712   1.1273152
 1.0185597  1.2634702  1.3363228  1.2858565  1.1791978  1.1164467
 1.1327789  1.0630652  1.1428013  1.4646271  1.9204372  3.0344276
 3.7608638  4.1243944  4.4201727  4.6141543  3.729303   2.7663953
 1.5537679  1.6853088  1.57779    1.6560472  1.6376797  1.3571116
 0.6788663  0.6192219  1.222507   1.8549887  2.3189795  2.499018
 2.7090902  2.8948574  3.3782012  3.9299078  4.0965343  4.6083603
 4.0524116  3.5346122  3.2350056  3.3421822  2.8364327  2.9091737
 2.4181547  1.3540907  1.2749478  1.6571183  2.1670477  2.0730448
 1.6840217  1.1291994  0.5421948  0.34216046 0.526796   0.5759265
 0.5270174  0.6802332  0.59963214 0.6944501  0.656905   0.45961234
 0.46356454 0.72915524 0.8454454  1.045566   1.3018242  1.2137887
 1.0433729  0.89726746 0.9211665  1.098632   0.7621246  0.67472386
 0.8438686  1.3010325  2.4223635  2.945276   3.3593707  3.720127
 3.4600205  2.5058572  1.7628319  1.3877213  1.3154957  1.6570426
 2.226029   2.1213179  1.185219   0.3535039  1.6198393  2.4320211
 3.4079552  3.896161   4.04651    3.8903923  3.4669697  2.9749875
 2.909281   3.1824033  3.6125069  3.616868   3.142374   3.0102925
 2.6256285  2.1908808  2.48174    2.2222464  1.2543815  1.026829
 1.6601421  1.9044814  2.0351808  1.3219438  0.77756506 0.4064125
 0.33990586 0.4604702  0.21694072 0.17131652 0.35769966 0.67435473
 1.3428928  1.3592842  0.9585456  0.458582   0.74433535 1.0463294
 1.1928145  1.3369433  1.1062361  0.9277773  0.840773   0.84984285
 0.9594028  0.6587951  0.48514578 0.4701939  0.79814315 1.7292026
 2.1383324  2.6894906  2.7859788  2.6647482  1.5893387  1.1229435
 1.1959862  1.1837922  2.0649498  2.525118   1.6995918  0.39420208
 1.8982038  2.980344   3.1072946  3.419288   3.4256887  3.3640723
 3.4625177  3.4133558  3.5228505  2.7519255  2.5406601  2.7425592
 3.3134322  2.9403265  2.4669046  1.8997438  1.8286196  1.9839369
 2.0330093  1.1881574  0.7284953  1.2921864  1.6259379  1.7767333
 1.2114549  0.7358397  0.33363757 0.19940332 0.36661172 0.23009367
 0.16351147 0.09823276 0.75263286 1.4291255  1.6203026  1.1268609
 0.5110858  0.6026958  0.84271145 0.99160194 1.1052557  1.0348731
 0.8809615  0.8417816  0.7841296  0.6848329  0.6762779  0.43531716
 0.41821572 0.5125758  0.8493816  1.6265718  1.9797024  2.020162
 2.0641308  1.1596873  0.9188753  0.65024847 1.4318086  2.0207894
 1.8145148  0.8292025  1.4809096  2.6559267  2.6008437  2.8830502
 3.2756255  3.4323123  3.2186613  2.8711185  2.81533    2.685533
 2.5401292  2.6446724  2.2358727  2.2311227  2.8221126  2.334176
 1.9316312  1.4621291  1.3578248  1.8261595  1.2757021  0.8358351
 0.57104534 1.1164367  1.4836776  1.0935113  0.84916407 0.4020883
 0.14880472 0.2170172  0.24288979 0.38485208 0.1926742  0.5134218
 0.8055984  1.358827   1.1422533  0.61441565 0.39458057 0.43411323
 0.57649904 0.62220526 0.8740662  0.9469892  0.8767916  0.8218194
 0.5672671  0.5117106  0.31487742 0.43282813 0.5563119  0.6037896
 0.9634251  1.5203861  1.5971932  1.4322598  1.3333644  0.7344145
 0.67165077 1.3920293  1.8307573  1.1919471  0.37549898 1.9090561
 2.3171384  2.230833   2.6015053  2.964699  ]
