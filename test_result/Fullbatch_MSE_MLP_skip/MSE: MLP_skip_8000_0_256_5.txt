time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 103.16%, model saved.
Epoch: 0 Train: 3592.49512 Test: 3800.25366
Epoch: 80 Train: 283.59164 Test: 288.15985
Epoch 160: New minimal relative error: 35.04%, model saved.
Epoch: 160 Train: 30.60130 Test: 29.96337
Epoch 240: New minimal relative error: 17.94%, model saved.
Epoch: 240 Train: 19.83834 Test: 16.35797
Epoch: 320 Train: 21.60342 Test: 22.75634
Epoch 400: New minimal relative error: 16.50%, model saved.
Epoch: 400 Train: 11.25173 Test: 9.43957
Epoch 480: New minimal relative error: 11.98%, model saved.
Epoch: 480 Train: 7.22984 Test: 7.74664
Epoch: 560 Train: 10.21179 Test: 10.28186
Epoch: 640 Train: 27.08981 Test: 16.40321
Epoch 720: New minimal relative error: 11.34%, model saved.
Epoch: 720 Train: 5.25845 Test: 5.31086
Epoch: 800 Train: 4.99959 Test: 5.20180
Epoch: 880 Train: 4.63380 Test: 4.79277
Epoch: 960 Train: 4.27812 Test: 4.57146
Epoch: 1040 Train: 4.87573 Test: 4.58555
Epoch: 1120 Train: 4.16449 Test: 4.52881
Epoch: 1200 Train: 4.93087 Test: 4.51671
Epoch: 1280 Train: 4.59815 Test: 5.27293
Epoch: 1360 Train: 4.42085 Test: 4.28997
Epoch 1440: New minimal relative error: 8.01%, model saved.
Epoch: 1440 Train: 4.67420 Test: 4.00521
Epoch: 1520 Train: 5.20815 Test: 5.19596
Epoch: 1600 Train: 2.24085 Test: 2.31187
Epoch: 1680 Train: 2.13718 Test: 2.19938
Epoch: 1760 Train: 1.78258 Test: 1.81084
Epoch: 1840 Train: 1.75955 Test: 2.01804
Epoch: 1920 Train: 2.68092 Test: 3.02561
Epoch: 2000 Train: 4.02535 Test: 3.64308
Epoch: 2080 Train: 1.68732 Test: 1.23971
Epoch: 2160 Train: 4.71389 Test: 5.09042
Epoch: 2240 Train: 2.51881 Test: 1.67095
Epoch: 2320 Train: 1.55484 Test: 2.07584
Epoch: 2400 Train: 10.68893 Test: 8.45566
Epoch: 2480 Train: 0.70018 Test: 0.84234
Epoch: 2560 Train: 0.76342 Test: 0.90862
Epoch: 2640 Train: 0.74841 Test: 1.04253
Epoch: 2720 Train: 3.03029 Test: 2.08190
Epoch: 2800 Train: 2.39524 Test: 2.67425
Epoch: 2880 Train: 2.26052 Test: 2.79834
Epoch: 2960 Train: 2.88085 Test: 3.76555
Epoch: 3040 Train: 0.84573 Test: 1.19856
Epoch: 3120 Train: 0.51365 Test: 0.66569
Epoch: 3200 Train: 0.45699 Test: 0.60747
Epoch: 3280 Train: 3.72125 Test: 4.74195
Epoch: 3360 Train: 0.38532 Test: 0.55419
Epoch: 3440 Train: 0.43669 Test: 0.61752
Epoch: 3520 Train: 0.41802 Test: 0.62697
Epoch 3600: New minimal relative error: 6.25%, model saved.
Epoch: 3600 Train: 0.44134 Test: 0.60107
Epoch 3680: New minimal relative error: 5.74%, model saved.
Epoch: 3680 Train: 0.40836 Test: 0.56007
Epoch: 3760 Train: 0.33197 Test: 0.48428
Epoch: 3840 Train: 0.32015 Test: 0.47550
Epoch 3920: New minimal relative error: 3.82%, model saved.
Epoch: 3920 Train: 0.31714 Test: 0.51758
Epoch: 4000 Train: 0.59431 Test: 0.82214
Epoch: 4080 Train: 1.03232 Test: 0.99431
Epoch: 4160 Train: 0.95760 Test: 0.82172
Epoch: 4240 Train: 2.36701 Test: 2.47346
Epoch: 4320 Train: 0.27113 Test: 0.42150
Epoch: 4400 Train: 0.42805 Test: 0.66988
Epoch: 4480 Train: 0.26213 Test: 0.40714
Epoch: 4560 Train: 0.24867 Test: 0.39915
Epoch: 4640 Train: 0.34479 Test: 0.42750
Epoch: 4720 Train: 0.25037 Test: 0.39111
Epoch: 4800 Train: 0.40751 Test: 0.64524
Epoch: 4880 Train: 0.22020 Test: 0.37166
Epoch: 4960 Train: 0.26487 Test: 0.45830
Epoch: 5040 Train: 0.22098 Test: 0.36178
Epoch: 5120 Train: 0.21033 Test: 0.36217
Epoch: 5200 Train: 0.24635 Test: 0.37382
Epoch: 5280 Train: 0.22064 Test: 0.38811
Epoch: 5360 Train: 0.19827 Test: 0.34184
Epoch: 5440 Train: 0.39472 Test: 0.48708
Epoch 5520: New minimal relative error: 3.00%, model saved.
Epoch: 5520 Train: 0.18841 Test: 0.33007
Epoch: 5600 Train: 0.18773 Test: 0.32896
Epoch: 5680 Train: 0.80868 Test: 1.02553
Epoch: 5760 Train: 0.17681 Test: 0.31983
Epoch: 5840 Train: 0.18477 Test: 0.34916
Epoch: 5920 Train: 0.21963 Test: 0.39289
Epoch: 6000 Train: 0.16900 Test: 0.31447
Epoch: 6080 Train: 0.23127 Test: 0.38214
Epoch: 6160 Train: 0.16266 Test: 0.30030
Epoch: 6240 Train: 0.16071 Test: 0.30694
Epoch: 6320 Train: 0.23538 Test: 0.31025
Epoch: 6400 Train: 0.16538 Test: 0.35480
Epoch: 6480 Train: 0.15251 Test: 0.28892
Epoch: 6560 Train: 0.28390 Test: 0.34004
Epoch: 6640 Train: 0.14782 Test: 0.28358
Epoch: 6720 Train: 0.26852 Test: 0.48733
Epoch: 6800 Train: 0.14467 Test: 0.28223
Epoch: 6880 Train: 0.19564 Test: 0.38496
Epoch: 6960 Train: 0.13970 Test: 0.27372
Epoch: 7040 Train: 0.96546 Test: 0.97095
Epoch: 7120 Train: 0.13633 Test: 0.27039
Epoch: 7200 Train: 0.14604 Test: 0.28232
Epoch: 7280 Train: 0.14547 Test: 0.28866
Epoch: 7360 Train: 0.13167 Test: 0.26030
Epoch: 7440 Train: 0.16347 Test: 0.33023
Epoch: 7520 Train: 0.12776 Test: 0.25712
Epoch: 7600 Train: 0.25338 Test: 0.31415
Epoch: 7680 Train: 0.12482 Test: 0.25421
Epoch: 7760 Train: 0.14385 Test: 0.26262
Epoch: 7840 Train: 0.18942 Test: 0.29085
Epoch: 7920 Train: 0.12081 Test: 0.24720
Epoch: 7999 Train: 0.13906 Test: 0.26607
Training Loss: tensor(0.1391)
Test Loss: tensor(0.2661)
Learned LE: [ 0.7936052 -0.0492476 -3.4786656]
True LE: [ 8.4094077e-01  9.9987956e-03 -1.4528626e+01]
Relative Error: [ 4.3055134  4.4112763  4.516162   4.6301203  4.8355255  4.7938833
  4.6907544  4.867776   4.887061   4.6080084  4.332368   4.0285106
  4.0322413  4.60824    4.078605   3.395483   3.5338867  4.8469687
  6.580522   8.078415   9.18422    9.401111   9.678742   9.985579
 10.179264  10.284786  10.523386  10.677635  10.600742  10.599465
 10.772472  10.490115  10.075526   9.327247   8.6177435  8.041236
  7.4858108  7.0468535  6.3616366  5.829948   5.6027737  5.484093
  5.213657   4.5540056  4.289464   3.86279    3.2050085  3.117373
  3.5276248  4.012058   4.265171   4.1826067  3.5795412  3.0189707
  2.7536292  2.7345793  3.127516   3.4119124  3.598699   3.6985822
  3.764928   3.9251242  4.03131    4.1166296  4.2060404  4.267261
  4.3818846  4.3996315  4.2704625  4.4202676  4.5961814  4.29801
  4.0634537  3.7991292  3.8570528  4.2985935  4.3090262  3.530342
  3.742439   5.133654   6.7523665  7.8331122  8.209887   8.3089485
  8.578534   8.8675     9.017648   9.0671215  9.267401   9.470389
  9.837215   9.868075   9.890141   9.968822   9.821508   9.452379
  8.622398   7.892757   7.2562294  6.725975   6.171458   5.3841915
  5.1169624  4.8704643  4.734212   3.9499116  3.5088577  3.1226993
  2.4549878  2.4437509  3.1379368  3.8140986  4.154984   3.7390995
  3.0894523  2.5358818  2.3347354  2.3783174  2.7838924  3.1008227
  3.2822876  3.3755898  3.5075862  3.704857   3.819916   3.8457189
  3.8628738  3.8895543  3.948323   3.981918   4.0238953  4.0059624
  4.2385077  4.1412854  3.8646057  3.7144523  3.690991   3.8594973
  4.1629596  3.4582171  3.6484654  4.9141397  6.146229   7.2465687
  7.2454476  7.3623023  7.5667424  7.8229256  7.9505973  8.007625
  8.11136    8.174856   8.327254   8.786694   9.059686   8.98813
  9.254588   9.231572   8.690982   7.7929535  7.028973   6.441045
  5.9717937  5.085505   4.575156   4.290484   4.267067   3.513425
  2.928866   2.435465   1.9866122  1.9652404  2.8449073  3.7176006
  3.8573086  3.241015   2.448664   2.0086753  1.9010384  1.9349664
  2.3463352  2.6806483  2.908801   3.0398483  3.2342632  3.4679036
  3.6027446  3.6425304  3.5928648  3.5483618  3.4701376  3.5892613
  3.77297    3.7604122  3.8516874  4.0138288  3.8566246  3.607475
  3.5155342  3.4861023  3.8391714  3.5147235  3.4621313  4.401932
  5.585264   6.3380027  6.4045916  6.4596205  6.6469727  7.0212626
  7.169698   7.0740776  6.980231   6.96984    6.995208   7.250526
  7.6497135  8.15572    8.2306385  8.457968   8.48176    7.8120766
  6.894141   6.120376   5.602489   4.96149    4.1501274  3.8362246
  3.5635955  3.1347387  2.5020714  2.134384   1.7735997  1.4989564
  2.4159672  3.465002   3.3225965  2.8088274  2.1250823  1.623811
  1.4103812  1.4655199  1.8938986  2.254256   2.501865   2.6659868
  2.9088502  3.1464245  3.3189907  3.4280148  3.4048195  3.3101103
  3.2435331  3.1593335  3.353527   3.5550542  3.5531893  3.716277
  3.767091   3.5890915  3.5062287  3.3286977  3.417558   3.591584
  3.2555473  3.8530636  4.872516   5.5986056  5.6131     5.7172084
  6.2374344  6.994225   7.3641005  7.3628078  7.199057   6.612134
  6.0838084  6.0442095  6.402275   6.8989053  7.718314   7.5397463
  7.4564657  7.5981975  6.879526   5.964396   5.2071834  4.764118
  3.9596517  3.3116157  2.9980085  2.811314   2.2401814  1.9758303
  1.6818663  1.4600134  1.8140143  2.8644056  2.7962825  2.448065
  1.9327979  1.5440834  1.2690469  1.0738627  1.4542904  1.8484019
  2.153611   2.3690772  2.6194682  2.8726919  3.0339527  3.124212
  3.1700857  3.1697168  3.1029773  2.9440765  2.8701205  3.1163344
  3.3345733  3.391577   3.51642    3.3380926  3.1346366  3.3635848
  3.317507   3.4462154  3.1879008  3.316558   4.079536   4.935279
  4.9674225  5.4224663  6.3312097  6.4769387  6.4359717  6.1947165
  6.0575323  6.2009506  6.627398   6.3259406  5.5869045  5.5937824
  6.077745   6.796781   6.8867335  6.655828   6.546831   5.936135
  5.020218   4.3009     3.9461207  3.0756257  2.584589   2.544614
  2.3938477  2.2118905  1.7847672  1.4660736  1.3078526  2.1198916
  2.2478178  2.127812   1.8157208  1.5204113  1.3406823  1.0982239
  1.1306584  1.5040643  1.8092064  2.0379884  2.2647393  2.5394416
  2.7205684  2.8206284  2.8890898  3.0173497  3.0804906  2.8910491
  2.6879277  2.605907   2.8693917  3.1727881  3.2523048  3.238817
  2.9976795  2.8764303  3.1421971  3.2418041  3.3473063  3.1127625
  3.3223114  4.2604423  4.4976244  5.424293   5.1529093  4.854756
  5.0691004  5.0150437  4.7748785  4.765375 ]
