time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 97.29%, model saved.
Epoch: 0 Train: 59798.56641 Test: 3814.16748
Epoch: 80 Train: 13411.77930 Test: 1273.19019
Epoch: 160 Train: 11329.64160 Test: 968.23199
Epoch: 240 Train: 12126.70996 Test: 1109.75879
Epoch: 320 Train: 12465.01172 Test: 1344.29028
Epoch 400: New minimal relative error: 90.31%, model saved.
Epoch: 400 Train: 10962.83887 Test: 881.65021
Epoch 480: New minimal relative error: 50.94%, model saved.
Epoch: 480 Train: 12408.05566 Test: 1012.66437
Epoch: 560 Train: 11753.82227 Test: 1101.10449
Epoch: 640 Train: 9663.26953 Test: 795.24780
Epoch: 720 Train: 9201.85352 Test: 713.73114
Epoch: 800 Train: 8557.11914 Test: 590.79144
Epoch: 880 Train: 8182.10645 Test: 547.70355
Epoch: 960 Train: 6793.62354 Test: 426.23581
Epoch: 1040 Train: 5641.40088 Test: 287.20264
Epoch: 1120 Train: 4808.29297 Test: 275.05978
Epoch: 1200 Train: 3220.24683 Test: 198.54762
Epoch: 1280 Train: 2080.11548 Test: 60.88431
Epoch: 1360 Train: 1503.49573 Test: 94.30998
Epoch: 1440 Train: 1381.42017 Test: 38.43695
Epoch 1520: New minimal relative error: 11.99%, model saved.
Epoch: 1520 Train: 1258.33533 Test: 21.76115
Epoch: 1600 Train: 898.68634 Test: 12.29223
Epoch: 1680 Train: 850.92108 Test: 10.47813
Epoch: 1760 Train: 785.82306 Test: 9.43211
Epoch: 1840 Train: 651.55109 Test: 7.16462
Epoch: 1920 Train: 638.66632 Test: 13.88132
Epoch: 2000 Train: 544.25922 Test: 5.00777
Epoch: 2080 Train: 516.29150 Test: 5.90588
Epoch: 2160 Train: 482.27036 Test: 5.10676
Epoch: 2240 Train: 448.57220 Test: 5.12930
Epoch: 2320 Train: 407.01614 Test: 5.66660
Epoch: 2400 Train: 390.99866 Test: 4.84145
Epoch 2480: New minimal relative error: 10.86%, model saved.
Epoch: 2480 Train: 346.09070 Test: 3.64852
Epoch: 2560 Train: 340.24469 Test: 2.74891
Epoch: 2640 Train: 345.91351 Test: 4.51931
Epoch 2720: New minimal relative error: 10.14%, model saved.
Epoch: 2720 Train: 310.47318 Test: 2.04519
Epoch: 2800 Train: 294.01718 Test: 1.91240
Epoch: 2880 Train: 317.10828 Test: 2.84230
Epoch: 2960 Train: 294.51053 Test: 3.10727
Epoch 3040: New minimal relative error: 8.63%, model saved.
Epoch: 3040 Train: 276.81680 Test: 2.36954
Epoch: 3120 Train: 278.15802 Test: 2.01414
Epoch: 3200 Train: 266.18372 Test: 10.02766
Epoch 3280: New minimal relative error: 7.06%, model saved.
Epoch: 3280 Train: 271.59378 Test: 1.73799
Epoch 3360: New minimal relative error: 5.16%, model saved.
Epoch: 3360 Train: 269.22012 Test: 1.88291
Epoch: 3440 Train: 257.77863 Test: 2.40083
Epoch: 3520 Train: 257.66794 Test: 2.54098
Epoch: 3600 Train: 228.90633 Test: 1.67297
Epoch: 3680 Train: 244.65869 Test: 17.58150
Epoch: 3760 Train: 209.11052 Test: 1.02439
Epoch: 3840 Train: 215.41910 Test: 3.47399
Epoch: 3920 Train: 227.63876 Test: 1.50689
Epoch: 4000 Train: 208.42967 Test: 1.85710
Epoch: 4080 Train: 231.08788 Test: 2.17452
Epoch: 4160 Train: 225.94342 Test: 3.82325
Epoch: 4240 Train: 195.15935 Test: 2.67342
Epoch 4320: New minimal relative error: 5.06%, model saved.
Epoch: 4320 Train: 182.77470 Test: 1.28169
Epoch: 4400 Train: 186.16350 Test: 1.39973
Epoch: 4480 Train: 178.75052 Test: 1.25870
Epoch: 4560 Train: 171.30446 Test: 1.20816
Epoch: 4640 Train: 172.83023 Test: 1.10152
Epoch: 4720 Train: 186.69139 Test: 2.62820
Epoch: 4800 Train: 161.38536 Test: 0.90903
Epoch: 4880 Train: 160.57962 Test: 1.29067
Epoch: 4960 Train: 159.23567 Test: 0.89737
Epoch: 5040 Train: 157.00156 Test: 0.91086
Epoch: 5120 Train: 139.31383 Test: 0.70104
Epoch 5200: New minimal relative error: 3.08%, model saved.
Epoch: 5200 Train: 130.84441 Test: 0.62719
Epoch: 5280 Train: 132.01567 Test: 0.79812
Epoch: 5360 Train: 159.13531 Test: 1.33999
Epoch: 5440 Train: 151.84770 Test: 1.57151
Epoch: 5520 Train: 131.77338 Test: 1.05470
Epoch: 5600 Train: 140.33749 Test: 1.51641
Epoch: 5680 Train: 132.13370 Test: 0.84722
Epoch: 5760 Train: 130.61572 Test: 0.65379
Epoch: 5840 Train: 119.25213 Test: 0.55220
Epoch: 5920 Train: 114.44195 Test: 0.46876
Epoch: 6000 Train: 117.17505 Test: 0.99386
Epoch: 6080 Train: 116.80256 Test: 1.22247
Epoch 6160: New minimal relative error: 3.03%, model saved.
Epoch: 6160 Train: 108.17906 Test: 0.56000
Epoch: 6240 Train: 112.21907 Test: 0.70932
Epoch: 6320 Train: 115.32988 Test: 0.64249
Epoch: 6400 Train: 113.38330 Test: 0.66160
Epoch: 6480 Train: 112.66286 Test: 0.55705
Epoch: 6560 Train: 129.39565 Test: 0.84844
Epoch: 6640 Train: 110.08336 Test: 0.64676
Epoch: 6720 Train: 105.49787 Test: 0.50660
Epoch: 6800 Train: 94.19009 Test: 0.40971
Epoch: 6880 Train: 92.70254 Test: 0.48732
Epoch: 6960 Train: 87.20083 Test: 0.53256
Epoch: 7040 Train: 84.22060 Test: 0.29452
Epoch: 7120 Train: 81.87024 Test: 0.51146
Epoch: 7200 Train: 82.91174 Test: 0.31905
Epoch: 7280 Train: 85.98338 Test: 0.37437
Epoch: 7360 Train: 79.76671 Test: 0.29847
Epoch: 7440 Train: 82.07095 Test: 0.84313
Epoch: 7520 Train: 80.33909 Test: 0.40862
Epoch: 7600 Train: 76.50361 Test: 0.35086
Epoch: 7680 Train: 71.30516 Test: 0.21026
Epoch: 7760 Train: 74.99072 Test: 0.50400
Epoch: 7840 Train: 74.44963 Test: 0.43632
Epoch: 7920 Train: 78.71765 Test: 0.31061
Epoch: 7999 Train: 90.62049 Test: 0.70373
Training Loss: tensor(90.6205)
Test Loss: tensor(0.7037)
Learned LE: [  0.82855856   0.0323581  -14.530632  ]
True LE: [ 8.6866802e-01  1.0186301e-02 -1.4557983e+01]
Relative Error: [2.6937103  2.202538   1.7990923  1.5299922  1.3097479  1.2766585
 1.450261   1.5377916  1.6363807  1.6487831  1.7193651  1.8170972
 1.9266965  1.8600663  1.7575262  1.7828329  1.9827117  1.868185
 1.8026495  1.7322755  1.7724065  1.6975509  1.7303256  1.9763336
 1.6275376  1.4976174  1.4627787  1.3168825  1.3419658  1.5377238
 1.9021282  2.4096417  2.5262496  2.3904703  1.9474823  1.6993039
 1.4759969  1.5326011  1.6836982  1.7133156  1.6133008  1.4201546
 1.3985199  1.6539084  1.7400109  2.0788383  2.3924294  2.7034943
 3.152758   3.5804     4.1167164  3.7749796  3.1996067  2.8346312
 2.753729   2.8071613  2.9117324  3.0744143  3.4688299  3.7977006
 3.4109752  2.7879722  2.2471147  1.8561412  1.4536029  1.2329001
 1.1743158  1.1726999  1.3451142  1.5423628  1.6792947  1.6694288
 1.7095063  1.7583247  1.813047   1.7008694  1.5466956  1.5586375
 1.7505685  1.778495   1.6675465  1.5158325  1.5168209  1.4937047
 1.460793   1.7688813  1.5224844  1.3627431  1.3655677  1.2166481
 1.2653666  1.5542004  1.9871709  2.5348265  2.3822677  2.1553717
 1.7731929  1.4910513  1.3399221  1.3766187  1.587872   1.5865796
 1.5004385  1.2865225  1.2419064  1.4426408  1.5742666  1.900469
 2.1045961  2.428649   2.7553303  3.1720243  3.6093764  3.5403104
 2.9850807  2.7567508  2.6836953  2.756331   2.791154   2.9635515
 3.3644094  3.475127   2.9856882  2.4383805  1.9593881  1.552942
 1.2080998  0.99240094 1.138459   1.2167706  1.3624934  1.6542627
 1.8106825  1.7784972  1.7474303  1.7239869  1.7360611  1.5717057
 1.4046677  1.3640738  1.5326357  1.7677816  1.5886049  1.3749542
 1.3498555  1.3547002  1.24493    1.4771185  1.41119    1.1849346
 1.1753646  1.0721745  1.172552   1.5003258  2.005711   2.5103557
 2.312659   1.9691937  1.6697528  1.4064811  1.3322306  1.3389282
 1.6183274  1.5915655  1.4486781  1.1737953  1.0786792  1.214633
 1.3608869  1.6558962  1.8464432  2.17619    2.4259486  2.71539
 3.0810666  3.3991563  2.9778206  2.7441254  2.6217277  2.690972
 2.6614182  2.8449235  3.265132   3.1909878  2.622838   2.1197805
 1.7400218  1.3653172  1.0738448  0.8887402  1.0908817  1.3734441
 1.4918363  1.7876983  1.9629961  1.9531372  1.8135716  1.7259796
 1.7000692  1.4738069  1.3010821  1.2512158  1.3876556  1.6089033
 1.5362319  1.3002717  1.2207522  1.2526273  1.0909181  1.1280167
 1.2490022  0.8808608  0.7909836  0.7996235  0.923857   1.2713429
 1.862726   2.389112   2.287256   1.8844893  1.6155918  1.3557769
 1.1797009  1.23661    1.539093   1.6524699  1.5100945  1.2133691
 1.0144985  1.0744987  1.202439   1.4272989  1.5778387  1.8468682
 2.0791428  2.3065045  2.60903    3.1980064  3.0011883  2.728318
 2.5566123  2.5645661  2.5199945  2.7036989  3.1487896  2.963089
 2.3303807  1.8469728  1.5126009  1.2443702  1.0150362  0.91888523
 1.1469718  1.550018   1.7683996  1.9184374  2.0379038  2.0415795
 1.81121    1.7000169  1.6315215  1.3883884  1.2083933  1.1550624
 1.2771804  1.4596094  1.5611557  1.30535    1.1357291  1.1566366
 0.9805083  0.8351536  0.8548838  0.5480805  0.4484046  0.52331316
 0.61180943 0.9778673  1.5960889  2.0839942  2.1472135  1.8678112
 1.4942484  1.2750676  1.0881763  1.2054372  1.3643918  1.59299
 1.5368962  1.4125831  1.0804545  1.0306396  1.0217179  1.2159843
 1.361961   1.5393252  1.6495128  1.8275768  2.2476091  2.9241025
 3.0297122  2.6908572  2.4798546  2.4379199  2.4149728  2.6123893
 3.0630074  2.837433   2.207491   1.7228379  1.4149503  1.1895212
 1.0827873  1.0940765  1.3259664  1.6967436  1.9828081  1.9700013
 2.0829897  2.1206539  1.937197   1.7784411  1.6026359  1.378103
 1.1432043  1.064863   1.1764088  1.3263038  1.468069   1.3696456
 1.1174138  1.09909    1.0092211  0.6992405  0.54214704 0.4935708
 0.39875895 0.4021191  0.46504563 0.6857296  1.2552922  1.8103597
 1.8830475  1.6391565  1.4356178  1.2520366  1.1273361  1.1702169
 1.2785989  1.5755901  1.5706016  1.4759603  1.2723151  1.0686376
 0.90671647 1.0439897  1.1408489  1.169749   1.3096435  1.4606044
 1.841949   2.4674075  3.0965936  2.6808865  2.4207325  2.3173172
 2.2178953  2.3580334  2.79497    2.5673385  2.0303466  1.6276326
 1.434095   1.3384732  1.3500495  1.4172856  1.4855644  1.6245402
 1.8344996  2.0021424  2.03677    2.205759   2.2497113  2.0025072
 1.700153   1.4727863  1.1593105  0.9881231  1.087861   1.1873968
 1.2645668  1.454019   1.2195616  1.0940273  1.0288941  0.825829
 0.6043857  0.42071822 0.42662397 0.37989506]
