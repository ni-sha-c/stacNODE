time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.79%, model saved.
Epoch: 0 Train: 58948.31250 Test: 3945.31641
Epoch: 100 Train: 15261.94727 Test: 1452.24963
Epoch 200: New minimal relative error: 79.87%, model saved.
Epoch: 200 Train: 12932.72852 Test: 1138.08533
Epoch: 300 Train: 13148.49414 Test: 1102.70740
Epoch 400: New minimal relative error: 65.56%, model saved.
Epoch: 400 Train: 12503.23633 Test: 1194.78760
Epoch: 500 Train: 14553.70117 Test: 1325.82056
Epoch: 600 Train: 11831.26855 Test: 1054.06421
Epoch: 700 Train: 12303.88672 Test: 1150.70020
Epoch: 800 Train: 10408.79980 Test: 898.44189
Epoch: 900 Train: 9281.35449 Test: 656.78552
Epoch: 1000 Train: 6594.81055 Test: 383.70193
Epoch: 1100 Train: 4241.10107 Test: 173.26003
Epoch: 1200 Train: 3199.52319 Test: 100.03398
Epoch: 1300 Train: 2745.29346 Test: 80.60896
Epoch 1400: New minimal relative error: 40.66%, model saved.
Epoch: 1400 Train: 2568.74170 Test: 66.60940
Epoch: 1500 Train: 2059.65576 Test: 52.82516
Epoch: 1600 Train: 1581.00012 Test: 282.91342
Epoch 1700: New minimal relative error: 21.11%, model saved.
Epoch: 1700 Train: 1040.89783 Test: 15.22435
Epoch: 1800 Train: 1101.87439 Test: 23.71403
Epoch 1900: New minimal relative error: 14.68%, model saved.
Epoch: 1900 Train: 888.50311 Test: 13.39304
Epoch 2000: New minimal relative error: 7.07%, model saved.
Epoch: 2000 Train: 776.35437 Test: 13.21271
Epoch: 2100 Train: 718.28064 Test: 17.87331
Epoch: 2200 Train: 608.20520 Test: 8.92394
Epoch 2300: New minimal relative error: 6.84%, model saved.
Epoch: 2300 Train: 572.75403 Test: 6.04880
Epoch: 2400 Train: 593.52509 Test: 15.77564
Epoch: 2500 Train: 613.99969 Test: 10.34500
Epoch: 2600 Train: 547.05212 Test: 6.79873
Epoch: 2700 Train: 652.47845 Test: 33.49899
Epoch: 2800 Train: 604.29950 Test: 14.10205
Epoch: 2900 Train: 529.99231 Test: 12.02375
Epoch 3000: New minimal relative error: 4.84%, model saved.
Epoch: 3000 Train: 534.13141 Test: 6.63457
Epoch: 3100 Train: 564.07556 Test: 9.29704
Epoch: 3200 Train: 527.27136 Test: 7.26314
Epoch: 3300 Train: 513.19800 Test: 6.85540
Epoch: 3400 Train: 508.29068 Test: 20.47101
Epoch: 3500 Train: 443.47537 Test: 5.06344
Epoch: 3600 Train: 422.26099 Test: 4.34238
Epoch: 3700 Train: 413.67133 Test: 7.58834
Epoch: 3800 Train: 389.34998 Test: 4.35401
Epoch: 3900 Train: 372.81195 Test: 3.86085
Epoch: 4000 Train: 382.11115 Test: 5.23574
Epoch: 4100 Train: 382.34222 Test: 4.51935
Epoch: 4200 Train: 357.97842 Test: 6.63115
Epoch: 4300 Train: 400.79504 Test: 5.77695
Epoch: 4400 Train: 332.61862 Test: 3.04056
Epoch 4500: New minimal relative error: 3.13%, model saved.
Epoch: 4500 Train: 313.73575 Test: 2.40269
Epoch 4600: New minimal relative error: 2.51%, model saved.
Epoch: 4600 Train: 302.91736 Test: 2.75516
Epoch: 4700 Train: 295.50842 Test: 2.18669
Epoch: 4800 Train: 301.59775 Test: 2.98808
Epoch: 4900 Train: 311.79126 Test: 10.53590
Epoch: 5000 Train: 312.56967 Test: 3.25092
Epoch: 5100 Train: 284.66232 Test: 3.11716
Epoch: 5200 Train: 296.96326 Test: 3.00126
Epoch: 5300 Train: 390.71558 Test: 8.53844
Epoch: 5400 Train: 361.05823 Test: 4.30305
Epoch: 5500 Train: 328.66699 Test: 9.11998
Epoch: 5600 Train: 380.28201 Test: 7.06068
Epoch: 5700 Train: 308.41760 Test: 13.45308
Epoch: 5800 Train: 261.99524 Test: 2.45576
Epoch: 5900 Train: 289.82443 Test: 3.87237
Epoch: 6000 Train: 283.07294 Test: 3.86727
Epoch: 6100 Train: 253.05679 Test: 3.35162
Epoch: 6200 Train: 278.89410 Test: 2.88592
Epoch: 6300 Train: 254.74934 Test: 2.59864
Epoch: 6400 Train: 243.10489 Test: 2.12442
Epoch: 6500 Train: 259.34079 Test: 3.77342
Epoch: 6600 Train: 254.70117 Test: 2.51075
Epoch: 6700 Train: 247.54643 Test: 1.91669
Epoch: 6800 Train: 221.39348 Test: 1.79044
Epoch: 6900 Train: 264.81235 Test: 2.63403
Epoch: 7000 Train: 302.87183 Test: 3.31157
Epoch: 7100 Train: 269.03928 Test: 3.02916
Epoch: 7200 Train: 212.78436 Test: 1.73681
Epoch: 7300 Train: 230.64238 Test: 3.40427
Epoch: 7400 Train: 203.19936 Test: 1.59044
Epoch: 7500 Train: 250.64098 Test: 2.42451
Epoch: 7600 Train: 226.11365 Test: 1.92766
Epoch: 7700 Train: 199.52707 Test: 1.44513
Epoch: 7800 Train: 213.02313 Test: 1.71833
Epoch: 7900 Train: 201.57880 Test: 1.66433
Epoch: 8000 Train: 175.91324 Test: 1.25203
Epoch: 8100 Train: 178.48587 Test: 1.32997
Epoch: 8200 Train: 177.29546 Test: 1.20182
Epoch: 8300 Train: 186.27528 Test: 1.38240
Epoch: 8400 Train: 180.93660 Test: 1.12965
Epoch: 8500 Train: 202.63686 Test: 1.61625
Epoch: 8600 Train: 198.44986 Test: 1.48322
Epoch: 8700 Train: 203.00667 Test: 1.74002
Epoch: 8800 Train: 194.39899 Test: 1.51808
Epoch: 8900 Train: 203.96587 Test: 2.13537
Epoch: 9000 Train: 203.08481 Test: 2.25845
Epoch: 9100 Train: 193.62698 Test: 1.80105
Epoch: 9200 Train: 204.33316 Test: 2.51114
Epoch: 9300 Train: 200.32817 Test: 2.47355
Epoch: 9400 Train: 190.39748 Test: 1.70575
Epoch: 9500 Train: 182.12621 Test: 1.53057
Epoch: 9600 Train: 170.15685 Test: 1.29559
Epoch 9700: New minimal relative error: 2.47%, model saved.
Epoch: 9700 Train: 163.70164 Test: 1.44204
Epoch: 9800 Train: 164.92117 Test: 1.33325
Epoch: 9900 Train: 181.80800 Test: 1.88192
Epoch: 9999 Train: 189.72093 Test: 1.82121
Training Loss: tensor(189.7209)
Test Loss: tensor(1.8212)
Learned LE: [  0.88254035  -0.03826271 -14.548726  ]
True LE: [ 8.7128180e-01 -6.1653648e-03 -1.4541613e+01]
Relative Error: [1.9206368  2.1940203  2.1003845  1.9220558  1.9592807  2.1769693
 1.959835   1.5423315  1.3491613  1.3276445  1.4843297  1.13034
 0.78904575 0.4793657  0.28018013 0.33792454 0.54697853 0.6888315
 0.9154753  1.0817784  1.2717785  1.4550719  1.7789414  2.0988894
 2.248799   2.406977   2.639622   2.8099298  2.907417   2.9769874
 3.0377333  3.2036424  3.4282365  2.692144   2.2040527  1.8672149
 1.6648216  1.5855223  1.7838931  2.2307987  2.7693691  1.9264929
 1.1958048  0.7740328  0.7290824  0.7935036  0.8381024  0.82003325
 0.76250386 0.8461798  1.111524   1.5440223  2.073038   2.3107316
 2.5320728  2.7635138  2.9616125  2.7165365  2.279163   1.9192964
 1.6740699  1.6187083  1.7883496  1.9668117  1.8991281  1.7291043
 1.7092861  1.8770442  1.7768478  1.3738374  1.2269464  1.2648635
 1.3686054  1.0338871  0.795825   0.5242336  0.32948342 0.40478393
 0.5721177  0.73969203 0.9495588  1.1455278  1.4051875  1.6174558
 1.8578526  2.2287977  2.5540655  2.7953897  2.9918833  3.082405
 3.0978942  3.097863   3.0682566  3.119271   3.2343335  3.117866
 2.5047688  2.0461798  1.6727345  1.5662408  1.6803246  2.0628178
 2.5889764  1.8296968  1.160318   0.87091273 0.88670206 0.96188056
 0.96707195 0.9375867  0.83759296 0.98714966 1.3160628  1.7466108
 2.3230486  2.6548057  2.8875911  3.0891147  3.229082   3.1912882
 2.727597   2.2605228  1.8616449  1.6068472  1.6271567  1.7242795
 1.7088253  1.5450788  1.4701818  1.5327461  1.673392   1.2843391
 1.172675   1.2699491  1.2667933  1.0657142  0.93760526 0.8026633
 0.6516263  0.61526823 0.64762115 0.7952816  0.9853462  1.2516652
 1.5811228  1.8814924  2.0893054  2.4061272  2.792287   3.1790442
 3.4832456  3.5447695  3.4824126  3.3679035  3.2585833  3.1888578
 3.166073   3.3740706  2.9205103  2.2641811  1.8096628  1.5792832
 1.5865527  1.8609881  2.3131902  1.8006444  1.1989539  1.0046816
 1.0131723  1.1003826  1.0793008  1.086048   0.9659287  1.0681379
 1.4859997  1.9932362  2.5400326  3.010581   3.3064702  3.5139675
 3.621199   3.5000336  3.3020504  2.7388387  2.1907032  1.7204987
 1.5070015  1.4832251  1.4407252  1.3744786  1.2901522  1.3320966
 1.7969942  1.3541758  1.2452978  1.3184947  1.1771438  0.9952325
 0.9739345  0.98936075 0.94706285 0.8186271  0.7274523  0.8009035
 0.99366605 1.3296866  1.7734805  2.1739092  2.4742172  2.7278666
 3.0150256  3.2967594  3.7171881  4.0808277  4.068737   3.809366
 3.6163542  3.438808   3.2688043  3.3365595  3.4868505  2.6343932
 2.0681202  1.6914977  1.5416298  1.6963999  2.0622606  2.0294936
 1.385782   1.1933188  1.1490254  1.1990105  1.2478027  1.2059668
 1.1579579  1.2356141  1.5738444  2.1876774  2.8059688  3.331095
 3.7476223  4.011987   4.1322546  4.027987   3.7953086  3.3845022
 2.7096744  2.0440464  1.5351647  1.2975974  1.1831269  1.2593565
 1.1622057  1.1601535  1.6697822  1.45326    1.2463776  1.313545
 1.2066001  0.92863387 0.98827237 1.0852944  1.0700493  1.0119095
 1.0189991  0.9439199  1.1184281  1.4622848  1.985929   2.5080404
 2.6631947  2.6922655  2.7583516  3.0083878  3.3541794  3.858153
 4.161025   4.074397   4.0202694  3.883099   3.5750606  3.4646728
 3.4489822  3.2614355  2.497991   1.9392252  1.6542091  1.6805766
 1.9116932  2.2287407  1.6591576  1.5187794  1.5669582  1.5093055
 1.3968912  1.3388648  1.3680338  1.4714524  1.8107336  2.2696273
 3.012498   3.5893333  4.1601686  4.5252337  4.7222085  4.727019
 4.3816013  4.153193   3.4580429  2.64254    1.8646667  1.284268
 1.0429894  1.0917643  1.0252548  0.9871675  1.3739349  1.6155329
 1.308349   1.3341631  1.308598   0.8996694  0.9443743  1.1164452
 1.1282498  1.0931444  1.1572425  1.149248   1.1789247  1.5083411
 2.1191025  2.452307   2.693018   2.757925   2.7945232  2.9033675
 3.183221   3.6197503  4.056669   4.2768116  4.11078    3.9467118
 3.910366   3.8200445  3.6383662  3.4040027  3.195706   2.4248784
 1.970411   1.8025454  1.8413186  2.0101576  2.0427122  1.685715
 1.7936645  1.8652813  1.7498411  1.6017824  1.446685   1.5800018
 1.9592719  2.541285   3.1029298  3.860677   4.3123713  4.661227
 5.025882   5.452042   5.2476687  4.846253   4.466921   3.552179
 2.5882483  1.7024558  1.1760311  0.9036895  0.93213713 0.84059846
 1.0619447  1.7179314  1.4670718  1.2955855  1.3680402  0.85470706
 0.88728666 1.1632028  1.3241313  1.2128175  1.1427238  1.2576394
 1.3285482  1.5732616  1.9269187  2.2515721  2.620821   2.9278545
 3.0637257  3.0752754  3.074297   3.342008  ]
