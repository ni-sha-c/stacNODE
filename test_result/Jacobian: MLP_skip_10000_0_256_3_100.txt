time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.57%, model saved.
Epoch: 0 Train: 9520.06543 Test: 3966.30298
Epoch: 100 Train: 2682.75244 Test: 1118.77722
Epoch: 200 Train: 2322.42432 Test: 913.30371
Epoch: 300 Train: 1240.05652 Test: 538.42712
Epoch: 400 Train: 434.62067 Test: 88.50507
Epoch 500: New minimal relative error: 32.43%, model saved.
Epoch: 500 Train: 226.65247 Test: 33.76313
Epoch 600: New minimal relative error: 9.84%, model saved.
Epoch: 600 Train: 158.41443 Test: 18.62057
Epoch: 700 Train: 115.16821 Test: 14.26618
Epoch: 800 Train: 110.23450 Test: 14.47566
Epoch: 900 Train: 66.05819 Test: 5.89268
Epoch 1000: New minimal relative error: 3.39%, model saved.
Epoch: 1000 Train: 57.44312 Test: 5.00690
Epoch: 1100 Train: 51.28249 Test: 6.14715
Epoch: 1200 Train: 45.70378 Test: 4.89833
Epoch: 1300 Train: 41.66723 Test: 5.06462
Epoch: 1400 Train: 37.29689 Test: 2.82763
Epoch: 1500 Train: 35.17185 Test: 2.64189
Epoch: 1600 Train: 33.44218 Test: 3.30515
Epoch: 1700 Train: 31.02336 Test: 2.25807
Epoch: 1800 Train: 32.16169 Test: 5.92830
Epoch: 1900 Train: 27.87561 Test: 2.01168
Epoch: 2000 Train: 26.53955 Test: 2.42647
Epoch: 2100 Train: 25.07264 Test: 1.78107
Epoch 2200: New minimal relative error: 2.12%, model saved.
Epoch: 2200 Train: 24.17303 Test: 1.69206
Epoch: 2300 Train: 27.83784 Test: 8.14042
Epoch: 2400 Train: 22.51068 Test: 1.48950
Epoch: 2500 Train: 22.06251 Test: 1.39144
Epoch: 2600 Train: 22.08424 Test: 1.43231
Epoch: 2700 Train: 20.91886 Test: 1.28931
Epoch: 2800 Train: 20.74126 Test: 1.38568
Epoch 2900: New minimal relative error: 1.69%, model saved.
Epoch: 2900 Train: 20.04508 Test: 1.21090
Epoch: 3000 Train: 19.15791 Test: 1.12654
Epoch: 3100 Train: 19.40280 Test: 1.31173
Epoch 3200: New minimal relative error: 1.45%, model saved.
Epoch: 3200 Train: 18.61329 Test: 1.15656
Epoch: 3300 Train: 19.75429 Test: 3.20070
Epoch: 3400 Train: 17.81596 Test: 1.32359
Epoch: 3500 Train: 17.26033 Test: 1.06176
Epoch: 3600 Train: 16.77750 Test: 1.02564
Epoch: 3700 Train: 16.62439 Test: 1.00819
Epoch: 3800 Train: 16.22590 Test: 0.97308
Epoch: 3900 Train: 15.84908 Test: 0.97253
Epoch: 4000 Train: 15.49132 Test: 0.94076
Epoch: 4100 Train: 15.60363 Test: 1.42942
Epoch: 4200 Train: 15.03341 Test: 0.93922
Epoch: 4300 Train: 15.10885 Test: 0.83669
Epoch: 4400 Train: 14.51742 Test: 0.78232
Epoch: 4500 Train: 14.33159 Test: 0.84283
Epoch: 4600 Train: 13.90454 Test: 0.74621
Epoch: 4700 Train: 13.72648 Test: 0.74501
Epoch: 4800 Train: 13.65959 Test: 0.83267
Epoch: 4900 Train: 13.14046 Test: 0.68917
Epoch: 5000 Train: 12.89660 Test: 0.68461
Epoch: 5100 Train: 12.73990 Test: 0.70153
Epoch 5200: New minimal relative error: 1.17%, model saved.
Epoch: 5200 Train: 13.07829 Test: 0.69704
Epoch: 5300 Train: 12.97878 Test: 0.73351
Epoch: 5400 Train: 12.70219 Test: 0.66342
Epoch: 5500 Train: 12.62126 Test: 0.83121
Epoch: 5600 Train: 12.54753 Test: 0.67481
Epoch: 5700 Train: 12.93689 Test: 1.17994
Epoch: 5800 Train: 12.43351 Test: 0.68239
Epoch: 5900 Train: 12.52171 Test: 0.67701
Epoch: 6000 Train: 12.66221 Test: 0.79824
Epoch: 6100 Train: 12.31979 Test: 0.72246
Epoch: 6200 Train: 12.44914 Test: 1.16629
Epoch: 6300 Train: 12.73450 Test: 1.17594
Epoch: 6400 Train: 12.26913 Test: 1.29902
Epoch: 6500 Train: 11.59050 Test: 0.99715
Epoch: 6600 Train: 11.01142 Test: 0.56364
Epoch: 6700 Train: 10.85577 Test: 0.61785
Epoch: 6800 Train: 10.72382 Test: 0.52042
Epoch: 6900 Train: 10.58276 Test: 0.52152
Epoch: 7000 Train: 10.44886 Test: 0.47911
Epoch: 7100 Train: 10.48748 Test: 0.49678
Epoch: 7200 Train: 10.47171 Test: 0.51050
Epoch: 7300 Train: 10.23388 Test: 0.44945
Epoch: 7400 Train: 10.12278 Test: 0.45910
Epoch: 7500 Train: 9.90769 Test: 0.42997
Epoch: 7600 Train: 9.82885 Test: 0.42231
Epoch: 7700 Train: 9.84627 Test: 0.42239
Epoch: 7800 Train: 9.76545 Test: 0.42178
Epoch: 7900 Train: 9.94229 Test: 0.46055
Epoch: 8000 Train: 9.78829 Test: 0.42337
Epoch: 8100 Train: 9.95579 Test: 0.44850
Epoch: 8200 Train: 9.82623 Test: 0.39896
Epoch: 8300 Train: 9.83906 Test: 0.43516
Epoch: 8400 Train: 9.59210 Test: 0.38914
Epoch: 8500 Train: 9.48048 Test: 0.39132
Epoch: 8600 Train: 9.38120 Test: 0.39720
Epoch: 8700 Train: 9.12342 Test: 0.39492
Epoch: 8800 Train: 9.04842 Test: 0.40443
Epoch: 8900 Train: 8.99439 Test: 0.44275
Epoch 9000: New minimal relative error: 0.89%, model saved.
Epoch: 9000 Train: 8.77772 Test: 0.40012
Epoch: 9100 Train: 8.72233 Test: 0.39457
Epoch 9200: New minimal relative error: 0.74%, model saved.
Epoch: 9200 Train: 8.68699 Test: 0.39148
Epoch: 9300 Train: 8.65263 Test: 0.39254
Epoch: 9400 Train: 9.87091 Test: 1.77540
Epoch: 9500 Train: 8.60993 Test: 0.39608
Epoch: 9600 Train: 8.47117 Test: 0.38488
Epoch: 9700 Train: 8.61536 Test: 0.44187
Epoch: 9800 Train: 8.50789 Test: 0.37970
Epoch: 9900 Train: 8.43357 Test: 0.38775
Epoch: 9999 Train: 9.06779 Test: 0.97883
Training Loss: tensor(9.0678)
Test Loss: tensor(0.9788)
Learned LE: [ 8.4248579e-01  7.5469352e-03 -1.4513058e+01]
True LE: [ 8.6252856e-01 -3.5234098e-03 -1.4536466e+01]
Relative Error: [1.0705082  1.0392966  0.9697398  0.76531714 0.48533672 0.2748338
 0.22574726 0.6026046  0.7921312  0.64663917 0.25068855 0.03500915
 0.1567827  0.2881653  0.16436185 0.24331145 0.22293746 0.33639097
 0.610267   0.67095345 0.67470574 0.54712486 0.34917387 0.308744
 0.25484556 0.22951986 0.2750107  0.45604283 0.48465565 0.46655494
 0.47061798 0.45103273 0.5692745  0.79707354 0.8214414  0.9091758
 0.8681639  0.68682635 0.5416786  0.5590557  0.4920827  0.40386385
 0.35094857 0.25318325 0.33723313 0.32865068 0.5217319  0.42106324
 0.26795933 0.11502055 0.419197   0.7580794  0.91184366 0.96767974
 1.0658042  1.0122529  0.8040129  0.6445893  0.6114973  0.7625677
 0.78965557 1.0358157  1.0601686  1.0581136  0.94378555 0.74603504
 0.5225973  0.26061848 0.08529217 0.3710851  0.7648789  0.77830064
 0.3306327  0.0684642  0.16620068 0.21520236 0.1304287  0.2702373
 0.12505579 0.4018282  0.61880124 0.6609797  0.596081   0.49780068
 0.3154227  0.29525355 0.24745697 0.20646597 0.4400406  0.671241
 0.73444605 0.65854675 0.48193032 0.40385658 0.42763925 0.58550406
 0.7729907  0.69657606 0.7550876  0.7049847  0.5657529  0.54495406
 0.49151176 0.39915517 0.35198256 0.28207496 0.38354188 0.3259852
 0.40784684 0.47939116 0.30105972 0.09516045 0.38568953 0.6731871
 0.8285584  0.802477   0.7937545  0.80830604 0.6251445  0.40353256
 0.33061266 0.5098942  0.61366916 0.9082754  0.98528063 1.038614
 0.88188446 0.71257055 0.6020888  0.27782393 0.28096023 0.36904067
 0.7235038  0.692935   0.40626022 0.12809914 0.16798747 0.19102909
 0.16001436 0.3179494  0.15430368 0.4415954  0.57690954 0.5847378
 0.4541322  0.29766575 0.2676091  0.14645156 0.23375033 0.38644212
 0.5939457  0.8292187  0.9740442  0.89017403 0.7025588  0.6000794
 0.48749447 0.42083547 0.6440098  0.5249727  0.5796639  0.64725184
 0.57566434 0.5455187  0.57724726 0.46592247 0.4235834  0.2933194
 0.42867115 0.3557408  0.36061144 0.5635879  0.35727167 0.06103832
 0.34175718 0.5811308  0.6788135  0.65964913 0.5416226  0.47507244
 0.33242008 0.1375592  0.09289699 0.27303562 0.455754   0.73375803
 0.8528789  0.9786831  0.819242   0.65892977 0.6851023  0.40393934
 0.43408945 0.42597777 0.5034655  0.5824987  0.5385167  0.21963935
 0.19210137 0.20433275 0.22371763 0.38518175 0.28066453 0.46831003
 0.5366585  0.4618548  0.26972854 0.08463376 0.10202869 0.28054228
 0.44511062 0.500503   0.6322504  0.82892954 1.0638665  1.0195001
 0.86003876 0.7649868  0.6522399  0.51288575 0.41735935 0.6272806
 0.38888466 0.49522817 0.5489421  0.5648159  0.47835135 0.58816534
 0.5195836  0.29863223 0.43525052 0.40378654 0.28572747 0.5819295
 0.4233264  0.15562026 0.35544446 0.53079927 0.5697515  0.4831156
 0.34255102 0.3193321  0.27282628 0.3472868  0.42645118 0.47652292
 0.6247798  0.64238465 0.7145581  0.85993975 0.67689896 0.57395464
 0.6855021  0.55083746 0.48233825 0.4548648  0.24776587 0.46195436
 0.5507087  0.3247515  0.2424953  0.2303939  0.31184912 0.4927113
 0.466135   0.6116568  0.64948153 0.48451182 0.22182845 0.17397656
 0.2525085  0.5307239  0.60573965 0.63582563 0.58478945 0.6769828
 0.9126319  1.0353631  0.89479834 0.8018282  0.76791286 0.63953364
 0.35011998 0.32748562 0.5562627  0.32663575 0.43121326 0.5513313
 0.4338074  0.50748837 0.5935208  0.3018955  0.41774595 0.5022666
 0.24908763 0.44203135 0.4795456  0.31140533 0.42060822 0.6092207
 0.60830766 0.4875495  0.31881618 0.4738762  0.48534465 0.45186305
 0.5809612  0.8115361  1.0333133  0.85729426 0.7467161  0.65763414
 0.505219   0.38469058 0.53142947 0.6037673  0.4204736  0.5473826
 0.37909168 0.14567564 0.35917616 0.46365938 0.30130726 0.21920718
 0.31086162 0.5240992  0.61650115 0.7848396  0.8126353  0.64421344
 0.31927752 0.1498434  0.2720375  0.60657233 0.75146204 0.70893115
 0.624522   0.4929825  0.56473154 0.8389919  0.80406505 0.68237746
 0.65374213 0.5793592  0.3842559  0.22194985 0.24697888 0.51183426
 0.37102726 0.44082972 0.469904   0.30610168 0.69883573 0.44022924
 0.4204817  0.6723419  0.54215497 0.09004281 0.32444146 0.4574669
 0.52477705 0.7062196  0.7642442  0.70800334 0.64215183 0.61407816
 0.6124041  0.51633173 0.52007335 0.6747695  0.8686099  1.2142512
 1.140575   0.7055295  0.5318735  0.25835192 0.24203517 0.5179615
 0.4266501  0.56818944 0.58243304 0.2604337  0.2959321  0.41607198
 0.49439844 0.2905296  0.3685135  0.5671681  0.6096736  0.6230124
 0.7103303  0.6631712  0.5438593  0.43931895 0.30246195 0.4474766
 0.6256756  0.61813563 0.49497527 0.43419605]
