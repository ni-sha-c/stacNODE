time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 3856.17700 Test: 4267.31445
Epoch: 80 Train: 377.45508 Test: 383.79123
Epoch 160: New minimal relative error: 53.27%, model saved.
Epoch: 160 Train: 54.79279 Test: 42.66475
Epoch 240: New minimal relative error: 38.88%, model saved.
Epoch: 240 Train: 4.79021 Test: 6.45511
Epoch 320: New minimal relative error: 27.33%, model saved.
Epoch: 320 Train: 24.19753 Test: 22.10247
Epoch: 400 Train: 16.64213 Test: 20.94136
Epoch: 480 Train: 2.31331 Test: 1.79803
Epoch 560: New minimal relative error: 12.96%, model saved.
Epoch: 560 Train: 3.68574 Test: 1.69770
Epoch: 640 Train: 7.67357 Test: 11.51492
Epoch 720: New minimal relative error: 10.80%, model saved.
Epoch: 720 Train: 3.96141 Test: 5.44148
Epoch: 800 Train: 6.09549 Test: 5.49338
Epoch: 880 Train: 4.67430 Test: 7.09848
Epoch: 960 Train: 4.14064 Test: 2.41109
Epoch: 1040 Train: 2.51684 Test: 3.98991
Epoch: 1120 Train: 1.81976 Test: 2.49630
Epoch: 1200 Train: 1.16706 Test: 1.46974
Epoch: 1280 Train: 1.87307 Test: 2.37424
Epoch: 1360 Train: 1.51788 Test: 1.82918
Epoch: 1440 Train: 2.54996 Test: 3.71003
Epoch: 1520 Train: 1.06842 Test: 1.09850
Epoch: 1600 Train: 1.95313 Test: 2.87797
Epoch: 1680 Train: 1.13132 Test: 1.43326
Epoch 1760: New minimal relative error: 9.76%, model saved.
Epoch: 1760 Train: 0.59400 Test: 1.38078
Epoch: 1840 Train: 0.73510 Test: 0.94527
Epoch: 1920 Train: 6.00800 Test: 6.21039
Epoch: 2000 Train: 0.71635 Test: 1.06964
Epoch: 2080 Train: 0.94023 Test: 0.99414
Epoch: 2160 Train: 0.31045 Test: 0.31768
Epoch: 2240 Train: 0.81662 Test: 1.08055
Epoch 2320: New minimal relative error: 7.52%, model saved.
Epoch: 2320 Train: 1.70556 Test: 1.92472
Epoch: 2400 Train: 0.77125 Test: 1.14710
Epoch: 2480 Train: 2.59098 Test: 2.43129
Epoch: 2560 Train: 0.74886 Test: 0.94625
Epoch: 2640 Train: 1.31379 Test: 1.20440
Epoch: 2720 Train: 0.42730 Test: 0.26647
Epoch: 2800 Train: 0.30166 Test: 0.24445
Epoch 2880: New minimal relative error: 7.46%, model saved.
Epoch: 2880 Train: 0.23593 Test: 0.28777
Epoch: 2960 Train: 0.15347 Test: 0.16245
Epoch: 3040 Train: 0.09553 Test: 0.12571
Epoch: 3120 Train: 0.19509 Test: 0.20705
Epoch 3200: New minimal relative error: 7.12%, model saved.
Epoch: 3200 Train: 0.60327 Test: 1.04061
Epoch: 3280 Train: 0.33299 Test: 0.55032
Epoch: 3360 Train: 0.14110 Test: 0.25265
Epoch 3440: New minimal relative error: 6.14%, model saved.
Epoch: 3440 Train: 0.15062 Test: 0.15685
Epoch: 3520 Train: 0.07823 Test: 0.09189
Epoch: 3600 Train: 0.07144 Test: 0.08757
Epoch: 3680 Train: 0.14231 Test: 0.15991
Epoch: 3760 Train: 0.16105 Test: 0.19840
Epoch: 3840 Train: 0.25278 Test: 0.30866
Epoch: 3920 Train: 1.74362 Test: 2.34740
Epoch: 4000 Train: 0.30164 Test: 0.39862
Epoch: 4080 Train: 0.19885 Test: 0.25919
Epoch: 4160 Train: 0.15935 Test: 0.20375
Epoch: 4240 Train: 1.74730 Test: 2.30161
Epoch: 4320 Train: 0.25465 Test: 0.42684
Epoch: 4400 Train: 0.68044 Test: 0.84251
Epoch: 4480 Train: 0.63692 Test: 0.75847
Epoch: 4560 Train: 1.26385 Test: 1.73776
Epoch: 4640 Train: 0.21337 Test: 0.32594
Epoch: 4720 Train: 0.21073 Test: 0.27637
Epoch 4800: New minimal relative error: 5.72%, model saved.
Epoch: 4800 Train: 0.30464 Test: 0.35446
Epoch: 4880 Train: 0.45060 Test: 0.41614
Epoch: 4960 Train: 0.09052 Test: 0.11282
Epoch: 5040 Train: 0.10228 Test: 0.11388
Epoch: 5120 Train: 0.38383 Test: 0.45013
Epoch: 5200 Train: 0.28804 Test: 0.34857
Epoch: 5280 Train: 0.17276 Test: 0.12555
Epoch: 5360 Train: 0.34668 Test: 0.26843
Epoch: 5440 Train: 0.03297 Test: 0.04464
Epoch: 5520 Train: 0.03990 Test: 0.04957
Epoch: 5600 Train: 0.16587 Test: 0.19039
Epoch: 5680 Train: 0.68727 Test: 0.65278
Epoch: 5760 Train: 0.13023 Test: 0.21362
Epoch 5840: New minimal relative error: 5.20%, model saved.
Epoch: 5840 Train: 0.18724 Test: 0.21027
Epoch: 5920 Train: 0.04148 Test: 0.05918
Epoch 6000: New minimal relative error: 2.90%, model saved.
Epoch: 6000 Train: 0.05106 Test: 0.07521
Epoch: 6080 Train: 0.14464 Test: 0.12931
Epoch: 6160 Train: 0.16802 Test: 0.22157
Epoch: 6240 Train: 0.45167 Test: 0.59510
Epoch: 6320 Train: 0.45552 Test: 0.64878
Epoch: 6400 Train: 0.09890 Test: 0.10080
Epoch: 6480 Train: 0.04976 Test: 0.07472
Epoch: 6560 Train: 0.06846 Test: 0.04431
Epoch: 6640 Train: 0.12096 Test: 0.15634
Epoch: 6720 Train: 0.67891 Test: 0.79100
Epoch: 6800 Train: 0.10077 Test: 0.11763
Epoch: 6880 Train: 0.07148 Test: 0.07114
Epoch: 6960 Train: 0.03485 Test: 0.04953
Epoch: 7040 Train: 0.02552 Test: 0.03654
Epoch: 7120 Train: 0.05047 Test: 0.07517
Epoch: 7200 Train: 0.03239 Test: 0.05476
Epoch: 7280 Train: 0.08744 Test: 0.07199
Epoch: 7360 Train: 0.09477 Test: 0.08684
Epoch: 7440 Train: 0.25448 Test: 0.28902
Epoch: 7520 Train: 0.04379 Test: 0.04832
Epoch: 7600 Train: 0.12643 Test: 0.17694
Epoch: 7680 Train: 0.22115 Test: 0.25631
Epoch: 7760 Train: 0.02451 Test: 0.03454
Epoch: 7840 Train: 0.02480 Test: 0.03675
Epoch: 7920 Train: 0.02230 Test: 0.03239
Epoch: 7999 Train: 0.02180 Test: 0.03291
Training Loss: tensor(0.0218)
Test Loss: tensor(0.0329)
Learned LE: [ 0.83569354  0.01623854 -4.252906  ]
True LE: [ 8.9022386e-01 -3.4178088e-03 -1.4565269e+01]
Relative Error: [4.1231647  4.3453717  4.170397   3.8503296  4.2737517  4.18312
 4.5909777  4.602053   3.84426    3.2809415  3.1265576  2.9606297
 2.7031155  2.977529   3.4933898  4.207175   4.6664095  4.7648954
 4.8316994  5.151679   5.4830976  5.283261   5.257423   4.9598455
 4.667574   4.3606653  4.115504   3.7311647  4.098798   3.4382408
 2.7169287  2.2633944  2.0591962  2.4158766  3.6335814  4.395856
 4.4930267  3.9437573  3.3899293  2.887871   2.22963    1.8685275
 1.7283165  1.9155072  1.9699829  2.0141366  2.1073406  2.1023464
 2.071909   2.1161962  2.274042   2.5661144  2.5840986  2.2769911
 2.207484   2.53929    3.0471935  2.605538   2.3002772  2.1953976
 2.3514001  2.599197   3.0754492  3.257738   3.3253129  2.9287477
 3.2072306  3.2588012  3.6814198  3.5257556  2.7746198  2.3376942
 2.1928253  2.1555815  1.9280185  1.9230857  2.5994313  3.1714294
 3.4820776  3.5084457  3.555727   3.8272269  4.3433394  3.9580085
 3.7696402  3.6912136  3.583459   3.3516235  3.1054876  2.933115
 2.8962052  2.924316   2.1989477  1.7152125  1.5968673  1.5551283
 2.5798826  3.508128   3.2122846  2.959338   2.5810826  2.0305972
 1.4999435  1.3660302  1.3320444  1.5110482  1.4239146  1.4765114
 1.6543733  1.6442508  1.538541   1.5369843  1.5053538  1.7803324
 2.0731637  1.7903093  1.7240067  2.0131278  2.4427104  2.3876286
 1.7258178  1.481357   1.4882809  1.6518381  2.0647259  2.4342701
 2.6114876  2.3998828  2.19788    2.3549628  2.7108912  2.6499045
 1.8792983  1.5815278  1.4880584  1.4895566  1.599682   1.4701841
 1.9062258  2.2668145  2.4417434  2.4054286  2.2948787  2.5051565
 2.9023643  2.65243    2.5866382  2.5202909  2.3776264  2.170195
 2.1785371  2.0014834  1.6278265  2.0726907  1.8609861  1.4076995
 1.3757503  1.1837493  1.7022212  2.7372983  2.3518865  2.2295377
 1.9464887  1.3788201  0.8857428  0.8439339  0.866712   1.0815803
 0.9974447  1.1066939  1.1638935  1.242233   1.1869293  1.063498
 0.98182034 1.1205943  1.5888376  1.410617   1.3413146  1.4920422
 1.8782701  1.9776635  1.6041962  1.116301   0.8065192  0.9650717
 1.2380303  1.7118976  1.9435793  1.8532166  1.5013489  1.5425514
 1.8625364  1.8731018  1.0865052  0.9910964  1.0534978  0.95627373
 1.1605456  1.4615227  1.5777276  1.5694404  1.5481569  1.5225357
 1.3775094  1.3915168  1.6892562  1.7425846  1.56872    1.522704
 1.3049015  1.2329892  1.3114531  1.2089965  1.0523838  1.019037
 1.4696895  1.0636667  1.1227756  0.95039606 1.0190657  1.7797695
 1.7020276  1.5538774  1.4456326  1.0261732  0.6052934  0.6552734
 0.7224603  0.83885705 0.72291493 0.7376476  0.7113183  0.7604713
 0.83482367 0.83677864 0.7854865  0.75102156 0.90988755 1.0184472
 0.9508213  1.0280023  1.2840519  1.4344345  1.4008352  0.96393514
 0.6299863  0.5574063  0.7006755  1.1220751  1.413069   1.3804191
 1.0296986  0.81175536 1.1685225  1.1373053  0.7696181  0.83371305
 0.80791277 0.6891237  0.84064555 1.1380521  1.280542   1.1758518
 0.96480334 0.8505061  0.8641135  0.8601227  0.9578596  1.1438457
 0.95845217 0.957814   0.7087634  0.53414255 0.6023467  0.7548107
 0.6589056  0.755199   0.8677041  0.98648846 0.8280463  0.83982897
 0.66080755 1.0524184  1.2019773  0.9781258  0.97998214 0.9329157
 0.51663756 0.62764573 0.61862063 0.6067432  0.5875553  0.49945843
 0.4870674  0.47954038 0.56380355 0.61115694 0.61813056 0.5546029
 0.46655726 0.53442687 0.50310165 0.5980052  0.67532426 0.94380325
 0.91935045 0.7778752  0.7375233  0.3052773  0.5163656  0.590209
 0.9498633  1.0293152  0.6687531  0.47901878 0.7551472  0.9335685
 0.8894228  0.74313676 0.95513797 0.79974234 0.7076325  0.92031956
 0.92991817 1.0501149  0.9070797  0.8324764  0.8779829  0.7363957
 0.77995425 0.7214051  0.8163886  0.99104905 1.0171059  0.6862816
 0.48853177 0.45117518 0.7161143  0.80143297 0.6884211  0.7688278
 0.9541023  0.7722235  0.88428587 0.5205559  0.78989637 0.60575193
 0.7948781  0.89221513 0.6773305  0.6697522  0.62502116 0.56135136
 0.6412984  0.37780315 0.4013194  0.38480425 0.47402856 0.46206647
 0.42403388 0.42262316 0.36529043 0.32211888 0.26949453 0.27022412
 0.35728887 0.52136016 0.6805481  0.40204036 0.3558583  0.5002244
 0.25849164 0.4282774  0.5454902  0.7002923  0.62501377 0.38744056
 0.5368739  0.942846   0.91321737 0.82009506 0.7670676  0.8445892
 0.5560137  0.69788903 0.96421456 0.75633544 0.81283647 0.8134422
 0.8116075  0.71765244 0.7759237  0.7771275  0.8962211  0.87068444
 0.884784   0.8577697  0.6867833  0.6698621 ]
