time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 8000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 512
n_layers: 3
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 127018.351562500 Test: 47890.816406250
Epoch 0: New minimal relative error: 47890.82%, model saved.
Epoch: 100 Train: 7581.971679688 Test: 6864.421875000
Epoch 100: New minimal relative error: 6864.42%, model saved.
Epoch: 200 Train: 2673.348144531 Test: 2146.500732422
Epoch 200: New minimal relative error: 2146.50%, model saved.
Epoch: 300 Train: 2719.982421875 Test: 1999.851196289
Epoch 300: New minimal relative error: 1999.85%, model saved.
Epoch: 400 Train: 2248.976318359 Test: 1762.817016602
Epoch 400: New minimal relative error: 1762.82%, model saved.
Epoch: 500 Train: 2108.860839844 Test: 1534.081787109
Epoch 500: New minimal relative error: 1534.08%, model saved.
Epoch: 600 Train: 1996.520263672 Test: 1412.877441406
Epoch 600: New minimal relative error: 1412.88%, model saved.
Epoch: 700 Train: 1895.899169922 Test: 1269.724365234
Epoch 700: New minimal relative error: 1269.72%, model saved.
Epoch: 800 Train: 1834.920043945 Test: 1202.382080078
Epoch 800: New minimal relative error: 1202.38%, model saved.
Epoch: 900 Train: 1781.747192383 Test: 1138.059692383
Epoch 900: New minimal relative error: 1138.06%, model saved.
Epoch: 1000 Train: 1760.560668945 Test: 1096.549194336
Epoch 1000: New minimal relative error: 1096.55%, model saved.
Epoch: 1100 Train: 1761.983398438 Test: 1088.740478516
Epoch 1100: New minimal relative error: 1088.74%, model saved.
Epoch: 1200 Train: 1830.221679688 Test: 1128.215820312
Epoch: 1300 Train: 1768.050537109 Test: 1062.693237305
Epoch 1300: New minimal relative error: 1062.69%, model saved.
Epoch: 1400 Train: 1743.145385742 Test: 1043.149658203
Epoch 1400: New minimal relative error: 1043.15%, model saved.
Epoch: 1500 Train: 1740.633300781 Test: 1040.949340820
Epoch 1500: New minimal relative error: 1040.95%, model saved.
Epoch: 1600 Train: 1737.156127930 Test: 1037.623779297
Epoch 1600: New minimal relative error: 1037.62%, model saved.
Epoch: 1700 Train: 1733.918212891 Test: 1033.385742188
Epoch 1700: New minimal relative error: 1033.39%, model saved.
Epoch: 1800 Train: 1768.544067383 Test: 1113.135498047
Epoch: 1900 Train: 1883.300292969 Test: 1148.066772461
Epoch: 2000 Train: 1747.769042969 Test: 1035.569335938
Epoch: 2100 Train: 1863.159423828 Test: 1066.728881836
Epoch: 2200 Train: 1751.926513672 Test: 1044.252441406
Epoch: 2300 Train: 1779.648681641 Test: 1079.378173828
Epoch: 2400 Train: 1752.492431641 Test: 1040.956909180
Epoch: 2500 Train: 1755.031738281 Test: 1038.775512695
Epoch: 2600 Train: 1756.457031250 Test: 1055.975952148
Epoch: 2700 Train: 1782.410034180 Test: 1062.681152344
Epoch: 2800 Train: 1763.071777344 Test: 1052.191284180
Epoch: 2900 Train: 1776.342651367 Test: 1057.679321289
Epoch: 3000 Train: 1764.062744141 Test: 1032.171752930
Epoch 3000: New minimal relative error: 1032.17%, model saved.
Epoch: 3100 Train: 1746.338867188 Test: 1027.571166992
Epoch 3100: New minimal relative error: 1027.57%, model saved.
Epoch: 3200 Train: 1823.185546875 Test: 1107.311035156
Epoch: 3300 Train: 1752.271240234 Test: 1036.144165039
Epoch: 3400 Train: 1746.643310547 Test: 1036.284545898
Epoch: 3500 Train: 1749.418212891 Test: 1042.921020508
Epoch: 3600 Train: 1759.621337891 Test: 1044.188720703
Epoch: 3700 Train: 1782.951049805 Test: 1075.853637695
Epoch: 3800 Train: 1759.207031250 Test: 1043.115600586
Epoch: 3900 Train: 1758.189697266 Test: 1035.296875000
Epoch: 4000 Train: 1771.046386719 Test: 1042.856689453
Epoch: 4100 Train: 1857.333251953 Test: 1122.339233398
Epoch: 4200 Train: 1772.192626953 Test: 1037.523559570
Epoch: 4300 Train: 1773.961669922 Test: 1041.765625000
Epoch: 4400 Train: 1779.102050781 Test: 1050.383056641
Epoch: 4500 Train: 1837.274414062 Test: 1095.594726562
Epoch: 4600 Train: 1794.675170898 Test: 1070.338134766
Epoch: 4700 Train: 1775.504638672 Test: 1041.601806641
Epoch: 4800 Train: 1775.840332031 Test: 1051.675781250
Epoch: 4900 Train: 1764.798583984 Test: 1040.062255859
Epoch: 5000 Train: 1770.085937500 Test: 1048.621215820
Epoch: 5100 Train: 1770.354614258 Test: 1045.527954102
Epoch: 5200 Train: 1796.752441406 Test: 1066.609497070
Epoch: 5300 Train: 1803.644042969 Test: 1062.331176758
Epoch: 5400 Train: 1789.733276367 Test: 1054.588989258
Epoch: 5500 Train: 1778.161132812 Test: 1047.415527344
Epoch: 5600 Train: 1789.499267578 Test: 1064.146850586
Epoch: 5700 Train: 1764.974365234 Test: 1043.230590820
Epoch: 5800 Train: 1770.088867188 Test: 1046.530639648
Epoch: 5900 Train: 1758.234008789 Test: 1035.493408203
Epoch: 6000 Train: 1796.862548828 Test: 1073.837402344
Epoch: 6100 Train: 1762.611816406 Test: 1036.368164062
Epoch: 6200 Train: 1763.750244141 Test: 1041.306762695
Epoch: 6300 Train: 1760.787353516 Test: 1040.146728516
Epoch: 6400 Train: 1753.212158203 Test: 1035.070434570
Epoch: 6500 Train: 1750.431152344 Test: 1037.751708984
Epoch: 6600 Train: 1745.970092773 Test: 1032.217651367
Epoch: 6700 Train: 1746.738647461 Test: 1034.289550781
Epoch: 6800 Train: 1750.071533203 Test: 1038.258789062
Epoch: 6900 Train: 1765.977416992 Test: 1057.656982422
Epoch: 7000 Train: 1741.589965820 Test: 1029.478393555
Epoch: 7100 Train: 1747.754882812 Test: 1038.282104492
Epoch: 7200 Train: 1744.084228516 Test: 1033.640014648
Epoch: 7300 Train: 1741.600341797 Test: 1031.234985352
Epoch: 7400 Train: 1740.032592773 Test: 1031.912353516
Epoch: 7500 Train: 1745.503295898 Test: 1038.185913086
Epoch: 7600 Train: 1739.688232422 Test: 1030.178344727
Epoch: 7700 Train: 1741.357299805 Test: 1030.977783203
Epoch: 7800 Train: 1745.158691406 Test: 1032.529907227
Epoch: 7900 Train: 1753.023193359 Test: 1039.564453125
Epoch: 8000 Train: 1751.195678711 Test: 1035.087768555
Epoch: 8100 Train: 1754.296020508 Test: 1036.958984375
Epoch: 8200 Train: 1754.026123047 Test: 1037.930053711
Epoch: 8300 Train: 1754.820190430 Test: 1039.430419922
Epoch: 8400 Train: 1756.554931641 Test: 1036.021728516
Epoch: 8500 Train: 1757.690429688 Test: 1037.913574219
Epoch: 8600 Train: 1761.295288086 Test: 1041.227416992
Epoch: 8700 Train: 1769.349609375 Test: 1047.158447266
Epoch: 8800 Train: 1766.249267578 Test: 1042.224121094
Epoch: 8900 Train: 1771.242919922 Test: 1044.478759766
Epoch: 9000 Train: 1775.737182617 Test: 1047.888671875
Epoch: 9100 Train: 1770.539306641 Test: 1045.187011719
Epoch: 9200 Train: 1776.302001953 Test: 1049.291381836
Epoch: 9300 Train: 1765.699951172 Test: 1038.599731445
Epoch: 9400 Train: 1773.822509766 Test: 1049.389404297
Epoch: 9500 Train: 1768.424804688 Test: 1041.767456055
Epoch: 9600 Train: 1767.904296875 Test: 1040.806884766
Epoch: 9700 Train: 1760.876220703 Test: 1036.939819336
Epoch: 9800 Train: 1767.383178711 Test: 1042.322021484
Epoch: 9900 Train: 1764.036865234 Test: 1040.433837891
Epoch: 9999 Train: 1761.199951172 Test: 1037.878417969
Training Loss: tensor(1761.2000)
Test Loss: tensor(1037.8784)
True Mean x: tensor(3.0839, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3413, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(1.5860)
Jacobian term Test Loss: tensor(1.6129)
Learned LE: [[3.2927074 2.405293 ]
 [3.3017342 2.2812748]]
True LE: [[ 0.69318753        -inf]
 [-3.0538914  -0.7150728 ]]
Norm Diff:: tensor(inf)
