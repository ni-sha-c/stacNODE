time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 800
num_test: 800
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 128
n_layers: 4
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 18.456817627 Test: 16.938438416
Epoch 0: New minimal relative error: 16.94%, model saved.
Epoch: 100 Train: 5.620403290 Test: 5.679014206
Epoch 100: New minimal relative error: 5.68%, model saved.
Epoch: 200 Train: 5.546852112 Test: 5.607573509
Epoch 200: New minimal relative error: 5.61%, model saved.
Epoch: 300 Train: 5.395128250 Test: 5.452302933
Epoch 300: New minimal relative error: 5.45%, model saved.
Epoch: 400 Train: 5.386869907 Test: 5.453568459
Epoch: 500 Train: 5.367932320 Test: 5.453423500
Epoch: 600 Train: 5.422379971 Test: 5.507667542
Epoch: 700 Train: 5.539684296 Test: 5.563305855
Epoch: 800 Train: 5.506484985 Test: 5.532201290
Epoch: 900 Train: 5.453861237 Test: 5.477080345
Epoch: 1000 Train: 5.337543488 Test: 5.402670383
Epoch 1000: New minimal relative error: 5.40%, model saved.
Epoch: 1100 Train: 5.390986443 Test: 5.465972900
Epoch: 1200 Train: 5.394046307 Test: 5.463885307
Epoch: 1300 Train: 5.360657692 Test: 5.443040848
Epoch: 1400 Train: 5.382527828 Test: 5.481517792
Epoch: 1500 Train: 5.417466164 Test: 5.472836494
Epoch: 1600 Train: 5.404198647 Test: 5.472228050
Epoch: 1700 Train: 5.337899208 Test: 5.419293880
Epoch: 1800 Train: 5.390063286 Test: 5.447679520
Epoch: 1900 Train: 5.404960632 Test: 5.470317841
Epoch: 2000 Train: 5.428965569 Test: 5.507122993
Epoch: 2100 Train: 5.443434238 Test: 5.516411781
Epoch: 2200 Train: 5.426144123 Test: 5.507205486
Epoch: 2300 Train: 5.369778156 Test: 5.407013893
Epoch: 2400 Train: 5.393796921 Test: 5.480662346
Epoch: 2500 Train: 5.453830242 Test: 5.514859200
Epoch: 2600 Train: 5.446490288 Test: 5.528478622
Epoch: 2700 Train: 5.472544670 Test: 5.519849777
Epoch: 2800 Train: 5.476419449 Test: 5.549938679
Epoch: 2900 Train: 5.485395432 Test: 5.549180031
Epoch: 3000 Train: 5.513403416 Test: 5.584586143
Epoch: 3100 Train: 5.486620903 Test: 5.561522007
Epoch: 3200 Train: 5.472142220 Test: 5.545722485
Epoch: 3300 Train: 5.468597412 Test: 5.543805122
Epoch: 3400 Train: 5.458008766 Test: 5.545319557
Epoch: 3500 Train: 5.444515228 Test: 5.534334183
Epoch: 3600 Train: 5.455206871 Test: 5.531383514
Epoch: 3700 Train: 5.472533226 Test: 5.553381443
Epoch: 3800 Train: 5.480661392 Test: 5.568868637
Epoch: 3900 Train: 5.462235451 Test: 5.540703297
Epoch: 4000 Train: 5.468125820 Test: 5.544123173
Epoch: 4100 Train: 5.475088120 Test: 5.545490742
Epoch: 4200 Train: 5.473058701 Test: 5.551179886
Epoch: 4300 Train: 5.468890190 Test: 5.541349888
Epoch: 4400 Train: 5.470295906 Test: 5.541146278
Epoch: 4500 Train: 5.475185394 Test: 5.542270184
Epoch: 4600 Train: 5.477427483 Test: 5.547574997
Epoch: 4700 Train: 5.474659920 Test: 5.547294617
Epoch: 4800 Train: 5.476865768 Test: 5.549551964
Epoch: 4900 Train: 5.478508949 Test: 5.538686752
Epoch: 5000 Train: 5.479867935 Test: 5.533930779
Epoch: 5100 Train: 5.478585243 Test: 5.533315182
Epoch: 5200 Train: 5.483712196 Test: 5.536110878
Epoch: 5300 Train: 5.481317520 Test: 5.533973694
Epoch: 5400 Train: 5.478962898 Test: 5.528878212
Epoch: 5500 Train: 5.451075554 Test: 5.507205486
Epoch: 5600 Train: 5.417734146 Test: 5.486122608
Epoch: 5700 Train: 5.421577454 Test: 5.482238770
Epoch: 5800 Train: 5.424059868 Test: 5.500937462
Epoch: 5900 Train: 5.436427116 Test: 5.530743599
Epoch: 6000 Train: 5.454591751 Test: 5.556980610
Epoch: 6100 Train: 5.469606400 Test: 5.562784195
Epoch: 6200 Train: 5.480070591 Test: 5.552107334
Epoch: 6300 Train: 5.477617741 Test: 5.559893131
Epoch: 6400 Train: 5.479747772 Test: 5.553320408
Epoch: 6500 Train: 5.479287624 Test: 5.555762768
Epoch: 6600 Train: 5.480516911 Test: 5.557939529
Epoch: 6700 Train: 5.484886169 Test: 5.556063652
Epoch: 6800 Train: 5.488028526 Test: 5.557722569
Epoch: 6900 Train: 5.492324829 Test: 5.560636997
Epoch: 7000 Train: 5.491516590 Test: 5.566958427
Epoch: 7100 Train: 5.493457794 Test: 5.568724632
Epoch: 7200 Train: 5.490630627 Test: 5.566836357
Epoch: 7300 Train: 5.484610558 Test: 5.565386772
Epoch: 7400 Train: 5.482602119 Test: 5.567916393
Epoch: 7500 Train: 5.482244492 Test: 5.566818237
Epoch: 7600 Train: 5.480977535 Test: 5.566516876
Epoch: 7700 Train: 5.487533092 Test: 5.576317787
Epoch: 7800 Train: 5.485836029 Test: 5.573512077
Epoch: 7900 Train: 5.486804485 Test: 5.574527740
Epoch: 8000 Train: 5.490448475 Test: 5.574259281
Epoch: 8100 Train: 5.493309975 Test: 5.571978569
Epoch: 8200 Train: 5.493058681 Test: 5.572649956
Epoch: 8300 Train: 5.492566109 Test: 5.574460030
Epoch: 8400 Train: 5.492481709 Test: 5.575744629
Epoch: 8500 Train: 5.491816521 Test: 5.576237679
Epoch: 8600 Train: 5.493438721 Test: 5.575063705
Epoch: 8700 Train: 5.492817879 Test: 5.575876713
Epoch: 8800 Train: 5.492543221 Test: 5.577607155
Epoch: 8900 Train: 5.493234634 Test: 5.579449654
Epoch: 9000 Train: 5.493975639 Test: 5.591668129
Epoch: 9100 Train: 5.494640350 Test: 5.594677925
Epoch: 9200 Train: 5.495373249 Test: 5.597884178
Epoch: 9300 Train: 5.495750427 Test: 5.596355438
Epoch: 9400 Train: 5.495651722 Test: 5.600374699
Epoch: 9500 Train: 5.495895386 Test: 5.596437454
Epoch: 9600 Train: 5.495882034 Test: 5.596673965
Epoch: 9700 Train: 5.495900154 Test: 5.597752571
Epoch: 9800 Train: 5.496122837 Test: 5.599237442
Epoch: 9900 Train: 5.497703552 Test: 5.599340439
Epoch: 9999 Train: 5.497614861 Test: 5.596473217
Training Loss: tensor(5.4976)
Test Loss: tensor(5.5965)
True Mean x: tensor(2.9868, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.5376, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.4853, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0059, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0053)
Jacobian term Test Loss: tensor(0.0054)
Learned LE: [0.9736231  0.21263786]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
