time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 98.48%, model saved.
Epoch: 0 Train: 9451.29688 Test: 4076.91162
Epoch: 80 Train: 2609.31152 Test: 1101.74585
Epoch: 160 Train: 2192.24194 Test: 821.75592
Epoch: 240 Train: 1079.87244 Test: 283.50589
Epoch: 320 Train: 461.06921 Test: 89.32721
Epoch 400: New minimal relative error: 84.28%, model saved.
Epoch: 400 Train: 263.04578 Test: 39.18513
Epoch 480: New minimal relative error: 28.20%, model saved.
Epoch: 480 Train: 157.46964 Test: 15.30491
Epoch 560: New minimal relative error: 12.66%, model saved.
Epoch: 560 Train: 122.69585 Test: 12.54638
Epoch: 640 Train: 121.27464 Test: 30.14288
Epoch: 720 Train: 72.52551 Test: 5.83409
Epoch: 800 Train: 61.66300 Test: 9.40446
Epoch: 880 Train: 53.53924 Test: 2.70121
Epoch 960: New minimal relative error: 3.62%, model saved.
Epoch: 960 Train: 46.46585 Test: 2.15283
Epoch: 1040 Train: 42.56612 Test: 1.90490
Epoch: 1120 Train: 38.28591 Test: 1.83751
Epoch: 1200 Train: 35.85437 Test: 1.73778
Epoch: 1280 Train: 77.44174 Test: 62.85705
Epoch: 1360 Train: 30.26190 Test: 1.13528
Epoch: 1440 Train: 39.56878 Test: 16.23569
Epoch 1520: New minimal relative error: 2.89%, model saved.
Epoch: 1520 Train: 25.20034 Test: 0.89786
Epoch: 1600 Train: 25.14602 Test: 0.85932
Epoch: 1680 Train: 47.50383 Test: 28.09876
Epoch: 1760 Train: 21.98147 Test: 0.69026
Epoch: 1840 Train: 20.93079 Test: 0.90493
Epoch: 1920 Train: 20.51250 Test: 0.61855
Epoch: 2000 Train: 21.66588 Test: 0.79889
Epoch: 2080 Train: 18.10548 Test: 0.47040
Epoch: 2160 Train: 18.17574 Test: 0.56109
Epoch: 2240 Train: 18.21849 Test: 2.59484
Epoch: 2320 Train: 16.36864 Test: 0.45021
Epoch: 2400 Train: 15.96720 Test: 0.44912
Epoch: 2480 Train: 16.46530 Test: 0.63759
Epoch: 2560 Train: 27.03563 Test: 5.48667
Epoch: 2640 Train: 16.11709 Test: 0.44632
Epoch 2720: New minimal relative error: 2.07%, model saved.
Epoch: 2720 Train: 14.74696 Test: 0.37261
Epoch: 2800 Train: 16.58326 Test: 0.76187
Epoch: 2880 Train: 14.00971 Test: 0.33210
Epoch: 2960 Train: 14.45779 Test: 0.48465
Epoch: 3040 Train: 12.83725 Test: 0.27618
Epoch: 3120 Train: 13.03679 Test: 0.30196
Epoch: 3200 Train: 12.16998 Test: 0.22398
Epoch: 3280 Train: 13.10681 Test: 0.26449
Epoch: 3360 Train: 11.95250 Test: 0.20714
Epoch: 3440 Train: 12.96515 Test: 0.42104
Epoch: 3520 Train: 11.42998 Test: 0.19042
Epoch: 3600 Train: 13.77900 Test: 3.21999
Epoch: 3680 Train: 10.81950 Test: 0.18535
Epoch: 3760 Train: 10.43469 Test: 0.14957
Epoch: 3840 Train: 10.27268 Test: 0.16925
Epoch: 3920 Train: 12.24652 Test: 3.38875
Epoch: 4000 Train: 10.11545 Test: 0.14390
Epoch: 4080 Train: 11.15204 Test: 1.25515
Epoch: 4160 Train: 10.72967 Test: 0.84996
Epoch: 4240 Train: 9.34021 Test: 0.11947
Epoch: 4320 Train: 9.61405 Test: 0.14140
Epoch: 4400 Train: 9.07890 Test: 0.13611
Epoch: 4480 Train: 9.01672 Test: 0.12278
Epoch 4560: New minimal relative error: 1.48%, model saved.
Epoch: 4560 Train: 8.78926 Test: 0.13215
Epoch: 4640 Train: 9.15478 Test: 0.15002
Epoch: 4720 Train: 8.32951 Test: 0.10454
Epoch: 4800 Train: 8.30947 Test: 0.10756
Epoch: 4880 Train: 11.78760 Test: 3.28274
Epoch: 4960 Train: 8.30672 Test: 0.23442
Epoch: 5040 Train: 11.09864 Test: 2.80260
Epoch: 5120 Train: 7.83775 Test: 0.08695
Epoch: 5200 Train: 7.76550 Test: 0.11336
Epoch: 5280 Train: 7.93646 Test: 0.34743
Epoch: 5360 Train: 7.57751 Test: 0.10417
Epoch: 5440 Train: 7.71556 Test: 0.09696
Epoch: 5520 Train: 7.58403 Test: 0.08713
Epoch: 5600 Train: 7.82323 Test: 0.40138
Epoch: 5680 Train: 9.57551 Test: 2.07358
Epoch 5760: New minimal relative error: 1.26%, model saved.
Epoch: 5760 Train: 7.25844 Test: 0.09564
Epoch: 5840 Train: 7.29101 Test: 0.07269
Epoch: 5920 Train: 7.25095 Test: 0.09143
Epoch: 6000 Train: 7.22098 Test: 0.07962
Epoch: 6080 Train: 7.72620 Test: 0.53965
Epoch: 6160 Train: 7.19411 Test: 0.15094
Epoch: 6240 Train: 7.46217 Test: 0.39210
Epoch: 6320 Train: 7.18433 Test: 0.14357
Epoch: 6400 Train: 7.05856 Test: 0.10235
Epoch: 6480 Train: 7.07711 Test: 0.15117
Epoch: 6560 Train: 7.03429 Test: 0.17720
Epoch: 6640 Train: 6.91700 Test: 0.16899
Epoch: 6720 Train: 7.04312 Test: 0.30091
Epoch: 6800 Train: 6.96732 Test: 0.18008
Epoch: 6880 Train: 7.43136 Test: 0.37925
Epoch: 6960 Train: 6.78429 Test: 0.06448
Epoch: 7040 Train: 6.78289 Test: 0.11751
Epoch: 7120 Train: 6.66791 Test: 0.06383
Epoch: 7200 Train: 6.69325 Test: 0.10595
Epoch: 7280 Train: 6.77177 Test: 0.19881
Epoch: 7360 Train: 6.90056 Test: 0.21573
Epoch: 7440 Train: 6.45778 Test: 0.06920
Epoch: 7520 Train: 6.42329 Test: 0.13482
Epoch: 7600 Train: 6.62976 Test: 0.28768
Epoch: 7680 Train: 6.26878 Test: 0.07155
Epoch: 7760 Train: 6.35120 Test: 0.18406
Epoch: 7840 Train: 6.19913 Test: 0.16233
Epoch: 7920 Train: 6.13511 Test: 0.13452
Epoch: 7999 Train: 6.04362 Test: 0.11031
Training Loss: tensor(6.0436)
Test Loss: tensor(0.1103)
Learned LE: [ 8.8988978e-01 -1.5330426e-03 -1.4577863e+01]
True LE: [ 8.8240069e-01  1.2687263e-02 -1.4568615e+01]
Relative Error: [6.310487   6.74603    6.9396324  7.2454586  7.4910455  7.457635
 7.1095533  6.8851004  6.185187   5.509399   4.683029   3.8428776
 3.0948796  2.4908175  2.1317334  2.5104032  3.1345003  4.00705
 5.0134754  5.193006   5.339371   5.1691537  4.408493   3.4918535
 2.8369105  2.4226532  2.2293406  2.305772   2.0777962  1.5979836
 1.2542416  1.2575969  1.146728   0.4999857  0.25843325 0.27088508
 0.37605682 0.62634784 0.9872721  1.1056743  1.6742421  2.2340097
 3.1984932  4.18449    5.224617   6.3208528  6.9307556  6.9263277
 6.5883493  6.5123377  6.1942334  6.1576266  6.4026766  6.4054003
 6.4565363  6.3746085  6.1776586  5.9818106  5.9813786  6.1532154
 6.1063414  5.8610106  5.7543087  5.8035436  6.1510715  6.465716
 6.557532   6.551691   6.280739   5.943598   5.484502   4.9161572
 4.3286924  3.5663466  2.7083824  1.971489   1.5202516  1.628384
 2.1311195  2.86468    3.8513854  4.4175115  4.6596956  4.62243
 4.1338024  3.259925   2.5892713  2.1738563  1.9494305  1.9148214
 1.8385338  1.3680742  0.92067665 0.9832346  1.0198783  0.70074505
 0.39406925 0.4302331  0.38368514 0.7808759  1.1240997  0.80868375
 1.1546732  1.5066867  2.3790352  3.471833   4.461882   5.464422
 6.2000613  6.298159   5.9166236  5.816154   5.5883384  5.52334
 5.760816   5.766388   5.799199   5.5563445  5.2677994  5.110069
 5.1610336  5.383327   5.625872   5.2157187  4.9806795  4.883827
 5.1108894  5.234665   5.3172445  5.457783   5.4867144  5.188553
 4.6909604  4.380907   3.827064   3.4881446  2.551488   1.6738408
 1.0917957  0.9137068  1.175738   1.7321925  2.5625868  3.314538
 3.9697993  4.094956   3.7209923  3.118549   2.3903027  1.9500129
 1.7392334  1.6908234  1.5496492  1.1783559  0.8118841  0.58063453
 0.7891016  1.0082332  0.571909   0.5479988  0.50222224 0.8261465
 1.2707376  0.7132891  0.85550517 0.93746156 1.6061206  2.7519236
 3.6689694  4.5432234  5.4342346  5.608009   5.2809896  5.1423903
 5.043509   4.940149   5.1398387  5.254974   5.262817   4.794227
 4.4578247  4.2726054  4.353811   4.552246   4.715443   4.7622457
 4.356692   4.1794243  4.2039375  4.166827   4.2007394  4.3512564
 4.4280624  4.280012   4.1004634  3.6892726  3.3887057  3.1113
 2.7043407  1.6880084  0.955539   0.5005853  0.3378012  0.7195694
 1.3596532  2.461188   3.0159411  3.6464925  3.4386086  3.050106
 2.3565724  1.7971283  1.5363313  1.4909047  1.4790641  0.98783207
 0.74937016 0.45921397 0.63209546 0.9500483  0.9113057  0.5867101
 0.5903371  0.6952826  1.3086836  0.8282386  0.80747753 0.6979513
 0.950711   1.813785   2.8781393  3.6378856  4.4624906  4.8000093
 4.79175    4.53022    4.5604258  4.327078   4.4054794  4.6531916
 4.599366   4.1355147  3.8086667  3.5538406  3.5279508  3.560118
 3.7936475  3.9829645  4.02132    3.7729342  3.561873   3.3883789
 3.3777356  3.4325664  3.4386654  3.4269507  3.2768059  2.9542696
 2.8797467  2.6981826  2.5655553  2.1112974  1.1965226  0.46599242
 0.16889402 0.3961206  0.56040347 1.3839432  2.176578   2.8264759
 3.1539428  2.9082372  2.493117   1.9217217  1.490499   1.3418472
 1.4816091  1.0726268  0.66167617 0.52930534 0.37936255 0.8203988
 1.1800352  0.8691282  0.6723107  0.72942144 1.1050278  1.0006238
 0.83299863 0.80073214 0.6270073  0.93882155 1.9627688  2.7671409
 3.3827436  4.08441    4.2701974  3.9948225  3.795917   3.6359944
 3.6148376  3.9904535  3.845664   3.4736593  3.14961    2.8896308
 2.670432   2.5358858  2.7551384  3.077122   3.397002   3.5387132
 3.2567685  2.8944921  2.7224247  2.7584023  2.7401855  2.7619016
 2.647007   2.289044   2.186472   2.202073   2.2138762  2.1975067
 1.7236648  0.8872484  0.36447173 0.72413737 0.63958675 0.56416553
 1.3072432  1.8687731  2.5453737  2.6702213  2.5502827  2.109512
 1.6343912  1.35954    1.3507977  1.2960347  0.93686444 0.5954155
 0.40175477 0.1902046  0.5892499  0.97237027 0.8281464  0.6984018
 0.73670673 1.1461537  0.84702426 0.9342702  0.8665252  0.42603648
 0.7729501  1.7563721  2.4009411  2.9702003  3.472799   3.3779461
 2.9968262  2.9893115  2.847821   3.2632346  3.2142487  2.7774847
 2.556505   2.2889414  1.9331994  1.7092775  1.6932431  1.9886765
 2.483186   2.9317248  3.1385546  2.7382686  2.3850176  2.1418517
 2.0522168  2.1436806  2.333519   2.1118424  1.6115978  1.552728
 1.6126411  1.8864526  2.0199616  1.476889   0.63428783 0.65095043
 1.0046281  0.89788574 0.6873696  1.1015077  1.5815873  2.1488304
 2.2204285  2.22995    1.8223015  1.4690579 ]
