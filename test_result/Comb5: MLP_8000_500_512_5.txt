time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 3727.06372 Test: 4323.61133
Epoch 80: New minimal relative error: 35.94%, model saved.
Epoch: 80 Train: 58.39404 Test: 64.17566
Epoch: 160 Train: 16.60530 Test: 53.38701
Epoch 240: New minimal relative error: 33.31%, model saved.
Epoch: 240 Train: 6.71554 Test: 8.74292
Epoch: 320 Train: 11.19435 Test: 9.49711
Epoch: 400 Train: 10.68446 Test: 5.28641
Epoch: 480 Train: 3.03391 Test: 2.56818
Epoch: 560 Train: 8.53954 Test: 15.07268
Epoch: 640 Train: 10.02623 Test: 10.66511
Epoch: 720 Train: 5.73421 Test: 6.35125
Epoch: 800 Train: 0.45409 Test: 0.39709
Epoch: 880 Train: 7.63741 Test: 7.47572
Epoch: 960 Train: 0.47433 Test: 1.01059
Epoch: 1040 Train: 1.23872 Test: 0.58098
Epoch: 1120 Train: 0.27127 Test: 0.26412
Epoch 1200: New minimal relative error: 30.36%, model saved.
Epoch: 1200 Train: 5.08704 Test: 4.89852
Epoch 1280: New minimal relative error: 22.90%, model saved.
Epoch: 1280 Train: 2.56266 Test: 3.08123
Epoch 1360: New minimal relative error: 20.25%, model saved.
Epoch: 1360 Train: 1.39456 Test: 0.62533
Epoch: 1440 Train: 1.46279 Test: 2.02641
Epoch: 1520 Train: 6.03630 Test: 3.37904
Epoch: 1600 Train: 0.30842 Test: 0.67570
Epoch: 1680 Train: 0.41502 Test: 0.43523
Epoch: 1760 Train: 3.70601 Test: 5.03543
Epoch: 1840 Train: 0.18010 Test: 0.29540
Epoch: 1920 Train: 2.10434 Test: 1.89059
Epoch: 2000 Train: 1.50121 Test: 1.58154
Epoch: 2080 Train: 2.57568 Test: 1.63953
Epoch: 2160 Train: 2.50729 Test: 2.02334
Epoch: 2240 Train: 0.55176 Test: 0.69725
Epoch: 2320 Train: 2.75555 Test: 3.00196
Epoch: 2400 Train: 6.72582 Test: 8.31881
Epoch: 2480 Train: 0.07509 Test: 0.08332
Epoch: 2560 Train: 0.20234 Test: 0.20672
Epoch 2640: New minimal relative error: 17.16%, model saved.
Epoch: 2640 Train: 0.20657 Test: 0.21615
Epoch: 2720 Train: 0.18430 Test: 0.26275
Epoch: 2800 Train: 1.27417 Test: 1.00948
Epoch: 2880 Train: 0.59111 Test: 0.63597
Epoch: 2960 Train: 0.10727 Test: 0.15154
Epoch: 3040 Train: 0.27637 Test: 0.55638
Epoch: 3120 Train: 0.31088 Test: 0.64191
Epoch: 3200 Train: 2.02627 Test: 1.60121
Epoch: 3280 Train: 1.28920 Test: 1.35561
Epoch: 3360 Train: 0.84225 Test: 1.11697
Epoch: 3440 Train: 1.42275 Test: 2.00818
Epoch: 3520 Train: 0.25366 Test: 0.40934
Epoch 3600: New minimal relative error: 15.34%, model saved.
Epoch: 3600 Train: 0.14558 Test: 0.18959
Epoch: 3680 Train: 0.14286 Test: 0.17262
Epoch: 3760 Train: 0.10351 Test: 0.14462
Epoch: 3840 Train: 0.06282 Test: 0.07509
Epoch: 3920 Train: 0.09829 Test: 0.10858
Epoch: 4000 Train: 1.00570 Test: 0.83003
Epoch: 4080 Train: 0.18707 Test: 0.24347
Epoch: 4160 Train: 0.05208 Test: 0.10198
Epoch: 4240 Train: 1.03143 Test: 0.95493
Epoch: 4320 Train: 0.08108 Test: 0.19602
Epoch: 4400 Train: 0.03615 Test: 0.04569
Epoch: 4480 Train: 0.05808 Test: 0.07440
Epoch: 4560 Train: 0.03607 Test: 0.04237
Epoch: 4640 Train: 0.05882 Test: 0.06877
Epoch: 4720 Train: 0.03551 Test: 0.05371
Epoch: 4800 Train: 0.18276 Test: 0.27284
Epoch: 4880 Train: 0.12811 Test: 0.17510
Epoch: 4960 Train: 0.60489 Test: 0.45757
Epoch: 5040 Train: 0.63886 Test: 0.89506
Epoch: 5120 Train: 0.02665 Test: 0.03331
Epoch: 5200 Train: 0.05502 Test: 0.05073
Epoch: 5280 Train: 0.05336 Test: 0.06434
Epoch: 5360 Train: 0.03067 Test: 0.03514
Epoch: 5440 Train: 0.03439 Test: 0.03201
Epoch: 5520 Train: 0.48004 Test: 0.67181
Epoch: 5600 Train: 0.02663 Test: 0.03354
Epoch: 5680 Train: 0.05243 Test: 0.06398
Epoch: 5760 Train: 0.02651 Test: 0.03442
Epoch: 5840 Train: 0.05216 Test: 0.04976
Epoch: 5920 Train: 0.16737 Test: 0.21501
Epoch: 6000 Train: 0.02205 Test: 0.02751
Epoch: 6080 Train: 0.07915 Test: 0.07216
Epoch: 6160 Train: 0.02109 Test: 0.02653
Epoch: 6240 Train: 0.02169 Test: 0.03090
Epoch: 6320 Train: 0.02198 Test: 0.02993
Epoch: 6400 Train: 0.02303 Test: 0.02844
Epoch: 6480 Train: 0.42732 Test: 0.48941
Epoch 6560: New minimal relative error: 9.24%, model saved.
Epoch: 6560 Train: 0.01844 Test: 0.02424
Epoch: 6640 Train: 0.56711 Test: 0.23773
Epoch: 6720 Train: 0.01821 Test: 0.02437
Epoch: 6800 Train: 0.01716 Test: 0.02279
Epoch: 6880 Train: 0.02589 Test: 0.02683
Epoch: 6960 Train: 0.01681 Test: 0.02259
Epoch: 7040 Train: 0.02054 Test: 0.02270
Epoch: 7120 Train: 0.03545 Test: 0.03044
Epoch: 7200 Train: 0.01616 Test: 0.02168
Epoch: 7280 Train: 0.27998 Test: 0.07369
Epoch: 7360 Train: 0.01598 Test: 0.02143
Epoch: 7440 Train: 0.01514 Test: 0.02073
Epoch: 7520 Train: 0.02376 Test: 0.02106
Epoch: 7600 Train: 0.23030 Test: 0.32642
Epoch: 7680 Train: 0.01499 Test: 0.02072
Epoch: 7760 Train: 0.13161 Test: 0.19913
Epoch: 7840 Train: 0.01484 Test: 0.01980
Epoch: 7920 Train: 0.01390 Test: 0.01931
Epoch: 7999 Train: 0.01687 Test: 0.02354
Training Loss: tensor(0.0169)
Test Loss: tensor(0.0235)
Learned LE: [ 0.8922991 -0.0196377 -5.5887403]
True LE: [ 8.4631860e-01  1.2351533e-02 -1.4530384e+01]
Relative Error: [5.352089   5.23062    5.073043   4.882207   4.6320915  4.2561326
 3.7115784  3.0859375  2.6914802  2.7672167  3.0312858  3.2254624
 3.2264714  3.0720747  2.9950743  3.189379   3.649167   4.2270536
 4.7651343  5.193068   5.519447   5.762155   5.928526   5.9618587
 5.6702724  5.181404   4.59979    3.9208863  3.2957509  2.8671534
 2.6642973  2.508106   2.4579496  2.5400352  2.6011145  2.6543074
 2.690073   2.6541462  2.4307597  2.2664871  2.4039063  2.7313929
 3.0940945  3.4084551  3.5955827  3.6172242  3.531352   3.4582398
 3.4353685  3.4507937  3.4796655  3.4990256  3.496125   3.4833794
 3.4900057  3.5407293  3.6559403  3.8482945  4.1095924  4.3947077
 4.606314   4.6558104  4.5820737  4.4715967  4.3417754  4.1956964
 4.037964   3.8234313  3.4713964  2.9379454  2.3636727  2.1676447
 2.4768386  2.8448496  3.048788   3.0034177  2.8073063  2.7495909
 3.0232131  3.578953   4.210873   4.7197304  5.042232   5.225506
 5.336324   5.4443755  5.5494914  5.3799024  4.983971   4.498085
 3.8588915  3.1717494  2.6167603  2.292989   2.0846653  1.960876
 1.9815123  1.9987828  2.0971878  2.228913   2.2337015  2.0232956
 1.8726084  2.021515   2.3177629  2.6546216  2.897299   2.9936838
 2.928123   2.7730145  2.6470997  2.5806797  2.5643237  2.5646293
 2.555185   2.532776   2.5096047  2.5057075  2.5381823  2.624927
 2.7837155  3.0159576  3.2968256  3.569283   3.7329495  3.73651
 3.6534905  3.528238   3.368483   3.2147295  3.0478096  2.7712004
 2.312546   1.7964926  1.6821135  2.087051   2.5435154  2.8205302
 2.831027   2.6383443  2.5635684  2.8561175  3.4700546  4.155562
 4.6548796  4.875572   4.8880954  4.7932663  4.6976624  4.732336
 4.7585816  4.550718   4.2379217  3.784152   3.1419835  2.5078666
 2.0424292  1.7814902  1.563205   1.51104    1.4698248  1.538184
 1.7683281  1.8551728  1.705582   1.5215164  1.6190364  1.8511596
 2.1337693  2.3162532  2.3657181  2.278543   2.105827   1.9583509
 1.8666495  1.8112513  1.7646625  1.709291   1.6593748  1.6238749
 1.6065594  1.6130521  1.6565055  1.7578754  1.9304206  2.1630254
 2.4244924  2.6692965  2.8177679  2.8372102  2.7887936  2.6690319
 2.4843938  2.3028038  2.1127582  1.8011484  1.3615834  1.1723218
 1.5058324  2.0276034  2.4232638  2.6234756  2.5498886  2.4165406
 2.591018   3.1649132  3.8970492  4.4726534  4.702586   4.625008
 4.371475   4.045152   3.7694256  3.7149808  3.6994457  3.6169045
 3.4342058  3.080722   2.5119638  1.9638098  1.5919138  1.3531816
 1.1449744  1.0896437  1.0241354  1.2062802  1.4855917  1.4390311
 1.2250272  1.2278771  1.3573815  1.5566721  1.7000158  1.7422382
 1.6912425  1.560185   1.4296603  1.3425384  1.2562628  1.1554998
 1.0512915  0.9611669  0.9072233  0.8795761  0.8602107  0.8579264
 0.89225984 0.98098665 1.129982   1.3241459  1.5347754  1.7406722
 1.9031254  1.9772408  1.9951111  1.9331262  1.7542845  1.5183878
 1.2960024  1.0166109  0.7321357  0.8235598  1.2350482  1.716977
 2.0874429  2.3254027  2.3142753  2.2520888  2.5189514  3.1505916
 3.8170886  4.2242126  4.2573977  4.0281067  3.683289   3.2736886
 2.8591042  2.6334565  2.5487556  2.5720406  2.550041   2.3761387
 1.9697696  1.5318916  1.2437681  1.0471991  0.81720203 0.76893765
 0.6829308  0.8810517  1.178354   1.026819   0.8553214  0.87817836
 0.96001065 1.0654639  1.1024344  1.1024586  1.0591677  0.9874582
 0.9412989  0.8939336  0.77683496 0.64432013 0.5279943  0.45007402
 0.41307184 0.39280364 0.35698867 0.32568744 0.3269436  0.37174997
 0.46005547 0.5850529  0.7246287  0.8632316  1.0114471  1.1461898
 1.2371631  1.2858763  1.217952   1.0060744  0.724396   0.44357786
 0.25323397 0.42699867 0.7330165  1.1439112  1.4999149  1.8307242
 1.990673   1.9342409  2.0079246  2.4505968  2.9825351  3.2757814
 3.2554026  3.0190742  2.7607598  2.5281935  2.2391922  1.8983626
 1.6616133  1.5279616  1.5974495  1.6018236  1.4473882  1.140342
 0.9289508  0.83138067 0.6565679  0.5457519  0.50467116 0.47737047
 0.8143567  0.73122543 0.45052707 0.41127816 0.39359623 0.49459505
 0.46624994 0.44679484 0.4775425  0.49297768 0.5387058  0.5502834
 0.43822765 0.29625583 0.20960225 0.21420808 0.20538363 0.19922219
 0.17796466 0.12773293 0.07530279 0.04849122 0.0484075  0.08655136
 0.16631396 0.24707152 0.30599406 0.37555403 0.48923263 0.6106732
 0.71699774 0.7099526  0.5494492  0.30124053 0.17368618 0.19582945
 0.18847334 0.37318766 0.6785017  0.9843221  1.2976657  1.4634491
 1.3818793  1.3549132  1.6319546  1.9278512 ]
