time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.54%, model saved.
Epoch: 0 Train: 9842.40430 Test: 3847.04810
Epoch: 100 Train: 2795.95703 Test: 1087.64722
Epoch: 200 Train: 2591.22437 Test: 959.85486
Epoch: 300 Train: 2144.38086 Test: 741.29657
Epoch: 400 Train: 842.36157 Test: 205.35167
Epoch: 500 Train: 301.84790 Test: 50.88431
Epoch 600: New minimal relative error: 8.76%, model saved.
Epoch: 600 Train: 148.69275 Test: 18.17030
Epoch: 700 Train: 111.90092 Test: 12.11903
Epoch: 800 Train: 93.42848 Test: 7.81159
Epoch: 900 Train: 68.21584 Test: 5.49780
Epoch 1000: New minimal relative error: 8.17%, model saved.
Epoch: 1000 Train: 60.01952 Test: 4.67434
Epoch: 1100 Train: 53.59301 Test: 3.86208
Epoch: 1200 Train: 49.88124 Test: 3.67045
Epoch: 1300 Train: 45.56219 Test: 4.53650
Epoch: 1400 Train: 41.41697 Test: 3.57079
Epoch: 1500 Train: 37.05514 Test: 3.20294
Epoch: 1600 Train: 39.43591 Test: 5.53491
Epoch: 1700 Train: 33.92763 Test: 3.98772
Epoch: 1800 Train: 32.61869 Test: 5.12399
Epoch: 1900 Train: 28.52525 Test: 2.68997
Epoch: 2000 Train: 26.68419 Test: 1.92636
Epoch: 2100 Train: 25.39312 Test: 1.82328
Epoch 2200: New minimal relative error: 5.90%, model saved.
Epoch: 2200 Train: 24.50474 Test: 1.83183
Epoch: 2300 Train: 23.89968 Test: 1.56035
Epoch: 2400 Train: 22.60550 Test: 1.77181
Epoch: 2500 Train: 22.32497 Test: 2.13437
Epoch: 2600 Train: 20.69547 Test: 1.40202
Epoch: 2700 Train: 20.56722 Test: 1.84031
Epoch: 2800 Train: 20.47648 Test: 1.49480
Epoch: 2900 Train: 20.03343 Test: 1.31222
Epoch 3000: New minimal relative error: 5.75%, model saved.
Epoch: 3000 Train: 20.00152 Test: 1.37120
Epoch: 3100 Train: 19.79226 Test: 1.67403
Epoch: 3200 Train: 27.58802 Test: 10.19258
Epoch: 3300 Train: 18.36082 Test: 1.18298
Epoch: 3400 Train: 18.53317 Test: 1.28432
Epoch: 3500 Train: 18.09048 Test: 1.23868
Epoch: 3600 Train: 18.09815 Test: 1.24102
Epoch 3700: New minimal relative error: 5.17%, model saved.
Epoch: 3700 Train: 18.48674 Test: 1.18969
Epoch: 3800 Train: 18.12580 Test: 1.21361
Epoch: 3900 Train: 16.81281 Test: 1.10805
Epoch: 4000 Train: 16.54653 Test: 1.10820
Epoch: 4100 Train: 16.37634 Test: 1.76085
Epoch: 4200 Train: 15.39833 Test: 0.98362
Epoch: 4300 Train: 16.70800 Test: 2.89269
Epoch 4400: New minimal relative error: 4.98%, model saved.
Epoch: 4400 Train: 14.84643 Test: 0.88378
Epoch: 4500 Train: 16.04632 Test: 3.25440
Epoch: 4600 Train: 14.62988 Test: 0.86498
Epoch: 4700 Train: 14.43281 Test: 0.88622
Epoch: 4800 Train: 14.98058 Test: 1.60666
Epoch: 4900 Train: 13.86700 Test: 0.79470
Epoch: 5000 Train: 13.46869 Test: 0.79346
Epoch: 5100 Train: 13.37067 Test: 0.77684
Epoch: 5200 Train: 16.48122 Test: 2.12616
Epoch: 5300 Train: 13.20589 Test: 0.75693
Epoch: 5400 Train: 12.88223 Test: 0.76141
Epoch 5500: New minimal relative error: 4.94%, model saved.
Epoch: 5500 Train: 13.13841 Test: 0.79894
Epoch: 5600 Train: 12.71105 Test: 0.76046
Epoch: 5700 Train: 12.52207 Test: 0.78029
Epoch: 5800 Train: 12.58633 Test: 0.78044
Epoch 5900: New minimal relative error: 4.48%, model saved.
Epoch: 5900 Train: 12.31288 Test: 0.74871
Epoch: 6000 Train: 12.11813 Test: 0.72819
Epoch: 6100 Train: 11.95070 Test: 0.85407
Epoch: 6200 Train: 11.70429 Test: 0.71613
Epoch: 6300 Train: 12.09725 Test: 0.99450
Epoch 6400: New minimal relative error: 3.98%, model saved.
Epoch: 6400 Train: 11.08276 Test: 0.66165
Epoch: 6500 Train: 11.02827 Test: 0.61540
Epoch: 6600 Train: 10.98629 Test: 0.63839
Epoch: 6700 Train: 11.19108 Test: 0.66283
Epoch: 6800 Train: 11.15421 Test: 0.66183
Epoch: 6900 Train: 11.18868 Test: 0.65074
Epoch: 7000 Train: 11.22052 Test: 0.66154
Epoch: 7100 Train: 11.14592 Test: 0.66978
Epoch: 7200 Train: 11.37521 Test: 0.68790
Epoch: 7300 Train: 11.47357 Test: 0.66444
Epoch: 7400 Train: 11.55306 Test: 0.73082
Epoch: 7500 Train: 11.53639 Test: 0.67676
Epoch: 7600 Train: 11.79305 Test: 0.82636
Epoch: 7700 Train: 11.35848 Test: 0.64479
Epoch: 7800 Train: 11.45363 Test: 0.68514
Epoch: 7900 Train: 10.84308 Test: 0.51505
Epoch 8000: New minimal relative error: 3.76%, model saved.
Epoch: 8000 Train: 10.88542 Test: 0.51574
Epoch: 8100 Train: 10.65953 Test: 0.52694
Epoch: 8200 Train: 10.56686 Test: 0.49733
Epoch: 8300 Train: 10.47697 Test: 0.50867
Epoch: 8400 Train: 10.46015 Test: 0.48880
Epoch: 8500 Train: 10.48542 Test: 0.48689
Epoch: 8600 Train: 10.57617 Test: 0.54639
Epoch: 8700 Train: 10.48003 Test: 0.53236
Epoch: 8800 Train: 10.41916 Test: 0.53850
Epoch: 8900 Train: 10.28498 Test: 0.53111
Epoch: 9000 Train: 10.22259 Test: 0.57074
Epoch: 9100 Train: 10.01358 Test: 0.56011
Epoch: 9200 Train: 9.76519 Test: 0.50732
Epoch: 9300 Train: 9.73090 Test: 0.48856
Epoch 9400: New minimal relative error: 3.69%, model saved.
Epoch: 9400 Train: 9.54219 Test: 0.48022
Epoch: 9500 Train: 9.37818 Test: 0.46636
Epoch: 9600 Train: 9.26443 Test: 0.46178
Epoch: 9700 Train: 9.25467 Test: 0.46689
Epoch: 9800 Train: 9.35702 Test: 0.57229
Epoch: 9900 Train: 9.28855 Test: 0.48848
Epoch: 9999 Train: 9.31731 Test: 0.50433
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.54%, model saved.
Epoch: 0 Train: 9842.40430 Test: 3847.04810
Epoch: 100 Train: 2795.95703 Test: 1087.64722
Epoch: 200 Train: 2591.22437 Test: 959.85486
Epoch: 300 Train: 2144.38086 Test: 741.29657
Epoch: 400 Train: 842.36157 Test: 205.35167
Epoch: 500 Train: 301.84790 Test: 50.88431
Epoch 600: New minimal relative error: 8.76%, model saved.
Epoch: 600 Train: 148.69275 Test: 18.17030
Epoch: 700 Train: 111.90092 Test: 12.11903
Epoch: 800 Train: 93.42848 Test: 7.81159
Epoch: 900 Train: 68.21584 Test: 5.49780
Epoch 1000: New minimal relative error: 8.17%, model saved.
Epoch: 1000 Train: 60.01952 Test: 4.67434
Epoch: 1100 Train: 53.59301 Test: 3.86208
Epoch: 1200 Train: 49.88124 Test: 3.67045
Epoch: 1300 Train: 45.56219 Test: 4.53650
Epoch: 1400 Train: 41.41697 Test: 3.57079
Epoch: 1500 Train: 37.05514 Test: 3.20294
Epoch: 1600 Train: 39.43591 Test: 5.53491
Epoch: 1700 Train: 33.92763 Test: 3.98772
Epoch: 1800 Train: 32.61869 Test: 5.12399
Epoch: 1900 Train: 28.52525 Test: 2.68997
Epoch: 2000 Train: 26.68419 Test: 1.92636
Epoch: 2100 Train: 25.39312 Test: 1.82328
Epoch 2200: New minimal relative error: 5.90%, model saved.
Epoch: 2200 Train: 24.50474 Test: 1.83183
Epoch: 2300 Train: 23.89968 Test: 1.56035
Epoch: 2400 Train: 22.60550 Test: 1.77181
Epoch: 2500 Train: 22.32497 Test: 2.13437
Epoch: 2600 Train: 20.69547 Test: 1.40202
Epoch: 2700 Train: 20.56722 Test: 1.84031
Epoch: 2800 Train: 20.47648 Test: 1.49480
Epoch: 2900 Train: 20.03343 Test: 1.31222
Epoch 3000: New minimal relative error: 5.75%, model saved.
Epoch: 3000 Train: 20.00152 Test: 1.37120
Epoch: 3100 Train: 19.79226 Test: 1.67403
Epoch: 3200 Train: 27.58802 Test: 10.19258
Epoch: 3300 Train: 18.36082 Test: 1.18298
Epoch: 3400 Train: 18.53317 Test: 1.28432
Epoch: 3500 Train: 18.09048 Test: 1.23868
Epoch: 3600 Train: 18.09815 Test: 1.24102
Epoch 3700: New minimal relative error: 5.17%, model saved.
Epoch: 3700 Train: 18.48674 Test: 1.18969
Epoch: 3800 Train: 18.12580 Test: 1.21361
Epoch: 3900 Train: 16.81281 Test: 1.10805
Epoch: 4000 Train: 16.54653 Test: 1.10820
Epoch: 4100 Train: 16.37634 Test: 1.76085
Epoch: 4200 Train: 15.39833 Test: 0.98362
Epoch: 4300 Train: 16.70800 Test: 2.89269
Epoch 4400: New minimal relative error: 4.98%, model saved.
Epoch: 4400 Train: 14.84643 Test: 0.88378
Epoch: 4500 Train: 16.04632 Test: 3.25440
Epoch: 4600 Train: 14.62988 Test: 0.86498
Epoch: 4700 Train: 14.43281 Test: 0.88622
Epoch: 4800 Train: 14.98058 Test: 1.60666
Epoch: 4900 Train: 13.86700 Test: 0.79470
Epoch: 5000 Train: 13.46869 Test: 0.79346
Epoch: 5100 Train: 13.37067 Test: 0.77684
Epoch: 5200 Train: 16.48122 Test: 2.12616
Epoch: 5300 Train: 13.20589 Test: 0.75693
Epoch: 5400 Train: 12.88223 Test: 0.76141
Epoch 5500: New minimal relative error: 4.94%, model saved.
Epoch: 5500 Train: 13.13841 Test: 0.79894
Epoch: 5600 Train: 12.71105 Test: 0.76046
Epoch: 5700 Train: 12.52207 Test: 0.78029
Epoch: 5800 Train: 12.58633 Test: 0.78044
Epoch 5900: New minimal relative error: 4.48%, model saved.
Epoch: 5900 Train: 12.31288 Test: 0.74871
Epoch: 6000 Train: 12.11813 Test: 0.72819
Epoch: 6100 Train: 11.95070 Test: 0.85407
Epoch: 6200 Train: 11.70429 Test: 0.71613
Epoch: 6300 Train: 12.09725 Test: 0.99450
Epoch 6400: New minimal relative error: 3.98%, model saved.
Epoch: 6400 Train: 11.08276 Test: 0.66165
Epoch: 6500 Train: 11.02827 Test: 0.61540
Epoch: 6600 Train: 10.98629 Test: 0.63839
Epoch: 6700 Train: 11.19108 Test: 0.66283
Epoch: 6800 Train: 11.15421 Test: 0.66183
Epoch: 6900 Train: 11.18868 Test: 0.65074
Epoch: 7000 Train: 11.22052 Test: 0.66154
Epoch: 7100 Train: 11.14592 Test: 0.66978
Epoch: 7200 Train: 11.37521 Test: 0.68790
Epoch: 7300 Train: 11.47357 Test: 0.66444
Epoch: 7400 Train: 11.55306 Test: 0.73082
Epoch: 7500 Train: 11.53639 Test: 0.67676
Epoch: 7600 Train: 11.79305 Test: 0.82636
Epoch: 7700 Train: 11.35848 Test: 0.64479
Epoch: 7800 Train: 11.45363 Test: 0.68514
Epoch: 7900 Train: 10.84308 Test: 0.51505
Epoch 8000: New minimal relative error: 3.76%, model saved.
Epoch: 8000 Train: 10.88542 Test: 0.51574
Epoch: 8100 Train: 10.65953 Test: 0.52694
Epoch: 8200 Train: 10.56686 Test: 0.49733
Epoch: 8300 Train: 10.47697 Test: 0.50867
Epoch: 8400 Train: 10.46015 Test: 0.48880
Epoch: 8500 Train: 10.48542 Test: 0.48689
Epoch: 8600 Train: 10.57617 Test: 0.54639
Epoch: 8700 Train: 10.48003 Test: 0.53236
Epoch: 8800 Train: 10.41916 Test: 0.53850
Epoch: 8900 Train: 10.28498 Test: 0.53111
Epoch: 9000 Train: 10.22259 Test: 0.57074
Epoch: 9100 Train: 10.01358 Test: 0.56011
Epoch: 9200 Train: 9.76519 Test: 0.50732
Epoch: 9300 Train: 9.73090 Test: 0.48856
Epoch 9400: New minimal relative error: 3.69%, model saved.
Epoch: 9400 Train: 9.54219 Test: 0.48022
Epoch: 9500 Train: 9.37818 Test: 0.46636
Epoch: 9600 Train: 9.26443 Test: 0.46178
Epoch: 9700 Train: 9.25467 Test: 0.46689
Epoch: 9800 Train: 9.35702 Test: 0.57229
Epoch: 9900 Train: 9.28855 Test: 0.48848
Epoch: 9999 Train: 9.31731 Test: 0.50433
Training Loss: tensor(9.3173)
Test Loss: tensor(0.5043)
Learned LE: [  0.8345601    0.01858733 -14.511293  ]
True LE: [ 8.6252856e-01 -3.5234098e-03 -1.4536466e+01]
Relative Error: [ 9.827089   8.504267   7.036294   5.474648   4.1650376  3.2625053
  2.8451383  2.140043   1.6291281  2.3450243  3.270683   4.062282
  4.6843925  5.145189   5.434078   5.5486364  5.5813775  5.5817385
  5.54071    5.5601435  5.443704   5.277343   5.255398   5.4095025
  5.5965357  5.689812   5.865551   6.0533714  6.300811   6.3698845
  6.3730874  6.265166   6.148102   5.5502067  4.909351   4.255134
  3.7161674  3.5851662  4.0612044  4.5210347  5.1450543  5.5749497
  6.0813003  5.91089    5.8591084  5.8360114  6.04546    6.4871798
  7.167367   7.6651444  7.842688   6.7093825  5.8322387  5.3408217
  5.420512   5.881426   6.448403   7.1339717  7.9736547  8.81846
  9.826755  10.692486   9.433053   8.185202   6.6514263  5.003418
  3.589635   2.6064823  2.1628098  2.1900754  1.738494   2.3562045
  3.2114909  3.9513369  4.5222387  4.926183   5.1835227  5.327001
  5.3556795  5.288636   5.252887   5.274662   5.3140793  5.1239314
  5.1317773  5.252636   5.5138736  5.6318684  5.7439785  5.921799
  6.205755   6.3384237  6.401286   6.358233   6.1545687  5.6275716
  4.9850416  4.3307395  3.7604551  3.4566581  3.7653718  4.087207
  4.759536   5.415555   5.797803   5.632628   5.4272203  5.338202
  5.5257597  5.9992557  6.7566743  7.0804186  7.066858   5.7829404
  4.8294578  4.2996373  4.1979723  4.5576515  5.243908   5.8999515
  6.645403   7.440086   8.40646    9.351559   9.195843   7.9450955
  6.415159   4.67524    3.1462605  2.0521545  1.5958512  2.0474062
  1.8991656  2.3779523  3.1297438  3.8088157  4.32938    4.6803164
  4.8781796  4.9560947  4.961879   4.9566126  4.9090157  4.9404516
  5.125139   4.9724307  4.928353   5.0415335  5.2525935  5.461045
  5.5773506  5.686633   5.9492483  6.1646247  6.2902412  6.328131
  6.0792694  5.626839   5.0106535  4.3671856  3.805614   3.3627224
  3.480422   3.6909342  4.201672   5.1896186  5.5621386  5.219995
  4.8095074  4.706264   4.9026313  5.3925853  5.9866595  6.160008
  6.2981987  5.101193   4.0625277  3.4429986  3.1553025  3.4030066
  4.0081444  4.8031774  5.4417257  6.208322   7.0315967  8.062129
  8.516635   7.861507   6.360213   4.525962   2.8643713  1.6262349
  1.3567349  1.8321935  2.1897678  2.3815122  2.9799132  3.5865564
  4.0643797  4.3817596  4.5470343  4.5677557  4.5011444  4.4291663
  4.4409657  4.5941715  4.8159103  4.748638   4.688653   4.729583
  4.899736   5.153437   5.257623   5.381533   5.524818   5.8271246
  6.0146065  6.148138   5.951034   5.528054   4.9735837  4.3763127
  3.8371296  3.3332183  3.2254186  3.3795636  3.7086434  4.5223274
  5.4052835  4.724289   4.2452826  4.092791   4.2672305  4.7573495
  5.1501746  5.291323   5.562838   4.457847   3.5164688  2.7812035
  2.4205022  2.4333737  2.819248   3.6502435  4.374057   5.095718
  5.7690477  6.81075    7.501029   7.615596   6.5296464  4.6052427
  2.784928   1.4252602  1.225698   1.6675495  2.3490076  2.3933175
  2.7912714  3.3159275  3.7508864  4.0291963  4.1447897  4.1232424
  4.0173726  3.8762836  3.8300486  4.0701814  4.390172   4.4223986
  4.3575225  4.355048   4.4368505  4.6434083  4.7773232  4.8931456
  4.9512134  5.279872   5.5428853  5.780151   5.738303   5.297803
  4.846994   4.335897   3.8304944  3.3217459  3.0594544  3.1821747
  3.2925289  3.8736675  4.7452645  4.335456   3.765609   3.5450983
  3.6547093  4.0867805  4.376069   4.476167   4.951961   3.9096274
  3.0812213  2.306098   1.8822836  1.6895738  1.9000094  2.4623325
  3.369638   4.046503   4.6445904  5.5332108  6.380569   7.0200353
  6.649284   4.9753046  2.9659965  1.5360117  1.165592   1.5496687
  2.185993   2.5248935  2.6258185  3.0282776  3.421326   3.6759083
  3.7556186  3.677918   3.4912593  3.2820935  3.1794105  3.3684473
  3.751299   3.9961402  3.9424512  3.8787708  3.8946922  3.971831
  4.1448994  4.2218     4.3249393  4.471477   4.841896   5.1502404
  5.3524837  4.936073   4.5853324  4.164944   3.730405   3.3279576
  2.9557176  2.9847865  2.996991   3.3258312  3.8545322  3.745724
  3.429901   3.074925   3.070475   3.4111109  3.693827   3.686891
  4.092326   3.5781689  2.6958544  2.028189   1.4544282  1.2666965
  1.1934743  1.4812453  2.1942437  3.1281085  3.7170343  4.4294004
  5.28465    5.9254208  6.1730223  5.6343412  3.4928098  1.8881649
  1.2285835  1.467906   2.0576859  2.5376093  2.5646029  2.7713315
  3.1010249  3.3298821  3.3889792  3.2722387  3.0149531  2.7044437
  2.5090246  2.6199296  2.8990254  3.2759554  3.477497   3.546998
  3.5891418  3.6399224  3.7123837  3.8385663]
