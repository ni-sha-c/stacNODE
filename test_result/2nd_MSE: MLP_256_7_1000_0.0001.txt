time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 4036.19995 Test: 3867.88574
Epoch 100: New minimal relative error: 69.18%, model saved.
Epoch: 100 Train: 152.85736 Test: 136.73828
Epoch 200: New minimal relative error: 62.81%, model saved.
Epoch: 200 Train: 23.39473 Test: 48.46577
Epoch 300: New minimal relative error: 44.51%, model saved.
Epoch: 300 Train: 19.93352 Test: 38.75462
Epoch 400: New minimal relative error: 41.74%, model saved.
Epoch: 400 Train: 15.26435 Test: 38.51865
Epoch 500: New minimal relative error: 22.11%, model saved.
Epoch: 500 Train: 2.13810 Test: 23.51078
Epoch: 600 Train: 4.43319 Test: 22.13513
Epoch: 700 Train: 1.36538 Test: 21.38883
Epoch: 800 Train: 21.60074 Test: 31.60077
Epoch: 900 Train: 0.80370 Test: 19.87392
Epoch: 1000 Train: 19.71243 Test: 44.70728
Epoch: 1100 Train: 1.09655 Test: 19.85765
Epoch: 1200 Train: 0.42443 Test: 19.42932
Epoch: 1300 Train: 0.92916 Test: 19.66364
Epoch: 1400 Train: 4.32533 Test: 21.52279
Epoch: 1500 Train: 0.36662 Test: 19.70493
Epoch: 1600 Train: 0.23594 Test: 19.00316
Epoch: 1700 Train: 1.00975 Test: 20.35512
Epoch 1800: New minimal relative error: 19.76%, model saved.
Epoch: 1800 Train: 1.16438 Test: 19.52945
Epoch: 1900 Train: 0.25993 Test: 19.22772
Epoch: 2000 Train: 0.36067 Test: 19.58566
Epoch: 2100 Train: 0.99253 Test: 20.35187
Epoch: 2200 Train: 0.17251 Test: 19.38307
Epoch: 2300 Train: 1.87222 Test: 19.83418
Epoch: 2400 Train: 0.17430 Test: 19.71199
Epoch: 2500 Train: 0.11593 Test: 19.73352
Epoch: 2600 Train: 2.53985 Test: 20.47586
Epoch 2700: New minimal relative error: 16.89%, model saved.
Epoch: 2700 Train: 0.49949 Test: 20.11300
Epoch: 2800 Train: 0.38388 Test: 20.06530
Epoch: 2900 Train: 0.10120 Test: 20.05215
Epoch: 3000 Train: 0.09445 Test: 20.36968
Epoch: 3100 Train: 0.13919 Test: 20.19228
Epoch: 3200 Train: 0.22642 Test: 20.55520
Epoch: 3300 Train: 0.19230 Test: 20.76332
Epoch: 3400 Train: 1.88328 Test: 21.37707
Epoch: 3500 Train: 0.19830 Test: 21.43663
Epoch: 3600 Train: 2.11372 Test: 24.56796
Epoch: 3700 Train: 0.78813 Test: 22.20485
Epoch: 3800 Train: 7.05415 Test: 25.48162
Epoch: 3900 Train: 0.34874 Test: 22.00933
Epoch: 4000 Train: 2.35685 Test: 24.39389
Epoch: 4100 Train: 3.55149 Test: 24.28019
Epoch: 4200 Train: 0.59779 Test: 22.13824
Epoch: 4300 Train: 0.25743 Test: 22.28392
Epoch: 4400 Train: 0.30382 Test: 22.63888
Epoch 4500: New minimal relative error: 15.22%, model saved.
Epoch: 4500 Train: 1.14089 Test: 23.48471
Epoch: 4600 Train: 0.16830 Test: 22.76256
Epoch: 4700 Train: 0.15533 Test: 22.62358
Epoch: 4800 Train: 1.51598 Test: 23.97022
Epoch: 4900 Train: 1.96765 Test: 25.26262
Epoch: 5000 Train: 0.15711 Test: 23.11913
Epoch: 5100 Train: 0.06970 Test: 23.32801
Epoch: 5200 Train: 0.12960 Test: 23.43198
Epoch: 5300 Train: 0.17664 Test: 23.70794
Epoch: 5400 Train: 0.05356 Test: 23.75931
Epoch: 5500 Train: 0.09230 Test: 23.89709
Epoch: 5600 Train: 0.21343 Test: 24.23100
Epoch: 5700 Train: 0.03146 Test: 24.21281
Epoch: 5800 Train: 0.60216 Test: 24.82886
Epoch: 5900 Train: 2.97539 Test: 26.73146
Epoch: 6000 Train: 0.04857 Test: 24.58665
Epoch: 6100 Train: 1.17950 Test: 26.19440
Epoch: 6200 Train: 0.68624 Test: 25.42936
Epoch: 6300 Train: 1.68334 Test: 25.95072
Epoch: 6400 Train: 0.62684 Test: 25.31498
Epoch: 6500 Train: 1.14039 Test: 25.87516
Epoch: 6600 Train: 2.69753 Test: 28.55879
Epoch: 6700 Train: 0.05004 Test: 25.52768
Epoch: 6800 Train: 0.02553 Test: 25.54713
Epoch: 6900 Train: 0.02399 Test: 25.66296
Epoch: 7000 Train: 0.37350 Test: 26.25154
Epoch: 7100 Train: 0.03950 Test: 25.85522
Epoch: 7200 Train: 0.02100 Test: 25.97244
Epoch: 7300 Train: 0.02123 Test: 26.15030
Epoch: 7400 Train: 0.04237 Test: 26.20630
Epoch: 7500 Train: 0.08330 Test: 26.29319
Epoch: 7600 Train: 0.09869 Test: 26.60820
Epoch: 7700 Train: 0.03347 Test: 26.56549
Epoch: 7800 Train: 1.99998 Test: 28.93061
Epoch 7900: New minimal relative error: 14.39%, model saved.
Epoch: 7900 Train: 0.01837 Test: 26.67876
Epoch: 8000 Train: 0.89322 Test: 28.06039
Epoch: 8100 Train: 0.01752 Test: 26.93333
Epoch: 8200 Train: 0.15285 Test: 26.95127
Epoch: 8300 Train: 0.01958 Test: 26.97302
Epoch: 8400 Train: 0.01618 Test: 27.11389
Epoch: 8500 Train: 0.05613 Test: 27.23726
Epoch: 8600 Train: 0.01555 Test: 27.31884
Epoch: 8700 Train: 0.04241 Test: 27.33390
Epoch: 8800 Train: 0.02239 Test: 27.36778
Epoch: 8900 Train: 0.01469 Test: 27.47547
Epoch: 9000 Train: 0.02105 Test: 27.57585
Epoch: 9100 Train: 0.01421 Test: 27.65415
Epoch: 9200 Train: 0.02149 Test: 27.73394
Epoch: 9300 Train: 0.10478 Test: 27.87642
Epoch: 9400 Train: 0.01358 Test: 27.86389
Epoch: 9500 Train: 0.12846 Test: 27.89976
Epoch: 9600 Train: 0.01316 Test: 27.98055
Epoch: 9700 Train: 0.01300 Test: 27.99341
Epoch: 9800 Train: 0.01809 Test: 28.08097
Epoch: 9900 Train: 0.01246 Test: 28.15617
Epoch: 9999 Train: 0.03819 Test: 28.23325
Training Loss: tensor(0.0382)
Test Loss: tensor(28.2333)
Learned LE: [ 0.9459317  -0.01968497 -5.171623  ]
True LE: [ 8.8412666e-01 -6.4741396e-03 -1.4550681e+01]
Relative Error: [3.6678486  3.9336894  4.174936   4.3659925  4.485569   4.5410113
 4.5749974  4.6211295  4.6512294  4.6077466  4.481359   4.316513
 4.1705875  4.073928   4.018185   3.9769735  3.9331768  3.8909311
 3.8598266  3.8355184  3.8041196  3.7533596  3.677545   3.580324
 3.4760256  3.379382   3.2922573  3.2040935  3.1017444  2.9884646
 2.8766563  2.7393508  2.5072958  2.143871   1.6968439  1.2860711
 1.0479581  1.0537598  1.4495747  2.2213376  3.1650503  4.083672
 4.8179235  5.2933326  5.487903   5.423378   5.170896   4.7843895
 4.2897983  3.707446   3.0826054  2.494192   2.036783   1.8019179
 1.8079255  1.9669706  2.1694324  2.3488772  2.5023901  2.663692
 2.8576577  3.0845854  3.330513   3.5800035  3.8177743  4.020447
 4.1566195  4.2149587  4.234059   4.270745   4.315703   4.2954335
 4.17789    4.009552   3.8628552  3.7771735  3.740049   3.7198331
 3.6973329  3.6720257  3.6426897  3.5993402  3.5370405  3.45927
 3.370938   3.279321   3.197544   3.1331346  3.0765705  3.011018
 2.9266438  2.8350365  2.7573416  2.6645732  2.470761   2.1277387
 1.6859455  1.2579169  0.995981   0.98930913 1.3602268  2.1329033
 3.1019106  4.044586   4.7704077  5.197991   5.31938    5.188747
 4.9032197  4.499913   3.9845817  3.3726938  2.7241163  2.1254218
 1.6670042  1.4446721  1.4816628  1.6720858  1.8976662  2.0931904
 2.2523696  2.405076   2.5774848  2.7754545  2.9902976  3.210805
 3.4300447  3.6359873  3.7973356  3.8799345  3.898536   3.9235332
 3.9818616  3.9969552  3.9020967  3.7330303  3.5777647  3.4913335
 3.4624913  3.4565513  3.4543116  3.4480073  3.4199364  3.3512816
 3.249609   3.1415708  3.0455434  2.9695935  2.9216986  2.8988373
 2.8777585  2.8335888  2.7598565  2.6769698  2.6190233  2.5662405
 2.4199936  2.1136017  1.6979887  1.2724372  0.97279716 0.930647
 1.2462543  1.9846224  2.958134   3.9231606  4.6507044  5.0406513
 5.10035    4.919935   4.617514   4.205524   3.6737483  3.0401962
 2.38109    1.7903957  1.3533599  1.1540917  1.2091234  1.4077129
 1.6379611  1.842247   2.00938    2.1602716  2.316907   2.4878557
 2.667065   2.8446324  3.0230083  3.2104208  3.3906572  3.516658
 3.5615218  3.5796094  3.641691   3.700702   3.651443   3.4948933
 3.3276994  3.2290947  3.195944   3.1914485  3.1981158  3.2050614
 3.1794271  3.0886059  2.947631   2.8122656  2.7217717  2.6818016
 2.6883898  2.7224622  2.7448583  2.7219748  2.6477444  2.550627
 2.481414   2.4485583  2.354186   2.098744   1.7231855  1.3245456
 0.99528486 0.886831   1.1173514  1.77503    2.7225235  3.7037275
 4.452982   4.829055   4.8485165  4.6353145  4.3313065  3.9241247
 3.3935957  2.758354   2.106177   1.5437088  1.1601326  1.0023896
 1.0400362  1.1931244  1.3905818  1.5878879  1.7652249  1.9274063
 2.08426    2.2410655  2.3896508  2.5157268  2.6286702  2.7631571
 2.9336019  3.1021805  3.2050247  3.2398376  3.2927823  3.3871417
 3.407879   3.2907534  3.118146   2.9994023  2.9533312  2.93939
 2.937766   2.9408703  2.913655   2.8111718  2.6429052  2.490962
 2.4292595  2.4584687  2.5494752  2.661952   2.74092    2.7473254
 2.670622   2.5375266  2.407753   2.3383765  2.2784877  2.0883257
 1.7609484  1.3995339  1.0717956  0.882036   0.99206936 1.5143503
 2.3907654  3.368717   4.1621523  4.566152   4.5788584  4.35068
 4.0608654  3.6822212  3.185082   2.577381   1.9496553  1.4279144
 1.1156254  1.0116812  1.0039338  1.0479441  1.1571592  1.3206054
 1.5071666  1.6964343  1.8767926  2.0443037  2.1850634  2.270469
 2.3048916  2.3486614  2.459454   2.6329722  2.8027189  2.8986049
 2.9503205  3.0479507  3.1433713  3.1032472  2.9452586  2.8007388
 2.7358563  2.7134416  2.697096   2.6781166  2.6344736  2.526289
 2.3526278  2.2048845  2.2016752  2.3383987  2.5391192  2.7447152
 2.8909948  2.9400775  2.8827517  2.7300532  2.5217724  2.3325794
 2.2228646  2.0886269  1.8250833  1.4989612  1.1867478  0.9477094
 0.904731   1.2350239  1.9710768  2.9080856  3.7489097  4.2374988
 4.2961416  4.0781918  3.8132873  3.491833   3.0644827  2.5157852
 1.9246181  1.4374782  1.1694618  1.095272   1.0494606  0.97088313
 0.9489486  1.0456936  1.2326742  1.4569002  1.6784104  1.879961
 2.0471642  2.1340995  2.1156352  2.0548253  2.0548956  2.1641054
 2.352711   2.5338192  2.6368515  2.715935   2.84342    2.9092221
 2.8079753  2.6367245  2.5323856  2.5038526  2.4903052  2.456111
 2.385279   2.2630544  2.0988197  1.9799083  2.051617   2.319866
 2.6409729  2.9349625  3.1493413  3.241358  ]
