time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.57%, model saved.
Epoch: 0 Train: 60973.28906 Test: 3998.87354
Epoch: 80 Train: 16171.73047 Test: 1529.92810
Epoch 160: New minimal relative error: 83.74%, model saved.
Epoch: 160 Train: 13332.10938 Test: 1349.72107
Epoch: 240 Train: 13204.64941 Test: 1378.17273
Epoch: 320 Train: 15220.62012 Test: 1263.22290
Epoch: 400 Train: 12985.31348 Test: 1073.54553
Epoch 480: New minimal relative error: 62.15%, model saved.
Epoch: 480 Train: 14647.37988 Test: 1372.09009
Epoch: 560 Train: 12491.79688 Test: 1182.98975
Epoch: 640 Train: 15071.64648 Test: 1308.86707
Epoch: 720 Train: 14149.72461 Test: 1265.26562
Epoch: 800 Train: 14449.87695 Test: 1272.91431
Epoch 880: New minimal relative error: 61.66%, model saved.
Epoch: 880 Train: 13001.83984 Test: 1111.68054
Epoch: 960 Train: 13631.46289 Test: 1133.52527
Epoch: 1040 Train: 11918.06738 Test: 1041.67981
Epoch: 1120 Train: 11382.71191 Test: 975.32471
Epoch: 1200 Train: 10901.28223 Test: 791.00073
Epoch: 1280 Train: 9818.20898 Test: 734.55029
Epoch: 1360 Train: 10088.92188 Test: 720.30475
Epoch: 1440 Train: 9527.78223 Test: 665.86700
Epoch 1520: New minimal relative error: 57.37%, model saved.
Epoch: 1520 Train: 8544.07910 Test: 606.20343
Epoch: 1600 Train: 8210.77539 Test: 470.05344
Epoch: 1680 Train: 7195.62744 Test: 372.78659
Epoch: 1760 Train: 6735.20752 Test: 357.90625
Epoch: 1840 Train: 5647.21045 Test: 297.51349
Epoch: 1920 Train: 3875.25879 Test: 119.17352
Epoch 2000: New minimal relative error: 27.19%, model saved.
Epoch: 2000 Train: 1959.98022 Test: 82.05809
Epoch: 2080 Train: 1457.85437 Test: 36.38198
Epoch: 2160 Train: 1034.12170 Test: 17.16836
Epoch 2240: New minimal relative error: 11.32%, model saved.
Epoch: 2240 Train: 764.87469 Test: 14.42700
Epoch: 2320 Train: 650.96826 Test: 16.17101
Epoch: 2400 Train: 498.76718 Test: 4.77299
Epoch: 2480 Train: 404.32199 Test: 13.88899
Epoch: 2560 Train: 357.61096 Test: 2.93118
Epoch: 2640 Train: 369.92081 Test: 8.57038
Epoch: 2720 Train: 331.83167 Test: 10.87651
Epoch: 2800 Train: 293.95084 Test: 3.02017
Epoch 2880: New minimal relative error: 4.57%, model saved.
Epoch: 2880 Train: 272.05081 Test: 2.81483
Epoch: 2960 Train: 248.39084 Test: 1.91471
Epoch: 3040 Train: 235.92862 Test: 1.53601
Epoch: 3120 Train: 220.81444 Test: 1.24054
Epoch: 3200 Train: 200.34589 Test: 2.23016
Epoch: 3280 Train: 191.14496 Test: 1.14979
Epoch: 3360 Train: 178.15442 Test: 0.91138
Epoch: 3440 Train: 166.56212 Test: 2.76267
Epoch: 3520 Train: 156.93880 Test: 0.98641
Epoch: 3600 Train: 188.47725 Test: 17.82362
Epoch: 3680 Train: 159.76733 Test: 1.63457
Epoch: 3760 Train: 164.62526 Test: 4.07595
Epoch: 3840 Train: 240.61725 Test: 31.46945
Epoch: 3920 Train: 176.57988 Test: 1.37156
Epoch: 4000 Train: 184.87206 Test: 1.93673
Epoch: 4080 Train: 198.28754 Test: 2.69018
Epoch: 4160 Train: 207.31863 Test: 2.26092
Epoch: 4240 Train: 171.22316 Test: 1.39766
Epoch: 4320 Train: 153.94731 Test: 1.10911
Epoch: 4400 Train: 146.99292 Test: 1.17157
Epoch: 4480 Train: 139.21172 Test: 0.93932
Epoch: 4560 Train: 132.37225 Test: 0.94147
Epoch: 4640 Train: 135.45378 Test: 2.48513
Epoch: 4720 Train: 136.34427 Test: 0.97927
Epoch: 4800 Train: 141.82594 Test: 1.09810
Epoch: 4880 Train: 149.22229 Test: 1.30884
Epoch: 4960 Train: 136.99670 Test: 1.20992
Epoch: 5040 Train: 127.38962 Test: 1.08083
Epoch: 5120 Train: 142.50568 Test: 5.43146
Epoch: 5200 Train: 119.75928 Test: 0.91079
Epoch: 5280 Train: 113.88567 Test: 0.83263
Epoch: 5360 Train: 102.75240 Test: 0.65989
Epoch: 5440 Train: 97.25258 Test: 1.06098
Epoch: 5520 Train: 93.87988 Test: 0.58504
Epoch 5600: New minimal relative error: 4.34%, model saved.
Epoch: 5600 Train: 90.71437 Test: 0.54826
Epoch: 5680 Train: 91.76570 Test: 0.55746
Epoch: 5760 Train: 108.91212 Test: 1.44923
Epoch: 5840 Train: 112.25620 Test: 1.07650
Epoch: 5920 Train: 105.18147 Test: 1.23106
Epoch: 6000 Train: 108.42716 Test: 4.16095
Epoch 6080: New minimal relative error: 3.63%, model saved.
Epoch: 6080 Train: 91.08760 Test: 0.65452
Epoch: 6160 Train: 84.51258 Test: 0.54724
Epoch: 6240 Train: 82.30728 Test: 0.50826
Epoch: 6320 Train: 79.62802 Test: 0.38577
Epoch: 6400 Train: 78.13229 Test: 0.44729
Epoch: 6480 Train: 81.38504 Test: 2.09713
Epoch: 6560 Train: 77.16149 Test: 0.40951
Epoch: 6640 Train: 76.79939 Test: 0.41553
Epoch: 6720 Train: 75.05460 Test: 0.38966
Epoch: 6800 Train: 71.99797 Test: 0.53824
Epoch: 6880 Train: 71.19884 Test: 0.31363
Epoch: 6960 Train: 70.87032 Test: 0.85337
Epoch: 7040 Train: 74.06460 Test: 0.39727
Epoch: 7120 Train: 95.55777 Test: 0.95871
Epoch: 7200 Train: 83.74372 Test: 0.61234
Epoch: 7280 Train: 75.69418 Test: 0.50844
Epoch: 7360 Train: 80.49847 Test: 0.67794
Epoch: 7440 Train: 97.03070 Test: 1.28576
Epoch: 7520 Train: 94.75267 Test: 2.49239
Epoch: 7600 Train: 74.65036 Test: 0.43781
Epoch: 7680 Train: 71.05323 Test: 0.41912
Epoch: 7760 Train: 70.97501 Test: 0.40538
Epoch: 7840 Train: 70.62762 Test: 0.33737
Epoch: 7920 Train: 67.04825 Test: 0.31358
Epoch: 7999 Train: 66.65657 Test: 0.40382
Training Loss: tensor(66.6566)
Test Loss: tensor(0.4038)
Learned LE: [  0.8110578   0.0760783 -14.518526 ]
True LE: [ 8.7599558e-01 -1.8137411e-04 -1.4552431e+01]
Relative Error: [3.9151793  4.7616687  5.653054   6.060664   6.1394854  6.0862255
 6.015369   5.8641944  5.4771023  4.8710217  4.3336263  3.420341
 2.6871204  1.875343   0.8989644  0.5195488  1.0962403  1.8960254
 2.6755672  3.2779343  3.6914089  4.0648804  4.197048   4.4227247
 4.4210644  4.2178483  3.8239844  3.4332519  2.9182935  2.4558504
 2.614937   2.924387   3.1438332  3.2985404  3.4719923  3.5347376
 3.4960978  3.4914825  3.2696087  2.902238   2.4210243  1.6831375
 0.9524895  0.9720799  1.3580029  1.3330892  1.1156055  1.0340753
 1.1130466  1.4363805  2.0557675  2.708868   2.775242   2.9753273
 3.073477   3.0988014  3.0470538  2.7924986  2.4769514  2.3699484
 2.422046   2.739687   3.3781059  4.2354436  5.200967   5.5455036
 5.6371675  5.3996882  5.4823895  5.180244   4.7408595  4.1707764
 3.6297019  3.2762132  2.6678712  1.8997043  1.002473   0.38045797
 1.0748129  1.5550443  2.0829961  2.679492   3.1381168  3.5430603
 3.7412376  3.9726284  3.986095   3.8751018  3.5425038  3.1533592
 2.6639428  2.252682   2.525838   2.8892484  3.1546733  3.3326893
 3.5090134  3.6639209  3.7168915  3.6631963  3.6368828  3.3301625
 2.943006   2.341491   1.6601043  1.02304    1.2721372  1.520934
 1.2321696  1.0203223  1.1797118  1.3498217  1.6549686  2.233275
 2.5707364  2.6854665  2.801565   2.8380148  2.9041302  2.828869
 2.5486145  2.3003397  2.1779099  2.4282053  2.9642403  3.6434577
 4.584146   5.098464   5.2034793  5.01278    4.6157913  4.357731
 4.0278497  3.5535204  2.9647446  2.5354855  2.2840104  1.8018448
 1.2103078  0.46097818 0.8903475  1.4135112  1.8081651  2.1215632
 2.5977428  2.9434886  3.243921   3.461989   3.591685   3.4371986
 3.287685   2.935987   2.4574096  2.1485107  2.3653944  2.6649995
 2.9657948  3.2095497  3.3944561  3.6063929  3.7244227  3.7881758
 3.67536    3.6074896  3.3568745  2.9121344  2.3231719  1.6626421
 1.2028005  1.3910005  1.4503319  1.319538   1.1765455  1.3595757
 1.5565864  1.8313626  2.1768484  2.3890839  2.4185002  2.4897206
 2.6099844  2.6997783  2.547368   2.260438   2.0375426  2.1025236
 2.4885216  3.059832   3.9064434  4.4895062  4.8379154  4.6702113
 4.1124268  3.6839035  3.4644358  3.1664577  2.6600504  2.0546281
 1.709296   1.5327677  1.0282269  0.61252093 0.5242291  1.1697872
 1.6706371  1.9253112  2.0825124  2.3443778  2.6821957  2.8711336
 3.1861193  3.175715   2.9704518  2.708314   2.2136955  1.7834759
 2.0797088  2.351333   2.596241   2.9407353  3.3095672  3.5150442
 3.64332    3.831925   4.0833654  4.136262   4.0712914  3.748839
 3.2354684  2.6833599  2.0486903  1.6655124  1.5362654  1.3728358
 1.358361   1.2886623  1.4186206  1.6929142  1.8511226  2.149075
 2.009924   1.9520564  2.16775    2.4357553  2.5059884  2.233327
 1.9526958  1.8004041  2.0801373  2.466188   3.023646   3.859587
 4.120415   4.1927166  3.7102926  3.1606739  2.8664799  2.8321977
 2.5779567  2.0414202  1.4728835  1.1321461  0.91300297 0.43419713
 0.46755522 0.5896207  1.2494718  1.7477705  1.9998312  1.9421206
 2.0582027  2.3522441  2.640235   2.844955   2.6864734  2.4452972
 2.0856462  1.4756371  1.4683872  1.8902994  2.2467961  2.5211048
 2.8732653  3.201686   3.4644186  3.7318003  4.000576   4.262316
 4.4251194  4.3553677  4.1698375  3.6734297  3.0144925  2.380988
 2.1035542  1.9262453  1.6548687  1.4133325  1.2246565  1.3064553
 1.608588   1.8050038  1.864787   1.7097785  1.6213185  1.9450061
 2.2348232  2.2630565  2.006659   1.6863655  1.4713068  1.7413255
 2.1433816  2.8292089  3.4721758  3.7749543  3.476168   2.9825246
 2.5470703  2.2153893  2.2611284  2.216321   1.7619288  1.1391361
 0.7515762  0.6352213  0.5129913  0.7313879  0.72848856 1.2163093
 1.7658429  2.0029445  1.8623121  1.7530875  1.9570136  2.239709
 2.3978739  2.2655463  1.9872699  1.5286635  1.196296   1.233669
 1.7233663  2.0946212  2.3669517  2.5710804  2.8985353  3.396694
 3.6957808  3.936762   4.131238   4.3209453  4.351055   4.2910066
 3.9800887  3.433549   2.7565243  2.2096026  2.1470928  1.9715637
 1.767857   1.4798942  1.0891863  1.2905946  1.5465459  1.6372943
 1.6439002  1.556969   1.4425495  1.824481   1.9277695  1.8731048
 1.6898477  1.393402   1.2613022  1.5268191  2.306389   3.116628
 3.4094434  2.9771395  2.4742663  2.2099822  1.8480126  1.7892268
 1.9115173  1.6731899  1.0724609  0.6736847  0.7623006  0.7399888
 0.86036474 1.0367374  1.1320655  1.6870277  2.0139003  1.873368
 1.3912301  1.4873712  1.7174846  1.8988513 ]
