time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.26%, model saved.
Epoch: 0 Train: 172946.57812 Test: 4258.89941
Epoch: 80 Train: 44243.71094 Test: 2284.79199
Epoch: 160 Train: 41190.74609 Test: 2355.49414
Epoch: 240 Train: 36389.30078 Test: 1354.31409
Epoch: 320 Train: 40023.05078 Test: 1444.48962
Epoch: 400 Train: 34686.65625 Test: 1340.51685
Epoch: 480 Train: 40277.26953 Test: 1960.12537
Epoch: 560 Train: 41638.14844 Test: 1675.30420
Epoch: 640 Train: 39157.33984 Test: 1641.96924
Epoch 720: New minimal relative error: 95.04%, model saved.
Epoch: 720 Train: 38109.94141 Test: 1484.88281
Epoch 800: New minimal relative error: 61.77%, model saved.
Epoch: 800 Train: 36312.56250 Test: 1337.65222
Epoch: 880 Train: 36033.46875 Test: 1550.23706
Epoch: 960 Train: 37885.75391 Test: 1469.81042
Epoch: 1040 Train: 34478.74219 Test: 1427.77588
Epoch 1120: New minimal relative error: 52.88%, model saved.
Epoch: 1120 Train: 34868.46484 Test: 1338.91614
Epoch: 1200 Train: 32501.92383 Test: 1176.90588
Epoch: 1280 Train: 28848.74609 Test: 961.69305
Epoch: 1360 Train: 28112.47852 Test: 860.66187
Epoch: 1440 Train: 26380.44141 Test: 799.05664
Epoch: 1520 Train: 24087.61914 Test: 652.49756
Epoch: 1600 Train: 23429.26172 Test: 584.39276
Epoch: 1680 Train: 20930.38672 Test: 494.03925
Epoch: 1760 Train: 19319.38867 Test: 492.13565
Epoch: 1840 Train: 18760.13477 Test: 367.42316
Epoch: 1920 Train: 16786.07422 Test: 276.26935
Epoch: 2000 Train: 15836.94629 Test: 246.10976
Epoch: 2080 Train: 14356.21777 Test: 188.01395
Epoch: 2160 Train: 15144.36719 Test: 233.76192
Epoch: 2240 Train: 12469.01465 Test: 151.48354
Epoch: 2320 Train: 11054.53711 Test: 132.63510
Epoch: 2400 Train: 9373.03320 Test: 119.19975
Epoch: 2480 Train: 9383.16406 Test: 124.51259
Epoch 2560: New minimal relative error: 34.16%, model saved.
Epoch: 2560 Train: 7937.65918 Test: 103.40875
Epoch: 2640 Train: 6349.02539 Test: 70.02854
Epoch: 2720 Train: 5948.65918 Test: 80.66771
Epoch 2800: New minimal relative error: 29.49%, model saved.
Epoch: 2800 Train: 5434.99219 Test: 46.13250
Epoch: 2880 Train: 5447.72998 Test: 113.31195
Epoch: 2960 Train: 5873.82617 Test: 175.23689
Epoch 3040: New minimal relative error: 27.55%, model saved.
Epoch: 3040 Train: 4462.20508 Test: 36.26902
Epoch 3120: New minimal relative error: 17.20%, model saved.
Epoch: 3120 Train: 3895.84302 Test: 28.06858
Epoch: 3200 Train: 3899.16968 Test: 29.34062
Epoch: 3280 Train: 3302.96362 Test: 17.07447
Epoch 3360: New minimal relative error: 11.91%, model saved.
Epoch: 3360 Train: 3121.67529 Test: 12.06713
Epoch: 3440 Train: 2918.31836 Test: 22.11252
Epoch: 3520 Train: 2974.07593 Test: 20.27415
Epoch: 3600 Train: 2976.82471 Test: 13.80932
Epoch: 3680 Train: 2880.33472 Test: 13.30126
Epoch: 3760 Train: 2708.69434 Test: 14.06019
Epoch 3840: New minimal relative error: 11.88%, model saved.
Epoch: 3840 Train: 2473.40894 Test: 13.25917
Epoch: 3920 Train: 2660.70117 Test: 39.87094
Epoch: 4000 Train: 2476.85620 Test: 14.64413
Epoch: 4080 Train: 2426.65869 Test: 13.68750
Epoch: 4160 Train: 2128.74854 Test: 15.64014
Epoch: 4240 Train: 2039.77625 Test: 19.23465
Epoch: 4320 Train: 2117.05469 Test: 13.30978
Epoch: 4400 Train: 2156.15088 Test: 12.97702
Epoch: 4480 Train: 2309.53076 Test: 19.43668
Epoch: 4560 Train: 2519.22192 Test: 13.20781
Epoch: 4640 Train: 2258.12915 Test: 9.75856
Epoch: 4720 Train: 2137.05469 Test: 7.91084
Epoch: 4800 Train: 1900.05762 Test: 10.23471
Epoch 4880: New minimal relative error: 9.73%, model saved.
Epoch: 4880 Train: 1792.45117 Test: 9.72939
Epoch: 4960 Train: 1648.68640 Test: 6.94009
Epoch: 5040 Train: 1577.00232 Test: 6.04555
Epoch: 5120 Train: 1519.60339 Test: 4.40973
Epoch: 5200 Train: 1471.90710 Test: 3.79939
Epoch 5280: New minimal relative error: 9.40%, model saved.
Epoch: 5280 Train: 1378.26562 Test: 3.53900
Epoch 5360: New minimal relative error: 8.39%, model saved.
Epoch: 5360 Train: 1347.50769 Test: 3.07927
Epoch: 5440 Train: 1453.02405 Test: 3.37151
Epoch 5520: New minimal relative error: 8.39%, model saved.
Epoch: 5520 Train: 1289.10449 Test: 2.88656
Epoch: 5600 Train: 1349.05212 Test: 3.46069
Epoch: 5680 Train: 1317.70691 Test: 7.16686
Epoch 5760: New minimal relative error: 5.62%, model saved.
Epoch: 5760 Train: 1279.29700 Test: 5.10916
Epoch: 5840 Train: 1265.31323 Test: 3.31715
Epoch: 5920 Train: 1177.21692 Test: 4.57362
Epoch: 6000 Train: 1400.26355 Test: 7.20940
Epoch: 6080 Train: 1371.08521 Test: 5.62728
Epoch: 6160 Train: 1189.23706 Test: 3.32050
Epoch: 6240 Train: 1254.02673 Test: 2.41340
Epoch: 6320 Train: 1286.38477 Test: 12.30031
Epoch: 6400 Train: 1218.70679 Test: 11.17399
Epoch: 6480 Train: 1055.23767 Test: 8.81477
Epoch: 6560 Train: 1199.50598 Test: 5.53407
Epoch: 6640 Train: 1375.95325 Test: 18.48213
Epoch: 6720 Train: 1102.04395 Test: 3.51111
Epoch: 6800 Train: 1167.52087 Test: 3.33722
Epoch: 6880 Train: 1125.64246 Test: 6.99454
Epoch: 6960 Train: 1154.27563 Test: 9.75404
Epoch: 7040 Train: 1160.98352 Test: 8.60025
Epoch: 7120 Train: 1192.79993 Test: 9.37470
Epoch: 7200 Train: 1291.55762 Test: 7.30619
Epoch: 7280 Train: 1261.42810 Test: 18.28368
Epoch: 7360 Train: 1296.24243 Test: 3.92776
Epoch: 7440 Train: 1247.33716 Test: 3.79549
Epoch: 7520 Train: 1154.07312 Test: 3.52236
Epoch: 7600 Train: 1158.59167 Test: 2.57633
Epoch: 7680 Train: 1108.00964 Test: 3.40487
Epoch: 7760 Train: 1095.08838 Test: 2.74370
Epoch: 7840 Train: 1041.68384 Test: 2.76055
Epoch: 7920 Train: 979.00671 Test: 2.26729
Epoch: 7999 Train: 941.62793 Test: 2.78761
Training Loss: tensor(941.6279)
Test Loss: tensor(2.7876)
Learned LE: [  0.93542933  -0.30639485 -14.440112  ]
True LE: [ 8.2414645e-01  6.1368677e-03 -1.4502056e+01]
Relative Error: [26.21116   27.837515  29.339949  29.95394   30.310211  30.509964
 30.26947   29.796263  29.169983  28.418812  27.62065   26.910187
 26.130312  25.289913  24.429693  23.591938  22.836594  24.836483
 27.532223  29.907469  31.97878   33.15004   34.22366   35.17963
 36.006046  36.798786  37.628857  38.365524  39.07562   39.473843
 39.965324  40.220814  40.302143  40.3467    39.612953  37.896877
 35.98058   34.013027  31.88067   29.555428  27.1491    24.677174
 22.17337   19.514563  16.826296  14.224548  11.75163    9.386116
  7.316871   7.550787   8.466363   9.622509  10.799651  12.019431
 13.23989   14.465324  15.689927  16.937996  18.199244  19.51128
 20.901737  22.371115  23.910418  25.541708  27.10222   28.141382
 28.60163   28.930033  28.856104  28.528732  28.046734  27.423838
 26.733849  26.153622  25.489422  24.744171  23.943356  23.093924
 22.245493  22.514154  25.326574  27.749527  29.816198  31.111797
 32.2099    33.156906  33.943798  34.572136  35.253906  35.863304
 36.40926   36.87434   36.995316  37.41581   37.49009   37.50336
 37.73115   36.323074  34.539917  32.672325  30.72369   28.517025
 26.237598  23.888998  21.417103  18.87723   16.262253  13.620442
 11.074332   8.773203   6.8212614  6.6671143  7.3497524  8.361938
  9.570328  10.775741  11.889485  12.989214  14.098184  15.211577
 16.360317  17.526108  18.776903  20.146244  21.601734  23.097187
 24.605665  26.107258  26.733017  27.199709  27.349127  27.184795
 26.875544  26.40951   25.85757   25.409534  24.8903    24.271164
 23.487347  22.541443  21.680141  20.980389  22.705082  25.21392
 27.266184  28.686104  30.034739  31.164515  31.971355  32.571846
 33.024452  33.52285   33.91084   34.294846  34.43375   34.469967
 34.744114  34.7415    35.035126  34.81554   33.224102  31.465712
 29.70552   27.637589  25.505478  23.263723  20.89193   18.46486
 15.970352  13.416769  10.834358   8.419176   6.5094175  5.9013186
  6.353478   7.207039   8.263317   9.358469  10.476524  11.468929
 12.473408  13.43933   14.43253   15.458922  16.539726  17.742006
 19.087626  20.543875  22.011753  23.448233  24.637508  25.233646
 25.67625   25.699394  25.584406  25.313461  24.92794   24.573278
 24.260912  23.795383  22.989868  22.112694  21.280064  20.434996
 19.521898  22.131878  24.410933  25.981897  27.482355  28.815102
 29.977491  30.66875   31.071505  31.363089  31.637526  31.818909
 32.033264  31.892508  31.835136  32.01655   32.328346  32.645554
 31.947115  30.39828   28.724707  26.906273  24.927616  22.810146
 20.57724   18.28763   15.925865  13.486552  11.004138   8.5467415
  6.4236693  5.4206057  5.521915   6.0195827  6.804927   7.7286015
  8.707666   9.675939  10.560671  11.402312  12.303602  13.271925
 14.223218  15.216213  16.364305  17.700298  19.136482  20.602388
 22.040302  23.01951   23.705065  24.037416  24.102129  24.023853
 23.84387   23.577415  23.496195  23.192802  22.518503  21.750248
 20.871254  19.824833  18.822706  18.49625   21.146372  22.767046
 24.401123  25.998152  27.456251  28.53437   29.17801   29.291565
 29.371416  29.444286  29.532608  29.680868  29.310093  29.152023
 29.531658  29.999443  29.866543  29.240349  27.87173   26.262072
 24.445484  22.522562  20.443443  18.315443  16.106636  13.827877
 11.479132   9.0883875  6.8459444  5.3304815  4.9846344  5.0796022
  5.545335   6.237045   7.0213227  7.840816   8.641718   9.408659
 10.055449  10.77023   11.5644    12.483295  13.525913  14.607319
 15.90219   17.408709  18.994234  20.54124   21.532295  22.198843
 22.536858  22.728487  22.724806  22.582714  22.421188  22.429436
 21.960306  21.369736  20.50605   19.468796  18.43595   17.401201
 17.189642  19.404787  21.049023  22.707916  24.283907  25.613493
 26.657736  27.294939  27.408594  27.264496  27.14003   27.020859
 27.039696  26.768276  26.518082  27.064425  27.604958  27.232477
 26.68704   25.643713  24.085007  22.336767  20.45163   18.50228
 16.502857  14.441858  12.278004   9.9909115  7.7156386  5.738584
  4.961647   4.542309   4.600679   4.9711866  5.5131063  6.121231
  6.767061   7.3771567  7.976258   8.477749   8.952876   9.533047
 10.252759  11.1736965 12.455649  13.860786  15.374128  16.95688
 18.549696  19.737389  20.419298  20.831814  21.193502  21.370728
 21.423496  21.528366  21.32015   20.800735  20.19869   19.337183
 18.291235  17.273857  16.13347   15.460059  17.19641   18.847708
 20.637026  22.252222  23.437592  24.456047 ]
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.26%, model saved.
Epoch: 0 Train: 172946.57812 Test: 4258.89941
Epoch: 80 Train: 44243.71094 Test: 2284.79199
Epoch: 160 Train: 41190.74609 Test: 2355.49414
Epoch: 240 Train: 36389.30078 Test: 1354.31409
Epoch: 320 Train: 40023.05078 Test: 1444.48962
Epoch: 400 Train: 34686.65625 Test: 1340.51685
Epoch: 480 Train: 40277.26953 Test: 1960.12537
Epoch: 560 Train: 41638.14844 Test: 1675.30420
Epoch: 640 Train: 39157.33984 Test: 1641.96924
Epoch 720: New minimal relative error: 95.04%, model saved.
Epoch: 720 Train: 38109.94141 Test: 1484.88281
Epoch 800: New minimal relative error: 61.77%, model saved.
Epoch: 800 Train: 36312.56250 Test: 1337.65222
Epoch: 880 Train: 36033.46875 Test: 1550.23706
Epoch: 960 Train: 37885.75391 Test: 1469.81042
Epoch: 1040 Train: 34478.74219 Test: 1427.77588
Epoch 1120: New minimal relative error: 52.88%, model saved.
Epoch: 1120 Train: 34868.46484 Test: 1338.91614
Epoch: 1200 Train: 32501.92383 Test: 1176.90588
Epoch: 1280 Train: 28848.74609 Test: 961.69305
Epoch: 1360 Train: 28112.47852 Test: 860.66187
Epoch: 1440 Train: 26380.44141 Test: 799.05664
Epoch: 1520 Train: 24087.61914 Test: 652.49756
Epoch: 1600 Train: 23429.26172 Test: 584.39276
Epoch: 1680 Train: 20930.38672 Test: 494.03925
Epoch: 1760 Train: 19319.38867 Test: 492.13565
Epoch: 1840 Train: 18760.13477 Test: 367.42316
Epoch: 1920 Train: 16786.07422 Test: 276.26935
Epoch: 2000 Train: 15836.94629 Test: 246.10976
Epoch: 2080 Train: 14356.21777 Test: 188.01395
Epoch: 2160 Train: 15144.36719 Test: 233.76192
Epoch: 2240 Train: 12469.01465 Test: 151.48354
Epoch: 2320 Train: 11054.53711 Test: 132.63510
Epoch: 2400 Train: 9373.03320 Test: 119.19975
Epoch: 2480 Train: 9383.16406 Test: 124.51259
Epoch 2560: New minimal relative error: 34.16%, model saved.
Epoch: 2560 Train: 7937.65918 Test: 103.40875
Epoch: 2640 Train: 6349.02539 Test: 70.02854
Epoch: 2720 Train: 5948.65918 Test: 80.66771
Epoch 2800: New minimal relative error: 29.49%, model saved.
Epoch: 2800 Train: 5434.99219 Test: 46.13250
Epoch: 2880 Train: 5447.72998 Test: 113.31195
Epoch: 2960 Train: 5873.82617 Test: 175.23689
Epoch 3040: New minimal relative error: 27.55%, model saved.
Epoch: 3040 Train: 4462.20508 Test: 36.26902
Epoch 3120: New minimal relative error: 17.20%, model saved.
Epoch: 3120 Train: 3895.84302 Test: 28.06858
Epoch: 3200 Train: 3899.16968 Test: 29.34062
Epoch: 3280 Train: 3302.96362 Test: 17.07447
Epoch 3360: New minimal relative error: 11.91%, model saved.
Epoch: 3360 Train: 3121.67529 Test: 12.06713
Epoch: 3440 Train: 2918.31836 Test: 22.11252
Epoch: 3520 Train: 2974.07593 Test: 20.27415
Epoch: 3600 Train: 2976.82471 Test: 13.80932
Epoch: 3680 Train: 2880.33472 Test: 13.30126
Epoch: 3760 Train: 2708.69434 Test: 14.06019
Epoch 3840: New minimal relative error: 11.88%, model saved.
Epoch: 3840 Train: 2473.40894 Test: 13.25917
Epoch: 3920 Train: 2660.70117 Test: 39.87094
Epoch: 4000 Train: 2476.85620 Test: 14.64413
Epoch: 4080 Train: 2426.65869 Test: 13.68750
Epoch: 4160 Train: 2128.74854 Test: 15.64014
Epoch: 4240 Train: 2039.77625 Test: 19.23465
Epoch: 4320 Train: 2117.05469 Test: 13.30978
Epoch: 4400 Train: 2156.15088 Test: 12.97702
Epoch: 4480 Train: 2309.53076 Test: 19.43668
Epoch: 4560 Train: 2519.22192 Test: 13.20781
Epoch: 4640 Train: 2258.12915 Test: 9.75856
Epoch: 4720 Train: 2137.05469 Test: 7.91084
Epoch: 4800 Train: 1900.05762 Test: 10.23471
Epoch 4880: New minimal relative error: 9.73%, model saved.
Epoch: 4880 Train: 1792.45117 Test: 9.72939
Epoch: 4960 Train: 1648.68640 Test: 6.94009
Epoch: 5040 Train: 1577.00232 Test: 6.04555
Epoch: 5120 Train: 1519.60339 Test: 4.40973
Epoch: 5200 Train: 1471.90710 Test: 3.79939
Epoch 5280: New minimal relative error: 9.40%, model saved.
Epoch: 5280 Train: 1378.26562 Test: 3.53900
Epoch 5360: New minimal relative error: 8.39%, model saved.
Epoch: 5360 Train: 1347.50769 Test: 3.07927
Epoch: 5440 Train: 1453.02405 Test: 3.37151
Epoch 5520: New minimal relative error: 8.39%, model saved.
Epoch: 5520 Train: 1289.10449 Test: 2.88656
Epoch: 5600 Train: 1349.05212 Test: 3.46069
Epoch: 5680 Train: 1317.70691 Test: 7.16686
Epoch 5760: New minimal relative error: 5.62%, model saved.
Epoch: 5760 Train: 1279.29700 Test: 5.10916
Epoch: 5840 Train: 1265.31323 Test: 3.31715
Epoch: 5920 Train: 1177.21692 Test: 4.57362
Epoch: 6000 Train: 1400.26355 Test: 7.20940
Epoch: 6080 Train: 1371.08521 Test: 5.62728
Epoch: 6160 Train: 1189.23706 Test: 3.32050
Epoch: 6240 Train: 1254.02673 Test: 2.41340
Epoch: 6320 Train: 1286.38477 Test: 12.30031
Epoch: 6400 Train: 1218.70679 Test: 11.17399
Epoch: 6480 Train: 1055.23767 Test: 8.81477
Epoch: 6560 Train: 1199.50598 Test: 5.53407
Epoch: 6640 Train: 1375.95325 Test: 18.48213
Epoch: 6720 Train: 1102.04395 Test: 3.51111
Epoch: 6800 Train: 1167.52087 Test: 3.33722
Epoch: 6880 Train: 1125.64246 Test: 6.99454
Epoch: 6960 Train: 1154.27563 Test: 9.75404
Epoch: 7040 Train: 1160.98352 Test: 8.60025
Epoch: 7120 Train: 1192.79993 Test: 9.37470
Epoch: 7200 Train: 1291.55762 Test: 7.30619
Epoch: 7280 Train: 1261.42810 Test: 18.28368
Epoch: 7360 Train: 1296.24243 Test: 3.92776
Epoch: 7440 Train: 1247.33716 Test: 3.79549
Epoch: 7520 Train: 1154.07312 Test: 3.52236
Epoch: 7600 Train: 1158.59167 Test: 2.57633
Epoch: 7680 Train: 1108.00964 Test: 3.40487
Epoch: 7760 Train: 1095.08838 Test: 2.74370
Epoch: 7840 Train: 1041.68384 Test: 2.76055
Epoch: 7920 Train: 979.00671 Test: 2.26729
Epoch: 7999 Train: 941.62793 Test: 2.78761
Training Loss: tensor(941.6279)
Test Loss: tensor(2.7876)
Learned LE: [  0.93542933  -0.30639485 -14.440112  ]
True LE: [ 8.2414645e-01  6.1368677e-03 -1.4502056e+01]
Relative Error: [26.21116   27.837515  29.339949  29.95394   30.310211  30.509964
 30.26947   29.796263  29.169983  28.418812  27.62065   26.910187
 26.130312  25.289913  24.429693  23.591938  22.836594  24.836483
 27.532223  29.907469  31.97878   33.15004   34.22366   35.17963
 36.006046  36.798786  37.628857  38.365524  39.07562   39.473843
 39.965324  40.220814  40.302143  40.3467    39.612953  37.896877
 35.98058   34.013027  31.88067   29.555428  27.1491    24.677174
 22.17337   19.514563  16.826296  14.224548  11.75163    9.386116
  7.316871   7.550787   8.466363   9.622509  10.799651  12.019431
 13.23989   14.465324  15.689927  16.937996  18.199244  19.51128
 20.901737  22.371115  23.910418  25.541708  27.10222   28.141382
 28.60163   28.930033  28.856104  28.528732  28.046734  27.423838
 26.733849  26.153622  25.489422  24.744171  23.943356  23.093924
 22.245493  22.514154  25.326574  27.749527  29.816198  31.111797
 32.2099    33.156906  33.943798  34.572136  35.253906  35.863304
 36.40926   36.87434   36.995316  37.41581   37.49009   37.50336
 37.73115   36.323074  34.539917  32.672325  30.72369   28.517025
 26.237598  23.888998  21.417103  18.87723   16.262253  13.620442
 11.074332   8.773203   6.8212614  6.6671143  7.3497524  8.361938
  9.570328  10.775741  11.889485  12.989214  14.098184  15.211577
 16.360317  17.526108  18.776903  20.146244  21.601734  23.097187
 24.605665  26.107258  26.733017  27.199709  27.349127  27.184795
 26.875544  26.40951   25.85757   25.409534  24.8903    24.271164
 23.487347  22.541443  21.680141  20.980389  22.705082  25.21392
 27.266184  28.686104  30.034739  31.164515  31.971355  32.571846
 33.024452  33.52285   33.91084   34.294846  34.43375   34.469967
 34.744114  34.7415    35.035126  34.81554   33.224102  31.465712
 29.70552   27.637589  25.505478  23.263723  20.89193   18.46486
 15.970352  13.416769  10.834358   8.419176   6.5094175  5.9013186
  6.353478   7.207039   8.263317   9.358469  10.476524  11.468929
 12.473408  13.43933   14.43253   15.458922  16.539726  17.742006
 19.087626  20.543875  22.011753  23.448233  24.637508  25.233646
 25.67625   25.699394  25.584406  25.313461  24.92794   24.573278
 24.260912  23.795383  22.989868  22.112694  21.280064  20.434996
 19.521898  22.131878  24.410933  25.981897  27.482355  28.815102
 29.977491  30.66875   31.071505  31.363089  31.637526  31.818909
 32.033264  31.892508  31.835136  32.01655   32.328346  32.645554
 31.947115  30.39828   28.724707  26.906273  24.927616  22.810146
 20.57724   18.28763   15.925865  13.486552  11.004138   8.5467415
  6.4236693  5.4206057  5.521915   6.0195827  6.804927   7.7286015
  8.707666   9.675939  10.560671  11.402312  12.303602  13.271925
 14.223218  15.216213  16.364305  17.700298  19.136482  20.602388
 22.040302  23.01951   23.705065  24.037416  24.102129  24.023853
 23.84387   23.577415  23.496195  23.192802  22.518503  21.750248
 20.871254  19.824833  18.822706  18.49625   21.146372  22.767046
 24.401123  25.998152  27.456251  28.53437   29.17801   29.291565
 29.371416  29.444286  29.532608  29.680868  29.310093  29.152023
 29.531658  29.999443  29.866543  29.240349  27.87173   26.262072
 24.445484  22.522562  20.443443  18.315443  16.106636  13.827877
 11.479132   9.0883875  6.8459444  5.3304815  4.9846344  5.0796022
  5.545335   6.237045   7.0213227  7.840816   8.641718   9.408659
 10.055449  10.77023   11.5644    12.483295  13.525913  14.607319
 15.90219   17.408709  18.994234  20.54124   21.532295  22.198843
 22.536858  22.728487  22.724806  22.582714  22.421188  22.429436
 21.960306  21.369736  20.50605   19.468796  18.43595   17.401201
 17.189642  19.404787  21.049023  22.707916  24.283907  25.613493
 26.657736  27.294939  27.408594  27.264496  27.14003   27.020859
 27.039696  26.768276  26.518082  27.064425  27.604958  27.232477
 26.68704   25.643713  24.085007  22.336767  20.45163   18.50228
 16.502857  14.441858  12.278004   9.9909115  7.7156386  5.738584
  4.961647   4.542309   4.600679   4.9711866  5.5131063  6.121231
  6.767061   7.3771567  7.976258   8.477749   8.952876   9.533047
 10.252759  11.1736965 12.455649  13.860786  15.374128  16.95688
 18.549696  19.737389  20.419298  20.831814  21.193502  21.370728
 21.423496  21.528366  21.32015   20.800735  20.19869   19.337183
 18.291235  17.273857  16.13347   15.460059  17.19641   18.847708
 20.637026  22.252222  23.437592  24.456047 ]
