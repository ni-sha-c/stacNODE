time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 20000
num_train: 3000
num_test: 3000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 4
reg_param: 100.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 212.834793091 Test: 6.913417816
Epoch 0: New minimal relative error: 6.91%, model saved.
Epoch: 200 Train: 16.108518600 Test: 10.951607704
Epoch: 400 Train: 17.004011154 Test: 10.921996117
Epoch: 600 Train: 12.907894135 Test: 10.562112808
Epoch: 800 Train: 12.635499954 Test: 10.810669899
Epoch: 1000 Train: 12.451250076 Test: 10.575713158
Epoch: 1200 Train: 13.725217819 Test: 10.264856339
Epoch: 1400 Train: 12.367745399 Test: 10.314345360
Epoch: 1600 Train: 12.715840340 Test: 10.585752487
Epoch: 1800 Train: 12.591472626 Test: 10.600596428
Epoch: 2000 Train: 13.815256119 Test: 10.407824516
Epoch: 2200 Train: 12.951910973 Test: 10.401113510
Epoch: 2400 Train: 13.748163223 Test: 11.007959366
Epoch: 2600 Train: 13.257608414 Test: 10.148295403
Epoch: 2800 Train: 12.735874176 Test: 10.262472153
Epoch: 3000 Train: 12.703819275 Test: 10.337018967
Epoch: 3200 Train: 12.941572189 Test: 10.201882362
Epoch: 3400 Train: 14.489297867 Test: 10.379914284
Epoch: 3600 Train: 13.222202301 Test: 10.347441673
Epoch: 3800 Train: 12.356751442 Test: 10.523994446
Epoch: 4000 Train: 12.083333015 Test: 10.376111984
Epoch: 4200 Train: 12.402870178 Test: 10.844502449
Epoch: 4400 Train: 12.166961670 Test: 10.559267044
Epoch: 4600 Train: 12.126437187 Test: 10.499758720
Epoch: 4800 Train: 12.344182014 Test: 10.904194832
Epoch: 5000 Train: 12.294070244 Test: 10.582265854
Epoch: 5200 Train: 12.837389946 Test: 11.253643036
Epoch: 5400 Train: 12.519157410 Test: 10.649125099
Epoch: 5600 Train: 12.269598007 Test: 10.595631599
Epoch: 5800 Train: 12.201720238 Test: 10.577818871
Epoch: 6000 Train: 12.164342880 Test: 10.603334427
Epoch: 6200 Train: 12.168324471 Test: 10.619827271
Epoch: 6400 Train: 11.980332375 Test: 10.459732056
Epoch: 6600 Train: 12.026020050 Test: 10.577322960
Epoch: 6800 Train: 12.071069717 Test: 10.609056473
Epoch: 7000 Train: 11.997944832 Test: 10.673900604
Epoch: 7200 Train: 12.006976128 Test: 10.514351845
Epoch: 7400 Train: 11.966694832 Test: 10.600712776
Epoch: 7600 Train: 11.978534698 Test: 10.593393326
Epoch: 7800 Train: 11.962432861 Test: 10.589612961
Epoch: 8000 Train: 11.968712807 Test: 10.608567238
Epoch: 8200 Train: 11.972814560 Test: 10.632056236
Epoch: 8400 Train: 11.976478577 Test: 10.596985817
Epoch: 8600 Train: 11.982772827 Test: 10.632395744
Epoch: 8800 Train: 11.984101295 Test: 10.639219284
Epoch: 9000 Train: 11.986383438 Test: 10.656937599
Epoch: 9200 Train: 11.992433548 Test: 10.648520470
Epoch: 9400 Train: 11.976154327 Test: 10.645833015
Epoch: 9600 Train: 11.996047974 Test: 10.602096558
Epoch: 9800 Train: 11.994028091 Test: 10.660451889
Epoch: 10000 Train: 11.999126434 Test: 10.677040100
Epoch: 10200 Train: 12.006063461 Test: 10.671842575
Epoch: 10400 Train: 12.004979134 Test: 10.682140350
Epoch: 10600 Train: 12.002190590 Test: 10.692023277
Epoch: 10800 Train: 11.999777794 Test: 10.672495842
Epoch: 11000 Train: 12.007160187 Test: 10.684940338
Epoch: 11200 Train: 11.998613358 Test: 10.670390129
Epoch: 11400 Train: 12.007430077 Test: 10.684033394
Epoch: 11600 Train: 12.008556366 Test: 10.694417000
Epoch: 11800 Train: 12.008030891 Test: 10.663887024
Epoch: 12000 Train: 12.007793427 Test: 10.643366814
Epoch: 12200 Train: 12.014749527 Test: 10.691707611
Epoch: 12400 Train: 12.012186050 Test: 10.727593422
Epoch: 12600 Train: 12.014522552 Test: 10.669036865
Epoch: 12800 Train: 12.009590149 Test: 10.691774368
Epoch: 13000 Train: 12.014238358 Test: 10.689232826
Epoch: 13200 Train: 12.015484810 Test: 10.702256203
Epoch: 13400 Train: 12.022053719 Test: 10.704075813
Epoch: 13600 Train: 12.015689850 Test: 10.662039757
Epoch: 13800 Train: 12.040347099 Test: 10.739257812
Epoch: 14000 Train: 12.033679962 Test: 10.738893509
Epoch: 14200 Train: 12.031455040 Test: 10.735887527
Epoch: 14400 Train: 12.020859718 Test: 10.688690186
Epoch: 14600 Train: 12.010500908 Test: 10.701951981
Epoch: 14800 Train: 12.026095390 Test: 10.698523521
Epoch: 15000 Train: 12.017854691 Test: 10.707589149
Epoch: 15200 Train: 12.017562866 Test: 10.711750984
Epoch: 15400 Train: 12.016098022 Test: 10.704310417
Epoch: 15600 Train: 12.014978409 Test: 10.631594658
Epoch: 15800 Train: 12.025106430 Test: 10.720587730
Epoch: 16000 Train: 12.014247894 Test: 10.712985039
Epoch: 16200 Train: 12.031199455 Test: 10.713784218
Epoch: 16400 Train: 12.012784958 Test: 10.692040443
Epoch: 16600 Train: 12.019766808 Test: 10.711161613
Epoch: 16800 Train: 12.020029068 Test: 10.685895920
Epoch: 17000 Train: 12.026333809 Test: 10.732053757
Epoch: 17200 Train: 12.025291443 Test: 10.699845314
Epoch: 17400 Train: 12.022357941 Test: 10.722620964
Epoch: 17600 Train: 12.026199341 Test: 10.690068245
Epoch: 17800 Train: 12.025493622 Test: 10.643409729
Epoch: 18000 Train: 12.021085739 Test: 10.710149765
Epoch: 18200 Train: 12.022192001 Test: 10.718618393
Epoch: 18400 Train: 12.022156715 Test: 10.724499702
Epoch: 18600 Train: 12.025548935 Test: 10.730586052
Epoch: 18800 Train: 12.027661324 Test: 10.754329681
Epoch: 19000 Train: 12.024816513 Test: 10.725724220
Epoch: 19200 Train: 12.022930145 Test: 10.706685066
Epoch: 19400 Train: 12.025888443 Test: 10.734021187
Epoch: 19600 Train: 12.030860901 Test: 10.745117188
Epoch: 19800 Train: 12.024632454 Test: 10.722218513
Epoch: 19999 Train: 12.027280807 Test: 10.729362488
Training Loss: tensor(12.0273)
Test Loss: tensor(10.7294)
True Mean x: tensor(3.2361, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(0.2620, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.1394, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0070, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0114)
Jacobian term Test Loss: tensor(0.0115)
Learned LE: [ 0.17980786 -0.17846252]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
