time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 2000
num_test: 2000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 3
reg_param: 200.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 390.825866699 Test: 5.288953781
Epoch 0: New minimal relative error: 5.29%, model saved.
Epoch: 100 Train: 21.901720047 Test: 15.233081818
Epoch: 200 Train: 21.079425812 Test: 13.563451767
Epoch: 300 Train: 14.142757416 Test: 11.561762810
Epoch: 400 Train: 13.591184616 Test: 11.526448250
Epoch: 500 Train: 14.905149460 Test: 11.204557419
Epoch: 600 Train: 13.087531090 Test: 11.681997299
Epoch: 700 Train: 16.730293274 Test: 10.844133377
Epoch: 800 Train: 13.777457237 Test: 11.539569855
Epoch: 900 Train: 13.254756927 Test: 11.554450989
Epoch: 1000 Train: 13.920221329 Test: 11.546961784
Epoch: 1100 Train: 13.898438454 Test: 11.518268585
Epoch: 1200 Train: 13.577277184 Test: 11.447857857
Epoch: 1300 Train: 13.331332207 Test: 11.408902168
Epoch: 1400 Train: 15.017000198 Test: 11.279888153
Epoch: 1500 Train: 13.356262207 Test: 11.118955612
Epoch: 1600 Train: 13.383180618 Test: 11.387750626
Epoch: 1700 Train: 14.609051704 Test: 11.482076645
Epoch: 1800 Train: 14.009946823 Test: 11.458086014
Epoch: 1900 Train: 14.779428482 Test: 11.527025223
Epoch: 2000 Train: 13.894088745 Test: 11.295292854
Epoch: 2100 Train: 13.667898178 Test: 11.267952919
Epoch: 2200 Train: 13.487129211 Test: 11.552435875
Epoch: 2300 Train: 13.651577950 Test: 11.662128448
Epoch: 2400 Train: 13.969175339 Test: 11.625128746
Epoch: 2500 Train: 14.203402519 Test: 11.542729378
Epoch: 2600 Train: 13.532973289 Test: 11.555685997
Epoch: 2700 Train: 14.252248764 Test: 11.768618584
Epoch: 2800 Train: 13.656072617 Test: 11.565193176
Epoch: 2900 Train: 13.816112518 Test: 11.640063286
Epoch: 3000 Train: 14.082605362 Test: 11.787699699
Epoch: 3100 Train: 15.524418831 Test: 11.647389412
Epoch: 3200 Train: 14.571574211 Test: 11.540917397
Epoch: 3300 Train: 14.073062897 Test: 11.498230934
Epoch: 3400 Train: 13.577190399 Test: 11.559912682
Epoch: 3500 Train: 13.606831551 Test: 11.588260651
Epoch: 3600 Train: 14.751992226 Test: 11.496991158
Epoch: 3700 Train: 16.024139404 Test: 11.521487236
Epoch: 3800 Train: 15.122411728 Test: 11.308644295
Epoch: 3900 Train: 15.227180481 Test: 11.338031769
Epoch: 4000 Train: 15.111089706 Test: 10.971745491
Epoch: 4100 Train: 14.199996948 Test: 10.999891281
Epoch: 4200 Train: 14.033845901 Test: 11.068694115
Epoch: 4300 Train: 14.216821671 Test: 11.206908226
Epoch: 4400 Train: 14.607128143 Test: 11.201834679
Epoch: 4500 Train: 14.870029449 Test: 11.177760124
Epoch: 4600 Train: 14.868141174 Test: 11.361004829
Epoch: 4700 Train: 15.624414444 Test: 11.635630608
Epoch: 4800 Train: 16.264236450 Test: 11.620923042
Epoch: 4900 Train: 15.735774994 Test: 11.294164658
Epoch: 5000 Train: 15.202894211 Test: 11.143310547
Epoch: 5100 Train: 15.630421638 Test: 11.019368172
Epoch: 5200 Train: 16.085760117 Test: 11.043269157
Epoch: 5300 Train: 16.608119965 Test: 10.849707603
Epoch: 5400 Train: 16.977008820 Test: 10.830170631
Epoch: 5500 Train: 16.848701477 Test: 10.871026993
Epoch: 5600 Train: 16.057693481 Test: 10.968421936
Epoch: 5700 Train: 16.274734497 Test: 11.048698425
Epoch: 5800 Train: 15.597657204 Test: 11.135963440
Epoch: 5900 Train: 17.002286911 Test: 11.091795921
Epoch: 6000 Train: 18.867111206 Test: 11.150235176
Epoch: 6100 Train: 18.576705933 Test: 11.129603386
Epoch: 6200 Train: 17.589817047 Test: 11.006820679
Epoch: 6300 Train: 16.587209702 Test: 10.844326019
Epoch: 6400 Train: 15.987765312 Test: 10.859316826
Epoch: 6500 Train: 16.200386047 Test: 10.887877464
Epoch: 6600 Train: 15.894510269 Test: 11.043650627
Epoch: 6700 Train: 16.044837952 Test: 11.104395866
Epoch: 6800 Train: 16.535408020 Test: 11.043424606
Epoch: 6900 Train: 16.364538193 Test: 11.059283257
Epoch: 7000 Train: 15.817003250 Test: 11.105295181
Epoch: 7100 Train: 15.643421173 Test: 11.209459305
Epoch: 7200 Train: 15.554278374 Test: 11.299782753
Epoch: 7300 Train: 15.387499809 Test: 11.313895226
Epoch: 7400 Train: 15.380488396 Test: 11.347360611
Epoch: 7500 Train: 15.259511948 Test: 11.363205910
Epoch: 7600 Train: 15.133147240 Test: 11.357332230
Epoch: 7700 Train: 15.281498909 Test: 11.355508804
Epoch: 7800 Train: 15.547409058 Test: 11.333985329
Epoch: 7900 Train: 15.715875626 Test: 11.294730186
Epoch: 8000 Train: 15.938377380 Test: 11.297305107
Epoch: 8100 Train: 15.708791733 Test: 11.248998642
Epoch: 8200 Train: 15.418112755 Test: 11.212388039
Epoch: 8300 Train: 15.341817856 Test: 11.231344223
Epoch: 8400 Train: 15.392004967 Test: 11.304544449
Epoch: 8500 Train: 15.641839981 Test: 11.355744362
Epoch: 8600 Train: 16.078231812 Test: 11.393491745
Epoch: 8700 Train: 16.275764465 Test: 11.355539322
Epoch: 8800 Train: 16.325492859 Test: 11.329105377
Epoch: 8900 Train: 16.571540833 Test: 11.289093971
Epoch: 9000 Train: 16.406101227 Test: 11.262742996
Epoch: 9100 Train: 16.141319275 Test: 11.247242928
Epoch: 9200 Train: 16.170705795 Test: 11.234907150
Epoch: 9300 Train: 16.259767532 Test: 11.213680267
Epoch: 9400 Train: 16.121713638 Test: 11.188912392
Epoch: 9500 Train: 15.916166306 Test: 11.153944969
Epoch: 9600 Train: 15.918095589 Test: 11.143129349
Epoch: 9700 Train: 15.955457687 Test: 11.147017479
Epoch: 9800 Train: 16.085643768 Test: 11.119078636
Epoch: 9900 Train: 16.658432007 Test: 11.088687897
Epoch: 9999 Train: 16.869585037 Test: 11.068658829
Training Loss: tensor(16.8696)
Test Loss: tensor(11.0687)
True Mean x: tensor(3.1528, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(-0.0899, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.1648, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0066, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0276)
Jacobian term Test Loss: tensor(0.0309)
Learned LE: [ 0.4905595  -0.03097379]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
