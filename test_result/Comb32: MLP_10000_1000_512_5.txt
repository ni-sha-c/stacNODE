time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.90%, model saved.
Epoch: 0 Train: 3924.91040 Test: 4173.56494
Epoch 100: New minimal relative error: 38.10%, model saved.
Epoch: 100 Train: 86.65911 Test: 50.43685
Epoch: 200 Train: 8.61803 Test: 12.30242
Epoch: 300 Train: 7.71031 Test: 12.66304
Epoch 400: New minimal relative error: 26.70%, model saved.
Epoch: 400 Train: 3.11123 Test: 7.41432
Epoch 500: New minimal relative error: 19.19%, model saved.
Epoch: 500 Train: 4.66923 Test: 4.99572
Epoch: 600 Train: 10.83302 Test: 10.67049
Epoch: 700 Train: 1.06357 Test: 4.15795
Epoch: 800 Train: 2.88247 Test: 4.58217
Epoch: 900 Train: 2.86542 Test: 7.56950
Epoch: 1000 Train: 1.78967 Test: 4.43789
Epoch: 1100 Train: 0.61170 Test: 3.14762
Epoch: 1200 Train: 0.17037 Test: 2.66492
Epoch: 1300 Train: 5.15393 Test: 6.21899
Epoch: 1400 Train: 2.60481 Test: 4.61552
Epoch: 1500 Train: 2.74849 Test: 5.25141
Epoch: 1600 Train: 2.52584 Test: 5.09665
Epoch: 1700 Train: 0.89621 Test: 3.17430
Epoch: 1800 Train: 0.81327 Test: 3.78338
Epoch: 1900 Train: 0.35417 Test: 2.92992
Epoch: 2000 Train: 1.11031 Test: 2.88325
Epoch 2100: New minimal relative error: 9.47%, model saved.
Epoch: 2100 Train: 1.86508 Test: 2.72578
Epoch: 2200 Train: 5.00659 Test: 7.80786
Epoch: 2300 Train: 0.41394 Test: 2.54387
Epoch: 2400 Train: 0.19753 Test: 2.36786
Epoch: 2500 Train: 1.46853 Test: 4.00852
Epoch: 2600 Train: 0.66069 Test: 2.66311
Epoch: 2700 Train: 0.79812 Test: 3.05610
Epoch: 2800 Train: 1.84622 Test: 4.18853
Epoch: 2900 Train: 2.78571 Test: 4.31236
Epoch: 3000 Train: 0.12693 Test: 2.33681
Epoch: 3100 Train: 0.83907 Test: 3.21967
Epoch: 3200 Train: 1.10231 Test: 3.19177
Epoch: 3300 Train: 2.35365 Test: 5.13039
Epoch: 3400 Train: 1.20314 Test: 3.72214
Epoch: 3500 Train: 0.51943 Test: 2.75934
Epoch: 3600 Train: 0.36668 Test: 2.69373
Epoch: 3700 Train: 1.93924 Test: 4.28816
Epoch: 3800 Train: 0.62009 Test: 2.96239
Epoch: 3900 Train: 1.50525 Test: 3.91721
Epoch: 4000 Train: 0.08830 Test: 2.43257
Epoch: 4100 Train: 0.10327 Test: 2.47072
Epoch: 4200 Train: 0.02610 Test: 2.41640
Epoch: 4300 Train: 0.47242 Test: 2.91184
Epoch: 4400 Train: 0.21117 Test: 2.63866
Epoch: 4500 Train: 1.20249 Test: 3.68549
Epoch: 4600 Train: 0.05889 Test: 2.48050
Epoch: 4700 Train: 0.30658 Test: 2.79382
Epoch: 4800 Train: 0.12428 Test: 2.58684
Epoch: 4900 Train: 0.12436 Test: 2.57658
Epoch: 5000 Train: 0.25044 Test: 2.74828
Epoch: 5100 Train: 0.30411 Test: 2.82990
Epoch: 5200 Train: 0.25372 Test: 2.77896
Epoch: 5300 Train: 0.20453 Test: 2.71191
Epoch: 5400 Train: 0.61794 Test: 3.08825
Epoch: 5500 Train: 0.04131 Test: 2.55793
Epoch: 5600 Train: 0.06902 Test: 2.60337
Epoch: 5700 Train: 0.33739 Test: 2.86392
Epoch: 5800 Train: 0.02037 Test: 2.55450
Epoch: 5900 Train: 0.10356 Test: 2.66493
Epoch: 6000 Train: 0.05272 Test: 2.57939
Epoch: 6100 Train: 0.02369 Test: 2.56278
Epoch: 6200 Train: 0.02562 Test: 2.58154
Epoch: 6300 Train: 0.07354 Test: 2.64393
Epoch: 6400 Train: 0.01427 Test: 2.54160
Epoch: 6500 Train: 0.01548 Test: 2.58411
Epoch: 6600 Train: 1.12054 Test: 3.92973
Epoch: 6700 Train: 0.01717 Test: 2.55445
Epoch: 6800 Train: 0.01510 Test: 2.56130
Epoch: 6900 Train: 0.41423 Test: 3.03505
Epoch: 7000 Train: 0.02670 Test: 2.57523
Epoch: 7100 Train: 0.02531 Test: 2.56913
Epoch: 7200 Train: 0.01974 Test: 2.57554
Epoch: 7300 Train: 0.02355 Test: 2.57633
Epoch: 7400 Train: 0.01418 Test: 2.54742
Epoch: 7500 Train: 0.01515 Test: 2.57128
Epoch: 7600 Train: 0.20374 Test: 2.68117
Epoch: 7700 Train: 0.05152 Test: 2.57612
Epoch: 7800 Train: 0.02471 Test: 2.57242
Epoch: 7900 Train: 0.04109 Test: 2.57576
Epoch: 8000 Train: 0.48919 Test: 2.82038
Epoch: 8100 Train: 0.01222 Test: 2.52581
Epoch: 8200 Train: 0.01094 Test: 2.54914
Epoch: 8300 Train: 0.01750 Test: 2.56734
Epoch: 8400 Train: 0.01084 Test: 2.53218
Epoch: 8500 Train: 0.01030 Test: 2.52990
Epoch: 8600 Train: 0.01186 Test: 2.52558
Epoch: 8700 Train: 0.01058 Test: 2.54844
Epoch: 8800 Train: 0.59475 Test: 2.90827
Epoch: 8900 Train: 0.03386 Test: 2.53092
Epoch: 9000 Train: 0.01305 Test: 2.53017
Epoch: 9100 Train: 0.03915 Test: 2.56147
Epoch: 9200 Train: 0.00960 Test: 2.50840
Epoch: 9300 Train: 0.01126 Test: 2.51121
Epoch: 9400 Train: 0.00905 Test: 2.50992
Epoch: 9500 Train: 0.11319 Test: 2.50358
Epoch: 9600 Train: 0.01935 Test: 2.49056
Epoch: 9700 Train: 0.00968 Test: 2.48464
Epoch: 9800 Train: 0.24805 Test: 2.79839
Epoch: 9900 Train: 0.01150 Test: 2.47509
Epoch: 9999 Train: 0.02645 Test: 2.50331
Training Loss: tensor(0.0265)
Test Loss: tensor(2.5033)
Learned LE: [ 0.84884745  0.07339854 -4.199541  ]
True LE: [ 8.6467743e-01  1.0484164e-02 -1.4550235e+01]
Relative Error: [ 5.453337   5.864596   6.2438965  6.5385594  6.726913   6.825194
  6.8644958  6.867916   6.8452663  6.8019996  6.7485385  6.699708
  6.6679773  6.6574717  6.6651225  6.6894727  6.739185   6.835479
  7.004825   7.260627   7.5940104  7.986927   8.430242   8.920589
  9.440473   9.951391  10.402472  10.744854  10.9420395 10.976776
 10.853576  10.582752  10.163748   9.603526   8.950112   8.28054
  7.669155   7.1657376  6.7728357  6.448282   6.142805   5.8281226
  5.4978976  5.1554303  4.8059106  4.456491   4.1194654  3.8145654
  3.566575   3.393944   3.2987356  3.2696269  3.2919333  3.352841
  3.4400826  3.5415134  3.6492314  3.7650995  3.9042447  4.090776
  4.344699   4.6696415  5.0467305  5.433948   5.7716928  6.006138
  6.1217303  6.145341   6.1168923  6.0634813  5.99817    5.9296904
  5.8705306  5.8340354  5.825372   5.838733   5.8657007  5.9055133
  5.9687376  6.0785246  6.262535   6.535188   6.886106   7.2961793
  7.757029   8.2648945  8.799364   9.319008   9.774811  10.12158
 10.324782  10.368887  10.26192   10.018422   9.6327505  9.099939
  8.462236   7.798639   7.196654   6.720787   6.3724413  6.095703
  5.8300824  5.5460525  5.238742   4.912127   4.570975   4.222896
  3.8822598  3.5728621  3.323014   3.1516585  3.0577614  3.026955
  3.0432765  3.093437   3.165124   3.2481134  3.3364854  3.4341533
  3.558609   3.7356925  3.984695   4.3036714  4.6664796  5.0259724
  5.31937    5.492503   5.535827   5.4865046  5.392859   5.28721
  5.185511   5.099772   5.0444684  5.0291076  5.0481935  5.084618
  5.126314   5.17693    5.2516894  5.373399   5.5691624  5.853482
  6.2160134  6.6368446  7.109242   7.6275945  8.1672     8.68524
  9.136928   9.483274   9.693378   9.752711   9.672104   9.470433
  9.135262   8.646316   8.036186   7.3831716  6.7893786  6.339406
  6.034267   5.802533   5.573885   5.3183255  5.033027   4.7221656
  4.3898354  4.0427084  3.6972451  3.381235   3.1271257  2.9539914
  2.8574443  2.8204036  2.8263793  2.8613265  2.9137485  2.9761524
  3.0457318  3.127773   3.2402442  3.4096856  3.6533422  3.9638448
  4.308466   4.6362686  4.883883   4.997625   4.972428   4.855168
  4.702712   4.5522118  4.4227114  4.3289347  4.285982   4.298568
  4.3485193  4.4061007  4.4573045  4.513399   4.5961466  4.7274437
  4.930891   5.2210264  5.5875077  6.0116124  6.4881845  7.010359
  7.5467796  8.052873   8.490295   8.829352   9.04548    9.125245
  9.079655   8.932299   8.665406   8.238725   7.670623   7.034005
  6.4460793  6.015975   5.74823    5.556502   5.3614945  5.132679
  4.869195   4.5762906  4.255483   3.911084   3.5607483  3.2367363
  2.9758418  2.798038   2.694993   2.6472008  2.6381283  2.6538503
  2.6839492  2.724907   2.7776856  2.847703   2.9512386  3.113593
  3.3501089  3.648415   3.970023   4.2624216  4.464726   4.5241413
  4.4381056  4.2617927  4.0607147  3.8772912  3.7319891  3.6389985
  3.6135964  3.6567643  3.7388194  3.8165576  3.8729503  3.9283764
  4.013207   4.1482763  4.3525515  4.6399107  5.001607   5.4199815
  5.892655   6.4122796  6.9402456  7.4269643  7.84109    8.1646385
  8.382334   8.484106   8.478975   8.394719   8.211546   7.866852
  7.35865    6.747421   6.1614347  5.7396593  5.498727   5.3400626
  5.1746716  4.971532   4.731513   4.460165   4.156101   3.8191144
  3.4661603  3.1336062  2.8641143  2.6794875  2.567471   2.5051284
  2.4769015  2.4703684  2.4767702  2.4968255  2.5356681  2.5975773
  2.6946573  2.8494728  3.0755641  3.3573537  3.651644   3.90537
  4.063819   4.07715    3.9420795  3.7197013  3.485181   3.2860632
  3.1401243  3.0560737  3.047678   3.1174293  3.229197   3.327185
  3.3883412  3.4387403  3.5173085  3.6464849  3.840486   4.11278
  4.45808    4.8600497  5.319179   5.830592   6.3482404  6.8135552
  7.198915   7.4987664  7.7097096  7.8296995  7.865388   7.8464823
  7.7586703  7.5153494  7.0870757  6.5145655  5.9267144  5.4963684
  5.2665334  5.1329274  4.993417   4.815525   4.6012874  4.356063
  4.0755773  3.7533567  3.4026432  3.062894   2.7834635  2.5911915
  2.470138   2.3919985  2.342237   2.3116984  2.2947247  2.2960336
  2.3239753  2.381326   2.4739594  2.6200886  2.8325408  3.0944211
  3.359064   3.5725946  3.6894474  3.6666338  3.497023   3.2446709
  2.9956357  2.8015628  2.6719182  2.602061   2.6030836  2.6862166
  2.81996    2.9391582  3.010607   3.0581741  3.125319   3.2382853
  3.408667   3.6495686  3.9617875  4.33253    4.764801   5.2594533
  5.7672987  6.2161593  6.573779   6.8452797]
