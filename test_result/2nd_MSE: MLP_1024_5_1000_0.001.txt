time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 101.14%, model saved.
Epoch: 0 Train: 3447.36401 Test: 4165.20312
Epoch: 100 Train: 45.50370 Test: 53.85009
Epoch 200: New minimal relative error: 38.36%, model saved.
Epoch: 200 Train: 5.53011 Test: 10.67365
Epoch: 300 Train: 6.62248 Test: 14.31852
Epoch: 400 Train: 8.03642 Test: 12.35270
Epoch: 500 Train: 1.10131 Test: 3.62832
Epoch 600: New minimal relative error: 33.89%, model saved.
Epoch: 600 Train: 0.57364 Test: 1.61888
Epoch: 700 Train: 4.91183 Test: 7.23431
Epoch 800: New minimal relative error: 27.19%, model saved.
Epoch: 800 Train: 4.70022 Test: 3.11896
Epoch 900: New minimal relative error: 26.78%, model saved.
Epoch: 900 Train: 4.26833 Test: 5.86886
Epoch: 1000 Train: 8.74451 Test: 8.65631
Epoch: 1100 Train: 2.49808 Test: 2.88159
Epoch: 1200 Train: 3.34137 Test: 6.29696
Epoch: 1300 Train: 7.59256 Test: 11.06516
Epoch: 1400 Train: 0.28034 Test: 0.59079
Epoch 1500: New minimal relative error: 12.56%, model saved.
Epoch: 1500 Train: 0.40622 Test: 0.66769
Epoch: 1600 Train: 6.39215 Test: 4.33508
Epoch: 1700 Train: 3.24506 Test: 4.09480
Epoch: 1800 Train: 2.91167 Test: 2.45302
Epoch: 1900 Train: 1.94196 Test: 1.14698
Epoch: 2000 Train: 0.23653 Test: 0.32927
Epoch: 2100 Train: 0.56729 Test: 0.74503
Epoch: 2200 Train: 0.20948 Test: 0.27322
Epoch: 2300 Train: 3.66897 Test: 5.36294
Epoch: 2400 Train: 0.32004 Test: 0.40481
Epoch: 2500 Train: 1.14738 Test: 1.91920
Epoch: 2600 Train: 0.40928 Test: 0.65992
Epoch: 2700 Train: 1.79836 Test: 1.03648
Epoch: 2800 Train: 0.49350 Test: 0.48112
Epoch: 2900 Train: 0.36335 Test: 0.48584
Epoch: 3000 Train: 1.40378 Test: 2.03117
Epoch: 3100 Train: 0.31743 Test: 0.41564
Epoch: 3200 Train: 0.32473 Test: 0.46415
Epoch 3300: New minimal relative error: 7.89%, model saved.
Epoch: 3300 Train: 0.48429 Test: 0.44210
Epoch: 3400 Train: 0.48528 Test: 0.60046
Epoch: 3500 Train: 1.42967 Test: 1.66469
Epoch: 3600 Train: 0.31753 Test: 0.45393
Epoch: 3700 Train: 0.20304 Test: 0.19784
Epoch: 3800 Train: 0.47351 Test: 0.56682
Epoch: 3900 Train: 0.71250 Test: 0.88067
Epoch: 4000 Train: 0.83268 Test: 1.07181
Epoch: 4100 Train: 0.16584 Test: 0.26616
Epoch: 4200 Train: 0.44576 Test: 0.47121
Epoch: 4300 Train: 0.46807 Test: 0.57411
Epoch: 4400 Train: 1.13917 Test: 1.28593
Epoch: 4500 Train: 0.03908 Test: 0.09668
Epoch: 4600 Train: 0.81381 Test: 0.96872
Epoch: 4700 Train: 0.36597 Test: 0.46487
Epoch: 4800 Train: 0.42216 Test: 0.43425
Epoch: 4900 Train: 0.95944 Test: 0.99417
Epoch: 5000 Train: 0.32011 Test: 0.37967
Epoch 5100: New minimal relative error: 7.69%, model saved.
Epoch: 5100 Train: 0.54051 Test: 0.63911
Epoch: 5200 Train: 0.10390 Test: 0.11950
Epoch: 5300 Train: 0.26004 Test: 0.32273
Epoch: 5400 Train: 0.28597 Test: 0.41572
Epoch: 5500 Train: 0.05165 Test: 0.04912
Epoch: 5600 Train: 0.10000 Test: 0.14138
Epoch: 5700 Train: 0.60950 Test: 0.61920
Epoch: 5800 Train: 0.21694 Test: 0.31467
Epoch: 5900 Train: 0.10460 Test: 0.12417
Epoch: 6000 Train: 0.02937 Test: 0.04771
Epoch: 6100 Train: 0.36790 Test: 0.44142
Epoch: 6200 Train: 0.27045 Test: 0.38732
Epoch: 6300 Train: 0.21377 Test: 0.34037
Epoch: 6400 Train: 0.04339 Test: 0.08192
Epoch: 6500 Train: 0.66724 Test: 0.85754
Epoch: 6600 Train: 0.16425 Test: 0.24054
Epoch 6700: New minimal relative error: 5.90%, model saved.
Epoch: 6700 Train: 0.14889 Test: 0.21557
Epoch: 6800 Train: 0.68911 Test: 0.84593
Epoch: 6900 Train: 0.51012 Test: 0.62133
Epoch: 7000 Train: 0.03139 Test: 0.04621
Epoch: 7100 Train: 0.02186 Test: 0.03313
Epoch 7200: New minimal relative error: 4.94%, model saved.
Epoch: 7200 Train: 0.10791 Test: 0.12405
Epoch: 7300 Train: 0.02875 Test: 0.04104
Epoch: 7400 Train: 0.03606 Test: 0.04901
Epoch: 7500 Train: 0.08459 Test: 0.09655
Epoch: 7600 Train: 0.24130 Test: 0.23674
Epoch: 7700 Train: 0.08148 Test: 0.09882
Epoch: 7800 Train: 0.01635 Test: 0.02094
Epoch: 7900 Train: 0.03174 Test: 0.03174
Epoch: 8000 Train: 0.01534 Test: 0.02049
Epoch: 8100 Train: 0.07426 Test: 0.09695
Epoch: 8200 Train: 0.24456 Test: 0.27167
Epoch: 8300 Train: 0.03210 Test: 0.04480
Epoch: 8400 Train: 0.04998 Test: 0.06250
Epoch: 8500 Train: 0.00770 Test: 0.01439
Epoch: 8600 Train: 0.00621 Test: 0.01317
Epoch: 8700 Train: 0.07101 Test: 0.10286
Epoch: 8800 Train: 0.02990 Test: 0.04120
Epoch: 8900 Train: 0.01995 Test: 0.02755
Epoch: 9000 Train: 0.00508 Test: 0.01373
Epoch: 9100 Train: 0.01921 Test: 0.02929
Epoch: 9200 Train: 0.08300 Test: 0.07241
Epoch: 9300 Train: 0.03969 Test: 0.06170
Epoch: 9400 Train: 0.02198 Test: 0.03395
Epoch: 9500 Train: 0.00929 Test: 0.01452
Epoch: 9600 Train: 0.02390 Test: 0.03561
Epoch: 9700 Train: 0.41172 Test: 0.53556
Epoch: 9800 Train: 0.01509 Test: 0.02666
Epoch: 9900 Train: 0.02329 Test: 0.02944
Epoch: 9999 Train: 0.07607 Test: 0.04719
Training Loss: tensor(0.0761)
Test Loss: tensor(0.0472)
Learned LE: [ 0.86164474 -0.03057673 -5.407639  ]
True LE: [ 8.7777609e-01  1.2759108e-03 -1.4556389e+01]
Relative Error: [ 2.5812888   2.589347    2.7316446   2.9508002   3.129692    3.1539445
  2.962611    2.5529566   1.951891    1.1914319   0.41104954  0.9957999
  2.1952024   3.500824    4.8291554   6.1205277   7.3357306   8.477213
  9.622645   10.560954   10.48852     9.819583    9.134603    8.410964
  7.6858416   7.0011206   6.37424     5.8194804   5.3596234   5.016432
  4.804271    4.727488    4.753631    4.785868    4.690752    4.357166
  3.7920814   3.1970923   2.8395412   2.7499366   2.730953    2.6488762
  2.4981108   2.3217566   2.1604376   2.0361135   1.9442577   1.8602878
  1.7678831   1.6263534   1.374677    1.2607511   1.5757464   2.0841808
  2.463634    2.7026968   2.848227    2.9065654   2.8714466   2.7485912
  2.568058    2.3825834   2.2529156   2.2317872   2.3449416   2.5537696
  2.7496886   2.8174202   2.6965714   2.3882728   1.9228641   1.3299413
  0.68121284  0.6876688   1.5996745   2.6911964   3.814313    4.90836
  5.945692    6.93064     7.9370184   8.949186    9.138589    8.53446
  7.9132156   7.260687    6.599744    5.9722786   5.390887    4.856861
  4.3874025   4.0145655   3.7669938   3.6595225   3.6772318   3.7413974
  3.7324283   3.5392728   3.1270232   2.6569624   2.4003942   2.3798687
  2.3792484   2.2838426   2.1165495   1.9323323   1.771508    1.6565045
  1.5867755   1.5348737   1.4762759   1.3918711   1.1896446   0.9725809
  1.1260371   1.5926037   1.9855486   2.2356308   2.4022176   2.4983044
  2.5113306   2.4341724   2.2841513   2.1081114   1.9668719   1.913007
  1.9815711   2.1602986   2.3598943   2.4655974   2.413995    2.2086513
  1.8833028   1.4631023   0.9723448   0.62824875  1.0386549   1.8582225
  2.750146    3.6228147   4.453506    5.2540755   6.076995    7.0351624
  7.625278    7.1713967   6.6081867   6.051026    5.4821095   4.9362454
  4.428792    3.9502609   3.4981587   3.1007924   2.8064194   2.6523724
  2.6425807   2.7185242   2.767857    2.6979544   2.4495018   2.1190257
  1.972624    2.0335407   2.0629194   1.964041    1.791206    1.6110357
  1.4593066   1.3555942   1.305866    1.2875366   1.2643906   1.2257236
  1.1073488   0.8449861   0.7911719   1.1385529   1.5319998   1.7801883
  1.947906    2.0687513   2.1311789   2.1149209   2.017032    1.8687993
  1.7281122   1.6481164   1.6649706   1.7920034   1.9744515   2.1044178
  2.1105316   1.9955931   1.7987782   1.5434344   1.2228112   0.86535484
  0.7359239   1.1334282   1.7554326   2.392003    2.9959767   3.5831923
  4.1963167   4.9344826   5.804946    5.782051    5.239417    4.778725
  4.3232126   3.8830292   3.4703422   3.0832636   2.6998453   2.319223
  1.9855626   1.7670915   1.7071692   1.780267    1.8663969   1.8671724
  1.7590303   1.5766948   1.5530953   1.7179214   1.7987696   1.7088459
  1.53899     1.371779    1.2353479   1.1418456   1.1035166   1.1119188
  1.1276183   1.1260368   1.0911611   0.8955924   0.6569663   0.7837806
  1.1408541   1.3831997   1.5249127   1.6372677   1.7274063   1.7715167
  1.745123    1.6504464   1.5301714   1.4406124   1.4162942   1.4791955
  1.6187817   1.7519996   1.7953919   1.7432127   1.6433586   1.5276836
  1.375157    1.1460123   0.876127    0.77471703  1.005437    1.378188
  1.7487545   2.1049106   2.4859228   2.9523785   3.6440597   4.2910576
  3.8946197   3.474697    3.1247962   2.8097115   2.5094664   2.2245657
  1.9520233   1.6625093   1.3603942   1.1105487   0.9799411   1.0144949
  1.1499538   1.1933128   1.1250157   1.0525717   1.1468586   1.4337772
  1.597276    1.5334942   1.372129    1.2233913   1.1062537   1.0191967
  0.97783154  0.9922337   1.0381768   1.0721143   1.0885587   1.0321736
  0.7673375   0.6178372   0.83649874  1.0896323   1.2007818   1.2639986
  1.3320253   1.4008052   1.4405761   1.422649    1.3518262   1.2742685
  1.2340474   1.2472132   1.3261282   1.4383376   1.4987615   1.4749619
  1.4193925   1.390408    1.3782853   1.3127701   1.1448023   0.9210713
  0.78002065  0.8096344   0.926989    1.0520526   1.191082    1.3889939
  1.7140574   2.3359833   2.711401    2.2519755   1.955672    1.7479254
  1.5888599   1.4193256   1.2403339   1.0729357   0.89367527  0.74231935
  0.70686793  0.6953998   0.76424354  0.9133155   0.8773794   0.7354352
  0.82452774  1.1813729   1.4526846   1.4429659   1.2898964   1.1618878
  1.0722233   0.9910779   0.9319158   0.9227518   0.9652851   1.024422
  1.0651798   1.0929049   1.0099697   0.7195991   0.64792615  0.87456864
  1.0199859   1.0363014   1.0350953   1.060778    1.1097846   1.1526456
  1.1559645   1.1199194   1.0859044   1.0842361   1.1185116   1.1920888
  1.2591181   1.2502383   1.1855764   1.1573509   1.2102857   1.2856112
  1.2823755   1.160991    0.97848326  0.8283193   0.7461311   0.6965525
  0.6537869   0.631569    0.6570623   0.7742517   1.1854386   1.3743842
  1.009222    0.86786944  0.8479509   0.8615475 ]
