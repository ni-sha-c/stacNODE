time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 124.17%, model saved.
Epoch: 0 Train: 3607.10986 Test: 3881.43311
Epoch 100: New minimal relative error: 55.87%, model saved.
Epoch: 100 Train: 246.50810 Test: 178.05377
Epoch 200: New minimal relative error: 10.02%, model saved.
Epoch: 200 Train: 13.38564 Test: 13.88988
Epoch: 300 Train: 8.52137 Test: 7.73897
Epoch: 400 Train: 9.92781 Test: 8.88211
Epoch: 500 Train: 20.30676 Test: 35.99434
Epoch: 600 Train: 5.89662 Test: 6.50856
Epoch: 700 Train: 21.44912 Test: 23.10942
Epoch: 800 Train: 6.13921 Test: 7.01091
Epoch: 900 Train: 3.33536 Test: 4.55108
Epoch: 1000 Train: 9.72845 Test: 9.41845
Epoch: 1100 Train: 4.27465 Test: 3.41310
Epoch: 1200 Train: 2.20020 Test: 2.14832
Epoch: 1300 Train: 6.15513 Test: 9.60678
Epoch: 1400 Train: 4.48111 Test: 3.68912
Epoch: 1500 Train: 1.33543 Test: 1.23766
Epoch: 1600 Train: 8.10163 Test: 8.01340
Epoch: 1700 Train: 11.33848 Test: 14.16477
Epoch: 1800 Train: 1.35952 Test: 0.83583
Epoch: 1900 Train: 5.52418 Test: 7.79390
Epoch: 2000 Train: 1.66930 Test: 1.35496
Epoch: 2100 Train: 3.19364 Test: 4.60207
Epoch: 2200 Train: 8.74753 Test: 3.48328
Epoch 2300: New minimal relative error: 9.12%, model saved.
Epoch: 2300 Train: 0.63738 Test: 0.52327
Epoch: 2400 Train: 1.05800 Test: 0.99223
Epoch 2500: New minimal relative error: 8.90%, model saved.
Epoch: 2500 Train: 1.37168 Test: 1.02182
Epoch: 2600 Train: 0.46129 Test: 0.46519
Epoch 2700: New minimal relative error: 8.32%, model saved.
Epoch: 2700 Train: 0.80286 Test: 0.99302
Epoch 2800: New minimal relative error: 7.56%, model saved.
Epoch: 2800 Train: 1.04428 Test: 0.76533
Epoch: 2900 Train: 0.65852 Test: 0.73487
Epoch: 3000 Train: 2.80804 Test: 4.16776
Epoch: 3100 Train: 1.10042 Test: 0.78713
Epoch: 3200 Train: 3.02767 Test: 4.16252
Epoch: 3300 Train: 1.15881 Test: 1.56437
Epoch: 3400 Train: 0.43244 Test: 0.52391
Epoch: 3500 Train: 0.30620 Test: 0.34881
Epoch: 3600 Train: 0.33071 Test: 0.44764
Epoch: 3700 Train: 0.51093 Test: 0.53168
Epoch: 3800 Train: 0.39284 Test: 0.39919
Epoch: 3900 Train: 0.26516 Test: 0.28516
Epoch 4000: New minimal relative error: 5.44%, model saved.
Epoch: 4000 Train: 2.59060 Test: 3.23213
Epoch: 4100 Train: 0.48990 Test: 0.48017
Epoch: 4200 Train: 0.42151 Test: 0.40588
Epoch: 4300 Train: 0.34529 Test: 0.67258
Epoch: 4400 Train: 0.32169 Test: 0.43709
Epoch: 4500 Train: 0.28214 Test: 0.27764
Epoch: 4600 Train: 0.16833 Test: 0.19517
Epoch: 4700 Train: 0.20598 Test: 0.24221
Epoch: 4800 Train: 0.48807 Test: 0.48697
Epoch: 4900 Train: 2.32228 Test: 3.33328
Epoch: 5000 Train: 2.09888 Test: 1.23073
Epoch: 5100 Train: 0.39364 Test: 0.34483
Epoch: 5200 Train: 0.28934 Test: 0.23032
Epoch: 5300 Train: 0.34339 Test: 0.48519
Epoch: 5400 Train: 0.19333 Test: 0.26619
Epoch: 5500 Train: 0.32867 Test: 0.47406
Epoch: 5600 Train: 0.17689 Test: 0.18128
Epoch: 5700 Train: 0.12431 Test: 0.14048
Epoch: 5800 Train: 0.19405 Test: 0.22598
Epoch: 5900 Train: 0.36114 Test: 0.27145
Epoch: 6000 Train: 0.43168 Test: 0.57861
Epoch: 6100 Train: 0.32028 Test: 0.25744
Epoch: 6200 Train: 3.04338 Test: 3.67478
Epoch: 6300 Train: 0.07731 Test: 0.11190
Epoch: 6400 Train: 0.09200 Test: 0.16040
Epoch: 6500 Train: 0.11265 Test: 0.19743
Epoch: 6600 Train: 0.16327 Test: 0.18537
Epoch: 6700 Train: 0.14699 Test: 0.21864
Epoch: 6800 Train: 0.17911 Test: 0.27421
Epoch: 6900 Train: 5.75035 Test: 5.34005
Epoch: 7000 Train: 0.07269 Test: 0.10453
Epoch: 7100 Train: 0.08390 Test: 0.13093
Epoch: 7200 Train: 0.71575 Test: 0.96649
Epoch: 7300 Train: 1.02043 Test: 1.37501
Epoch: 7400 Train: 0.97602 Test: 1.18881
Epoch: 7500 Train: 1.05170 Test: 1.46756
Epoch: 7600 Train: 0.35002 Test: 0.48341
Epoch: 7700 Train: 0.32405 Test: 0.43606
Epoch: 7800 Train: 0.76645 Test: 0.99737
Epoch: 7900 Train: 0.99370 Test: 1.14693
Epoch: 8000 Train: 0.12066 Test: 0.18877
Epoch: 8100 Train: 0.24199 Test: 0.27496
Epoch: 8200 Train: 0.90128 Test: 1.24044
Epoch: 8300 Train: 2.03441 Test: 2.19017
Epoch: 8400 Train: 0.06272 Test: 0.08881
Epoch: 8500 Train: 0.17816 Test: 0.23648
Epoch: 8600 Train: 0.18356 Test: 0.16782
Epoch: 8700 Train: 0.08909 Test: 0.13443
Epoch: 8800 Train: 0.10546 Test: 0.16863
Epoch: 8900 Train: 0.05247 Test: 0.07512
Epoch: 9000 Train: 0.78077 Test: 0.87664
Epoch: 9100 Train: 0.25576 Test: 0.31823
Epoch: 9200 Train: 0.06732 Test: 0.09503
Epoch: 9300 Train: 0.22747 Test: 0.23715
Epoch: 9400 Train: 0.09371 Test: 0.14041
Epoch: 9500 Train: 0.24074 Test: 0.19521
Epoch: 9600 Train: 0.07332 Test: 0.10122
Epoch: 9700 Train: 0.03987 Test: 0.06279
Epoch: 9800 Train: 0.38121 Test: 0.45169
Epoch: 9900 Train: 0.37896 Test: 0.47383
Epoch: 9999 Train: 0.26995 Test: 0.31197
Training Loss: tensor(0.2699)
Test Loss: tensor(0.3120)
Learned LE: [ 0.75228894 -0.03947102 -2.4201922 ]
True LE: [ 8.5949868e-01 -2.4073296e-03 -1.4539573e+01]
Relative Error: [10.170741  10.430263  10.711573  11.084334  11.426539  11.577964
 11.621184  11.295883  11.174217  11.103947  11.03165   10.844999
 10.687373  11.262909  11.768605  12.567897  13.332951  13.950303
 14.216578  14.953301  16.074253  16.709877  17.608044  18.354149
 18.65001   19.057098  19.472929  19.521982  19.37907   19.374466
 19.353563  19.262281  19.062847  18.813719  18.465971  18.463896
 18.350449  18.123188  17.881634  17.823763  18.03766   18.084627
 17.908388  17.909882  18.294247  18.570335  18.465906  17.665464
 16.477194  15.245835  14.212266  12.880582  11.778805  11.288817
 10.701494  10.34524    9.920749   9.699912   9.469197   9.32267
  9.376221   9.328009   9.719955   9.933642  10.087444  10.46997
 10.767722  11.029186  11.066683  10.76806   10.590438  10.3862505
 10.287724  10.18167   10.16351   10.42403   10.874504  11.595588
 12.482253  13.201576  13.645925  14.233483  15.18157   15.811976
 16.494898  17.210968  17.72027   18.193716  18.37166   18.334845
 18.30078   18.467918  18.574133  18.596228  18.426727  18.124205
 17.73182   17.589197  17.46265   17.154278  16.911976  16.691866
 16.80134   17.024343  17.03641   16.905176  17.142447  17.495964
 17.308432  16.83671   15.762222  14.565916  13.457626  12.142874
 11.040135  10.618525  10.210563   9.803971   9.261186   9.017057
  8.801219   8.780767   8.864776   8.8684435  9.143087   9.415679
  9.417331   9.696447  10.046566  10.4083805 10.4900875 10.281089
 10.029658   9.836542   9.638087   9.489099   9.523804   9.89346
 10.129908  10.483685  11.503537  12.376545  12.975754  13.421173
 13.714465  14.32062   14.817047  15.293457  15.441023  16.269264
 17.026737  17.3681    17.532877  17.598549  17.36097   17.123081
 17.386875  17.259693  16.862572  16.426548  16.513569  16.402336
 15.923107  15.629968  15.576649  15.917875  15.840231  15.724047
 15.701639  16.332806  15.940894  15.688437  14.969152  13.93683
 12.888883  11.594191  10.501871   9.983637   9.7231455  9.329806
  8.752585   8.358318   8.159755   8.237581   8.2611     8.308177
  8.34096    8.4876995  8.6022215  8.725867   8.938899   9.298746
  9.804128   9.792207   9.509992   9.36491    9.215255   8.946741
  8.816871   9.226854   9.432539   9.4577465 10.268193  11.372772
 11.949839  11.8554325 12.024101  12.628601  13.036142  12.853481
 13.062532  14.013571  15.123805  15.838218  16.623549  16.76153
 16.326876  15.998978  15.81289   15.972154  16.04622   15.480179
 15.240569  15.26805   14.9879675 14.70446   14.429892  14.441158
 14.535935  14.416277  14.491168  14.885683  14.693154  14.382068
 13.963744  13.029396  12.220748  11.325695  10.17665    9.591962
  9.241476   8.87645    8.274264   7.6856284  7.5619206  7.676898
  7.634086   7.451243   7.516499   7.5271077  7.706271   7.7217393
  7.842175   8.155684   8.865597   9.181955   8.928856   8.877763
  8.860614   8.679886   8.386861   8.562556   8.695301   8.804482
  9.083968  10.187802  10.5327835 10.446656  10.239265  10.56168
 10.983191  10.775683  10.977777  11.99363   13.182508  14.172341
 14.8340435 15.022633  15.054197  15.067275  14.740623  14.478617
 14.412347  14.7141695 14.114258  13.938643  13.908421  13.56083
 13.426985  13.05739   13.132879  13.167617  13.171661  13.244719
 13.464141  13.045973  12.858011  12.180937  11.441303  10.765811
  9.9085245  9.192244   8.765223   8.434777   7.916954   7.2045336
  7.1490355  7.252161   7.0295773  6.7469506  6.641768   6.589032
  6.642666   6.8620853  6.957506   7.2048     7.721437   8.52402
  8.603452   8.41627    8.348179   8.29126    8.030968   7.994572
  7.978776   8.22292    8.332916   8.781976   9.02457    9.230841
  8.5244465  8.522036   8.727416   8.799517   9.085893   9.612872
 10.794974  12.367315  12.799067  12.899584  13.125873  13.390249
 13.3034525 13.288793  12.960598  12.734057  13.09883   12.732151
 12.458033  12.456129  12.027824  11.852378  11.700347  11.757393
 11.838084  11.841523  12.009361  11.873951  11.5773535 11.231694
 10.575528  10.066398   9.333503   8.615438   8.235875   8.0373
  7.6083064  7.054925   6.6687574  6.8529234  6.570735   6.2733436
  6.1251564  5.900791   5.86306    6.014489   6.216095   6.391626
  6.667305   7.2632456  8.103261   8.122783   7.92768    7.8211093
  7.685925   7.2706723  7.438472   7.4498973  7.7385664  7.9381986
  7.4333854  7.8048377  7.4028516  6.984565   6.751357   6.934027
  6.849121   7.2787085  8.413189  10.168867 ]
