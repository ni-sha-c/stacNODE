time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 3785.78247 Test: 4323.74902
Epoch 80: New minimal relative error: 82.31%, model saved.
Epoch: 80 Train: 54.71024 Test: 71.88568
Epoch 160: New minimal relative error: 16.75%, model saved.
Epoch: 160 Train: 15.62146 Test: 18.73901
Epoch: 240 Train: 9.99429 Test: 7.29998
Epoch: 320 Train: 10.17999 Test: 6.13341
Epoch: 400 Train: 11.18492 Test: 13.69545
Epoch: 480 Train: 7.49718 Test: 8.84187
Epoch: 560 Train: 27.71301 Test: 26.94764
Epoch: 640 Train: 7.41849 Test: 8.56345
Epoch: 720 Train: 5.27607 Test: 4.22536
Epoch: 800 Train: 4.90175 Test: 5.87600
Epoch: 880 Train: 1.78670 Test: 2.67361
Epoch: 960 Train: 4.09272 Test: 7.02546
Epoch: 1040 Train: 6.96369 Test: 5.45004
Epoch: 1120 Train: 1.11865 Test: 0.42108
Epoch: 1200 Train: 2.17377 Test: 3.12629
Epoch: 1280 Train: 4.44648 Test: 6.01970
Epoch: 1360 Train: 5.31810 Test: 6.49713
Epoch: 1440 Train: 3.26538 Test: 2.74883
Epoch: 1520 Train: 0.25972 Test: 0.35134
Epoch: 1600 Train: 0.20055 Test: 0.29774
Epoch: 1680 Train: 0.16519 Test: 0.23309
Epoch: 1760 Train: 0.17829 Test: 0.39069
Epoch: 1840 Train: 0.66093 Test: 0.97112
Epoch: 1920 Train: 1.43595 Test: 1.25942
Epoch: 2000 Train: 0.30246 Test: 0.45624
Epoch: 2080 Train: 0.56842 Test: 0.65187
Epoch: 2160 Train: 3.68036 Test: 3.78020
Epoch: 2240 Train: 0.83647 Test: 1.00900
Epoch: 2320 Train: 0.29639 Test: 0.18182
Epoch: 2400 Train: 1.04301 Test: 1.27224
Epoch: 2480 Train: 1.05449 Test: 1.15137
Epoch: 2560 Train: 1.25126 Test: 1.03047
Epoch: 2640 Train: 0.26422 Test: 0.31668
Epoch: 2720 Train: 1.07138 Test: 0.87896
Epoch: 2800 Train: 1.25403 Test: 1.62531
Epoch: 2880 Train: 2.10510 Test: 2.69676
Epoch: 2960 Train: 0.54511 Test: 0.63420
Epoch: 3040 Train: 1.96863 Test: 3.16259
Epoch: 3120 Train: 0.74747 Test: 1.22547
Epoch: 3200 Train: 0.10175 Test: 0.09937
Epoch: 3280 Train: 0.81244 Test: 0.79084
Epoch: 3360 Train: 0.96607 Test: 0.90166
Epoch: 3440 Train: 0.05609 Test: 0.18616
Epoch: 3520 Train: 0.03079 Test: 0.06251
Epoch: 3600 Train: 0.06168 Test: 0.07919
Epoch: 3680 Train: 0.61519 Test: 0.57766
Epoch: 3760 Train: 1.52450 Test: 1.58631
Epoch 3840: New minimal relative error: 12.24%, model saved.
Epoch: 3840 Train: 0.03949 Test: 0.06324
Epoch: 3920 Train: 0.34572 Test: 0.48587
Epoch: 4000 Train: 0.26785 Test: 0.40704
Epoch: 4080 Train: 0.07295 Test: 0.08050
Epoch: 4160 Train: 0.09287 Test: 0.13613
Epoch: 4240 Train: 0.38211 Test: 0.50371
Epoch: 4320 Train: 0.68908 Test: 0.57538
Epoch: 4400 Train: 0.29236 Test: 0.26656
Epoch: 4480 Train: 0.12089 Test: 0.12135
Epoch: 4560 Train: 0.27822 Test: 0.36247
Epoch: 4640 Train: 0.01432 Test: 0.02509
Epoch: 4720 Train: 0.45411 Test: 0.50525
Epoch: 4800 Train: 0.23912 Test: 0.31373
Epoch: 4880 Train: 0.06699 Test: 0.08799
Epoch: 4960 Train: 0.15019 Test: 0.18092
Epoch: 5040 Train: 0.29830 Test: 0.40000
Epoch: 5120 Train: 0.05926 Test: 0.08637
Epoch: 5200 Train: 0.38418 Test: 0.37397
Epoch: 5280 Train: 0.91936 Test: 0.93380
Epoch: 5360 Train: 0.42696 Test: 0.56731
Epoch: 5440 Train: 0.21455 Test: 0.18881
Epoch: 5520 Train: 0.27661 Test: 0.26698
Epoch: 5600 Train: 0.12437 Test: 0.13335
Epoch: 5680 Train: 0.05189 Test: 0.06406
Epoch: 5760 Train: 0.01669 Test: 0.02833
Epoch: 5840 Train: 0.01327 Test: 0.02260
Epoch: 5920 Train: 0.03877 Test: 0.03648
Epoch: 6000 Train: 0.30272 Test: 0.30813
Epoch: 6080 Train: 0.06605 Test: 0.07766
Epoch: 6160 Train: 0.16799 Test: 0.11932
Epoch: 6240 Train: 0.34547 Test: 0.47149
Epoch: 6320 Train: 0.01505 Test: 0.02470
Epoch: 6400 Train: 0.03494 Test: 0.05201
Epoch: 6480 Train: 0.44437 Test: 0.51480
Epoch: 6560 Train: 0.26126 Test: 0.31685
Epoch: 6640 Train: 0.01198 Test: 0.02315
Epoch: 6720 Train: 0.00749 Test: 0.01797
Epoch: 6800 Train: 0.12335 Test: 0.13671
Epoch: 6880 Train: 0.24775 Test: 0.28272
Epoch 6960: New minimal relative error: 9.95%, model saved.
Epoch: 6960 Train: 0.00799 Test: 0.01912
Epoch: 7040 Train: 0.05696 Test: 0.07709
Epoch: 7120 Train: 0.01353 Test: 0.01820
Epoch: 7200 Train: 0.30585 Test: 0.32502
Epoch: 7280 Train: 0.13340 Test: 0.16068
Epoch: 7360 Train: 0.02874 Test: 0.02802
Epoch: 7440 Train: 0.06144 Test: 0.04153
Epoch: 7520 Train: 0.19649 Test: 0.13266
Epoch: 7600 Train: 0.03354 Test: 0.06908
Epoch: 7680 Train: 0.00604 Test: 0.01659
Epoch: 7760 Train: 0.01633 Test: 0.02836
Epoch: 7840 Train: 0.00644 Test: 0.01758
Epoch: 7920 Train: 0.00589 Test: 0.01653
Epoch: 7999 Train: 0.10553 Test: 0.12723
Training Loss: tensor(0.1055)
Test Loss: tensor(0.1272)
Learned LE: [ 0.89363086 -0.02124035 -6.104346  ]
True LE: [ 8.7557453e-01  7.6272129e-04 -1.4557181e+01]
Relative Error: [1.3563862  1.4823807  1.5701598  1.5939906  1.595476   1.7185926
 1.9998876  2.2624574  2.4435172  2.5661745  2.5804975  2.4939013
 2.4072762  2.4161663  2.5476792  2.7662027  3.0440445  3.4015284
 3.8913429  4.5371747  5.2421637  5.783068   5.982351   5.8486958
 5.4951553  5.0400763  4.59705    4.2674985  4.1076226  4.07336
 4.113687   4.1636214  4.0403547  3.612656   3.0458999  2.4594765
 1.9974599  1.7516862  1.6331743  1.5523182  1.5059961  1.474402
 1.4133866  1.3520167  1.3315296  1.351916   1.3946098  1.4008102
 1.2588652  0.8962458  0.4975798  0.51273936 0.71901596 0.80135065
 0.77724874 0.7324063  0.7235122  0.756574   0.81184775 0.8688584
 0.91661185 0.95880294 1.014787   1.1041332  1.2158188  1.303511
 1.3242161  1.2987747  1.3685343  1.6007298  1.8132885  1.9382874
 2.0100095  1.9743562  1.8464742  1.749752   1.7836522  1.9373354
 2.1348     2.34239    2.5927625  2.9576578  3.4831426  4.079081
 4.5129313  4.63418    4.5072284  4.2292695  3.856506   3.4791563
 3.182163   3.0190315  2.9714     2.98417    3.0413601  3.0165997
 2.7362025  2.3224971  1.8651236  1.4656163  1.2514622  1.1441001
 1.0544572  0.99766266 0.9740596  0.9221377  0.8652366  0.8309086
 0.8154356  0.84499896 0.929957   0.9383218  0.73419183 0.42289728
 0.3938287  0.59172225 0.67272276 0.6328815  0.5652448  0.53942853
 0.5643683  0.6181666  0.67756647 0.7275342  0.76391417 0.7950967
 0.8421694  0.9192313  1.0050958  1.0454109  1.0111201  0.99814487
 1.1555189  1.3464124  1.4411563  1.4880165  1.4434651  1.2927042
 1.1693491  1.2005851  1.357771   1.5229875  1.6504894  1.775172
 1.9754825  2.3305418  2.8023052  3.1678562  3.2702656  3.2125413
 3.0739324  2.8159196  2.5086858  2.263004   2.102408   2.0201795
 1.9742293  1.977027   2.0163915  1.9244226  1.675172   1.3983995
 1.0772916  0.8770646  0.77659243 0.6937837  0.60341793 0.58236283
 0.5480635  0.5059973  0.4761676  0.43284753 0.40287977 0.45729676
 0.5487145  0.5765595  0.4460281  0.25445506 0.36635235 0.498636
 0.48793164 0.40796915 0.3550264  0.35747483 0.39689425 0.45094094
 0.5019477  0.5426763  0.572827   0.5969822  0.62933636 0.6832603
 0.7441998  0.7548627  0.69305587 0.7105128  0.87787193 0.9855436
 1.021233   1.0132486  0.88982695 0.7255087  0.68630934 0.8172055
 0.967409   1.0482998  1.0794743  1.121742   1.2658536  1.5764315
 1.906302   2.0145     1.9896697  2.0019782  1.9267825  1.7005705
 1.4814374  1.3614569  1.2984016  1.2221481  1.1249936  1.0903279
 1.1192607  1.0291784  0.9143834  0.7780389  0.6076979  0.517471
 0.46274862 0.36784032 0.29214397 0.27754742 0.2528123  0.256847
 0.23585486 0.21484184 0.1939736  0.10202329 0.23396061 0.39572954
 0.35694948 0.1433711  0.23350239 0.345431   0.31236225 0.23742877
 0.20374453 0.2059343  0.23048449 0.26635522 0.29915974 0.32728523
 0.35393664 0.3770858  0.39630407 0.4237713  0.466631   0.48284438
 0.41714036 0.4201764  0.56336623 0.6294146  0.64532256 0.62976164
 0.52938914 0.41245952 0.38272536 0.4872789  0.5746387  0.589812
 0.56037337 0.53509367 0.6022361  0.8478189  1.0514933  0.98847634
 0.95039874 1.0646653  1.0603358  0.8948023  0.75467986 0.70517576
 0.7232139  0.67588645 0.533037   0.44886452 0.44763577 0.38414866
 0.3535881  0.36083028 0.32616147 0.2950236  0.2749914  0.19926096
 0.0946959  0.08566386 0.11810334 0.16081022 0.16444269 0.21889573
 0.242817   0.23628949 0.15629217 0.24325608 0.37494335 0.22171451
 0.07152434 0.23729524 0.23822835 0.18039198 0.15100206 0.12969212
 0.1118368  0.11199977 0.1163458  0.11986051 0.13186495 0.15397847
 0.17426078 0.18597749 0.20275204 0.23981887 0.23590873 0.17791627
 0.27763575 0.34964952 0.36331344 0.39688662 0.4244033  0.4316493
 0.3485976  0.2814205  0.2810485  0.2723952  0.233074   0.20801513
 0.19634937 0.30437717 0.5203895  0.39948657 0.23755002 0.36104235
 0.44811153 0.36354774 0.3047089  0.30202872 0.33464813 0.37218013
 0.3032192  0.23195185 0.26930964 0.20918594 0.16334781 0.05299224
 0.11198992 0.17782329 0.14953075 0.18236831 0.07663469 0.0269581
 0.09150448 0.13715418 0.12549026 0.18517686 0.25366253 0.34881508
 0.3692799  0.14724162 0.2519309  0.40128386 0.22157484 0.15514436
 0.24166997 0.20770326 0.15501438 0.12373757 0.0885603  0.05643249
 0.03748562 0.03034295 0.04509327 0.05714452 0.05229652 0.0444593
 0.05392159 0.06992418 0.09546614 0.14674172 0.14472266 0.18869905
 0.19551492 0.21778117 0.31086984 0.45069852 0.515192   0.43331635
 0.32341418 0.2338232  0.16987507 0.14234573]
