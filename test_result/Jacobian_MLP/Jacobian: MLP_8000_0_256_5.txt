time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.89%, model saved.
Epoch: 0 Train: 171179.67188 Test: 4005.88428
Epoch: 80 Train: 29509.37695 Test: 1708.47046
Epoch: 160 Train: 9260.40820 Test: 1948.10730
Epoch 240: New minimal relative error: 73.02%, model saved.
Epoch: 240 Train: 1450.78137 Test: 110.47861
Epoch 320: New minimal relative error: 24.63%, model saved.
Epoch: 320 Train: 1646.97656 Test: 29.08832
Epoch 400: New minimal relative error: 21.35%, model saved.
Epoch: 400 Train: 315.99042 Test: 12.18604
Epoch: 480 Train: 176.11497 Test: 24.05636
Epoch 560: New minimal relative error: 8.06%, model saved.
Epoch: 560 Train: 98.49020 Test: 3.22095
Epoch: 640 Train: 113.53479 Test: 0.72242
Epoch 720: New minimal relative error: 5.17%, model saved.
Epoch: 720 Train: 79.43110 Test: 2.24770
Epoch: 800 Train: 150.64259 Test: 17.13778
Epoch 880: New minimal relative error: 4.27%, model saved.
Epoch: 880 Train: 33.31380 Test: 0.94035
Epoch: 960 Train: 71.55143 Test: 1.10973
Epoch: 1040 Train: 29.43793 Test: 1.48283
Epoch: 1120 Train: 21.63228 Test: 0.06292
Epoch: 1200 Train: 34.17319 Test: 3.89873
Epoch: 1280 Train: 107.83372 Test: 6.36058
Epoch: 1360 Train: 319.22769 Test: 50.60835
Epoch 1440: New minimal relative error: 3.16%, model saved.
Epoch: 1440 Train: 23.23737 Test: 0.47109
Epoch: 1520 Train: 28.31533 Test: 2.12775
Epoch: 1600 Train: 144.89825 Test: 17.07538
Epoch: 1680 Train: 13.49217 Test: 0.57827
Epoch: 1760 Train: 65.78932 Test: 6.73221
Epoch: 1840 Train: 104.01656 Test: 7.24431
Epoch: 1920 Train: 14.54892 Test: 1.32546
Epoch 2000: New minimal relative error: 3.13%, model saved.
Epoch: 2000 Train: 9.75216 Test: 0.09824
Epoch: 2080 Train: 9.91147 Test: 0.30642
Epoch: 2160 Train: 198.93387 Test: 53.86746
Epoch: 2240 Train: 7.67884 Test: 0.03582
Epoch: 2320 Train: 76.78036 Test: 9.31297
Epoch: 2400 Train: 9.05222 Test: 0.03074
Epoch: 2480 Train: 14.21555 Test: 1.58015
Epoch: 2560 Train: 15.49440 Test: 1.16424
Epoch: 2640 Train: 25.97483 Test: 4.57546
Epoch: 2720 Train: 39.66763 Test: 3.91252
Epoch: 2800 Train: 153.57442 Test: 13.12426
Epoch: 2880 Train: 14.09331 Test: 1.47628
Epoch: 2960 Train: 9.84879 Test: 0.62432
Epoch: 3040 Train: 67.40886 Test: 8.28944
Epoch: 3120 Train: 8.52441 Test: 0.39110
Epoch: 3200 Train: 5.88288 Test: 0.18884
Epoch: 3280 Train: 12.41080 Test: 1.84845
Epoch: 3360 Train: 48.48975 Test: 9.08794
Epoch: 3440 Train: 20.36465 Test: 1.97146
Epoch 3520: New minimal relative error: 2.04%, model saved.
Epoch: 3520 Train: 4.12004 Test: 0.06986
Epoch: 3600 Train: 6.06909 Test: 0.19145
Epoch: 3680 Train: 25.36358 Test: 2.97796
Epoch: 3760 Train: 3.56131 Test: 0.01159
Epoch: 3840 Train: 3.83221 Test: 0.05483
Epoch: 3920 Train: 18.89787 Test: 1.67463
Epoch: 4000 Train: 7.33358 Test: 1.00767
Epoch: 4080 Train: 5.90695 Test: 0.37262
Epoch: 4160 Train: 4.89562 Test: 0.30626
Epoch: 4240 Train: 7.55062 Test: 0.65308
Epoch: 4320 Train: 38.71504 Test: 1.54803
Epoch: 4400 Train: 2.83825 Test: 0.00505
Epoch: 4480 Train: 4.95934 Test: 0.41159
Epoch: 4560 Train: 3.62169 Test: 0.03182
Epoch: 4640 Train: 2.76933 Test: 0.07487
Epoch: 4720 Train: 2.55306 Test: 0.01603
Epoch: 4800 Train: 5.23761 Test: 0.18596
Epoch: 4880 Train: 2.40845 Test: 0.00296
Epoch: 4960 Train: 3.50961 Test: 0.12744
Epoch: 5040 Train: 2.27000 Test: 0.00283
Epoch: 5120 Train: 12.65745 Test: 0.17586
Epoch: 5200 Train: 2.18120 Test: 0.00285
Epoch: 5280 Train: 7.36890 Test: 1.32533
Epoch: 5360 Train: 4.75703 Test: 0.15665
Epoch: 5440 Train: 3.49260 Test: 0.18924
Epoch: 5520 Train: 2.00203 Test: 0.00440
Epoch: 5600 Train: 27.92963 Test: 3.10287
Epoch: 5680 Train: 1.93834 Test: 0.00301
Epoch: 5760 Train: 13.84927 Test: 2.61509
Epoch: 5840 Train: 1.82819 Test: 0.00346
Epoch 5920: New minimal relative error: 0.83%, model saved.
Epoch: 5920 Train: 1.95052 Test: 0.01522
Epoch: 6000 Train: 19.83095 Test: 1.63573
Epoch: 6080 Train: 15.86109 Test: 2.25961
Epoch: 6160 Train: 1.92306 Test: 0.04995
Epoch: 6240 Train: 1.68080 Test: 0.00644
Epoch: 6320 Train: 8.20269 Test: 0.73083
Epoch: 6400 Train: 1.79171 Test: 0.00705
Epoch: 6480 Train: 2.15485 Test: 0.11202
Epoch: 6560 Train: 1.58737 Test: 0.00537
Epoch: 6640 Train: 1.87405 Test: 0.04057
Epoch: 6720 Train: 1.60371 Test: 0.01538
Epoch: 6800 Train: 1.53331 Test: 0.01876
Epoch: 6880 Train: 2.10946 Test: 0.02482
Epoch: 6960 Train: 1.40956 Test: 0.00249
Epoch: 7040 Train: 1.48736 Test: 0.04772
Epoch: 7120 Train: 4.55140 Test: 0.52455
Epoch: 7200 Train: 1.51320 Test: 0.01732
Epoch: 7280 Train: 1.34624 Test: 0.00446
Epoch: 7360 Train: 1.53083 Test: 0.05563
Epoch: 7440 Train: 11.99962 Test: 0.89731
Epoch 7520: New minimal relative error: 0.41%, model saved.
Epoch: 7520 Train: 1.26410 Test: 0.00337
Epoch: 7600 Train: 10.07671 Test: 0.45022
Epoch: 7680 Train: 1.22362 Test: 0.00271
Epoch: 7760 Train: 1.91391 Test: 0.05525
Epoch: 7840 Train: 1.18331 Test: 0.00232
Epoch: 7920 Train: 15.50906 Test: 1.09585
Epoch: 7999 Train: 1.15196 Test: 0.00271
Training Loss: tensor(1.1520)
Test Loss: tensor(0.0027)
Learned LE: [ 8.9331365e-01 -2.7338397e-03 -1.4519067e+01]
True LE: [ 8.6393923e-01  1.1363093e-02 -1.4547668e+01]
Relative Error: [0.16301623 0.25061846 0.34446517 0.43651047 0.51618266 0.57054234
 0.58910316 0.5738536  0.54588825 0.5334769  0.5483348  0.5786134
 0.6059127  0.6154154  0.5974279  0.55694675 0.5084974  0.4601342
 0.40935883 0.35381952 0.29606313 0.23813401 0.1759988  0.11306877
 0.10358235 0.16922238 0.24500896 0.31185833 0.36944658 0.42524016
 0.50499177 0.64810675 0.86145186 1.1169541  1.3896409  1.6133273
 1.6646808  1.4927633  1.1814847  0.85638064 0.58213204 0.38870654
 0.2918872  0.25853604 0.2213055  0.20933415 0.33051363 0.47759685
 0.5717858  0.6044474  0.59206855 0.5546777  0.50637364 0.4520038
 0.39144218 0.32597017 0.2608845  0.20415647 0.16208161 0.13581945
 0.12184089 0.11851157 0.13202406 0.17113262 0.23525855 0.31676382
 0.40378714 0.48185414 0.53355604 0.5460874  0.5232704  0.4900808
 0.47417274 0.48073563 0.49625927 0.5075358  0.5041624  0.4803069
 0.4432454  0.40408382 0.36237702 0.31298706 0.2576324  0.20246759
 0.1502997  0.09737751 0.06811465 0.11636779 0.18330084 0.24025193
 0.28713766 0.32821113 0.37649035 0.4599956  0.597105   0.76742893
 0.9461291  1.1396276  1.306135   1.3257513  1.1543756  0.87587905
 0.6090741  0.38659012 0.22260109 0.15604432 0.14098284 0.12189829
 0.17150794 0.310068   0.42707616 0.48452386 0.49222848 0.47044247
 0.43678913 0.40066084 0.36205602 0.31751537 0.2689321  0.22565846
 0.20007777 0.19535616 0.20223352 0.20923452 0.2112349  0.20999752
 0.21522409 0.24043606 0.29257682 0.36409017 0.43677795 0.4881939
 0.50036955 0.47442546 0.4353479  0.4114855  0.40611094 0.4077231
 0.40925238 0.40344298 0.38317677 0.35396212 0.32183287 0.28258356
 0.23332648 0.17916332 0.12790073 0.0812363  0.03548645 0.04951233
 0.11102369 0.16332391 0.20416018 0.23436874 0.2529973  0.27950686
 0.35400182 0.4740287  0.5908805  0.689367   0.81238216 0.94916093
 0.9791129  0.8492139  0.6192479  0.41924646 0.27014524 0.13501118
 0.10713562 0.12140835 0.06287102 0.12262949 0.25502822 0.3455513
 0.37769133 0.37261668 0.3519672  0.33009824 0.3127445  0.2954624
 0.27093136 0.24051534 0.21636188 0.21195444 0.2262156  0.2462649
 0.26109803 0.26588735 0.26054728 0.2497884  0.2447437  0.26117164
 0.30640143 0.36904144 0.42410073 0.44636032 0.42712623 0.3859558
 0.35245797 0.33445287 0.3233464  0.31858784 0.3167239  0.30517998
 0.2838469  0.2565323  0.22105521 0.1784147  0.13692246 0.10037488
 0.06718783 0.04064434 0.05084958 0.08871251 0.11980242 0.14857842
 0.17350473 0.17840137 0.16705939 0.20341116 0.3105632  0.40374538
 0.4498282  0.51075935 0.62544274 0.67988646 0.61092824 0.43420255
 0.25493225 0.1739932  0.11605914 0.0713689  0.16209288 0.11364496
 0.05345082 0.1727452  0.25036907 0.26833004 0.2539117  0.2363393
 0.22802307 0.23123433 0.23759723 0.23493919 0.22013669 0.20484006
 0.20573351 0.22529729 0.25112283 0.27107686 0.27945533 0.27518678
 0.26035392 0.24016199 0.22580637 0.23387429 0.2726026  0.32728782
 0.3689724  0.3736014  0.34344035 0.30515993 0.27680817 0.25444174
 0.24051335 0.24136715 0.24142638 0.2291537  0.2076145  0.18030594
 0.1546755  0.14255221 0.13688721 0.12264541 0.10330485 0.09194656
 0.0941084  0.09623167 0.10180548 0.11968311 0.12856993 0.10863077
 0.08029971 0.1535764  0.25269225 0.288611   0.30082318 0.3678056
 0.4563008  0.43587628 0.3484159  0.1842733  0.05558052 0.06666762
 0.06491744 0.11080367 0.18769561 0.08656956 0.06635696 0.15230751
 0.17466772 0.15482736 0.1271066  0.12345763 0.14390849 0.17236799
 0.19313248 0.19601884 0.18525638 0.17903323 0.19233096 0.21900937
 0.24383138 0.2574453  0.25711265 0.24404472 0.22173424 0.19482473
 0.17222624 0.168636   0.19648387 0.2455528  0.28558064 0.29232568
 0.26906967 0.23935004 0.21246728 0.18647668 0.17591447 0.18383296
 0.18590224 0.17375991 0.154522   0.14097705 0.14864033 0.17154065
 0.18039502 0.16495228 0.13943607 0.11957652 0.10485073 0.08662031
 0.08890331 0.10522707 0.1034315  0.07217371 0.06855087 0.15468158
 0.20408335 0.1790863  0.1733438  0.23185639 0.29952818 0.26042438
 0.21079345 0.08796689 0.05306146 0.05055616 0.05311365 0.10503455
 0.18255378 0.08022538 0.04233756 0.09671012 0.09916766 0.08569448
 0.04356734 0.04726489 0.09137782 0.13061245 0.15621899 0.16067861
 0.14927016 0.14306587 0.15911219 0.18753043 0.21048057 0.21926919
 0.21243763 0.1928339  0.16573302 0.13711472 0.11261006 0.09658074
 0.10095125 0.13646503 0.18289562 0.2093011  0.2063907  0.18971169
 0.168866   0.14277227 0.12981841 0.13958521 0.14609478 0.13853687
 0.1243321  0.12135545 0.14107996 0.17100424]
