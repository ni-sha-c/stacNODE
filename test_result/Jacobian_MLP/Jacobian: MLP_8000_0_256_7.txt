time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 170137.46875 Test: 4407.60938
Epoch: 80 Train: 38663.87109 Test: 2586.86157
Epoch: 160 Train: 14254.01465 Test: 562.05505
Epoch 240: New minimal relative error: 58.95%, model saved.
Epoch: 240 Train: 876.83936 Test: 46.61024
Epoch 320: New minimal relative error: 19.80%, model saved.
Epoch: 320 Train: 453.44016 Test: 33.88023
Epoch: 400 Train: 186.79388 Test: 8.47684
Epoch: 480 Train: 367.18015 Test: 59.52512
Epoch 560: New minimal relative error: 18.45%, model saved.
Epoch: 560 Train: 213.74001 Test: 9.13525
Epoch: 640 Train: 188.32405 Test: 25.88473
Epoch: 720 Train: 135.79425 Test: 12.94629
Epoch: 800 Train: 69.34789 Test: 17.23572
Epoch: 880 Train: 264.18167 Test: 47.01057
Epoch: 960 Train: 202.69153 Test: 22.41081
Epoch 1040: New minimal relative error: 8.64%, model saved.
Epoch: 1040 Train: 63.08702 Test: 10.96073
Epoch: 1120 Train: 23.58384 Test: 1.81088
Epoch: 1200 Train: 147.53554 Test: 18.63286
Epoch: 1280 Train: 48.49635 Test: 2.75637
Epoch: 1360 Train: 53.12927 Test: 12.92937
Epoch: 1440 Train: 21.47437 Test: 1.56425
Epoch: 1520 Train: 16.19586 Test: 0.28288
Epoch 1600: New minimal relative error: 5.03%, model saved.
Epoch: 1600 Train: 24.29480 Test: 0.79793
Epoch: 1680 Train: 100.63603 Test: 7.58740
Epoch: 1760 Train: 27.49568 Test: 0.97748
Epoch: 1840 Train: 69.83390 Test: 6.32774
Epoch: 1920 Train: 83.08820 Test: 0.70392
Epoch: 2000 Train: 18.45565 Test: 5.37185
Epoch: 2080 Train: 71.08214 Test: 7.26479
Epoch: 2160 Train: 84.22784 Test: 11.86964
Epoch: 2240 Train: 8.12304 Test: 0.71038
Epoch: 2320 Train: 60.19759 Test: 7.67115
Epoch: 2400 Train: 9.22279 Test: 0.34559
Epoch: 2480 Train: 50.06533 Test: 5.57020
Epoch: 2560 Train: 22.12544 Test: 0.93064
Epoch: 2640 Train: 15.77670 Test: 1.79546
Epoch: 2720 Train: 15.81995 Test: 1.39191
Epoch: 2800 Train: 9.49193 Test: 0.82641
Epoch: 2880 Train: 8.71362 Test: 0.38932
Epoch: 2960 Train: 9.41786 Test: 0.40336
Epoch: 3040 Train: 9.46725 Test: 0.86637
Epoch: 3120 Train: 20.10405 Test: 1.39237
Epoch: 3200 Train: 18.94221 Test: 5.20209
Epoch: 3280 Train: 4.05402 Test: 0.28645
Epoch: 3360 Train: 12.47358 Test: 1.19369
Epoch: 3440 Train: 8.57897 Test: 0.82299
Epoch: 3520 Train: 6.78158 Test: 0.58171
Epoch: 3600 Train: 16.01575 Test: 3.04105
Epoch: 3680 Train: 17.09389 Test: 1.99070
Epoch: 3760 Train: 6.91016 Test: 0.62119
Epoch: 3840 Train: 9.28336 Test: 0.93590
Epoch: 3920 Train: 6.43320 Test: 0.38443
Epoch: 4000 Train: 3.15653 Test: 0.15111
Epoch 4080: New minimal relative error: 4.47%, model saved.
Epoch: 4080 Train: 3.74336 Test: 0.20408
Epoch: 4160 Train: 6.47298 Test: 0.39066
Epoch: 4240 Train: 5.72683 Test: 0.70921
Epoch: 4320 Train: 7.14607 Test: 0.63014
Epoch: 4400 Train: 10.45572 Test: 0.99587
Epoch: 4480 Train: 22.85958 Test: 4.46194
Epoch: 4560 Train: 3.53777 Test: 0.26964
Epoch: 4640 Train: 1.73334 Test: 0.00245
Epoch 4720: New minimal relative error: 3.63%, model saved.
Epoch: 4720 Train: 4.02954 Test: 0.24578
Epoch: 4800 Train: 5.52236 Test: 0.40521
Epoch: 4880 Train: 4.69421 Test: 0.08462
Epoch: 4960 Train: 38.08662 Test: 5.80720
Epoch: 5040 Train: 9.05642 Test: 0.49022
Epoch: 5120 Train: 2.63367 Test: 0.34239
Epoch: 5200 Train: 4.00410 Test: 0.28832
Epoch: 5280 Train: 22.58358 Test: 2.49178
Epoch: 5360 Train: 7.98852 Test: 0.99602
Epoch 5440: New minimal relative error: 1.42%, model saved.
Epoch: 5440 Train: 2.16707 Test: 0.13002
Epoch: 5520 Train: 1.98608 Test: 0.11588
Epoch: 5600 Train: 2.43511 Test: 0.20995
Epoch: 5680 Train: 3.19301 Test: 0.31718
Epoch: 5760 Train: 1.45598 Test: 0.01137
Epoch 5840: New minimal relative error: 0.34%, model saved.
Epoch: 5840 Train: 1.21318 Test: 0.00259
Epoch: 5920 Train: 1.64916 Test: 0.03462
Epoch: 6000 Train: 1.36459 Test: 0.02144
Epoch: 6080 Train: 2.24357 Test: 0.12796
Epoch: 6160 Train: 2.21484 Test: 0.01866
Epoch: 6240 Train: 1.25018 Test: 0.05324
Epoch: 6320 Train: 8.05420 Test: 1.15967
Epoch: 6400 Train: 11.42172 Test: 2.49272
Epoch: 6480 Train: 1.09363 Test: 0.00699
Epoch: 6560 Train: 1.26597 Test: 0.04217
Epoch: 6640 Train: 1.36984 Test: 0.08068
Epoch: 6720 Train: 1.32340 Test: 0.04594
Epoch: 6800 Train: 1.32932 Test: 0.07689
Epoch: 6880 Train: 1.39144 Test: 0.04471
Epoch: 6960 Train: 2.42088 Test: 0.16154
Epoch: 7040 Train: 16.12701 Test: 2.84239
Epoch: 7120 Train: 1.25742 Test: 0.05113
Epoch: 7200 Train: 0.94157 Test: 0.01304
Epoch: 7280 Train: 1.21307 Test: 0.06879
Epoch: 7360 Train: 1.07479 Test: 0.01088
Epoch: 7440 Train: 4.10588 Test: 0.28175
Epoch: 7520 Train: 2.71178 Test: 0.17923
Epoch: 7600 Train: 1.44501 Test: 0.08566
Epoch: 7680 Train: 1.03023 Test: 0.01222
Epoch: 7760 Train: 1.07501 Test: 0.03517
Epoch: 7840 Train: 0.89480 Test: 0.02138
Epoch: 7920 Train: 6.38868 Test: 0.28655
Epoch: 7999 Train: 1.50893 Test: 0.11952
Training Loss: tensor(1.5089)
Test Loss: tensor(0.1195)
Learned LE: [ 8.8011324e-01 -8.2424935e-03 -1.4619557e+01]
True LE: [ 8.7445211e-01 -1.9823578e-03 -1.4543901e+01]
Relative Error: [0.1144387  0.11130944 0.11438697 0.12744445 0.14819635 0.16941892
 0.1825087  0.18485463 0.18018775 0.17156677 0.15674585 0.13476777
 0.1124026  0.09876829 0.09694515 0.10443386 0.11639915 0.12678927
 0.13293147 0.13658753 0.14052702 0.14584734 0.15237926 0.15835324
 0.16202399 0.162601   0.15979311 0.15455917 0.14708863 0.13712782
 0.1255648  0.11738649 0.12072664 0.13487515 0.15397383 0.17160207
 0.16642493 0.14024049 0.13345636 0.16146746 0.19468345 0.21547931
 0.2207179  0.20865086 0.18203698 0.14881608 0.11855856 0.09790626
 0.0879092  0.08547259 0.08639287 0.08862638 0.09108825 0.09359522
 0.09638018 0.09966863 0.10338459 0.10736107 0.11137002 0.11483383
 0.11659493 0.11558885 0.11175206 0.10715171 0.1060533  0.11327455
 0.13061818 0.15248948 0.16929485 0.1755059  0.17250039 0.16623281
 0.15596674 0.13755374 0.11400764 0.09504768 0.08683794 0.08911787
 0.09808567 0.10701165 0.11195259 0.11436696 0.11771327 0.12420559
 0.1332762  0.14280704 0.1501406  0.15347686 0.15245944 0.1477166
 0.14070764 0.1316834  0.12028944 0.10747512 0.10092115 0.10976802
 0.12959485 0.1540294  0.161524   0.1354593  0.11372998 0.13415705
 0.17357388 0.20261762 0.2126404  0.20072228 0.17065713 0.13456485
 0.10464053 0.08759537 0.08121442 0.08032706 0.08133162 0.08282268
 0.08441985 0.08614429 0.08829378 0.0911179  0.09454245 0.09814449
 0.10190163 0.10565894 0.10896188 0.11063731 0.10944135 0.10546825
 0.1016876  0.1030131  0.11411618 0.13369259 0.15393013 0.16552575
 0.16553566 0.16007346 0.15366612 0.14138721 0.12009415 0.09645087
 0.08030389 0.07495388 0.07917081 0.08689743 0.09089449 0.09137981
 0.09312154 0.09950708 0.11052955 0.12400883 0.1363619  0.14484112
 0.14744686 0.14437565 0.1371619  0.12822334 0.11805752 0.10534858
 0.09138699 0.08665597 0.10001583 0.12542816 0.15069093 0.13983287
 0.10390044 0.10346983 0.14333531 0.1833805  0.20204812 0.19276355
 0.16084427 0.12292805 0.0947122  0.08151474 0.07713055 0.07576137
 0.07533047 0.07575355 0.07679144 0.07827211 0.08039436 0.08336017
 0.08724495 0.0913673  0.09490798 0.09752238 0.10001384 0.10273787
 0.10459473 0.10383444 0.10053147 0.09788713 0.1014745  0.11473268
 0.13489711 0.15231709 0.15852837 0.1545306  0.14879069 0.14283523
 0.12871146 0.10518365 0.08136496 0.06596966 0.06207218 0.06683878
 0.07099089 0.07000916 0.06834395 0.0716878  0.08230774 0.09838428
 0.11609998 0.13130102 0.1408721  0.14307725 0.13820434 0.12872924
 0.11762588 0.10648362 0.09320226 0.07814474 0.0739032  0.08995885
 0.11969296 0.14141636 0.11394281 0.08160266 0.10267521 0.1518915
 0.18558472 0.18494518 0.15433349 0.1154697  0.08925942 0.07914069
 0.07587138 0.07381283 0.07194291 0.07090018 0.07102624 0.07251459
 0.07475334 0.07802606 0.08271814 0.08863638 0.094308   0.0976363
 0.09792555 0.09689218 0.09707128 0.09846231 0.09868722 0.09649364
 0.09509952 0.10004162 0.11408472 0.13313062 0.14800991 0.1505192
 0.14386807 0.13839857 0.13350624 0.11885413 0.09354906 0.068142
 0.051803   0.04840928 0.05268207 0.05287492 0.04773274 0.04461733
 0.04976008 0.06415078 0.08447269 0.10601389 0.1243392  0.1362087
 0.13928467 0.13395928 0.12271576 0.10973879 0.09769405 0.08453216
 0.06876562 0.06299314 0.07777917 0.10948209 0.12911895 0.0935012
 0.06766512 0.10207868 0.15480383 0.17459917 0.15245198 0.11361311
 0.08728782 0.07942424 0.0790973  0.07966533 0.078307   0.07525463
 0.07272042 0.07213263 0.07348634 0.07604726 0.08004115 0.0861937
 0.09459956 0.10327684 0.10838789 0.10746239 0.10131269 0.09493028
 0.09275191 0.09319646 0.09280118 0.09255652 0.09760624 0.1105132
 0.128061   0.14150175 0.14232531 0.13380392 0.12810889 0.12553015
 0.11264885 0.08661552 0.05844203 0.03939778 0.0354378  0.03905595
 0.03596266 0.02762457 0.02209711 0.02650047 0.0422422  0.06483322
 0.08942143 0.11148494 0.12702104 0.13343765 0.1302103  0.1192659
 0.10494221 0.09184313 0.07967038 0.06503829 0.05530579 0.0644348
 0.09239215 0.11747012 0.08438358 0.05638196 0.0949367  0.14792684
 0.15223934 0.11848952 0.08824544 0.07958382 0.08360923 0.09009425
 0.09082529 0.08508042 0.07820063 0.07440827 0.07423744 0.07628196
 0.07926531 0.08323361 0.08923586 0.09870107 0.1106476  0.1212386
 0.12469937 0.11821891 0.10423766 0.091723   0.08733083 0.08777126
 0.08886616 0.09312215 0.10380209 0.11927798 0.13237277 0.1345322
 0.12464778 0.11677951 0.11669891 0.11001782 0.086678   0.0558454
 0.03235973 0.02472434 0.02790925 0.02384202 0.01430584 0.00769579
 0.00444946 0.01638018 0.03735485 0.06256595]
