time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 2000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 5
reg_param: 50.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 83.544563293 Test: 17.557683945
Epoch 0: New minimal relative error: 17.56%, model saved.
Epoch: 20 Train: 15.847143173 Test: 10.189832687
Epoch 20: New minimal relative error: 10.19%, model saved.
Epoch: 40 Train: 9.324063301 Test: 5.644166946
Epoch 40: New minimal relative error: 5.64%, model saved.
Epoch: 60 Train: 7.915178299 Test: 4.821968555
Epoch 60: New minimal relative error: 4.82%, model saved.
Epoch: 80 Train: 7.549341202 Test: 4.477254868
Epoch 80: New minimal relative error: 4.48%, model saved.
Epoch: 100 Train: 7.564016342 Test: 4.261971951
Epoch 100: New minimal relative error: 4.26%, model saved.
Epoch: 120 Train: 7.384091377 Test: 4.214506626
Epoch 120: New minimal relative error: 4.21%, model saved.
Epoch: 140 Train: 7.259684563 Test: 4.199406624
Epoch 140: New minimal relative error: 4.20%, model saved.
Epoch: 160 Train: 7.702136040 Test: 4.081214428
Epoch 160: New minimal relative error: 4.08%, model saved.
Epoch: 180 Train: 7.657989025 Test: 4.276244640
Epoch: 200 Train: 8.370399475 Test: 4.026276588
Epoch 200: New minimal relative error: 4.03%, model saved.
Epoch: 220 Train: 7.659290791 Test: 4.336845875
Epoch: 240 Train: 8.129831314 Test: 3.893513203
Epoch 240: New minimal relative error: 3.89%, model saved.
Epoch: 260 Train: 7.530379295 Test: 4.484936237
Epoch: 280 Train: 7.183724880 Test: 4.157781124
Epoch: 300 Train: 7.037217617 Test: 4.016995907
Epoch: 320 Train: 7.040897369 Test: 3.981633902
Epoch: 340 Train: 6.954104424 Test: 4.036304951
Epoch: 360 Train: 6.691188812 Test: 4.233761787
Epoch: 380 Train: 6.647646427 Test: 3.972303152
Epoch: 400 Train: 6.533119202 Test: 4.229045868
Epoch: 420 Train: 6.392380714 Test: 4.198767185
Epoch: 440 Train: 6.300660610 Test: 4.262264729
Epoch: 460 Train: 6.284020424 Test: 4.284030914
Epoch: 480 Train: 6.188201904 Test: 4.236972809
Epoch: 500 Train: 6.167142868 Test: 4.257205486
Epoch: 520 Train: 6.063535213 Test: 4.306905270
Epoch: 540 Train: 6.102441788 Test: 4.373754978
Epoch: 560 Train: 6.128118992 Test: 4.358335972
Epoch: 580 Train: 6.035286903 Test: 4.416143894
Epoch: 600 Train: 6.172142029 Test: 4.315587044
Epoch: 620 Train: 6.109776497 Test: 4.345359325
Epoch: 640 Train: 6.106648922 Test: 4.336310863
Epoch: 660 Train: 6.104532719 Test: 4.347251892
Epoch: 680 Train: 6.089644909 Test: 4.270532131
Epoch: 700 Train: 6.307999611 Test: 4.351153851
Epoch: 720 Train: 6.239665985 Test: 4.262327671
Epoch: 740 Train: 6.133527279 Test: 4.365038872
Epoch: 760 Train: 6.243602753 Test: 4.239380360
Epoch: 780 Train: 6.180613995 Test: 4.284416199
Epoch: 800 Train: 6.077451229 Test: 4.336399555
Epoch: 820 Train: 5.987798214 Test: 4.416436195
Epoch: 840 Train: 5.993919373 Test: 4.415929794
Epoch: 860 Train: 6.002301216 Test: 4.400647640
Epoch: 880 Train: 5.956757545 Test: 4.449461937
Epoch: 900 Train: 5.887054920 Test: 4.506206989
Epoch: 920 Train: 5.980528831 Test: 4.426484108
Epoch: 940 Train: 5.990087032 Test: 4.296943665
Epoch: 960 Train: 6.223693371 Test: 4.331072330
Epoch: 980 Train: 6.099484921 Test: 4.268993855
Epoch: 1000 Train: 6.137747288 Test: 4.254375458
Epoch: 1020 Train: 6.058172226 Test: 4.301469326
Epoch: 1040 Train: 5.997632980 Test: 4.306114674
Epoch: 1060 Train: 5.827918053 Test: 4.367624760
Epoch: 1080 Train: 5.770337105 Test: 4.433607101
Epoch: 1100 Train: 5.911996365 Test: 4.332890511
Epoch: 1120 Train: 5.877994537 Test: 4.358216286
Epoch: 1140 Train: 6.083679676 Test: 4.229469776
Epoch: 1160 Train: 6.052774429 Test: 4.310319424
Epoch: 1180 Train: 6.009835243 Test: 4.304528713
Epoch: 1200 Train: 5.934000015 Test: 4.325302124
Epoch: 1220 Train: 5.906479359 Test: 4.344466209
Epoch: 1240 Train: 5.857107162 Test: 4.355781555
Epoch: 1260 Train: 5.875500202 Test: 4.365868092
Epoch: 1280 Train: 5.863156319 Test: 4.365077019
Epoch: 1300 Train: 5.830197334 Test: 4.376410007
Epoch: 1320 Train: 5.783625603 Test: 4.412004471
Epoch: 1340 Train: 5.744697094 Test: 4.429334641
Epoch: 1360 Train: 5.763379574 Test: 4.373159885
Epoch: 1380 Train: 5.776515007 Test: 4.380237579
Epoch: 1400 Train: 5.743175983 Test: 4.370017052
Epoch: 1420 Train: 5.724051952 Test: 4.382104874
Epoch: 1440 Train: 5.698633671 Test: 4.401687622
Epoch: 1460 Train: 5.687342644 Test: 4.411164761
Epoch: 1480 Train: 5.688086510 Test: 4.423608780
Epoch: 1500 Train: 5.682392597 Test: 4.432231426
Epoch: 1520 Train: 5.684446335 Test: 4.446182251
Epoch: 1540 Train: 5.677544594 Test: 4.454582214
Epoch: 1560 Train: 5.682942867 Test: 4.465666294
Epoch: 1580 Train: 5.674813747 Test: 4.480102062
Epoch: 1600 Train: 5.663286209 Test: 4.489422321
Epoch: 1620 Train: 5.671141624 Test: 4.487051487
Epoch: 1640 Train: 5.672286510 Test: 4.485369205
Epoch: 1660 Train: 5.666092396 Test: 4.491182804
Epoch: 1680 Train: 5.668886185 Test: 4.501779556
Epoch: 1700 Train: 5.645998955 Test: 4.504565239
Epoch: 1720 Train: 5.634332180 Test: 4.501961708
Epoch: 1740 Train: 5.626729965 Test: 4.501916409
Epoch: 1760 Train: 5.599830627 Test: 4.497042179
Epoch: 1780 Train: 5.590063095 Test: 4.503425598
Epoch: 1800 Train: 5.586147308 Test: 4.509293556
Epoch: 1820 Train: 5.579624176 Test: 4.503244400
Epoch: 1840 Train: 5.576933384 Test: 4.504955769
Epoch: 1860 Train: 5.582140446 Test: 4.507505417
Epoch: 1880 Train: 5.602578640 Test: 4.521670341
Epoch: 1900 Train: 5.615080357 Test: 4.519889355
Epoch: 1920 Train: 5.610022545 Test: 4.528343678
Epoch: 1940 Train: 5.613545418 Test: 4.613624573
Epoch: 1960 Train: 5.923789024 Test: 4.361162663
Epoch: 1980 Train: 6.026335239 Test: 4.269217014
Epoch: 1999 Train: 5.987042904 Test: 4.262454987
Training Loss: tensor(5.9870)
Test Loss: tensor(4.2625)
True Mean x: tensor(3.1300, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(1.1924e+11, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3949, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(1.8981e+23, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0350)
Jacobian term Test Loss: tensor(0.0006)
Learned LE: [1.7288282  0.38060558]
True LE: tensor([ 0.6931, -0.7266], dtype=torch.float64)
