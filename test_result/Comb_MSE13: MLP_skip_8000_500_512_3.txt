time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.83%, model saved.
Epoch: 0 Train: 3469.38550 Test: 3940.37402
Epoch 80: New minimal relative error: 46.01%, model saved.
Epoch: 80 Train: 156.04272 Test: 198.73373
Epoch 160: New minimal relative error: 15.20%, model saved.
Epoch: 160 Train: 22.02524 Test: 25.95984
Epoch: 240 Train: 10.75125 Test: 12.61893
Epoch: 320 Train: 7.33358 Test: 8.75672
Epoch: 400 Train: 8.08202 Test: 9.41771
Epoch: 480 Train: 4.76876 Test: 5.76979
Epoch: 560 Train: 4.44455 Test: 5.34732
Epoch: 640 Train: 5.05914 Test: 6.10541
Epoch 720: New minimal relative error: 10.44%, model saved.
Epoch: 720 Train: 3.22177 Test: 4.02834
Epoch: 800 Train: 3.14603 Test: 3.99459
Epoch: 880 Train: 10.48101 Test: 14.44538
Epoch: 960 Train: 12.31681 Test: 12.31167
Epoch 1040: New minimal relative error: 10.35%, model saved.
Epoch: 1040 Train: 3.03888 Test: 3.72644
Epoch 1120: New minimal relative error: 9.82%, model saved.
Epoch: 1120 Train: 2.34676 Test: 3.20991
Epoch: 1200 Train: 1.87884 Test: 2.34946
Epoch: 1280 Train: 1.67616 Test: 2.14534
Epoch: 1360 Train: 1.55867 Test: 2.03617
Epoch: 1440 Train: 1.56452 Test: 2.06721
Epoch 1520: New minimal relative error: 6.96%, model saved.
Epoch: 1520 Train: 1.28102 Test: 1.69144
Epoch: 1600 Train: 1.18373 Test: 1.57647
Epoch: 1680 Train: 1.89389 Test: 2.50374
Epoch 1760: New minimal relative error: 5.98%, model saved.
Epoch: 1760 Train: 1.01700 Test: 1.37201
Epoch: 1840 Train: 0.97684 Test: 1.33263
Epoch: 1920 Train: 0.92914 Test: 1.27820
Epoch: 2000 Train: 1.18650 Test: 1.52003
Epoch: 2080 Train: 0.86783 Test: 1.21111
Epoch: 2160 Train: 1.69268 Test: 2.82407
Epoch: 2240 Train: 0.68740 Test: 0.97079
Epoch: 2320 Train: 0.72577 Test: 1.01602
Epoch: 2400 Train: 0.69459 Test: 0.89916
Epoch: 2480 Train: 0.60156 Test: 0.85711
Epoch: 2560 Train: 0.74767 Test: 0.86887
Epoch: 2640 Train: 0.61210 Test: 0.91508
Epoch: 2720 Train: 0.50294 Test: 0.73802
Epoch: 2800 Train: 0.55171 Test: 0.86395
Epoch: 2880 Train: 1.02637 Test: 1.36097
Epoch: 2960 Train: 0.46171 Test: 0.65741
Epoch: 3040 Train: 0.61503 Test: 0.86461
Epoch: 3120 Train: 0.54320 Test: 0.71518
Epoch: 3200 Train: 0.40335 Test: 0.71450
Epoch: 3280 Train: 0.48386 Test: 0.72166
Epoch: 3360 Train: 0.35278 Test: 0.53544
Epoch: 3440 Train: 0.79008 Test: 0.75009
Epoch: 3520 Train: 0.34855 Test: 0.54792
Epoch 3600: New minimal relative error: 5.72%, model saved.
Epoch: 3600 Train: 0.32125 Test: 0.49151
Epoch: 3680 Train: 0.46437 Test: 0.62187
Epoch: 3760 Train: 0.32869 Test: 0.49731
Epoch: 3840 Train: 0.28098 Test: 0.44267
Epoch 3920: New minimal relative error: 3.70%, model saved.
Epoch: 3920 Train: 0.29321 Test: 0.48703
Epoch: 4000 Train: 0.31105 Test: 0.45066
Epoch: 4080 Train: 0.27539 Test: 0.41735
Epoch: 4160 Train: 0.25683 Test: 0.40632
Epoch: 4240 Train: 1.03598 Test: 1.38755
Epoch: 4320 Train: 0.23229 Test: 0.38294
Epoch: 4400 Train: 0.22960 Test: 0.37706
Epoch: 4480 Train: 0.21999 Test: 0.36687
Epoch: 4560 Train: 0.22625 Test: 0.37506
Epoch: 4640 Train: 0.55538 Test: 0.68217
Epoch: 4720 Train: 0.20359 Test: 0.34551
Epoch: 4800 Train: 0.21484 Test: 0.34402
Epoch: 4880 Train: 0.19392 Test: 0.33490
Epoch: 4960 Train: 0.21596 Test: 0.33921
Epoch: 5040 Train: 0.23375 Test: 0.36848
Epoch: 5120 Train: 0.18603 Test: 0.31965
Epoch: 5200 Train: 0.32191 Test: 0.53167
Epoch: 5280 Train: 0.17380 Test: 0.30776
Epoch: 5360 Train: 0.19096 Test: 0.31089
Epoch: 5440 Train: 0.18929 Test: 0.31551
Epoch: 5520 Train: 0.17025 Test: 0.31445
Epoch: 5600 Train: 0.16245 Test: 0.29606
Epoch: 5680 Train: 0.27049 Test: 0.46531
Epoch: 5760 Train: 0.15482 Test: 0.28138
Epoch: 5840 Train: 0.15853 Test: 0.28730
Epoch: 5920 Train: 0.15132 Test: 0.28348
Epoch: 6000 Train: 0.15829 Test: 0.29617
Epoch: 6080 Train: 0.14371 Test: 0.26707
Epoch: 6160 Train: 0.65311 Test: 0.77104
Epoch: 6240 Train: 0.13857 Test: 0.26165
Epoch: 6320 Train: 0.47892 Test: 0.60994
Epoch: 6400 Train: 0.13504 Test: 0.25909
Epoch 6480: New minimal relative error: 3.64%, model saved.
Epoch: 6480 Train: 0.13210 Test: 0.25445
Epoch: 6560 Train: 0.44312 Test: 0.49414
Epoch: 6640 Train: 0.12679 Test: 0.24493
Epoch: 6720 Train: 0.12517 Test: 0.24068
Epoch 6800: New minimal relative error: 2.84%, model saved.
Epoch: 6800 Train: 0.13412 Test: 0.24501
Epoch: 6880 Train: 0.14132 Test: 0.26949
Epoch: 6960 Train: 0.11906 Test: 0.23197
Epoch: 7040 Train: 0.12319 Test: 0.23334
Epoch: 7120 Train: 0.11570 Test: 0.22655
Epoch: 7200 Train: 0.21658 Test: 0.33373
Epoch: 7280 Train: 0.13334 Test: 0.22837
Epoch: 7360 Train: 0.36299 Test: 0.44914
Epoch: 7440 Train: 0.10944 Test: 0.21713
Epoch: 7520 Train: 0.16163 Test: 0.27730
Epoch: 7600 Train: 0.10655 Test: 0.21266
Epoch: 7680 Train: 0.11992 Test: 0.21796
Epoch: 7760 Train: 0.16967 Test: 0.26918
Epoch: 7840 Train: 0.10327 Test: 0.20543
Epoch: 7920 Train: 0.10135 Test: 0.20375
Epoch: 7999 Train: 0.18580 Test: 0.29333
Training Loss: tensor(0.1858)
Test Loss: tensor(0.2933)
Learned LE: [ 0.8069896  0.009042  -4.5978646]
True LE: [ 8.7610257e-01 -2.0455349e-04 -1.4552611e+01]
Relative Error: [1.9565681  2.8980908  4.0286617  4.6811852  5.40816    6.3453474
 7.277797   7.419724   7.3731823  7.105712   6.4368844  5.562354
 4.372811   3.3891802  2.3639708  2.4164522  2.02245    0.59601307
 0.4169478  1.0683277  1.1225684  2.091503   2.8796108  3.0370712
 2.9754252  3.0638933  3.237646   3.1865187  3.2301505  3.21706
 2.5618086  2.2530475  2.0877361  1.9076076  1.955287   2.0401275
 2.4230502  3.469123   2.7222123  2.655401   2.6476645  1.6872078
 1.1120254  1.1175716  1.2561468  1.3437846  0.9581831  0.98219717
 0.99136597 1.371886   2.021592   2.271003   2.5000846  2.3709676
 2.275976   2.3402379  3.0365906  3.2211914  2.4041035  1.8701438
 1.5457735  1.2309928  0.77351946 0.5596498  0.80119056 1.7348425
 2.5829766  3.5481772  4.5257916  5.3163047  5.9754543  5.9834805
 5.69688    5.30374    4.4908156  3.5280619  2.9844837  1.8653356
 1.0727707  1.3164067  0.4811617  0.31401265 0.7194241  0.9791369
 1.841698   2.5691383  2.9403346  3.0396874  3.2170124  3.3482656
 3.1940181  3.0044992  3.036807   2.8927097  2.1388783  2.2159214
 1.9024935  1.4179327  1.4010736  1.5552421  1.681033   2.5141473
 1.9116839  1.7694391  2.0231316  1.2342006  0.6452655  0.900551
 1.297895   1.0051643  0.6158586  0.7164962  0.94665456 1.3353491
 1.4575597  1.510817   1.5503669  1.2440922  1.3759487  1.6889212
 2.117515   1.6670344  1.1241639  0.9293901  0.84603363 0.77502126
 0.52548444 0.39341193 0.10825481 0.3390745  1.2562932  2.3314247
 3.3184524  3.806626   3.9763856  4.3270802  4.451049   4.1195197
 3.2050714  2.2445605  1.9068553  0.92668384 0.3283281  0.6025174
 0.8857593  0.9712416  1.5557284  1.5455687  2.02605    2.8135922
 2.9403057  2.9932656  3.079002   2.9308898  2.5905616  2.556774
 2.622638   2.5602562  1.9296418  1.6487058  1.6685007  1.4144864
 1.1019869  0.8749001  0.8251238  0.9817362  1.3826488  1.0489008
 1.1254355  0.9054129  0.3012876  0.38566414 0.7047256  0.5818401
 0.4317836  0.44530305 0.78245497 0.71332186 0.8516201  0.9293866
 0.7442244  0.768324   0.7410542  0.9436056  1.0543473  1.0559735
 0.75285983 0.5636556  0.5146564  0.5046619  0.49728867 0.4048727
 0.3454365  0.26990908 0.48353592 1.0830034  1.5688224  1.9624068
 2.3085265  2.6145587  2.8522243  3.2933528  2.7358203  1.6719526
 0.8928504  0.6183072  0.38083684 0.8521607  1.4452542  1.5710028
 1.9703305  2.6331716  2.5856006  2.7189796  2.8662965  3.0457203
 2.390703   1.6807643  1.3595536  1.0823351  0.97974956 1.0466378
 0.93499553 0.67166984 0.9736271  0.9020562  0.8753404  0.6359496
 0.6496232  0.5138426  0.5585536  0.5869481  0.64293474 0.6277325
 0.27493513 0.49990317 0.15220605 0.16132307 0.15195154 0.33914015
 0.5452617  0.29970047 0.5884069  0.64768374 0.46308818 0.2901872
 0.15024261 0.5394303  0.7489732  0.7539339  0.67880654 0.5479292
 0.35339057 0.3134648  0.3803153  0.55179507 0.6365487  0.59875625
 0.32251123 0.37287155 0.604796   0.65154964 0.8780306  1.212942
 1.4734527  1.5595684  1.7921023  1.9939959  0.901858   0.58049524
 0.5646262  0.97669864 1.2850995  2.3484907  2.718179   3.1737392
 3.7127905  3.690674   3.0976105  3.0225818  2.6433012  2.025828
 1.1414205  0.6178176  0.6481544  0.6345482  0.6606667  0.5526085
 0.17161809 0.25864452 0.40821922 0.39730212 0.34835705 0.25733745
 0.6006299  0.5086828  0.29185423 0.13184342 0.5615021  0.20358895
 0.54134125 0.6108732  0.60220623 0.5648686  0.5672835  0.53629404
 0.3174503  0.4099196  0.1796885  0.31563985 0.29209015 0.27940163
 0.23133236 0.44284585 0.40352303 0.23783346 0.4390738  0.4835883
 0.25158083 0.4493801  0.77318716 0.94656795 0.9443714  0.92779464
 0.41000554 0.3039306  0.24007289 0.4892143  0.586298   0.5514261
 0.5703495  0.5495579  0.8037012  0.9647631  0.73705536 0.95076513
 1.4262456  1.5987508  1.7366071  3.1889842  3.6909733  3.8694866
 4.5503807  4.1564956  3.7597735  3.408524   2.651688   1.8616077
 1.2422276  1.1242222  1.3372645  1.4751896  1.0927641  0.5807553
 0.23818782 0.16039714 0.24854235 0.21446307 0.5045949  0.37171513
 0.4599475  0.20465726 0.12077511 0.20360246 0.42853868 0.52843153
 0.21227218 0.1574533  0.27569112 0.47217932 0.5748786  0.4769465
 0.48533726 0.5031743  0.10373481 0.26734287 0.21494979 0.23575437
 0.22777674 0.12509528 0.215377   0.2361306  0.4083948  0.3301115
 0.42861012 0.64338857 0.7206934  0.7845522  0.914104   0.8412762
 0.64381564 0.4377567  0.16678008 0.3879778  0.57207465 0.8909254
 0.5588806  0.3662357  0.38417843 0.31468877]
