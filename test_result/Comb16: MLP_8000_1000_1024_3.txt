time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.91%, model saved.
Epoch: 0 Train: 3834.01001 Test: 3924.32397
Epoch 80: New minimal relative error: 70.83%, model saved.
Epoch: 80 Train: 46.46240 Test: 48.18507
Epoch 160: New minimal relative error: 19.44%, model saved.
Epoch: 160 Train: 9.46405 Test: 8.94943
Epoch 240: New minimal relative error: 16.08%, model saved.
Epoch: 240 Train: 5.44018 Test: 5.05092
Epoch: 320 Train: 3.42742 Test: 3.09280
Epoch: 400 Train: 3.82788 Test: 2.65846
Epoch: 480 Train: 8.25029 Test: 7.11793
Epoch: 560 Train: 2.11821 Test: 2.71746
Epoch: 640 Train: 8.59238 Test: 10.70269
Epoch: 720 Train: 2.95392 Test: 1.69239
Epoch: 800 Train: 1.16469 Test: 0.87085
Epoch: 880 Train: 2.65280 Test: 2.27779
Epoch: 960 Train: 0.93261 Test: 0.84027
Epoch: 1040 Train: 0.68350 Test: 0.62400
Epoch: 1120 Train: 0.57775 Test: 0.56409
Epoch: 1200 Train: 0.48363 Test: 0.44403
Epoch: 1280 Train: 0.51386 Test: 0.51191
Epoch: 1360 Train: 0.37106 Test: 0.34440
Epoch: 1440 Train: 0.47003 Test: 0.51964
Epoch: 1520 Train: 4.76120 Test: 5.96207
Epoch: 1600 Train: 0.29711 Test: 0.28981
Epoch: 1680 Train: 0.27376 Test: 0.28413
Epoch 1760: New minimal relative error: 15.01%, model saved.
Epoch: 1760 Train: 0.93342 Test: 1.09736
Epoch: 1840 Train: 0.30441 Test: 0.34039
Epoch: 1920 Train: 0.25286 Test: 0.27667
Epoch: 2000 Train: 0.20326 Test: 0.19349
Epoch 2080: New minimal relative error: 14.17%, model saved.
Epoch: 2080 Train: 1.15280 Test: 1.41862
Epoch: 2160 Train: 0.53688 Test: 0.70761
Epoch 2240: New minimal relative error: 9.30%, model saved.
Epoch: 2240 Train: 0.36962 Test: 0.44096
Epoch: 2320 Train: 1.83175 Test: 2.14994
Epoch: 2400 Train: 0.37837 Test: 0.26902
Epoch: 2480 Train: 0.50306 Test: 0.47681
Epoch: 2560 Train: 0.21779 Test: 0.18844
Epoch: 2640 Train: 0.63297 Test: 0.34657
Epoch: 2720 Train: 0.18948 Test: 0.21935
Epoch: 2800 Train: 0.18087 Test: 0.41587
Epoch: 2880 Train: 1.87649 Test: 1.99646
Epoch: 2960 Train: 1.00116 Test: 1.19648
Epoch: 3040 Train: 0.25264 Test: 0.29021
Epoch: 3120 Train: 0.36668 Test: 0.45752
Epoch: 3200 Train: 0.67842 Test: 0.89079
Epoch: 3280 Train: 0.93975 Test: 1.27200
Epoch: 3360 Train: 0.32359 Test: 0.18520
Epoch: 3440 Train: 0.40754 Test: 0.26037
Epoch: 3520 Train: 0.28263 Test: 0.33545
Epoch: 3600 Train: 0.31427 Test: 0.27243
Epoch: 3680 Train: 0.10460 Test: 0.12480
Epoch 3760: New minimal relative error: 8.42%, model saved.
Epoch: 3760 Train: 0.08198 Test: 0.09743
Epoch: 3840 Train: 1.19075 Test: 1.39449
Epoch: 3920 Train: 0.07340 Test: 0.08983
Epoch: 4000 Train: 0.14889 Test: 0.09038
Epoch: 4080 Train: 0.06967 Test: 0.08734
Epoch: 4160 Train: 0.06895 Test: 0.09122
Epoch: 4240 Train: 0.06755 Test: 0.08516
Epoch: 4320 Train: 0.37534 Test: 0.48617
Epoch: 4400 Train: 0.06247 Test: 0.08087
Epoch: 4480 Train: 0.15753 Test: 0.10512
Epoch: 4560 Train: 0.09392 Test: 0.12314
Epoch: 4640 Train: 0.05878 Test: 0.07779
Epoch: 4720 Train: 0.09559 Test: 0.10730
Epoch: 4800 Train: 0.46691 Test: 0.37938
Epoch: 4880 Train: 0.06635 Test: 0.09048
Epoch: 4960 Train: 0.05403 Test: 0.07509
Epoch 5040: New minimal relative error: 8.00%, model saved.
Epoch: 5040 Train: 0.05508 Test: 0.07740
Epoch: 5120 Train: 0.09584 Test: 0.12912
Epoch: 5200 Train: 0.05034 Test: 0.07014
Epoch: 5280 Train: 0.07321 Test: 0.08052
Epoch: 5360 Train: 0.08541 Test: 0.11484
Epoch: 5440 Train: 0.04743 Test: 0.06795
Epoch: 5520 Train: 0.06076 Test: 0.09525
Epoch: 5600 Train: 0.34948 Test: 0.43509
Epoch: 5680 Train: 0.04481 Test: 0.06535
Epoch: 5760 Train: 0.04716 Test: 0.06593
Epoch: 5840 Train: 0.05058 Test: 0.07331
Epoch: 5920 Train: 0.04277 Test: 0.06324
Epoch: 6000 Train: 0.08979 Test: 0.12353
Epoch: 6080 Train: 0.04041 Test: 0.06160
Epoch: 6160 Train: 0.04432 Test: 0.07217
Epoch: 6240 Train: 0.03907 Test: 0.06043
Epoch: 6320 Train: 0.04967 Test: 0.06797
Epoch: 6400 Train: 0.03782 Test: 0.05910
Epoch: 6480 Train: 0.03798 Test: 0.06015
Epoch 6560: New minimal relative error: 3.97%, model saved.
Epoch: 6560 Train: 0.03659 Test: 0.05801
Epoch: 6640 Train: 0.06053 Test: 0.07530
Epoch: 6720 Train: 0.03552 Test: 0.05689
Epoch: 6800 Train: 0.06657 Test: 0.06555
Epoch: 6880 Train: 0.03449 Test: 0.05590
Epoch: 6960 Train: 0.11367 Test: 0.10066
Epoch: 7040 Train: 0.03358 Test: 0.05491
Epoch: 7120 Train: 0.03292 Test: 0.05431
Epoch: 7200 Train: 0.04481 Test: 0.06481
Epoch: 7280 Train: 0.03210 Test: 0.05348
Epoch: 7360 Train: 0.23112 Test: 0.14859
Epoch: 7440 Train: 0.03128 Test: 0.05264
Epoch: 7520 Train: 0.11355 Test: 0.16200
Epoch: 7600 Train: 0.03077 Test: 0.05211
Epoch: 7680 Train: 0.02995 Test: 0.05120
Epoch: 7760 Train: 0.04862 Test: 0.05357
Epoch: 7840 Train: 0.02936 Test: 0.05055
Epoch: 7920 Train: 0.02887 Test: 0.05006
Epoch: 7999 Train: 0.03723 Test: 0.07525
Training Loss: tensor(0.0372)
Test Loss: tensor(0.0753)
Learned LE: [ 0.8790279   0.02720132 -5.4571238 ]
True LE: [ 8.6011153e-01  9.1871880e-03 -1.4540720e+01]
Relative Error: [0.6003587  0.6644525  0.64619225 0.60541296 0.5859417  0.617021
 0.6925519  0.7472461  0.7377436  0.80765617 0.9427728  0.9440836
 0.81340754 0.6631155  0.551385   0.5405087  0.8300172  1.2954373
 1.8479439  2.443796   2.871389   3.0849838  3.208065   3.1178362
 2.9516041  2.771481   2.6796405  2.6022403  2.4020572  2.153386
 1.8703314  1.7284055  1.6624811  1.4951522  1.2635558  1.118987
 1.0630825  0.8732884  0.8552993  0.86406267 0.7222147  0.56413835
 0.40165642 0.24315265 0.19733429 0.34048244 0.36850876 0.37595433
 0.29267895 0.21437335 0.24532953 0.22513342 0.1625092  0.08825954
 0.07804663 0.08542126 0.13734289 0.0972093  0.04300352 0.14436966
 0.15666567 0.11051836 0.22697367 0.38659212 0.47271103 0.4639228
 0.40794286 0.3526854  0.34138125 0.39636338 0.45359516 0.5074865
 0.7033187  0.81224805 0.69283307 0.51006573 0.4473723  0.5840493
 0.828181   1.2533618  1.7123824  2.1286318  2.4952154  2.6569047
 2.6068432  2.568284   2.381386   2.1176376  1.8676864  1.7624717
 1.7917613  1.7349594  1.6077601  1.3606704  1.1237707  1.1169008
 1.0543493  0.87681675 0.65161234 0.5915768  0.48287386 0.47025177
 0.5367913  0.5016076  0.46167198 0.36863428 0.16433696 0.11325897
 0.24569009 0.2727818  0.24468368 0.19381002 0.1735829  0.21675129
 0.22962639 0.11567846 0.03675329 0.09821782 0.11029986 0.14616875
 0.05821421 0.09521761 0.17017487 0.15792131 0.084185   0.08621226
 0.18937466 0.27802706 0.3056464  0.26477504 0.18975918 0.10747129
 0.11983324 0.21331728 0.31851736 0.4708407  0.4842096  0.26784024
 0.25393814 0.549463   0.85739607 1.096251   1.4352453  1.8745223
 2.2003162  2.3167121  2.188031   1.994369   1.8191956  1.771148
 1.6115013  1.267888   1.0130132  0.9257255  0.93053764 0.92740154
 0.85978985 0.74991953 0.6095278  0.56119573 0.50708413 0.39577508
 0.21859445 0.20582741 0.14667585 0.18299824 0.20744193 0.21455528
 0.26996452 0.25388792 0.16916966 0.18834965 0.23336306 0.11825442
 0.074775   0.10864364 0.13210164 0.22927763 0.1472799  0.05153817
 0.08486827 0.07382824 0.12062178 0.0451997  0.10870589 0.149641
 0.11344474 0.05189031 0.05280227 0.07467145 0.09120543 0.13702047
 0.14995302 0.11138827 0.08005661 0.17234391 0.26556665 0.19818093
 0.1423033  0.13542934 0.08102015 0.41673672 0.78783834 1.0889268
 1.2896143  1.4495196  1.717874   1.9217372  1.7790681  1.4428755
 1.2618357  1.0189505  0.7805032  0.84332955 0.8363335  0.7208478
 0.58680356 0.3306631  0.33460185 0.35494372 0.30999327 0.37382004
 0.25092953 0.1855271  0.22441769 0.14158325 0.19298975 0.05000851
 0.06959832 0.08120341 0.07142323 0.1160036  0.1765074  0.16471438
 0.08897617 0.13535394 0.19615872 0.14441317 0.04495855 0.07279682
 0.08208419 0.14525078 0.10078199 0.08688042 0.08264439 0.02374613
 0.04514514 0.06409363 0.11889638 0.11016344 0.09557278 0.12396903
 0.13201514 0.08910067 0.04790172 0.06919859 0.0691703  0.07977606
 0.22342272 0.34373313 0.2818754  0.0792812  0.09612662 0.15160984
 0.3886396  0.6823194  1.0113127  1.260186   1.4721746  1.6363133
 1.4578416  1.33383    1.0679114  0.77113944 0.6318432  0.5223537
 0.30884653 0.2921459  0.2718843  0.28427395 0.27658883 0.26695818
 0.10054246 0.11988146 0.07957196 0.04334271 0.14934517 0.0736386
 0.14523014 0.06553417 0.18230854 0.04857661 0.06040229 0.03881467
 0.09510031 0.09586585 0.06214444 0.06076078 0.0763576  0.07610305
 0.0792017  0.07869896 0.1183511  0.12498326 0.08026649 0.12897474
 0.12486672 0.08163371 0.09811825 0.03748561 0.05711156 0.08221164
 0.0788238  0.0727149  0.09282982 0.15358305 0.1489373  0.12398369
 0.13925798 0.16382664 0.18479961 0.22751139 0.28440732 0.2534448
 0.15456659 0.05439445 0.09080984 0.18494861 0.40996465 0.6010428
 0.76583457 0.99559075 1.296662   1.366064   1.1162826  0.9140812
 0.6333529  0.47415885 0.37414223 0.33936888 0.29618093 0.17723161
 0.15173484 0.1137141  0.17470013 0.15707807 0.08014679 0.2568121
 0.04748778 0.04881592 0.06213704 0.13998799 0.10907281 0.09365671
 0.14453703 0.03809211 0.08852052 0.0961355  0.06060153 0.11277282
 0.12261487 0.08657358 0.07999397 0.07325217 0.05542033 0.03277366
 0.06170521 0.0509522  0.14442757 0.04986313 0.09805196 0.12910135
 0.08187225 0.03186781 0.06158867 0.06648293 0.04855096 0.04003943
 0.03360055 0.01558459 0.08817179 0.11629863 0.1370882  0.19437858
 0.2628613  0.33045942 0.27035156 0.16655953 0.14393315 0.17270625
 0.1718664  0.10277883 0.15155478 0.3276403  0.44430974 0.58164227
 0.7982559  0.97456735 0.63841033 0.5727421 ]
