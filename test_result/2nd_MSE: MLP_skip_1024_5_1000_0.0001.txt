time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 102.86%, model saved.
Epoch: 0 Train: 3769.63354 Test: 4329.21582
Epoch 100: New minimal relative error: 32.05%, model saved.
Epoch: 100 Train: 173.23267 Test: 39.83370
Epoch 200: New minimal relative error: 27.81%, model saved.
Epoch: 200 Train: 37.03314 Test: 11.80845
Epoch 300: New minimal relative error: 21.94%, model saved.
Epoch: 300 Train: 11.50624 Test: 16.47191
Epoch 400: New minimal relative error: 16.21%, model saved.
Epoch: 400 Train: 6.48959 Test: 10.70221
Epoch: 500 Train: 14.23464 Test: 16.29481
Epoch: 600 Train: 11.85266 Test: 13.85526
Epoch 700: New minimal relative error: 15.77%, model saved.
Epoch: 700 Train: 6.93250 Test: 6.46053
Epoch 800: New minimal relative error: 11.32%, model saved.
Epoch: 800 Train: 10.60489 Test: 10.41547
Epoch 900: New minimal relative error: 8.49%, model saved.
Epoch: 900 Train: 1.65375 Test: 1.73960
Epoch: 1000 Train: 8.11506 Test: 12.16727
Epoch: 1100 Train: 21.04111 Test: 26.95150
Epoch: 1200 Train: 1.07670 Test: 1.97312
Epoch: 1300 Train: 1.18229 Test: 4.06976
Epoch: 1400 Train: 6.91344 Test: 7.87829
Epoch: 1500 Train: 0.49124 Test: 0.78246
Epoch: 1600 Train: 1.22708 Test: 1.18322
Epoch: 1700 Train: 1.13370 Test: 0.91063
Epoch: 1800 Train: 1.84876 Test: 0.98913
Epoch: 1900 Train: 2.70503 Test: 3.21100
Epoch: 2000 Train: 8.35023 Test: 6.84426
Epoch: 2100 Train: 0.86459 Test: 0.45961
Epoch: 2200 Train: 2.83372 Test: 2.09362
Epoch: 2300 Train: 2.89312 Test: 3.50014
Epoch: 2400 Train: 0.18499 Test: 0.36013
Epoch: 2500 Train: 0.57094 Test: 0.71799
Epoch: 2600 Train: 0.36297 Test: 0.36069
Epoch: 2700 Train: 0.28042 Test: 0.16895
Epoch 2800: New minimal relative error: 7.38%, model saved.
Epoch: 2800 Train: 1.18527 Test: 1.00670
Epoch: 2900 Train: 3.32297 Test: 5.85400
Epoch: 3000 Train: 1.59875 Test: 2.10838
Epoch: 3100 Train: 2.29633 Test: 2.53297
Epoch: 3200 Train: 1.30816 Test: 1.37055
Epoch: 3300 Train: 0.58165 Test: 0.46897
Epoch: 3400 Train: 3.89210 Test: 2.03335
Epoch: 3500 Train: 0.41329 Test: 0.26492
Epoch: 3600 Train: 1.23026 Test: 1.38053
Epoch: 3700 Train: 1.06539 Test: 0.60585
Epoch: 3800 Train: 0.53346 Test: 0.66251
Epoch: 3900 Train: 0.37472 Test: 1.16224
Epoch: 4000 Train: 2.82833 Test: 2.50938
Epoch: 4100 Train: 1.93644 Test: 2.31847
Epoch 4200: New minimal relative error: 5.11%, model saved.
Epoch: 4200 Train: 0.23098 Test: 0.12647
Epoch: 4300 Train: 0.53703 Test: 0.44273
Epoch: 4400 Train: 0.80221 Test: 1.14257
Epoch: 4500 Train: 0.07933 Test: 0.18695
Epoch: 4600 Train: 1.06555 Test: 1.10149
Epoch: 4700 Train: 0.17200 Test: 0.15348
Epoch: 4800 Train: 0.27648 Test: 0.26570
Epoch: 4900 Train: 0.82978 Test: 1.01643
Epoch: 5000 Train: 0.10029 Test: 0.10316
Epoch: 5100 Train: 0.50471 Test: 0.73912
Epoch: 5200 Train: 0.09020 Test: 0.09752
Epoch: 5300 Train: 0.18248 Test: 0.20745
Epoch: 5400 Train: 1.06005 Test: 1.32895
Epoch: 5500 Train: 0.09135 Test: 0.09358
Epoch: 5600 Train: 0.05965 Test: 0.07362
Epoch: 5700 Train: 0.71221 Test: 0.86369
Epoch: 5800 Train: 0.04065 Test: 0.05246
Epoch: 5900 Train: 0.55785 Test: 0.78315
Epoch: 6000 Train: 0.14412 Test: 0.15075
Epoch: 6100 Train: 0.53329 Test: 0.76386
Epoch: 6200 Train: 0.30622 Test: 0.42962
Epoch: 6300 Train: 0.40649 Test: 0.49144
Epoch: 6400 Train: 0.75154 Test: 0.81539
Epoch: 6500 Train: 0.06812 Test: 0.09883
Epoch: 6600 Train: 0.17066 Test: 0.22344
Epoch: 6700 Train: 0.87220 Test: 1.15924
Epoch: 6800 Train: 0.05411 Test: 0.07019
Epoch: 6900 Train: 0.04589 Test: 0.06353
Epoch: 7000 Train: 0.35154 Test: 0.24219
Epoch 7100: New minimal relative error: 4.29%, model saved.
Epoch: 7100 Train: 0.03865 Test: 0.04866
Epoch: 7200 Train: 0.20235 Test: 0.27908
Epoch: 7300 Train: 0.15332 Test: 0.19213
Epoch: 7400 Train: 0.04215 Test: 0.04467
Epoch: 7500 Train: 0.04572 Test: 0.05668
Epoch 7600: New minimal relative error: 3.97%, model saved.
Epoch: 7600 Train: 0.08392 Test: 0.06858
Epoch: 7700 Train: 0.24629 Test: 0.34064
Epoch: 7800 Train: 0.11340 Test: 0.11251
Epoch: 7900 Train: 0.11932 Test: 0.09287
Epoch: 8000 Train: 1.39094 Test: 0.97088
Epoch: 8100 Train: 0.03002 Test: 0.04152
Epoch: 8200 Train: 0.02202 Test: 0.03096
Epoch: 8300 Train: 0.38079 Test: 0.52315
Epoch: 8400 Train: 0.15155 Test: 0.14111
Epoch: 8500 Train: 0.03999 Test: 0.04577
Epoch: 8600 Train: 0.16960 Test: 0.19387
Epoch: 8700 Train: 0.03610 Test: 0.04007
Epoch: 8800 Train: 0.09596 Test: 0.10277
Epoch: 8900 Train: 0.06950 Test: 0.08030
Epoch: 9000 Train: 0.06205 Test: 0.05692
Epoch: 9100 Train: 0.12980 Test: 0.12092
Epoch: 9200 Train: 0.09680 Test: 0.12187
Epoch: 9300 Train: 0.02759 Test: 0.03401
Epoch 9400: New minimal relative error: 1.94%, model saved.
Epoch: 9400 Train: 0.42424 Test: 0.38725
Epoch: 9500 Train: 0.05377 Test: 0.06680
Epoch: 9600 Train: 0.33497 Test: 0.30438
Epoch: 9700 Train: 0.02836 Test: 0.04120
Epoch: 9800 Train: 0.12084 Test: 0.16682
Epoch: 9900 Train: 0.02815 Test: 0.03472
Epoch: 9999 Train: 0.03906 Test: 0.05631
Training Loss: tensor(0.0391)
Test Loss: tensor(0.0563)
Learned LE: [ 0.8454394  -0.06236183 -4.2139134 ]
True LE: [ 8.5515732e-01  2.6622473e-04 -1.4540108e+01]
Relative Error: [2.2584999  2.1937025  2.2746575  2.4751685  2.812593   3.038978
 3.2061079  3.176364   3.0111787  2.8509893  2.7821937  2.785966
 2.8176875  2.7820106  2.711856   2.4688768  2.2996507  2.1465502
 1.8561046  1.52098    1.246157   1.5234183  1.6019175  1.615116
 1.9196566  2.1147869  1.5884277  1.4805034  1.2777666  1.396783
 0.76290834 2.313646   3.6336374  4.8507953  6.0608506  6.81675
 7.5655727  6.4440713  4.885409   3.776249   2.9616354  2.5535572
 2.4855173  2.232414   1.4900626  1.0340437  0.7445454  0.4788441
 1.0416741  1.4376739  1.4318838  1.4615235  2.0252674  2.0401185
 2.1119647  1.9438988  2.0020473  2.3942757  2.2766075  2.1395814
 1.9892613  2.0544991  1.9672217  1.8778285  1.8255002  1.9630117
 2.2081625  2.395353   2.547591   2.6329386  2.5131562  2.326354
 2.2609558  2.2937956  2.313257   2.3103824  2.2616942  1.9968252
 1.6132511  1.9184026  1.8971184  1.5668105  1.0280658  1.0682517
 1.377329   1.3045393  1.667851   1.8035805  1.6240332  1.3276427
 1.4211376  1.6340647  1.1498494  2.0146847  3.1660035  4.5770884
 5.6666865  6.4899435  6.851816   6.784319   5.191106   3.8704727
 3.0805314  2.4826047  2.194399   1.9723678  1.4860268  0.82846916
 0.5005708  0.2018493  0.595557   1.1584313  1.3599019  1.3620917
 1.5924852  1.9411156  2.0059998  1.8156006  1.6581414  1.7815087
 2.0556111  1.808144   1.7693768  1.810468   1.7232513  1.6040083
 1.5615456  1.5666889  1.7917815  1.9372797  2.0200691  2.1001625
 2.099287   1.9801697  1.8574791  1.9010963  1.965736   1.9041634
 1.8923072  1.739933   1.2585995  1.4982303  1.823834   1.7135192
 1.2418338  0.66226804 1.150036   1.2315291  1.3363824  1.5365455
 1.5626291  1.1836393  1.5947404  1.4856187  1.3767323  1.3387859
 2.4572837  3.7711396  4.77045    5.3362927  5.1778226  5.1342444
 5.271662   3.819238   2.9707096  2.3616192  1.6279045  1.4574647
 1.0910047  0.79823136 0.24293505 0.10992101 0.4574664  0.8920758
 1.3264534  1.4307936  1.4258076  1.7773958  1.9558635  1.7742326
 1.5345249  1.4391497  1.7077968  1.6242061  1.5693752  1.6205363
 1.6746618  1.4752395  1.3549494  1.3490653  1.4227188  1.629129
 1.7703536  1.7971864  1.7058966  1.6875134  1.5813181  1.5337054
 1.6139202  1.616152   1.6603755  1.570849   1.3456243  1.2659168
 1.5068246  1.7922474  1.5689137  0.95506626 0.8303099  1.1926081
 1.2388686  1.2209477  1.2385689  1.355263   1.6317527  1.282967
 1.3981988  1.090419   1.4770366  2.2075658  2.6550286  2.7905014
 2.5617492  2.0754087  1.6750312  1.8372955  2.202671   1.3703823
 0.9306719  0.56523937 0.48838872 0.39862064 0.22143668 0.34001327
 0.7350259  0.9718089  1.2476206  1.5562077  1.611291   1.5443178
 1.9537196  1.9824218  1.7350442  1.485677   1.5333734  1.7816318
 1.4841081  1.6083016  1.6943557  1.5699439  1.3631146  1.287079
 1.2092247  1.4061669  1.4939067  1.531135   1.5297128  1.3815701
 1.3192621  1.255924   1.2106932  1.3622081  1.3985878  1.4211771
 1.3674593  1.0077769  1.3152103  1.5629413  1.6741998  1.4599566
 0.8677275  0.8969717  1.2366478  1.2670652  1.0387713  1.1156609
 1.3527195  1.2324924  1.3112377  1.0618361  1.2562273  0.835903
 0.7609494  0.64371973 0.6881723  0.8105129  0.9633253  1.377811
 1.4060458  0.70883304 0.721446   0.84637207 0.9884773  1.2286829
 1.4612387  1.2432846  1.2000941  1.4125686  1.5992906  1.6053137
 1.9078261  1.8360589  1.6542922  2.1242774  2.0796244  1.833559
 1.6023967  1.702264   1.7632802  1.5767416  1.6856809  1.6138675
 1.5451448  1.3076509  1.2233804  1.2022882  1.3044777  1.2867016
 1.3251953  1.3037833  1.2075216  1.19873    1.1660191  1.0874612
 1.1070913  1.1252083  1.1701007  1.1259596  0.8740735  1.2329243
 1.4055291  1.4979258  1.3600135  1.0327724  0.98678106 1.259983
 1.3340126  0.9004088  1.1256368  1.2126278  1.0292207  1.1595671
 0.7473665  0.77964664 0.80909365 0.77887785 0.7697126  1.1154501
 1.5390115  1.3437439  1.5087907  1.7015733  1.562468   1.3131429
 1.6144806  1.5322596  1.6885734  1.778737   1.8542625  1.675111
 1.7573303  2.0355296  1.6741763  2.0618973  2.1398475  1.6162564
 2.0047846  1.9625435  1.9010189  1.6558654  1.787597   1.7316844
 1.5554698  1.5695224  1.4924192  1.4984417  1.3721751  1.22045
 1.1012046  1.1747353  1.1032481  1.0771924  1.1562214  1.1519816
 1.1222377  1.056086   0.99053746 0.89193714 1.0307384  1.0334607
 1.06864    0.8761394  1.2329713  1.3867178  1.3498163  1.3711863
 1.0466608  1.095417   1.2407655  1.5354316 ]
