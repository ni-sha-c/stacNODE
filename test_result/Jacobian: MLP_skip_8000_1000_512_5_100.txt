time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.75%, model saved.
Epoch: 0 Train: 9753.96973 Test: 4162.13770
Epoch: 80 Train: 2679.23877 Test: 1101.04541
Epoch: 160 Train: 2289.44165 Test: 827.50232
Epoch: 240 Train: 1107.97485 Test: 279.12244
Epoch: 320 Train: 710.37262 Test: 162.08031
Epoch: 400 Train: 291.65326 Test: 90.94930
Epoch 480: New minimal relative error: 43.06%, model saved.
Epoch: 480 Train: 231.52435 Test: 135.87402
Epoch 560: New minimal relative error: 17.68%, model saved.
Epoch: 560 Train: 94.58292 Test: 15.83060
Epoch 640: New minimal relative error: 11.65%, model saved.
Epoch: 640 Train: 74.47588 Test: 8.63488
Epoch: 720 Train: 65.14143 Test: 4.93059
Epoch: 800 Train: 64.63026 Test: 4.41142
Epoch: 880 Train: 41.22677 Test: 4.51618
Epoch 960: New minimal relative error: 5.65%, model saved.
Epoch: 960 Train: 48.74093 Test: 3.12904
Epoch: 1040 Train: 39.11836 Test: 1.55547
Epoch 1120: New minimal relative error: 5.24%, model saved.
Epoch: 1120 Train: 31.80550 Test: 3.78921
Epoch: 1200 Train: 31.35278 Test: 6.26104
Epoch: 1280 Train: 31.35581 Test: 3.73965
Epoch: 1360 Train: 26.21319 Test: 3.05032
Epoch: 1440 Train: 22.77769 Test: 0.71141
Epoch: 1520 Train: 23.72581 Test: 2.57490
Epoch: 1600 Train: 20.97831 Test: 0.98145
Epoch: 1680 Train: 31.36681 Test: 8.83484
Epoch: 1760 Train: 23.48836 Test: 4.56419
Epoch: 1840 Train: 22.15154 Test: 3.66724
Epoch: 1920 Train: 18.15620 Test: 0.92323
Epoch: 2000 Train: 23.80365 Test: 5.46753
Epoch: 2080 Train: 17.26213 Test: 2.03845
Epoch: 2160 Train: 17.74651 Test: 1.47455
Epoch 2240: New minimal relative error: 5.07%, model saved.
Epoch: 2240 Train: 17.34733 Test: 1.44922
Epoch: 2320 Train: 18.28644 Test: 2.12943
Epoch: 2400 Train: 17.89961 Test: 1.69088
Epoch: 2480 Train: 15.36041 Test: 0.79269
Epoch: 2560 Train: 15.08545 Test: 2.17938
Epoch 2640: New minimal relative error: 4.74%, model saved.
Epoch: 2640 Train: 14.11484 Test: 2.45691
Epoch: 2720 Train: 14.05485 Test: 1.81324
Epoch: 2800 Train: 13.52261 Test: 1.48733
Epoch: 2880 Train: 13.07822 Test: 0.80447
Epoch: 2960 Train: 12.70252 Test: 0.83837
Epoch: 3040 Train: 14.91239 Test: 3.28365
Epoch 3120: New minimal relative error: 3.64%, model saved.
Epoch: 3120 Train: 10.59946 Test: 0.20784
Epoch: 3200 Train: 12.18616 Test: 0.85597
Epoch: 3280 Train: 11.35582 Test: 1.36137
Epoch: 3360 Train: 12.10366 Test: 1.86288
Epoch: 3440 Train: 10.88800 Test: 0.88136
Epoch: 3520 Train: 10.53493 Test: 1.86488
Epoch: 3600 Train: 10.54964 Test: 1.03059
Epoch: 3680 Train: 10.21379 Test: 1.09794
Epoch: 3760 Train: 8.61588 Test: 0.18154
Epoch: 3840 Train: 9.87037 Test: 1.03333
Epoch: 3920 Train: 8.68141 Test: 0.18297
Epoch: 4000 Train: 9.31986 Test: 0.92438
Epoch: 4080 Train: 8.19220 Test: 0.10267
Epoch: 4160 Train: 8.34429 Test: 0.11231
Epoch: 4240 Train: 8.73573 Test: 0.36655
Epoch: 4320 Train: 11.44056 Test: 4.29086
Epoch: 4400 Train: 7.89249 Test: 0.09427
Epoch: 4480 Train: 9.32144 Test: 0.37424
Epoch: 4560 Train: 7.74622 Test: 0.10222
Epoch: 4640 Train: 7.79011 Test: 0.17320
Epoch: 4720 Train: 7.61925 Test: 0.09768
Epoch: 4800 Train: 7.64565 Test: 0.12918
Epoch: 4880 Train: 9.64782 Test: 0.43194
Epoch: 4960 Train: 7.58190 Test: 0.33403
Epoch: 5040 Train: 7.92469 Test: 0.31956
Epoch 5120: New minimal relative error: 3.60%, model saved.
Epoch: 5120 Train: 7.91514 Test: 0.32434
Epoch 5200: New minimal relative error: 3.59%, model saved.
Epoch: 5200 Train: 8.32450 Test: 0.70179
Epoch: 5280 Train: 7.21169 Test: 0.15864
Epoch: 5360 Train: 6.86742 Test: 0.08398
Epoch: 5440 Train: 7.66972 Test: 0.17483
Epoch: 5520 Train: 9.60602 Test: 1.82465
Epoch: 5600 Train: 10.42680 Test: 2.70551
Epoch: 5680 Train: 6.59197 Test: 0.08398
Epoch: 5760 Train: 6.67727 Test: 0.10440
Epoch: 5840 Train: 7.12732 Test: 0.33757
Epoch: 5920 Train: 6.63674 Test: 0.08337
Epoch: 6000 Train: 7.70603 Test: 0.40933
Epoch: 6080 Train: 6.84902 Test: 0.08781
Epoch: 6160 Train: 6.76356 Test: 0.08592
Epoch: 6240 Train: 6.62126 Test: 0.09290
Epoch: 6320 Train: 6.56492 Test: 0.41545
Epoch: 6400 Train: 6.44045 Test: 0.08676
Epoch: 6480 Train: 6.43380 Test: 0.11248
Epoch: 6560 Train: 8.69829 Test: 1.36609
Epoch: 6640 Train: 6.22145 Test: 0.09648
Epoch: 6720 Train: 6.22087 Test: 0.10043
Epoch: 6800 Train: 6.28026 Test: 0.08251
Epoch: 6880 Train: 6.44504 Test: 0.42756
Epoch: 6960 Train: 6.61728 Test: 0.27573
Epoch: 7040 Train: 6.39223 Test: 0.20599
Epoch: 7120 Train: 6.21353 Test: 0.14403
Epoch: 7200 Train: 6.22400 Test: 0.21986
Epoch: 7280 Train: 6.67378 Test: 0.84259
Epoch: 7360 Train: 5.94010 Test: 0.08412
Epoch: 7440 Train: 10.50532 Test: 3.50048
Epoch: 7520 Train: 6.17821 Test: 0.07110
Epoch: 7600 Train: 5.91008 Test: 0.07353
Epoch: 7680 Train: 6.02493 Test: 0.21674
Epoch 7760: New minimal relative error: 2.87%, model saved.
Epoch: 7760 Train: 5.94328 Test: 0.10083
Epoch: 7840 Train: 5.92426 Test: 0.07561
Epoch: 7920 Train: 6.05935 Test: 0.16982
Epoch 7999: New minimal relative error: 2.54%, model saved.
Epoch: 7999 Train: 5.88412 Test: 0.06944
Training Loss: tensor(5.8841)
Test Loss: tensor(0.0694)
Learned LE: [ 8.5944223e-01 -4.0542395e-03 -1.4521532e+01]
True LE: [ 8.6082947e-01  3.3862835e-03 -1.4547920e+01]
Relative Error: [1.5278735  2.1001348  2.4742842  2.5672252  2.384316   2.0356805
 1.6424096  1.403502   1.6766405  1.9685844  2.3349164  2.7769425
 3.0648928  3.2255     2.5727487  1.82456    1.3428364  0.9480985
 0.6847583  0.34501588 0.7030976  1.4139907  1.8104875  1.8488401
 1.7702738  1.3987892  0.84313804 0.35550013 0.7858076  1.0784131
 1.7741243  2.1892693  2.2812898  2.605704   2.5984163  1.7701466
 1.3860419  0.9482546  0.4206108  0.55895954 0.9690967  1.4029969
 1.6062607  1.4557734  1.2574505  1.0158036  0.98474324 0.93344206
 1.071759   1.1483846  1.3590915  1.3808299  1.487762   1.6271789
 1.9115586  1.9869361  1.718029   1.1075056  0.46290243 0.16786237
 0.63017493 0.93202496 1.2128215  1.6244141  2.0870202  2.2241788
 2.3658636  2.1529176  1.902719   1.3752306  1.2626941  1.4902343
 1.7645361  2.3955777  2.7268958  2.957799   2.7768354  2.0382586
 1.4236683  1.0103071  0.7116059  0.62888104 0.5747537  1.0880079
 1.702001   1.8454006  1.7198927  1.4700116  0.9733389  0.28699014
 0.45242125 0.89149547 1.3340759  2.2334301  2.2837842  2.2100496
 2.2586396  2.0373697  1.4304365  0.9725729  0.36256108 0.6169505
 0.79354626 1.2130716  1.4893322  1.5345567  1.3576181  1.0362542
 0.78943557 0.74418247 0.8765726  0.9838563  1.4076409  1.5572089
 1.645978   1.733273   1.8861599  2.0363908  1.9127971  1.3376889
 0.6974036  0.20369303 0.2680112  0.5223243  1.0340097  1.3205566
 1.5120916  1.8784506  2.118285   2.1911485  2.0153646  1.6292797
 1.1149763  1.1737458  1.295659   1.7090884  2.0437138  2.4927526
 2.754829   2.3978488  1.6708198  1.0935991  0.81424236 0.7231625
 0.56737626 0.7029933  1.1480982  1.6709834  1.6909676  1.465105
 1.0823498  0.5269405  0.32657477 0.63482237 1.1076877  1.6383376
 2.3678188  2.1740518  2.0691166  1.9350866  1.8648995  1.292202
 0.4518484  0.51048195 0.91567177 1.1641214  1.123503   1.3263679
 1.2609832  0.9990459  0.65426207 0.5263383  0.63958716 0.81773216
 1.2381037  1.5886922  1.6464723  1.7439696  1.8740734  2.0615454
 2.0265632  1.593505   0.9786496  0.36765906 0.18792309 0.18475841
 0.4387176  0.95957965 1.2442472  1.4036914  1.4866017  1.792661
 1.9000702  1.8751721  1.4752402  1.0613637  1.1124535  1.2472084
 1.3929052  1.6064683  2.0260096  2.420349   2.1106105  1.4889853
 0.9507676  0.70924175 0.78694487 0.52962315 0.80793333 1.1022781
 1.4863782  1.4183642  1.0840746  0.6796296  0.29853153 0.5661593
 0.6370818  0.7583845  1.1369898  1.8299527  1.4485179  1.413601
 1.4989282  1.829237   1.2479193  0.23571473 0.6383775  1.1614264
 1.3175102  1.0594124  0.995629   0.99483496 0.6969237  0.41299406
 0.29170966 0.420446   0.93445826 1.3973994  1.6598053  1.7187381
 1.7060214  1.9441645  1.9651271  1.8339074  1.375062   0.7914434
 0.21722554 0.12800598 0.1700539  0.25100228 0.6125641  0.9374677
 1.0666825  1.1195159  1.1637306  1.3985832  1.5817337  1.3156068
 1.0773249  1.1004424  1.1675475  1.1509596  1.2512478  1.462497
 1.9606405  1.794015   1.5126449  0.8447961  0.5646014  0.6574343
 0.47301632 0.78424764 1.133762   1.379546   1.2198957  0.76407653
 0.42551526 0.38822946 0.26511538 0.39280638 0.3199432  0.697063
 1.2385342  0.97893065 0.9207407  0.9790739  1.3977646  1.4433668
 0.28864205 0.50988287 1.1211061  1.296589   1.0669259  0.7225836
 0.81137055 0.5276058  0.29508796 0.27566937 0.4483548  0.8849902
 1.3527977  1.4997262  1.5624611  1.5792185  1.7525828  1.8102576
 1.6030533  1.3410231  0.7887507  0.2581883  0.15804355 0.14819174
 0.23877104 0.3169453  0.5370524  0.6054328  0.6580676  0.64494854
 0.6122283  1.0132391  0.9628742  0.8224795  1.0588369  1.1276711
 1.1019764  0.95702857 0.9351911  1.2517668  1.4959363  1.3279235
 0.9591868  0.54504335 0.5354521  0.48200843 0.6334154  1.0253867
 1.1925845  1.1716717  0.72408986 0.5545566  0.37301743 0.39484194
 0.21837462 0.34296024 0.25210696 0.6980143  0.8558811  0.6803754
 0.7667257  0.98463196 1.2629801  0.6071472  0.21306157 0.7951455
 1.2166494  1.1084809  0.74396837 0.46439385 0.39537895 0.38905832
 0.31420258 0.36646792 0.6513692  1.0463185  1.2299341  1.3417448
 1.3418413  1.5619205  1.573367   1.4053314  1.1247193  0.59999007
 0.37714195 0.14520647 0.34864768 0.30334058 0.1710128  0.28263366
 0.38501993 0.34780094 0.27197585 0.3344957  0.5045281  0.5122486
 0.20489714 0.5754862  1.019823   1.0470408  0.95489675 0.75533545
 0.6117913  1.0076094  1.0449637  0.88720393 0.6751189  0.48002717
 0.61737853 0.30578867 0.7684075  0.99288803]
