time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 20.102523804 Test: 16.930412292
Epoch 0: New minimal relative error: 16.93%, model saved.
Epoch: 100 Train: 5.297562599 Test: 5.371820927
Epoch 100: New minimal relative error: 5.37%, model saved.
Epoch: 200 Train: 5.114443302 Test: 5.207235336
Epoch 200: New minimal relative error: 5.21%, model saved.
Epoch: 300 Train: 5.161662102 Test: 5.221676350
Epoch: 400 Train: 5.149228096 Test: 5.219140053
Epoch: 500 Train: 5.227600098 Test: 5.281183243
Epoch: 600 Train: 5.202477455 Test: 5.291794300
Epoch: 700 Train: 5.106520653 Test: 5.198142052
Epoch 700: New minimal relative error: 5.20%, model saved.
Epoch: 800 Train: 5.114620686 Test: 5.183716774
Epoch 800: New minimal relative error: 5.18%, model saved.
Epoch: 900 Train: 5.202775478 Test: 5.314475060
Epoch: 1000 Train: 5.175034046 Test: 5.268625259
Epoch: 1100 Train: 5.218708038 Test: 5.294791222
Epoch: 1200 Train: 5.188274384 Test: 5.284573555
Epoch: 1300 Train: 5.214497566 Test: 5.277518272
Epoch: 1400 Train: 5.235895157 Test: 5.302662849
Epoch: 1500 Train: 5.183352470 Test: 5.268146515
Epoch: 1600 Train: 5.246762276 Test: 5.309162140
Epoch: 1700 Train: 5.179455280 Test: 5.281976223
Epoch: 1800 Train: 5.168591022 Test: 5.281247139
Epoch: 1900 Train: 5.149735451 Test: 5.252457619
Epoch: 2000 Train: 5.157299042 Test: 5.276671410
Epoch: 2100 Train: 5.181564331 Test: 5.292087078
Epoch: 2200 Train: 5.179757595 Test: 5.295545578
Epoch: 2300 Train: 5.196950912 Test: 5.303982258
Epoch: 2400 Train: 5.207804680 Test: 5.309633255
Epoch: 2500 Train: 5.210079670 Test: 5.308475494
Epoch: 2600 Train: 5.202068329 Test: 5.311755180
Epoch: 2700 Train: 5.180840492 Test: 5.305262089
Epoch: 2800 Train: 5.126495361 Test: 5.280573845
Epoch: 2900 Train: 5.191580772 Test: 5.350906372
Epoch: 3000 Train: 5.203008652 Test: 5.337556362
Epoch: 3100 Train: 5.176765442 Test: 5.331499100
Epoch: 3200 Train: 5.201750755 Test: 5.363132954
Epoch: 3300 Train: 5.180814743 Test: 5.302176476
Epoch: 3400 Train: 5.176149368 Test: 5.296154499
Epoch: 3500 Train: 5.224077225 Test: 5.347051620
Epoch: 3600 Train: 5.206812859 Test: 5.329874992
Epoch: 3700 Train: 5.226644516 Test: 5.339550972
Epoch: 3800 Train: 5.253108978 Test: 5.356200218
Epoch: 3900 Train: 5.232948303 Test: 5.334424019
Epoch: 4000 Train: 5.218277931 Test: 5.323904037
Epoch: 4100 Train: 5.216721535 Test: 5.327788353
Epoch: 4200 Train: 5.222124577 Test: 5.328008652
Epoch: 4300 Train: 5.218579292 Test: 5.327346802
Epoch: 4400 Train: 5.222058773 Test: 5.331364632
Epoch: 4500 Train: 5.222458839 Test: 5.337432861
Epoch: 4600 Train: 5.220612526 Test: 5.342142582
Epoch: 4700 Train: 5.219099998 Test: 5.335114479
Epoch: 4800 Train: 5.219136238 Test: 5.336050034
Epoch: 4900 Train: 5.217748165 Test: 5.338578224
Epoch: 5000 Train: 5.224912167 Test: 5.346205235
Epoch: 5100 Train: 5.226209164 Test: 5.350573540
Epoch: 5200 Train: 5.228390694 Test: 5.356146336
Epoch: 5300 Train: 5.234708786 Test: 5.360719681
Epoch: 5400 Train: 5.241205215 Test: 5.360613823
Epoch: 5500 Train: 5.249749660 Test: 5.361017227
Epoch: 5600 Train: 5.236052036 Test: 5.349379063
Epoch: 5700 Train: 5.237660408 Test: 5.345240593
Epoch: 5800 Train: 5.239447594 Test: 5.349185944
Epoch: 5900 Train: 5.238189697 Test: 5.348710537
Epoch: 6000 Train: 5.237757683 Test: 5.353611946
Epoch: 6100 Train: 5.242938995 Test: 5.358949661
Epoch: 6200 Train: 5.243638992 Test: 5.362595558
Epoch: 6300 Train: 5.245792389 Test: 5.376252174
Epoch: 6400 Train: 5.251435280 Test: 5.381378174
Epoch: 6500 Train: 5.256795883 Test: 5.386090279
Epoch: 6600 Train: 5.254501343 Test: 5.382372856
Epoch: 6700 Train: 5.258894920 Test: 5.383150101
Epoch: 6800 Train: 5.262232780 Test: 5.386052132
Epoch: 6900 Train: 5.265444756 Test: 5.381071091
Epoch: 7000 Train: 5.263209343 Test: 5.382616997
Epoch: 7100 Train: 5.258916378 Test: 5.378747940
Epoch: 7200 Train: 5.256338120 Test: 5.383360386
Epoch: 7300 Train: 5.251519203 Test: 5.379532337
Epoch: 7400 Train: 5.247821808 Test: 5.376068115
Epoch: 7500 Train: 5.246922970 Test: 5.372028828
Epoch: 7600 Train: 5.244525433 Test: 5.375501633
Epoch: 7700 Train: 5.244778633 Test: 5.378315449
Epoch: 7800 Train: 5.244354248 Test: 5.377926826
Epoch: 7900 Train: 5.244832516 Test: 5.372943878
Epoch: 8000 Train: 5.246129990 Test: 5.375862598
Epoch: 8100 Train: 5.247175694 Test: 5.376838207
Epoch: 8200 Train: 5.247532845 Test: 5.380206108
Epoch: 8300 Train: 5.246140480 Test: 5.380263329
Epoch: 8400 Train: 5.246494770 Test: 5.385356426
Epoch: 8500 Train: 5.249244690 Test: 5.381751060
Epoch: 8600 Train: 5.247500420 Test: 5.385190010
Epoch: 8700 Train: 5.248546600 Test: 5.382312775
Epoch: 8800 Train: 5.247591019 Test: 5.384848118
Epoch: 8900 Train: 5.247185707 Test: 5.385931015
Epoch: 9000 Train: 5.248736858 Test: 5.386659622
Epoch: 9100 Train: 5.248076439 Test: 5.389677048
Epoch: 9200 Train: 5.247535706 Test: 5.393504143
Epoch: 9300 Train: 5.245807648 Test: 5.386551380
Epoch: 9400 Train: 5.243328094 Test: 5.390541077
Epoch: 9500 Train: 5.246705055 Test: 5.395788193
Epoch: 9600 Train: 5.245369434 Test: 5.399823189
Epoch: 9700 Train: 5.242699623 Test: 5.399130821
Epoch: 9800 Train: 5.244206905 Test: 5.393677235
Epoch: 9900 Train: 5.246317863 Test: 5.394550323
Epoch: 9999 Train: 5.244950771 Test: 5.400409698
Training Loss: tensor(5.2450)
Test Loss: tensor(5.4004)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.3713, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0047, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0052)
Jacobian term Test Loss: tensor(0.0054)
Learned LE: [0.6522112 0.5770701]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
