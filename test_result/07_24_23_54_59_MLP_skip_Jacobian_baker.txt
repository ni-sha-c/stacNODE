time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 3000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 5
reg_param: 50.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 83.544563293 Test: 17.557683945
Epoch 0: New minimal relative error: 17.56%, model saved.
Epoch: 30 Train: 11.634592056 Test: 3.336086512
Epoch 30: New minimal relative error: 3.34%, model saved.
Epoch: 60 Train: 7.913743973 Test: 4.817137241
Epoch: 90 Train: 7.530745983 Test: 4.323691368
Epoch: 120 Train: 7.392122746 Test: 4.177427769
Epoch: 150 Train: 7.463565350 Test: 4.345419884
Epoch: 180 Train: 7.908824921 Test: 4.226560593
Epoch: 210 Train: 7.679575920 Test: 4.195911407
Epoch: 240 Train: 7.903046131 Test: 3.879117966
Epoch: 270 Train: 7.116868496 Test: 4.090706825
Epoch: 300 Train: 7.283071518 Test: 3.854743242
Epoch: 330 Train: 6.872371197 Test: 4.282883644
Epoch: 360 Train: 6.520153046 Test: 4.213635445
Epoch: 390 Train: 6.373167515 Test: 4.120104313
Epoch: 420 Train: 6.287302494 Test: 4.409405708
Epoch: 450 Train: 6.243197918 Test: 4.279951572
Epoch: 480 Train: 6.179646492 Test: 4.211541653
Epoch: 510 Train: 6.117149353 Test: 4.273309231
Epoch: 540 Train: 6.019241333 Test: 4.408633232
Epoch: 570 Train: 6.215134621 Test: 4.056792259
Epoch: 600 Train: 5.978926182 Test: 4.450111389
Epoch: 630 Train: 6.012586594 Test: 4.511097908
Epoch: 660 Train: 6.235502243 Test: 4.332083225
Epoch: 690 Train: 6.189311028 Test: 4.364814281
Epoch: 720 Train: 6.396204948 Test: 4.256299973
Epoch: 750 Train: 6.240667343 Test: 4.297037601
Epoch: 780 Train: 6.116346836 Test: 4.263975620
Epoch: 810 Train: 6.045858860 Test: 4.263436317
Epoch: 840 Train: 5.986364365 Test: 4.260751247
Epoch: 870 Train: 5.915621281 Test: 4.299904823
Epoch: 900 Train: 5.919018745 Test: 4.363298416
Epoch: 930 Train: 6.014063835 Test: 4.287714958
Epoch: 960 Train: 5.773624420 Test: 4.455160618
Epoch: 990 Train: 5.711805344 Test: 4.499928951
Epoch: 1020 Train: 5.648410797 Test: 4.539493561
Epoch: 1050 Train: 5.772078514 Test: 4.468226910
Epoch: 1080 Train: 5.710280895 Test: 4.451828480
Epoch: 1110 Train: 5.757188797 Test: 4.415313721
Epoch: 1140 Train: 5.962615013 Test: 4.349600792
Epoch: 1170 Train: 5.896107674 Test: 4.380151272
Epoch: 1200 Train: 5.892127991 Test: 4.425453663
Epoch: 1230 Train: 5.836160660 Test: 4.432738781
Epoch: 1260 Train: 5.861886978 Test: 4.449629784
Epoch: 1290 Train: 5.823600769 Test: 4.474888802
Epoch: 1320 Train: 5.802220345 Test: 4.507511616
Epoch: 1350 Train: 5.943243980 Test: 4.450210571
Epoch: 1380 Train: 5.825703621 Test: 4.560239792
Epoch: 1410 Train: 5.812089443 Test: 4.502877712
Epoch: 1440 Train: 5.810418606 Test: 4.487279892
Epoch: 1470 Train: 5.785544872 Test: 4.518669605
Epoch: 1500 Train: 5.740987778 Test: 4.537235737
Epoch: 1530 Train: 5.744626999 Test: 4.574043274
Epoch: 1560 Train: 5.707126617 Test: 4.574213028
Epoch: 1590 Train: 5.680346012 Test: 4.498844624
Epoch: 1620 Train: 5.765328407 Test: 4.536288261
Epoch: 1650 Train: 5.712411880 Test: 4.550427914
Epoch: 1680 Train: 5.719858170 Test: 4.615247250
Epoch: 1710 Train: 5.706137657 Test: 4.565155506
Epoch: 1740 Train: 5.706601143 Test: 4.573413849
Epoch: 1770 Train: 5.960925102 Test: 4.454674721
Epoch: 1800 Train: 5.864396572 Test: 4.572517872
Epoch: 1830 Train: 5.845919132 Test: 4.564651012
Epoch: 1860 Train: 5.830328465 Test: 4.573902130
Epoch: 1890 Train: 5.826071262 Test: 4.574821949
Epoch: 1920 Train: 5.808904648 Test: 4.577569962
Epoch: 1950 Train: 5.808030605 Test: 4.581004620
Epoch: 1980 Train: 5.811461449 Test: 4.581497669
Epoch: 2010 Train: 5.803855896 Test: 4.587220192
Epoch: 2040 Train: 5.805072784 Test: 4.593679428
Epoch: 2070 Train: 5.820469379 Test: 4.599767208
Epoch: 2100 Train: 5.833513737 Test: 4.611857891
Epoch: 2130 Train: 5.828116894 Test: 4.616511345
Epoch: 2160 Train: 5.826272964 Test: 4.624775410
Epoch: 2190 Train: 5.816979408 Test: 4.626715660
Epoch: 2220 Train: 5.799861908 Test: 4.627542019
Epoch: 2250 Train: 5.768950939 Test: 4.626258373
Epoch: 2280 Train: 5.739200115 Test: 4.624213219
Epoch: 2310 Train: 5.762383461 Test: 4.625670910
Epoch: 2340 Train: 5.780801296 Test: 4.622333050
Epoch: 2370 Train: 5.789378166 Test: 4.619931698
Epoch: 2400 Train: 5.792285919 Test: 4.614848614
Epoch: 2430 Train: 5.765325546 Test: 4.609755993
Epoch: 2460 Train: 5.744765282 Test: 4.604403496
Epoch: 2490 Train: 5.718610287 Test: 4.603237629
Epoch: 2520 Train: 5.707061291 Test: 4.596837997
Epoch: 2550 Train: 5.698939800 Test: 4.593935013
Epoch: 2580 Train: 5.702973843 Test: 4.593004227
Epoch: 2610 Train: 5.708860397 Test: 4.595862389
Epoch: 2640 Train: 5.703418732 Test: 4.594365597
Epoch: 2670 Train: 5.707094669 Test: 4.592075348
Epoch: 2700 Train: 5.705606461 Test: 4.591039181
Epoch: 2730 Train: 5.707300186 Test: 4.593384266
Epoch: 2760 Train: 5.713119507 Test: 4.595593929
Epoch: 2790 Train: 5.718661308 Test: 4.603235722
Epoch: 2820 Train: 5.719589710 Test: 4.601787567
Epoch: 2850 Train: 5.731181145 Test: 4.606262207
Epoch: 2880 Train: 5.735608101 Test: 4.613471508
Epoch: 2910 Train: 5.740850449 Test: 4.616120338
Epoch: 2940 Train: 5.743052959 Test: 4.624279022
Epoch: 2970 Train: 5.751227379 Test: 4.630012512
Epoch: 2999 Train: 5.762242317 Test: 4.632138729
Training Loss: tensor(5.7622)
Test Loss: tensor(4.6321)
True Mean x: tensor(3.4447, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.3416e+11, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.5065, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(1.4952e+24, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0234)
Jacobian term Test Loss: tensor(0.0006)
Learned LE: [1.3105828  0.44374338]
True LE: tensor([ 0.6932, -0.7446], dtype=torch.float64)
