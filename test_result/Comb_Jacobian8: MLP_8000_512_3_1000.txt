time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 98.89%, model saved.
Epoch: 0 Train: 59543.64453 Test: 4104.12061
Epoch: 80 Train: 7652.48633 Test: 832.66949
Epoch 160: New minimal relative error: 56.80%, model saved.
Epoch: 160 Train: 874.91583 Test: 39.77106
Epoch 240: New minimal relative error: 47.57%, model saved.
Epoch: 240 Train: 212.56900 Test: 45.99174
Epoch 320: New minimal relative error: 16.95%, model saved.
Epoch: 320 Train: 87.90183 Test: 6.35794
Epoch 400: New minimal relative error: 11.52%, model saved.
Epoch: 400 Train: 49.22861 Test: 1.14859
Epoch: 480 Train: 37.42584 Test: 3.09625
Epoch: 560 Train: 152.12714 Test: 45.04111
Epoch: 640 Train: 164.15872 Test: 73.54243
Epoch: 720 Train: 203.85727 Test: 109.22247
Epoch: 800 Train: 19.63704 Test: 3.04247
Epoch: 880 Train: 24.88554 Test: 6.27649
Epoch: 960 Train: 20.86044 Test: 7.24569
Epoch 1040: New minimal relative error: 9.77%, model saved.
Epoch: 1040 Train: 11.91030 Test: 2.27281
Epoch: 1120 Train: 32.37172 Test: 12.83219
Epoch: 1200 Train: 18.87004 Test: 7.62288
Epoch: 1280 Train: 29.02883 Test: 7.11517
Epoch: 1360 Train: 7.12391 Test: 0.61361
Epoch: 1440 Train: 18.43100 Test: 7.84068
Epoch 1520: New minimal relative error: 4.68%, model saved.
Epoch: 1520 Train: 8.26911 Test: 0.82688
Epoch: 1600 Train: 3.91154 Test: 0.10025
Epoch 1680: New minimal relative error: 2.55%, model saved.
Epoch: 1680 Train: 4.66269 Test: 0.33560
Epoch: 1760 Train: 14.30912 Test: 2.76437
Epoch: 1840 Train: 8.44258 Test: 2.13529
Epoch: 1920 Train: 6.88294 Test: 0.63526
Epoch: 2000 Train: 8.50336 Test: 3.85151
Epoch: 2080 Train: 11.82150 Test: 2.27356
Epoch 2160: New minimal relative error: 1.13%, model saved.
Epoch: 2160 Train: 3.64843 Test: 0.52406
Epoch: 2240 Train: 5.10591 Test: 1.19564
Epoch: 2320 Train: 4.06040 Test: 0.85464
Epoch: 2400 Train: 4.07430 Test: 0.29663
Epoch: 2480 Train: 2.29904 Test: 0.07041
Epoch: 2560 Train: 19.78600 Test: 6.25182
Epoch: 2640 Train: 27.87310 Test: 10.20138
Epoch: 2720 Train: 3.47332 Test: 0.29008
Epoch: 2800 Train: 3.77374 Test: 0.49986
Epoch: 2880 Train: 20.55805 Test: 12.04328
Epoch: 2960 Train: 5.13524 Test: 0.29463
Epoch: 3040 Train: 8.73478 Test: 3.02106
Epoch: 3120 Train: 2.67256 Test: 0.55055
Epoch: 3200 Train: 2.98432 Test: 0.51484
Epoch: 3280 Train: 5.81273 Test: 2.42909
Epoch: 3360 Train: 20.47718 Test: 8.44110
Epoch: 3440 Train: 1.60527 Test: 0.20804
Epoch: 3520 Train: 2.21501 Test: 0.25678
Epoch: 3600 Train: 1.41325 Test: 0.12065
Epoch: 3680 Train: 1.10204 Test: 0.03868
Epoch: 3760 Train: 1.34050 Test: 0.19417
Epoch: 3840 Train: 5.19240 Test: 1.98704
Epoch: 3920 Train: 3.06033 Test: 0.49416
Epoch 4000: New minimal relative error: 0.28%, model saved.
Epoch: 4000 Train: 0.99151 Test: 0.02308
Epoch: 4080 Train: 1.18374 Test: 0.15149
Epoch: 4160 Train: 1.01514 Test: 0.04802
Epoch: 4240 Train: 1.00701 Test: 0.05026
Epoch: 4320 Train: 1.06217 Test: 0.18930
Epoch: 4400 Train: 2.69974 Test: 0.86467
Epoch: 4480 Train: 4.77822 Test: 0.86991
Epoch: 4560 Train: 1.55010 Test: 0.38842
Epoch: 4640 Train: 1.62122 Test: 0.53561
Epoch: 4720 Train: 5.36219 Test: 0.99108
Epoch: 4800 Train: 0.77819 Test: 0.05270
Epoch: 4880 Train: 0.67637 Test: 0.02592
Epoch: 4960 Train: 0.72436 Test: 0.06261
Epoch: 5040 Train: 0.64398 Test: 0.04424
Epoch: 5120 Train: 0.61447 Test: 0.02772
Epoch: 5200 Train: 0.80498 Test: 0.03835
Epoch: 5280 Train: 0.75788 Test: 0.09280
Epoch: 5360 Train: 4.38719 Test: 1.40895
Epoch: 5440 Train: 5.61242 Test: 2.24752
Epoch: 5520 Train: 1.61860 Test: 0.23099
Epoch: 5600 Train: 1.53811 Test: 0.56192
Epoch: 5680 Train: 2.46203 Test: 0.79670
Epoch: 5760 Train: 0.78879 Test: 0.18771
Epoch: 5840 Train: 0.76766 Test: 0.06565
Epoch: 5920 Train: 0.51658 Test: 0.02816
Epoch: 6000 Train: 0.49681 Test: 0.02712
Epoch: 6080 Train: 0.92075 Test: 0.20906
Epoch: 6160 Train: 3.32674 Test: 1.58374
Epoch: 6240 Train: 1.74421 Test: 0.53562
Epoch: 6320 Train: 0.79646 Test: 0.18606
Epoch: 6400 Train: 0.38225 Test: 0.01670
Epoch: 6480 Train: 0.44770 Test: 0.02280
Epoch: 6560 Train: 0.37193 Test: 0.01786
Epoch: 6640 Train: 2.83695 Test: 0.32672
Epoch: 6720 Train: 0.34856 Test: 0.01602
Epoch 6800: New minimal relative error: 0.22%, model saved.
Epoch: 6800 Train: 0.39404 Test: 0.02971
Epoch: 6880 Train: 0.33257 Test: 0.01483
Epoch: 6960 Train: 0.50112 Test: 0.04797
Epoch: 7040 Train: 4.50255 Test: 2.07421
Epoch: 7120 Train: 0.31191 Test: 0.01510
Epoch: 7200 Train: 0.31658 Test: 0.01933
Epoch: 7280 Train: 0.37242 Test: 0.02221
Epoch: 7360 Train: 1.22266 Test: 0.32512
Epoch: 7440 Train: 2.02652 Test: 0.74353
Epoch: 7520 Train: 0.31000 Test: 0.03032
Epoch: 7600 Train: 0.33881 Test: 0.02854
Epoch: 7680 Train: 0.46830 Test: 0.09935
Epoch: 7760 Train: 0.42676 Test: 0.10705
Epoch: 7840 Train: 0.29426 Test: 0.02384
Epoch: 7920 Train: 2.03758 Test: 0.75005
Epoch: 7999 Train: 0.27398 Test: 0.02557
Training Loss: tensor(0.2740)
Test Loss: tensor(0.0256)
Learned LE: [  0.86137635   0.01924509 -14.556026  ]
True LE: [ 8.7581837e-01  1.0324543e-03 -1.4549361e+01]
Relative Error: [0.12712364 0.1277265  0.13058981 0.1351337  0.13945799 0.14118046
 0.13748783 0.12643808 0.10960038 0.09265786 0.08273469 0.08230261
 0.08595581 0.08681381 0.08799052 0.09716148 0.11449492 0.13128807
 0.14001954 0.14176603 0.14264527 0.14714852 0.15453124 0.16193408
 0.1642871  0.15861821 0.14521943 0.1269113  0.10732226 0.09015771
 0.07901299 0.0758692  0.07942595 0.08725515 0.09746875 0.10862514
 0.11871459 0.12870027 0.14533915 0.17421332 0.20042108 0.20657422
 0.20972887 0.21897419 0.21423437 0.19173221 0.16989942 0.15760551
 0.15214005 0.14291348 0.13215199 0.13077825 0.13665798 0.14287992
 0.14490306 0.14227617 0.13714813 0.13126354 0.12561777 0.1202816
 0.11545724 0.1112817  0.10843916 0.10760605 0.10965015 0.11444708
 0.12065834 0.12581393 0.12682001 0.12053653 0.10566749 0.08572163
 0.06888132 0.06195284 0.06353544 0.06487796 0.064817   0.07167917
 0.08935108 0.10938252 0.12186237 0.1267701  0.13066357 0.13824287
 0.14897871 0.15924312 0.16385937 0.15935826 0.14670995 0.12905926
 0.10927217 0.08987097 0.07372184 0.06360453 0.06120636 0.06515928
 0.07341412 0.08533484 0.09965736 0.11342867 0.12731148 0.15004168
 0.1830662  0.20198023 0.20383745 0.21431491 0.21875557 0.20219935
 0.17976925 0.16303459 0.1532185  0.14162497 0.12798658 0.12430526
 0.12896095 0.13328096 0.1333738  0.12987466 0.12522465 0.12063316
 0.11598673 0.11094154 0.10533395 0.09974848 0.09480511 0.09121946
 0.09014312 0.09252598 0.09814445 0.10520684 0.11089206 0.11129448
 0.10266697 0.08486608 0.06419859 0.05011731 0.04698027 0.04840889
 0.04790126 0.05021828 0.06346364 0.08433282 0.10044044 0.10814677
 0.11400706 0.1237522  0.13702296 0.15040345 0.15825164 0.15675114
 0.14725922 0.13322678 0.11656889 0.09816045 0.0798415  0.06335071
 0.05201463 0.04827453 0.05146325 0.0595517  0.07318558 0.09136406
 0.10931347 0.12691158 0.15453753 0.18693987 0.1963346  0.20112325
 0.21495023 0.21056393 0.19244997 0.17321001 0.15762654 0.14386666
 0.12709436 0.11941965 0.1220968  0.12519585 0.12523413 0.12328552
 0.12144083 0.11978364 0.11690454 0.11189741 0.10507369 0.09772886
 0.09052563 0.08396515 0.07863156 0.0756254  0.07627154 0.08107457
 0.08821279 0.0943434  0.09502285 0.08553219 0.06733473 0.04983536
 0.04229084 0.04214942 0.04245819 0.04186919 0.04554513 0.05984038
 0.07724249 0.08714637 0.09311762 0.10263756 0.11700988 0.13313387
 0.14509654 0.1481758  0.14378883 0.13603699 0.12575534 0.11225937
 0.0958586  0.07787395 0.06006328 0.04635222 0.04106673 0.04347182
 0.05089553 0.06541441 0.08670691 0.10829574 0.12796298 0.1570029
 0.1845798  0.18613029 0.19641167 0.20837341 0.20191751 0.18764986
 0.16765143 0.15036763 0.13132559 0.11746747 0.1171177  0.1197621
 0.12135901 0.12253603 0.12453341 0.12621072 0.12486333 0.11960214
 0.11133619 0.10211878 0.09356929 0.08607629 0.07905591 0.07183608
 0.06553895 0.06199482 0.06322086 0.06899405 0.07610925 0.07875814
 0.07162289 0.05706533 0.04783499 0.04762719 0.04854807 0.0481813
 0.04792737 0.04885732 0.05829943 0.06853992 0.07273378 0.07818638
 0.09023827 0.10660795 0.12245198 0.13079537 0.13199577 0.13121545
 0.12949026 0.1235768  0.11284406 0.09815147 0.08068368 0.06204194
 0.04746946 0.04213832 0.04534937 0.05303689 0.06672992 0.0880225
 0.11107444 0.13059302 0.15680782 0.1775795  0.17284621 0.1857442
 0.1964716  0.19629723 0.18493178 0.16233443 0.14240381 0.12109669
 0.11431718 0.11639976 0.12005699 0.1239541  0.12839913 0.13163607
 0.13006899 0.12327799 0.11332971 0.10291558 0.09391714 0.08752311
 0.08315638 0.07876728 0.07197481 0.06248829 0.05271549 0.04634079
 0.04771568 0.05603272 0.06359026 0.0630211  0.05645107 0.05571981
 0.05966948 0.06041902 0.05984942 0.05955653 0.05748634 0.06069436
 0.06396513 0.06264571 0.06558958 0.07643412 0.09148637 0.10424414
 0.10947122 0.11293669 0.11893724 0.12207739 0.11851957 0.11014661
 0.09762806 0.08005258 0.06116028 0.04836863 0.04665291 0.05509634
 0.0682951  0.08152805 0.09751454 0.11801111 0.13545756 0.15493858
 0.17001936 0.15811032 0.1672141  0.18026948 0.19181076 0.18285337
 0.15919465 0.13562569 0.11526039 0.11409145 0.11859463 0.12416577
 0.12890658 0.13096526 0.1262971  0.11515812 0.1016906  0.08972079
 0.08081482 0.0755339  0.07416026 0.07598173 0.07800834 0.07611291
 0.06748237 0.05312351 0.03675741 0.02644626 0.03424641 0.04953187
 0.05946623 0.06245543 0.06638993 0.07185084 0.07239109 0.071086
 0.07115531 0.06861377 0.06860325 0.06876526 0.06372678 0.06215032
 0.06727078 0.07582165 0.0825339  0.08371451]
