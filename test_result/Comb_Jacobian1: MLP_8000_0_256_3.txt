time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.55%, model saved.
Epoch: 0 Train: 172110.70312 Test: 3975.56226
Epoch: 80 Train: 29959.54688 Test: 1608.82275
Epoch: 160 Train: 13190.11914 Test: 664.52844
Epoch 240: New minimal relative error: 74.98%, model saved.
Epoch: 240 Train: 2651.15503 Test: 113.80495
Epoch 320: New minimal relative error: 32.13%, model saved.
Epoch: 320 Train: 721.21790 Test: 11.31897
Epoch 400: New minimal relative error: 13.60%, model saved.
Epoch: 400 Train: 361.59305 Test: 4.35011
Epoch: 480 Train: 207.28979 Test: 2.05361
Epoch 560: New minimal relative error: 12.26%, model saved.
Epoch: 560 Train: 164.84552 Test: 1.62073
Epoch: 640 Train: 135.92369 Test: 3.03531
Epoch: 720 Train: 93.60323 Test: 0.63969
Epoch: 800 Train: 79.38706 Test: 1.29962
Epoch: 880 Train: 157.71591 Test: 3.50834
Epoch: 960 Train: 61.30457 Test: 1.21257
Epoch 1040: New minimal relative error: 5.83%, model saved.
Epoch: 1040 Train: 51.00643 Test: 0.32151
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.55%, model saved.
Epoch: 0 Train: 172110.70312 Test: 3975.56226
Epoch: 80 Train: 29959.52344 Test: 1608.83728
Epoch: 160 Train: 13143.38965 Test: 640.43579
Epoch 240: New minimal relative error: 69.28%, model saved.
Epoch: 240 Train: 2403.64990 Test: 95.59778
Epoch 320: New minimal relative error: 16.55%, model saved.
Epoch: 320 Train: 641.02618 Test: 8.55680
Epoch 400: New minimal relative error: 10.96%, model saved.
Epoch: 400 Train: 386.58936 Test: 4.15446
Epoch: 480 Train: 188.78542 Test: 1.55106
Epoch 560: New minimal relative error: 7.01%, model saved.
Epoch: 560 Train: 178.93396 Test: 2.46355
Epoch: 640 Train: 103.35820 Test: 0.98301
Epoch: 720 Train: 127.88332 Test: 2.08139
Epoch: 800 Train: 124.63802 Test: 2.22246
Epoch 880: New minimal relative error: 5.53%, model saved.
Epoch: 880 Train: 62.50146 Test: 0.54750
Epoch: 960 Train: 59.99154 Test: 0.64878
Epoch: 1040 Train: 56.93787 Test: 1.30118
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.55%, model saved.
Epoch: 0 Train: 172110.70312 Test: 3975.56226
Epoch: 80 Train: 29959.52344 Test: 1608.83728
Epoch: 160 Train: 13143.38965 Test: 640.43579
Epoch 240: New minimal relative error: 35.69%, model saved.
Epoch: 240 Train: 2403.64990 Test: 95.59778
Epoch 320: New minimal relative error: 29.95%, model saved.
Epoch: 320 Train: 641.02618 Test: 8.55680
Epoch 400: New minimal relative error: 10.09%, model saved.
Epoch: 400 Train: 386.58936 Test: 4.15446
Epoch 480: New minimal relative error: 5.27%, model saved.
Epoch: 480 Train: 188.78542 Test: 1.55106
Epoch 560: New minimal relative error: 4.33%, model saved.
Epoch: 560 Train: 178.93396 Test: 2.46355
Epoch: 640 Train: 103.35820 Test: 0.98301
Epoch: 720 Train: 127.88332 Test: 2.08139
Epoch: 800 Train: 124.63802 Test: 2.22246
Epoch: 880 Train: 62.50146 Test: 0.54750
Epoch: 960 Train: 59.99154 Test: 0.64878
Epoch: 1040 Train: 56.93787 Test: 1.30118
Epoch: 1120 Train: 69.68418 Test: 1.25144
Epoch: 1200 Train: 42.99608 Test: 1.14197
Epoch: 1280 Train: 34.00694 Test: 0.33780
Epoch: 1360 Train: 31.42295 Test: 0.35830
Epoch: 1440 Train: 50.51479 Test: 1.61132
Epoch: 1520 Train: 27.50602 Test: 0.68851
Epoch 1600: New minimal relative error: 4.09%, model saved.
Epoch: 1600 Train: 33.89004 Test: 0.46713
Epoch 1680: New minimal relative error: 3.59%, model saved.
Epoch: 1680 Train: 27.45226 Test: 0.32215
Epoch 1760: New minimal relative error: 1.41%, model saved.
Epoch: 1760 Train: 24.04855 Test: 0.24031
Epoch: 1840 Train: 29.20830 Test: 0.51220
Epoch: 1920 Train: 20.03807 Test: 0.63822
Epoch: 2000 Train: 18.64519 Test: 0.25178
Epoch 2080: New minimal relative error: 1.04%, model saved.
Epoch: 2080 Train: 17.14019 Test: 0.22384
Epoch: 2160 Train: 17.31517 Test: 0.20013
Epoch: 2240 Train: 40.34185 Test: 1.68634
Epoch: 2320 Train: 16.68058 Test: 0.28360
Epoch: 2400 Train: 31.22955 Test: 0.62693
Epoch: 2480 Train: 17.12076 Test: 0.64430
Epoch: 2560 Train: 21.24930 Test: 0.88737
Epoch: 2640 Train: 23.52723 Test: 0.94772
Epoch: 2720 Train: 13.34774 Test: 0.29430
Epoch: 2800 Train: 12.28555 Test: 0.29279
Epoch: 2880 Train: 16.16010 Test: 0.92758
Epoch: 2960 Train: 11.05139 Test: 0.14783
Epoch: 3040 Train: 18.09671 Test: 1.03578
Epoch: 3120 Train: 10.35200 Test: 0.20671
Epoch: 3200 Train: 19.05011 Test: 2.21223
Epoch: 3280 Train: 19.95083 Test: 0.59717
Epoch: 3360 Train: 10.34858 Test: 0.38587
Epoch: 3440 Train: 7.81538 Test: 0.12893
Epoch: 3520 Train: 7.97799 Test: 0.15703
Epoch: 3600 Train: 7.96523 Test: 0.15768
Epoch: 3680 Train: 10.10918 Test: 0.32289
Epoch: 3760 Train: 14.51382 Test: 1.09375
Epoch: 3840 Train: 6.63123 Test: 0.14014
Epoch: 3920 Train: 8.29559 Test: 0.19128
Epoch: 4000 Train: 6.51505 Test: 0.16206
Epoch: 4080 Train: 6.03246 Test: 0.13677
Epoch: 4160 Train: 8.91469 Test: 0.45772
Epoch 4240: New minimal relative error: 1.02%, model saved.
Epoch: 4240 Train: 5.63221 Test: 0.14166
Epoch: 4320 Train: 5.62594 Test: 0.14662
Epoch: 4400 Train: 17.90173 Test: 1.41833
Epoch: 4480 Train: 5.23646 Test: 0.14864
Epoch: 4560 Train: 5.12931 Test: 0.14870
Epoch: 4640 Train: 5.00512 Test: 0.14184
Epoch: 4720 Train: 4.94143 Test: 0.14328
Epoch: 4800 Train: 4.80409 Test: 0.14370
Epoch: 4880 Train: 4.57454 Test: 0.13790
Epoch: 4960 Train: 5.25985 Test: 0.17297
Epoch: 5040 Train: 4.36387 Test: 0.14932
Epoch: 5120 Train: 4.97334 Test: 0.17679
Epoch: 5200 Train: 5.68368 Test: 0.17476
Epoch 5280: New minimal relative error: 0.92%, model saved.
Epoch: 5280 Train: 4.07897 Test: 0.14710
Epoch: 5360 Train: 3.96998 Test: 0.13357
Epoch: 5440 Train: 3.79084 Test: 0.13364
Epoch: 5520 Train: 10.14386 Test: 0.91886
Epoch 5600: New minimal relative error: 0.68%, model saved.
Epoch: 5600 Train: 3.62201 Test: 0.13589
Epoch: 5680 Train: 3.53330 Test: 0.13443
Epoch: 5760 Train: 3.46470 Test: 0.13304
Epoch 5840: New minimal relative error: 0.38%, model saved.
Epoch: 5840 Train: 3.37572 Test: 0.12971
Epoch: 5920 Train: 5.15577 Test: 0.29477
Epoch: 6000 Train: 3.23366 Test: 0.12926
Epoch: 6080 Train: 4.84209 Test: 0.20725
Epoch: 6160 Train: 3.10018 Test: 0.12906
Epoch: 6240 Train: 3.06164 Test: 0.13001
Epoch: 6320 Train: 4.18619 Test: 0.18790
Epoch: 6400 Train: 2.90869 Test: 0.12750
Epoch: 6480 Train: 5.12992 Test: 0.60018
Epoch: 6560 Train: 4.59615 Test: 0.21200
Epoch: 6640 Train: 2.73985 Test: 0.12679
Epoch: 6720 Train: 2.72132 Test: 0.12873
Epoch 6800: New minimal relative error: 0.27%, model saved.
Epoch: 6800 Train: 2.63252 Test: 0.12607
Epoch: 6880 Train: 2.73628 Test: 0.13909
Epoch: 6960 Train: 2.53578 Test: 0.12596
Epoch: 7040 Train: 2.54819 Test: 0.13369
Epoch: 7120 Train: 2.44293 Test: 0.12514
Epoch: 7200 Train: 3.10394 Test: 0.21005
Epoch: 7280 Train: 2.35622 Test: 0.12475
Epoch: 7360 Train: 4.30405 Test: 0.43674
Epoch: 7440 Train: 2.27838 Test: 0.12479
Epoch: 7520 Train: 2.25424 Test: 0.12546
Epoch: 7600 Train: 2.53344 Test: 0.13046
Epoch: 7680 Train: 2.16054 Test: 0.12364
Epoch: 7760 Train: 5.05255 Test: 0.40630
Epoch: 7840 Train: 2.19683 Test: 0.12545
Epoch: 7920 Train: 2.05578 Test: 0.12309
Epoch: 7999 Train: 2.01475 Test: 0.12290
Training Loss: tensor(2.0147)
Test Loss: tensor(0.1229)
Learned LE: [ 8.7214357e-01  4.3806134e-04 -1.4544025e+01]
True LE: [ 8.6217010e-01  7.6111332e-03 -1.4541243e+01]
Relative Error: [2.7519739  2.3971443  2.0378482  1.7433673  1.6246011  1.7870009
 2.224767   2.845742   3.5492666  4.2396474  4.8300395  5.258038
 5.5026784  5.5853543  5.550524   5.4421144  5.2895584  5.1056104
 4.892341   4.64464    4.3583145  4.0368843  3.6941967  3.351454
 3.028805   2.7388344  2.4834068  2.254067   2.037959   1.822115
 1.597593   1.361003   1.1183504  0.90372145 0.8241056  1.0162404
 1.4425718  1.9448895  2.317556   2.3704576  2.0938797  1.6783148
 1.3474199  1.1991569  1.1613535  1.133668   1.1345683  1.2722211
 1.5804302  1.9827423  2.3887916  2.7400703  3.0128663  3.2113984
 3.3527489  3.453961   3.522934   3.5562038  3.541484   3.4630597
 3.308155   3.0717769  2.7600834  2.3912368  1.994832   1.622509
 1.374366   1.4088345  1.7902411  2.412491   3.1440463  3.870847
 4.494149   4.9462805  5.2086916  5.3089123  5.29865    5.2226534
 5.1057053  4.954739   4.7645955  4.5249004  4.2297196  3.885907
 3.514413   3.1447842  2.8045223  2.510732   2.2667325  2.06281
 1.8826976  1.7099247  1.5335779  1.3488541  1.157169   0.97085226
 0.8474638  0.9221441  1.264844   1.7544376  2.1620927  2.2453706
 1.9576136  1.5028551  1.1432722  1.014981   1.0234779  1.0212682
 1.0221972  1.1679088  1.5088214  1.9462724  2.3693     2.714173
 2.964972   3.1384974  3.2622352  3.3577483  3.4333334  3.4836662
 3.4922552  3.4394753  3.3084617  3.0909352  2.791067   2.4241204
 2.014147   1.5941126  1.2273229  1.0745478  1.3318865  1.9265847
 2.6765802  3.4411936  4.106034   4.593546   4.882521   5.007017
 5.025498   4.9851193  4.9071136  4.791333   4.6251397  4.3925123
 4.085892   3.7153716  3.3098276  2.9062266  2.5398457  2.233411
 1.9933307  1.8102117  1.6651956  1.5372535  1.4121984  1.283726
 1.151021   1.0141965  0.8891913  0.86073875 1.0749744  1.5199026
 1.9699134  2.1213815  1.8553214  1.3668152  0.9540323  0.82356405
 0.8849239  0.9205411  0.9216878  1.0650451  1.4303769  1.8953735
 2.3247652  2.6513505  2.8715062  3.0162978  3.1221871  3.2146747
 3.301774   3.3750176  3.4142761  3.395981   3.2991192  3.112159
 2.836831   2.4874794  2.085578   1.6543213  1.2215483  0.87216616
 0.8833833  1.388795   2.1352136  2.9338424  3.649173   4.186228
 4.514099   4.6692414  4.7190437  4.716469   4.680304   4.604081
 4.467523   4.2473116  3.9332244  3.5389085  3.0993764  2.6589432
 2.2590814  1.9285079  1.6785079  1.5036176  1.385588   1.3003374
 1.2281318  1.1592077  1.0909983  1.0182297  0.9319694  0.8536938
 0.91258585 1.2479281  1.7220185  1.9820446  1.7912815  1.2927202
 0.80300933 0.61701226 0.7301466  0.82149494 0.82937145 0.9546085
 1.3301462  1.8134004  2.2398007  2.5394795  2.7232084  2.8371053
 2.9254553  3.0160303  3.1167877  3.2169077  3.2927315  3.316221
 3.2631874  3.1189773  2.8824198  2.5660322  2.1911252  1.7793102
 1.3438866  0.9006936  0.5735029  0.8222863  1.5180465  2.3328454
 3.1024723  3.706019   4.0899997  4.2844424  4.3669276  4.401827
 4.4101734  4.380428   4.2832503  4.0876923  3.778625   3.3713703
 2.906007   2.4328032  1.9975299  1.6334335  1.3570212  1.1686634
 1.0562611  0.99944526 0.97550654 0.96852165 0.97039723 0.9705838
 0.94935876 0.8895142  0.8355429  0.9798168  1.4032592  1.7892858
 1.7498629  1.2976266  0.74195445 0.40788385 0.5417951  0.70945215
 0.7414863  0.83069587 1.1931082  1.6854378  2.104223   2.3723478
 2.5168023  2.597409   2.6646335  2.7500465  2.863513   2.9925292
 3.1079557  3.1778183  3.1751547  3.0833852  2.8985562  2.6310797
 2.3012004  1.9317534  1.5358396  1.1123565  0.65754753 0.357153
 0.8487311  1.6342074  2.4436972  3.1275046  3.5921936  3.8417861
 3.9583914  4.026566   4.078556   4.1009254  4.0568686  3.9041572
 3.6202357  3.218986   2.7443888  2.2507732  1.7874064  1.3906451
 1.0795783  0.85832936 0.72070944 0.6563833  0.65364647 0.69990945
 0.77802014 0.86261797 0.92065644 0.92306304 0.8594173  0.81456035
 1.0414462  1.4883758  1.6765177  1.3681421  0.8166323  0.30448404
 0.3081516  0.5662509  0.65254873 0.69632846 1.008189   1.4990519
 1.9126719  2.152126   2.2560608  2.2988698  2.336452   2.4077332
 2.5295935  2.6877587  2.845048   2.9624553  3.0115561  2.9751563
 2.8483226  2.6393042  2.368082   2.0586007  1.7262092  1.3708619
 0.97870964 0.53275055 0.2724808  0.87720865 1.6662549  2.4198596
 2.9927063  3.3285062  3.4884229  3.5808563  3.66693    3.7415314
 3.762462   3.6750321  3.442524   3.0731895  2.6117938  2.1162035
 1.6389489  1.2201624  0.88318044 0.63254225]
