time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.04%, model saved.
Epoch: 0 Train: 59500.10938 Test: 3857.05322
Epoch: 80 Train: 15605.08887 Test: 1514.35510
Epoch 160: New minimal relative error: 63.51%, model saved.
Epoch: 160 Train: 14474.62598 Test: 1250.23303
Epoch 240: New minimal relative error: 56.61%, model saved.
Epoch: 240 Train: 13846.69336 Test: 1307.29822
Epoch: 320 Train: 11457.09277 Test: 1191.34766
Epoch: 400 Train: 14735.54590 Test: 1440.08728
Epoch: 480 Train: 11581.74023 Test: 1135.50635
Epoch: 560 Train: 12047.39746 Test: 1036.61938
Epoch 640: New minimal relative error: 48.36%, model saved.
Epoch: 640 Train: 12102.28027 Test: 1090.29968
Epoch: 720 Train: 11510.93555 Test: 932.86963
Epoch: 800 Train: 11448.98828 Test: 1125.46350
Epoch: 880 Train: 9797.36523 Test: 743.27271
Epoch: 960 Train: 7209.67285 Test: 546.22845
Epoch 1040: New minimal relative error: 10.79%, model saved.
Epoch: 1040 Train: 5281.93164 Test: 331.66119
Epoch: 1120 Train: 2691.73438 Test: 94.95812
Epoch: 1200 Train: 1266.61877 Test: 28.81977
Epoch: 1280 Train: 823.09998 Test: 14.79000
Epoch: 1360 Train: 853.15192 Test: 34.97705
Epoch: 1440 Train: 701.77808 Test: 19.35845
Epoch 1520: New minimal relative error: 9.63%, model saved.
Epoch: 1520 Train: 525.56146 Test: 8.24526
Epoch: 1600 Train: 468.39700 Test: 11.63841
Epoch: 1680 Train: 500.99985 Test: 19.02833
Epoch: 1760 Train: 444.02261 Test: 12.88519
Epoch: 1840 Train: 384.51758 Test: 7.17028
Epoch: 1920 Train: 332.11572 Test: 6.19177
Epoch: 2000 Train: 323.84384 Test: 8.64124
Epoch: 2080 Train: 293.98892 Test: 4.70499
Epoch: 2160 Train: 274.66928 Test: 7.54453
Epoch: 2240 Train: 277.51422 Test: 6.98568
Epoch: 2320 Train: 238.74687 Test: 5.15534
Epoch 2400: New minimal relative error: 4.69%, model saved.
Epoch: 2400 Train: 273.70642 Test: 5.16962
Epoch: 2480 Train: 262.97476 Test: 14.16330
Epoch: 2560 Train: 218.80116 Test: 2.38714
Epoch: 2640 Train: 230.19753 Test: 2.32013
Epoch: 2720 Train: 217.31889 Test: 8.10587
Epoch: 2800 Train: 206.09708 Test: 2.03431
Epoch: 2880 Train: 187.94371 Test: 1.54476
Epoch: 2960 Train: 181.79405 Test: 1.71039
Epoch: 3040 Train: 173.20126 Test: 1.40471
Epoch: 3120 Train: 165.39722 Test: 2.30663
Epoch: 3200 Train: 166.72244 Test: 1.37547
Epoch: 3280 Train: 160.92899 Test: 2.64732
Epoch: 3360 Train: 164.34573 Test: 6.69956
Epoch: 3440 Train: 148.00615 Test: 1.08878
Epoch 3520: New minimal relative error: 2.73%, model saved.
Epoch: 3520 Train: 157.32568 Test: 2.66130
Epoch: 3600 Train: 147.24760 Test: 1.43612
Epoch: 3680 Train: 160.88872 Test: 6.41491
Epoch: 3760 Train: 145.02658 Test: 1.22906
Epoch: 3840 Train: 133.65950 Test: 1.66716
Epoch: 3920 Train: 135.99118 Test: 4.60193
Epoch: 4000 Train: 133.69772 Test: 2.47063
Epoch: 4080 Train: 126.49062 Test: 4.42355
Epoch: 4160 Train: 114.60134 Test: 0.79429
Epoch: 4240 Train: 105.64423 Test: 0.83545
Epoch: 4320 Train: 107.74889 Test: 1.01306
Epoch: 4400 Train: 100.10718 Test: 1.11411
Epoch: 4480 Train: 98.69195 Test: 0.66107
Epoch: 4560 Train: 101.74606 Test: 1.00632
Epoch: 4640 Train: 113.45667 Test: 1.99550
Epoch: 4720 Train: 104.21507 Test: 1.02428
Epoch 4800: New minimal relative error: 2.44%, model saved.
Epoch: 4800 Train: 96.83334 Test: 2.33522
Epoch: 4880 Train: 87.51389 Test: 0.62259
Epoch: 4960 Train: 82.75983 Test: 0.55497
Epoch: 5040 Train: 90.54469 Test: 1.30118
Epoch: 5120 Train: 92.84422 Test: 0.79044
Epoch 5200: New minimal relative error: 1.95%, model saved.
Epoch: 5200 Train: 100.77625 Test: 0.73201
Epoch: 5280 Train: 99.54401 Test: 1.32003
Epoch: 5360 Train: 91.98476 Test: 0.75970
Epoch: 5440 Train: 89.36549 Test: 0.96950
Epoch: 5520 Train: 100.85094 Test: 1.09178
Epoch: 5600 Train: 103.65260 Test: 1.24263
Epoch: 5680 Train: 94.79995 Test: 0.73704
Epoch: 5760 Train: 89.30970 Test: 0.82667
Epoch: 5840 Train: 87.44902 Test: 0.93022
Epoch: 5920 Train: 98.71305 Test: 2.67653
Epoch: 6000 Train: 82.17220 Test: 0.65626
Epoch: 6080 Train: 85.54485 Test: 1.55631
Epoch 6160: New minimal relative error: 1.68%, model saved.
Epoch: 6160 Train: 87.34563 Test: 0.59145
Epoch: 6240 Train: 92.41663 Test: 0.80669
Epoch: 6320 Train: 87.65347 Test: 0.70726
Epoch: 6400 Train: 80.85143 Test: 0.62637
Epoch: 6480 Train: 84.52029 Test: 0.70719
Epoch: 6560 Train: 86.26994 Test: 0.78917
Epoch: 6640 Train: 91.66148 Test: 0.70088
Epoch: 6720 Train: 91.65746 Test: 0.98523
Epoch: 6800 Train: 90.70518 Test: 0.65443
Epoch: 6880 Train: 88.29398 Test: 0.68959
Epoch: 6960 Train: 86.22149 Test: 0.72791
Epoch: 7040 Train: 85.20304 Test: 0.61604
Epoch 7120: New minimal relative error: 1.18%, model saved.
Epoch: 7120 Train: 82.75764 Test: 0.57981
Epoch: 7200 Train: 78.45730 Test: 0.50369
Epoch: 7280 Train: 77.17089 Test: 0.50321
Epoch: 7360 Train: 75.86517 Test: 0.51097
Epoch: 7440 Train: 74.79420 Test: 0.48035
Epoch: 7520 Train: 72.49319 Test: 0.58208
Epoch: 7600 Train: 75.10847 Test: 0.64780
Epoch: 7680 Train: 76.43944 Test: 0.56081
Epoch: 7760 Train: 76.83020 Test: 0.65253
Epoch: 7840 Train: 74.23608 Test: 1.42817
Epoch: 7920 Train: 73.39884 Test: 0.75192
Epoch: 7999 Train: 71.39899 Test: 0.65292
Training Loss: tensor(71.3990)
Test Loss: tensor(0.6529)
Learned LE: [  0.9040174   -0.03954997 -14.536399  ]
True LE: [ 8.7830925e-01 -4.4778632e-03 -1.4551398e+01]
Relative Error: [0.54209113 0.6035162  0.64608264 0.634216   0.598223   0.48501837
 0.4264148  0.29711586 0.41250154 0.6707458  0.8886058  0.91580003
 0.89661443 0.9800897  1.0867963  0.93163776 0.7934519  0.5008479
 0.23230274 0.15005222 0.15027153 0.18803917 0.28485283 0.54824066
 0.85493046 0.8949547  0.7639469  0.74318016 0.6639218  0.689693
 0.6459748  0.6888069  0.4685003  0.24241829 0.27900782 0.26128116
 0.3018925  0.39888382 0.52879804 0.7072207  0.77971065 0.8189898
 0.875491   0.74725294 0.6701399  0.7272803  0.94646674 0.9856699
 0.94862145 0.84673667 0.7938022  0.91686857 1.0180953  1.0102543
 1.0038025  1.0108281  0.8664886  0.6705614  0.5692311  0.4143449
 0.36138004 0.3293583  0.48204952 0.50925684 0.668387   0.6678216
 0.63455296 0.55259836 0.37738788 0.26967895 0.14221485 0.4143516
 0.75576675 0.8485797  0.93761843 0.83717453 1.0218322  1.0313202
 0.9713082  0.9087679  0.5753713  0.34883913 0.20451611 0.21714155
 0.3165075  0.43388057 0.64708614 0.9206065  0.8381025  0.6613062
 0.56362164 0.5820286  0.5279648  0.65823156 0.5432928  0.28211161
 0.20225127 0.3437729  0.31786314 0.45027444 0.60006887 0.72353595
 0.8564837  0.877587   0.9657171  1.1296899  1.1190617  0.80462134
 0.74443394 0.9285159  0.83546436 0.84153914 0.8986432  0.9485922
 1.0778811  1.0961843  1.0833461  1.0348454  0.9255887  0.7029776
 0.47031012 0.33124772 0.23419034 0.31400478 0.46324247 0.49535626
 0.69958264 0.8130143  0.8100043  0.69679976 0.57307756 0.403015
 0.31818026 0.4406495  0.6638544  0.82992303 0.91614026 0.94134146
 0.98310244 1.0224825  1.0617425  1.0736476  1.0177071  0.7313468
 0.5220235  0.32041162 0.4091642  0.4595743  0.5571824  0.7167089
 0.87021387 0.73192495 0.50104654 0.4608572  0.41842404 0.4949477
 0.5392686  0.35833672 0.11034288 0.17064713 0.33536306 0.30805254
 0.3617779  0.4479974  0.58922905 0.71626616 0.7849092  0.90453464
 1.1723055  1.1626146  0.80482435 0.8250218  0.809359   0.68303144
 0.7531177  0.92759997 1.0753826  1.1828765  1.1643122  1.0854778
 0.9974867  0.8136656  0.5349244  0.33775595 0.33101967 0.38186166
 0.46604252 0.43453375 0.6531305  0.8333493  0.89828765 0.85541177
 0.8471181  0.80152124 0.75726354 0.60507274 0.5697239  0.6935989
 0.7211923  0.7614994  0.824345   0.8635128  0.9739553  1.0990579
 1.1028186  1.1514649  0.92063195 0.6360551  0.38488582 0.47903425
 0.46999738 0.5143258  0.618504   0.71374625 0.5387045  0.37498873
 0.28853315 0.32334834 0.3672866  0.35722345 0.25890395 0.09267135
 0.17849773 0.22789785 0.13397287 0.28621006 0.42528686 0.5441546
 0.6641816  0.7762759  0.8961466  1.056006   1.1376235  0.9828249
 0.90841746 0.77947557 0.68733066 0.7106314  0.9527455  1.1338775
 1.247721   1.1473638  1.0483235  0.92424226 0.76955384 0.49440333
 0.41825873 0.40503916 0.40588492 0.4125847  0.50819844 0.63491076
 0.74353105 0.8838877  0.8446528  0.8873703  0.93515563 0.8366106
 0.7431669  0.5892258  0.50366193 0.5750319  0.5537779  0.68780196
 0.7999958  0.7902204  1.0817075  1.2111001  1.3581052  1.0888877
 0.84454817 0.55767834 0.5049059  0.48415124 0.3909335  0.43538752
 0.5485248  0.39077556 0.30328423 0.22561695 0.28431362 0.24298774
 0.1757593  0.2819411  0.21856833 0.20861845 0.18677108 0.28028992
 0.32949632 0.45293775 0.5539351  0.7538784  0.86504805 0.8508661
 0.98506117 1.012476   1.0079514  1.0297812  0.8511411  0.71182173
 0.7910028  0.90139794 0.9882854  1.1530892  1.1267267  0.98525935
 0.792122   0.6599429  0.51404065 0.53189814 0.42794254 0.3765425
 0.314596   0.459891   0.49261755 0.51121324 0.62095153 0.6969712
 0.781604   0.88598615 0.84961456 0.842218   0.84355885 0.8442511
 0.81418407 0.7827864  0.76644677 0.83364797 0.9038589  1.0211242
 1.1292462  1.4059671  1.2933306  0.9929579  0.7608296  0.48420653
 0.46160853 0.506215   0.39798072 0.44524172 0.36796942 0.43625543
 0.266775   0.2243092  0.2128941  0.12707357 0.21227106 0.3244635
 0.2502094  0.30278525 0.35812137 0.3743448  0.40045726 0.54881513
 0.7019586  0.66218317 0.7407174  0.78537196 0.8124757  0.7545955
 0.9182991  1.0369202  0.8913977  0.72682226 0.7918557  0.8039488
 1.010086   0.97110486 0.91944885 0.72660637 0.5620046  0.5275087
 0.4983777  0.37605926 0.3014284  0.22398797 0.2949893  0.19403586
 0.28600442 0.40320972 0.5329874  0.50106615 0.6529195  0.73516333
 0.68521106 0.78065485 0.9117739  1.0323585  1.1017547  1.1033181
 0.98074347 0.9701403  1.1976682  1.1453371  1.077736   1.1086553
 1.1385751  0.94278467 0.7227909  0.46760625]
