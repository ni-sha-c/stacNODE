time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.75%, model saved.
Epoch: 0 Train: 9753.96973 Test: 4162.13770
Epoch: 100 Train: 2713.74683 Test: 1089.32593
Epoch: 200 Train: 1817.35864 Test: 610.58563
Epoch: 300 Train: 849.07495 Test: 172.90613
Epoch: 400 Train: 423.55011 Test: 66.52029
Epoch: 500 Train: 306.66559 Test: 53.61480
Epoch 600: New minimal relative error: 16.55%, model saved.
Epoch: 600 Train: 81.50372 Test: 7.39697
Epoch 700: New minimal relative error: 10.87%, model saved.
Epoch: 700 Train: 69.75642 Test: 6.08484
Epoch: 800 Train: 59.16488 Test: 5.69213
Epoch: 900 Train: 46.76778 Test: 9.07056
Epoch: 1000 Train: 43.79928 Test: 6.45209
Epoch: 1100 Train: 49.38016 Test: 13.30122
Epoch 1200: New minimal relative error: 6.95%, model saved.
Epoch: 1200 Train: 26.54193 Test: 1.42840
Epoch: 1300 Train: 30.49606 Test: 7.57495
Epoch: 1400 Train: 36.02004 Test: 6.76351
Epoch: 1500 Train: 28.27446 Test: 5.93357
Epoch: 1600 Train: 32.82754 Test: 15.07836
Epoch: 1700 Train: 39.04345 Test: 15.73413
Epoch: 1800 Train: 21.56741 Test: 7.21424
Epoch: 1900 Train: 21.56230 Test: 4.29001
Epoch: 2000 Train: 20.22206 Test: 1.22373
Epoch: 2100 Train: 15.23342 Test: 1.18954
Epoch: 2200 Train: 14.09028 Test: 0.48963
Epoch: 2300 Train: 16.55772 Test: 2.38436
Epoch: 2400 Train: 13.36128 Test: 1.31514
Epoch: 2500 Train: 20.41566 Test: 8.63948
Epoch: 2600 Train: 16.52471 Test: 4.63028
Epoch: 2700 Train: 25.90622 Test: 7.41690
Epoch 2800: New minimal relative error: 5.44%, model saved.
Epoch: 2800 Train: 12.51145 Test: 0.63058
Epoch: 2900 Train: 14.56821 Test: 2.21105
Epoch 3000: New minimal relative error: 5.10%, model saved.
Epoch: 3000 Train: 11.63705 Test: 1.87519
Epoch: 3100 Train: 11.24636 Test: 1.60049
Epoch: 3200 Train: 10.56534 Test: 1.23644
Epoch: 3300 Train: 13.08290 Test: 1.04193
Epoch: 3400 Train: 9.99368 Test: 0.51393
Epoch 3500: New minimal relative error: 4.64%, model saved.
Epoch: 3500 Train: 10.43762 Test: 0.72001
Epoch: 3600 Train: 13.33439 Test: 1.66287
Epoch: 3700 Train: 10.40174 Test: 1.32483
Epoch: 3800 Train: 9.29061 Test: 1.04427
Epoch 3900: New minimal relative error: 4.04%, model saved.
Epoch: 3900 Train: 9.41371 Test: 0.80360
Epoch: 4000 Train: 8.97603 Test: 1.01183
Epoch: 4100 Train: 13.25020 Test: 2.11890
Epoch 4200: New minimal relative error: 3.41%, model saved.
Epoch: 4200 Train: 7.83263 Test: 0.26480
Epoch 4300: New minimal relative error: 2.72%, model saved.
Epoch: 4300 Train: 7.50297 Test: 0.07900
Epoch: 4400 Train: 10.44187 Test: 1.55459
Epoch: 4500 Train: 11.95605 Test: 5.18010
Epoch: 4600 Train: 8.19519 Test: 0.14016
Epoch: 4700 Train: 7.12015 Test: 0.24629
Epoch: 4800 Train: 7.15855 Test: 0.41992
Epoch: 4900 Train: 7.85072 Test: 0.65774
Epoch: 5000 Train: 7.29367 Test: 0.91840
Epoch: 5100 Train: 6.37681 Test: 0.06823
Epoch: 5200 Train: 9.33956 Test: 2.28809
Epoch: 5300 Train: 6.56704 Test: 0.45852
Epoch: 5400 Train: 6.25960 Test: 0.18503
Epoch: 5500 Train: 7.39242 Test: 0.86819
Epoch: 5600 Train: 7.72023 Test: 1.04942
Epoch: 5700 Train: 6.20257 Test: 0.07195
Epoch: 5800 Train: 6.13987 Test: 0.24699
Epoch: 5900 Train: 7.04575 Test: 0.63260
Epoch: 6000 Train: 8.25956 Test: 1.53186
Epoch: 6100 Train: 6.88032 Test: 0.58019
Epoch: 6200 Train: 5.95708 Test: 0.07616
Epoch: 6300 Train: 12.41992 Test: 5.30661
Epoch: 6400 Train: 9.28346 Test: 3.46964
Epoch: 6500 Train: 7.66124 Test: 0.49223
Epoch: 6600 Train: 6.00137 Test: 0.25181
Epoch: 6700 Train: 6.28784 Test: 0.32466
Epoch: 6800 Train: 6.77136 Test: 0.86219
Epoch: 6900 Train: 5.76284 Test: 0.14649
Epoch 7000: New minimal relative error: 2.44%, model saved.
Epoch: 7000 Train: 5.53369 Test: 0.09137
Epoch: 7100 Train: 5.76497 Test: 0.34894
Epoch: 7200 Train: 6.03133 Test: 0.33506
Epoch: 7300 Train: 5.23865 Test: 0.07555
Epoch: 7400 Train: 5.10871 Test: 0.05960
Epoch: 7500 Train: 5.32619 Test: 0.22116
Epoch: 7600 Train: 5.18670 Test: 0.06065
Epoch: 7700 Train: 5.43515 Test: 0.32677
Epoch: 7800 Train: 5.54669 Test: 0.44323
Epoch: 7900 Train: 5.02095 Test: 0.14387
Epoch: 8000 Train: 5.08538 Test: 0.30356
Epoch: 8100 Train: 4.77193 Test: 0.07672
Epoch: 8200 Train: 4.98419 Test: 0.35209
Epoch: 8300 Train: 5.08250 Test: 0.07660
Epoch: 8400 Train: 4.83106 Test: 0.07428
Epoch: 8500 Train: 4.61174 Test: 0.05543
Epoch: 8600 Train: 4.60745 Test: 0.06731
Epoch: 8700 Train: 4.39407 Test: 0.04826
Epoch: 8800 Train: 4.46269 Test: 0.06684
Epoch: 8900 Train: 4.74929 Test: 0.13366
Epoch: 9000 Train: 4.32180 Test: 0.04762
Epoch: 9100 Train: 4.34967 Test: 0.15391
Epoch: 9200 Train: 4.50273 Test: 0.45265
Epoch: 9300 Train: 4.09124 Test: 0.04937
Epoch: 9400 Train: 4.08977 Test: 0.05002
Epoch: 9500 Train: 4.43728 Test: 0.18630
Epoch: 9600 Train: 5.32445 Test: 0.64240
Epoch: 9700 Train: 4.10265 Test: 0.04817
Epoch: 9800 Train: 4.07931 Test: 0.05465
Epoch: 9900 Train: 4.12241 Test: 0.04532
Epoch: 9999 Train: 3.95804 Test: 0.04205
Training Loss: tensor(3.9580)
Test Loss: tensor(0.0420)
Learned LE: [  0.7911035    0.06894996 -14.536794  ]
True LE: [ 8.6082995e-01  3.3862288e-03 -1.4547920e+01]
Relative Error: [6.1721306  6.211526   6.009121   5.8211274  5.5731745  5.153495
 4.3547378  3.7995875  3.4941607  3.134787   2.949143   2.7528985
 2.2774577  1.8688327  1.5842922  1.6623836  1.9621664  2.082682
 2.361132   2.7150335  2.719884   2.9257286  2.9430063  3.0755115
 3.3057761  3.4637456  3.8123713  4.0999527  4.0003185  4.2212925
 4.327591   4.410486   4.032344   4.3397303  5.0699983  6.288738
 6.2901773  5.9925294  5.869781   5.377857   5.100881   4.8332486
 4.305317   3.5085306  2.851737   2.0623634  1.3675954  0.90569955
 0.6794835  0.5195506  0.654898   0.96542454 1.4516727  1.908843
 2.445228   3.2482498  4.2521005  4.942375   5.179883   5.1347632
 4.887024   4.848801   5.2125406  5.377462   5.365674   5.129097
 4.9711113  4.593128   3.956537   3.483045   3.1675906  2.8877552
 2.5179543  2.2980814  2.0881252  1.8308113  1.6539816  1.535312
 1.7269479  1.6538428  1.7711265  2.2104766  2.400435   2.5628886
 2.7341888  2.8238251  2.9782052  2.7163064  2.490654   2.9045227
 3.205984   3.0035715  3.3582144  3.299201   2.970598   2.9862595
 3.3869033  4.6863546  5.313521   5.0424433  4.7963696  4.3739147
 4.2936997  4.17405    4.150652   3.5932918  2.8582006  2.0537097
 1.2918336  0.8285797  0.5029629  0.3605711  0.25964317 0.44145775
 0.9229335  1.584703   1.9543369  2.4641373  3.3292358  4.080889
 4.3598404  4.5193377  4.272959   4.1099358  4.1533494  4.354849
 4.4488482  4.521902   4.47062    4.3075457  3.6941285  3.085628
 2.7288234  2.6447794  2.2715545  1.9991616  1.8136094  1.8213861
 1.684783   1.5864581  1.5119385  1.2734028  1.2902322  1.5390071
 2.0454266  2.2499373  2.5930605  2.6007643  2.6195486  2.5487933
 2.0132225  1.7514657  2.0666614  2.3412433  2.1929467  2.570913
 2.1840177  1.9117398  2.009483   2.765996   3.973692   4.475651
 4.020685   3.3889022  3.1949706  3.2283635  3.312118   3.239509
 2.7803009  2.1344452  1.3703022  0.8105232  0.5315148  0.41947094
 0.40179807 0.33637327 0.20799087 0.92024875 1.4822735  1.9081169
 2.516129   3.2835338  3.6233711  3.7023153  3.755369   3.4874644
 3.475996   3.5299618  3.5758083  3.710001   3.7574217  3.7494938
 3.5587966  3.0232904  2.434702   2.1324036  1.893471   1.8095059
 1.6199058  1.6149275  1.7064114  1.615653   1.4057958  1.1314982
 0.98088396 1.0082613  1.2177091  1.563614   1.8960227  2.2996027
 2.3780296  2.4852002  1.8438206  1.5314724  1.2009531  1.1785618
 1.6449811  1.600033   1.7131362  1.3559256  1.1290178  1.2475717
 2.2883422  3.175395   3.6475236  2.9457796  2.3397527  2.1836023
 2.2401798  2.3324203  2.218539   1.9827045  1.4986632  0.9332119
 0.6626609  0.62406707 0.56243026 0.61535525 0.5256115  0.13905619
 0.79577714 1.2929559  1.7982595  2.3856237  2.8469973  2.9782956
 3.047762   3.1068473  2.8592966  2.8466187  2.8320107  2.7708564
 2.7439446  2.7958019  2.76069    2.634448   2.1120796  1.7628355
 1.7376665  1.2229943  1.1516769  1.3326102  1.4328754  1.5819911
 1.4954787  1.2367376  0.91009384 0.6358565  0.64178276 0.83975446
 1.1412663  1.5172484  1.7791411  1.9870739  1.8800879  1.3525165
 0.99523383 0.8886584  0.5731931  0.99456143 1.034778   1.0471702
 0.7872057  0.59352183 0.6527471  1.7223285  2.3965373  2.759693
 2.180817   1.5125418  1.3584936  1.4104835  1.4914232  1.581132
 1.4585348  1.3458084  0.8748654  0.5676289  0.55392814 0.66906375
 0.7535369  0.6306268  0.23573884 0.5039166  1.1145098  1.5006784
 1.8334528  2.123897   2.3288338  2.5221632  2.4577608  2.4410439
 2.3615322  2.0829017  1.8033462  1.6980048  1.8038923  1.7361443
 1.7656971  1.5527357  1.0732646  1.1987668  0.8537286  0.5141919
 0.6393021  1.029675   1.3130276  1.2010067  1.2567824  0.6355329
 0.36599025 0.41144267 0.5947425  0.85961175 1.1105874  1.3417034
 1.5991477  1.4233305  0.8721653  0.6187212  0.43000796 0.5250519
 0.36504722 0.7702756  0.575794   0.5644163  0.3925851  0.2581353
 1.0440153  1.7433641  2.0227647  1.7350527  1.2166451  0.7807063
 0.7047605  0.9163072  1.0768924  1.1204282  1.1529346  0.85835254
 0.5688748  0.5359531  0.67575204 0.65916175 0.7140532  0.5465123
 0.20731552 0.7304874  1.1554528  1.347413   1.4509473  1.6852292
 1.9801939  2.0850909  2.0213675  1.8543816  1.4975893  1.2601794
 1.083659   0.8885499  0.9361466  0.8565939  0.9474812  0.70096856
 0.7309416  0.687973   0.23097984 0.1522128  0.3323596  0.86924964
 1.0141476  0.9909216  0.6604404  0.34499636 0.2160062  0.3190864
 0.6003838  0.8760797  1.066172   1.2143887 ]
