time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 97.46%, model saved.
Epoch: 0 Train: 60354.27734 Test: 3952.15283
Epoch: 100 Train: 12509.86914 Test: 1084.92944
Epoch 200: New minimal relative error: 69.27%, model saved.
Epoch: 200 Train: 11549.92676 Test: 886.68475
Epoch: 300 Train: 13473.52734 Test: 1239.02942
Epoch 400: New minimal relative error: 62.23%, model saved.
Epoch: 400 Train: 10813.63672 Test: 927.62531
Epoch 500: New minimal relative error: 61.46%, model saved.
Epoch: 500 Train: 13807.40918 Test: 1089.86987
Epoch: 600 Train: 10794.87695 Test: 971.90051
Epoch: 700 Train: 10374.95312 Test: 915.94574
Epoch 800: New minimal relative error: 59.09%, model saved.
Epoch: 800 Train: 10390.81250 Test: 766.21228
Epoch: 900 Train: 8234.41406 Test: 606.01752
Epoch: 1000 Train: 7077.25146 Test: 407.63129
Epoch: 1100 Train: 5988.00342 Test: 309.29865
Epoch: 1200 Train: 4394.05078 Test: 236.97200
Epoch: 1300 Train: 2672.98315 Test: 231.39859
Epoch: 1400 Train: 1301.77112 Test: 56.55650
Epoch 1500: New minimal relative error: 44.42%, model saved.
Epoch: 1500 Train: 1105.48792 Test: 30.76818
Epoch 1600: New minimal relative error: 18.16%, model saved.
Epoch: 1600 Train: 816.11902 Test: 16.18275
Epoch 1700: New minimal relative error: 9.97%, model saved.
Epoch: 1700 Train: 651.10547 Test: 7.83450
Epoch: 1800 Train: 600.02167 Test: 8.45674
Epoch 1900: New minimal relative error: 9.16%, model saved.
Epoch: 1900 Train: 514.51678 Test: 7.43191
Epoch: 2000 Train: 452.49213 Test: 22.54570
Epoch: 2100 Train: 427.53403 Test: 25.56127
Epoch 2200: New minimal relative error: 6.10%, model saved.
Epoch: 2200 Train: 385.05197 Test: 5.02921
Epoch: 2300 Train: 335.21072 Test: 3.72613
Epoch: 2400 Train: 471.06323 Test: 21.22955
Epoch: 2500 Train: 293.98718 Test: 2.29489
Epoch: 2600 Train: 294.61237 Test: 11.04432
Epoch: 2700 Train: 254.72130 Test: 4.19703
Epoch: 2800 Train: 257.76056 Test: 8.01160
Epoch: 2900 Train: 281.05884 Test: 7.02065
Epoch: 3000 Train: 238.25710 Test: 2.30786
Epoch: 3100 Train: 219.42174 Test: 1.63713
Epoch: 3200 Train: 226.27769 Test: 1.63674
Epoch: 3300 Train: 219.07602 Test: 1.58091
Epoch: 3400 Train: 197.10129 Test: 1.81453
Epoch: 3500 Train: 192.42966 Test: 1.12272
Epoch: 3600 Train: 190.11990 Test: 1.12503
Epoch: 3700 Train: 198.21416 Test: 5.01589
Epoch: 3800 Train: 216.65343 Test: 5.36208
Epoch: 3900 Train: 175.52533 Test: 1.17499
Epoch: 4000 Train: 167.13454 Test: 1.28977
Epoch: 4100 Train: 196.67958 Test: 3.80520
Epoch: 4200 Train: 167.45674 Test: 2.11996
Epoch: 4300 Train: 168.11093 Test: 0.86627
Epoch: 4400 Train: 152.97778 Test: 2.51225
Epoch: 4500 Train: 177.64110 Test: 22.28285
Epoch: 4600 Train: 140.76302 Test: 0.84126
Epoch: 4700 Train: 147.23042 Test: 2.36907
Epoch 4800: New minimal relative error: 3.80%, model saved.
Epoch: 4800 Train: 131.22928 Test: 0.52056
Epoch: 4900 Train: 136.97504 Test: 0.71959
Epoch: 5000 Train: 140.51445 Test: 1.03588
Epoch: 5100 Train: 144.20648 Test: 16.04571
Epoch: 5200 Train: 155.81726 Test: 3.34776
Epoch: 5300 Train: 137.31657 Test: 1.06874
Epoch: 5400 Train: 160.65521 Test: 1.13308
Epoch: 5500 Train: 147.32396 Test: 0.80668
Epoch: 5600 Train: 154.36740 Test: 3.97804
Epoch: 5700 Train: 146.44205 Test: 1.71637
Epoch: 5800 Train: 135.20924 Test: 1.08849
Epoch: 5900 Train: 147.98987 Test: 1.18407
Epoch: 6000 Train: 129.05301 Test: 0.71789
Epoch: 6100 Train: 127.60485 Test: 3.94788
Epoch: 6200 Train: 110.39374 Test: 0.79578
Epoch 6300: New minimal relative error: 3.42%, model saved.
Epoch: 6300 Train: 107.57584 Test: 0.47533
Epoch: 6400 Train: 124.55074 Test: 3.69323
Epoch: 6500 Train: 135.89107 Test: 0.93063
Epoch: 6600 Train: 128.68462 Test: 0.84718
Epoch: 6700 Train: 117.82019 Test: 0.66300
Epoch: 6800 Train: 114.67240 Test: 0.65774
Epoch: 6900 Train: 122.31376 Test: 0.74109
Epoch: 7000 Train: 116.22108 Test: 0.64578
Epoch: 7100 Train: 106.68015 Test: 0.64558
Epoch: 7200 Train: 111.18935 Test: 1.04468
Epoch: 7300 Train: 100.33688 Test: 0.45111
Epoch: 7400 Train: 98.67265 Test: 0.52633
Epoch: 7500 Train: 92.56325 Test: 0.38708
Epoch: 7600 Train: 93.66282 Test: 0.39190
Epoch: 7700 Train: 95.24570 Test: 0.44079
Epoch: 7800 Train: 109.91542 Test: 0.68146
Epoch: 7900 Train: 98.79678 Test: 0.61836
Epoch: 8000 Train: 85.48263 Test: 0.31177
Epoch: 8100 Train: 84.97002 Test: 0.42275
Epoch: 8200 Train: 84.13995 Test: 0.34179
Epoch: 8300 Train: 83.11856 Test: 0.32801
Epoch: 8400 Train: 81.20367 Test: 0.43227
Epoch 8500: New minimal relative error: 3.20%, model saved.
Epoch: 8500 Train: 83.47261 Test: 0.38149
Epoch: 8600 Train: 84.84505 Test: 0.45455
Epoch: 8700 Train: 75.23081 Test: 0.69899
Epoch: 8800 Train: 72.60154 Test: 0.26876
Epoch: 8900 Train: 71.13447 Test: 0.35675
Epoch: 9000 Train: 71.91873 Test: 0.23912
Epoch: 9100 Train: 77.32255 Test: 0.32898
Epoch: 9200 Train: 75.06438 Test: 0.35275
Epoch: 9300 Train: 81.19741 Test: 0.37232
Epoch: 9400 Train: 76.97025 Test: 0.33284
Epoch: 9500 Train: 73.86251 Test: 0.27695
Epoch: 9600 Train: 81.21390 Test: 0.94722
Epoch 9700: New minimal relative error: 2.35%, model saved.
Epoch: 9700 Train: 70.29339 Test: 0.31165
Epoch: 9800 Train: 109.32858 Test: 0.56082
Epoch: 9900 Train: 85.46965 Test: 0.47282
Epoch: 9999 Train: 136.45006 Test: 1.00950
Training Loss: tensor(136.4501)
Test Loss: tensor(1.0095)
Learned LE: [  0.8494847    0.03356238 -14.535202  ]
True LE: [ 8.6866802e-01  1.0186301e-02 -1.4557983e+01]
Relative Error: [0.18991928 0.36480033 0.5802851  0.8536259  0.9402451  1.2181892
 1.6228693  2.1011245  2.5376182  3.1393387  3.362425   3.2185893
 2.9499726  2.6500833  2.4574375  2.3844798  2.335021   2.2252634
 2.2776625  2.5847206  3.0199046  3.6037061  4.0783257  4.2882404
 4.494304   4.7467804  5.013643   5.0247006  4.8162093  4.5664587
 4.2536006  3.904359   3.5976164  3.359953   3.0897686  2.7360675
 2.3837748  2.1156104  2.068612   1.8850194  2.100434   2.4037623
 2.6823456  2.8585665  3.0330126  2.9788158  2.808322   2.6600459
 2.5229146  2.4733255  2.1867697  1.8597087  1.7211232  1.701065
 1.4529274  1.1530144  0.9244606  0.7990731  0.6513222  0.51507753
 0.29333836 0.25437587 0.4020223  0.61731535 0.85724014 1.0564697
 1.0997404  1.2863146  1.5157286  1.781354   2.1885343  2.8571634
 3.221674   3.2854445  2.9863992  2.634166   2.4254155  2.4112039
 2.3813024  2.1665518  2.2165813  2.5053132  2.9606228  3.5724916
 3.9412146  4.1458607  4.3618345  4.624791   4.8749723  4.8119307
 4.5977917  4.388487   4.105875   3.7668293  3.4234278  3.1574662
 2.872895   2.5232413  2.144384   1.9160511  1.8396941  1.6947392
 1.8938668  2.1425006  2.4058447  2.5891874  2.8587973  2.8796427
 2.7307131  2.782451   2.609456   2.5317843  2.2633915  1.9709114
 1.8517743  1.7606499  1.5452114  1.2207762  1.0018278  0.83655566
 0.6838891  0.48251334 0.31463656 0.4413804  0.59178007 0.7884981
 1.1195842  1.2330846  1.3702288  1.3001242  1.3018297  1.5583653
 1.9536339  2.613038   3.049827   3.1997676  3.083884   2.6836052
 2.4432485  2.340724   2.2572644  2.1787677  2.1668596  2.4228244
 2.888399   3.478165   3.7610059  3.9836726  4.230262   4.4752297
 4.630808   4.4924474  4.3175178  4.1544623  3.9015603  3.589404
 3.2964814  2.946819   2.656971   2.3150532  1.9260383  1.7538186
 1.622365   1.4800262  1.6980561  1.9086527  2.151041   2.363088
 2.6168256  2.7735581  2.7936785  2.910743   2.6695967  2.5515764
 2.3721478  2.0810025  1.9180443  1.7970642  1.6167183  1.2964386
 1.0758026  0.92715096 0.7166195  0.40873337 0.35111403 0.5146884
 0.69688    0.9179457  1.200159   1.4346     1.4296533  1.3420303
 1.2637013  1.4478092  1.8708225  2.4083648  2.8540814  3.0269747
 3.1002045  2.8559363  2.5266805  2.2953277  2.1202753  2.046709
 2.1718183  2.329994   2.787643   3.370444   3.5947368  3.779318
 3.9977412  4.294277   4.439899   4.260479   4.03265    3.8516588
 3.5917082  3.377052   3.1017892  2.7584605  2.414089   2.047608
 1.7059783  1.5653421  1.419595   1.2998486  1.4758372  1.7092016
 1.9062659  2.2272348  2.419386   2.7277968  2.8490415  2.9772635
 2.7730794  2.5561678  2.4761188  2.1656823  1.9417332  1.8420504
 1.6694298  1.3833193  1.1464094  0.95006496 0.6335279  0.30449942
 0.26523554 0.51245624 0.71831506 1.0017687  1.2288275  1.4866463
 1.4842993  1.4552768  1.4157295  1.378861   1.5672038  1.9720944
 2.4555068  2.7025871  2.8552828  3.0152957  2.720707   2.3535955
 2.0620291  1.9545748  2.0076706  2.2633033  2.6839519  3.2017853
 3.3549056  3.526796   3.8013167  4.1361847  4.2763424  4.0784464
 3.8839188  3.615975   3.321929   3.1315536  2.894522   2.5884721
 2.2028782  1.778998   1.4365059  1.2898927  1.163626   1.1736962
 1.3050771  1.527802   1.6641289  2.042792   2.2030478  2.5752335
 2.8612602  2.973737   2.860649   2.5862608  2.4716396  2.2270572
 1.9850681  1.8430628  1.7808884  1.4827585  1.2281426  0.9874033
 0.5481706  0.23036611 0.1822804  0.42418745 0.70208365 1.0070965
 1.2583262  1.4079672  1.5299591  1.5442289  1.4970652  1.372544
 1.4003433  1.6774611  2.2050035  2.3746762  2.4595451  2.6231217
 2.7672875  2.4609635  2.0936298  1.9466219  1.8799742  2.141498
 2.5596077  2.992649   3.130987   3.2823224  3.587126   3.9794226
 4.1526394  3.9848678  3.7444584  3.5098474  3.1888497  3.0124772
 2.7890098  2.457111   2.060092   1.5984039  1.2184924  0.97296983
 0.7878638  0.9854775  1.2032584  1.3068453  1.45294    1.8108817
 2.0321937  2.4089677  2.819355   2.8599317  2.9473162  2.6556559
 2.4560673  2.3583877  2.1074998  1.8778292  1.7980275  1.5325296
 1.2685626  0.9366599  0.5482694  0.28229418 0.09355413 0.31413722
 0.66283184 1.0254421  1.2472317  1.3446298  1.4949056  1.5380766
 1.5315075  1.5107942  1.3953509  1.5489873  1.8064564  2.1099885
 2.1522057  2.2750037  2.4273252  2.5105338  2.148842   1.9497517
 1.8431809  2.016773   2.3235624  2.7585344  2.9157715  3.0496495
 3.378135   3.7961252  4.0119696  3.8791049 ]
