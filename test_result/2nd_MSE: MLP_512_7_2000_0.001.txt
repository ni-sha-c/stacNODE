time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.99%, model saved.
Epoch: 0 Train: 3733.06274 Test: 4094.26978
Epoch 100: New minimal relative error: 59.47%, model saved.
Epoch: 100 Train: 66.18434 Test: 90.47591
Epoch 200: New minimal relative error: 26.37%, model saved.
Epoch: 200 Train: 7.13259 Test: 27.31471
Epoch: 300 Train: 5.96122 Test: 25.45895
Epoch: 400 Train: 1.28621 Test: 16.61409
Epoch: 500 Train: 0.82627 Test: 17.55779
Epoch: 600 Train: 2.76475 Test: 19.74206
Epoch: 700 Train: 1.73890 Test: 19.71067
Epoch: 800 Train: 2.73086 Test: 21.91008
Epoch: 900 Train: 2.86493 Test: 22.61177
Epoch: 1000 Train: 2.39034 Test: 23.53522
Epoch: 1100 Train: 1.19305 Test: 24.52051
Epoch: 1200 Train: 4.12442 Test: 25.85441
Epoch 1300: New minimal relative error: 8.51%, model saved.
Epoch: 1300 Train: 0.17883 Test: 25.58929
Epoch: 1400 Train: 0.80194 Test: 26.54779
Epoch: 1500 Train: 0.32418 Test: 28.23977
Epoch: 1600 Train: 0.61133 Test: 30.74443
Epoch: 1700 Train: 0.64182 Test: 30.47929
Epoch: 1800 Train: 1.69509 Test: 36.67947
Epoch: 1900 Train: 5.27886 Test: 42.47691
Epoch: 2000 Train: 2.89494 Test: 40.24668
Epoch: 2100 Train: 0.33437 Test: 39.72313
Epoch: 2200 Train: 0.35484 Test: 41.90849
Epoch: 2300 Train: 1.20918 Test: 45.85004
Epoch: 2400 Train: 0.61677 Test: 45.91331
Epoch: 2500 Train: 0.16581 Test: 47.05248
Epoch: 2600 Train: 0.18714 Test: 49.69437
Epoch: 2700 Train: 0.68921 Test: 51.94529
Epoch: 2800 Train: 2.39349 Test: 55.84393
Epoch: 2900 Train: 1.31724 Test: 54.46350
Epoch: 3000 Train: 0.78776 Test: 58.44433
Epoch: 3100 Train: 1.01904 Test: 61.63221
Epoch: 3200 Train: 0.07502 Test: 62.25334
Epoch: 3300 Train: 0.57522 Test: 65.15675
Epoch: 3400 Train: 0.40951 Test: 66.22144
Epoch: 3500 Train: 1.47450 Test: 70.59605
Epoch: 3600 Train: 0.19650 Test: 72.01083
Epoch: 3700 Train: 0.62491 Test: 73.59411
Epoch: 3800 Train: 0.05522 Test: 75.33582
Epoch: 3900 Train: 0.97738 Test: 78.60056
Epoch: 4000 Train: 0.90345 Test: 79.53709
Epoch: 4100 Train: 1.34352 Test: 81.31912
Epoch: 4200 Train: 0.36820 Test: 81.88924
Epoch: 4300 Train: 0.27328 Test: 86.25937
Epoch: 4400 Train: 1.59474 Test: 88.89722
Epoch: 4500 Train: 1.42365 Test: 90.02323
Epoch: 4600 Train: 0.07446 Test: 89.76960
Epoch: 4700 Train: 0.09709 Test: 94.48249
Epoch: 4800 Train: 1.18512 Test: 97.03818
Epoch: 4900 Train: 0.14168 Test: 97.02941
Epoch: 5000 Train: 0.65259 Test: 101.47098
Epoch: 5100 Train: 0.05422 Test: 100.72822
Epoch: 5200 Train: 0.58369 Test: 102.50925
Epoch: 5300 Train: 0.07841 Test: 103.37975
Epoch: 5400 Train: 0.22664 Test: 105.56870
Epoch: 5500 Train: 0.70512 Test: 107.68779
Epoch: 5600 Train: 0.07564 Test: 109.31042
Epoch: 5700 Train: 0.15698 Test: 111.38324
Epoch: 5800 Train: 0.53745 Test: 112.85292
Epoch: 5900 Train: 0.10572 Test: 116.16276
Epoch: 6000 Train: 0.52174 Test: 117.53542
Epoch: 6100 Train: 0.18797 Test: 118.56394
Epoch: 6200 Train: 0.14941 Test: 121.39568
Epoch: 6300 Train: 0.24342 Test: 121.22128
Epoch: 6400 Train: 0.01563 Test: 123.64363
Epoch: 6500 Train: 0.04522 Test: 125.62505
Epoch: 6600 Train: 0.07950 Test: 127.43861
Epoch: 6700 Train: 0.03673 Test: 128.43529
Epoch: 6800 Train: 0.06674 Test: 129.37143
Epoch: 6900 Train: 0.06418 Test: 130.92319
Epoch: 7000 Train: 0.11200 Test: 133.95445
Epoch: 7100 Train: 0.04626 Test: 134.48274
Epoch: 7200 Train: 0.13184 Test: 137.50687
Epoch: 7300 Train: 0.03196 Test: 138.48006
Epoch: 7400 Train: 0.01875 Test: 139.50786
Epoch: 7500 Train: 0.30889 Test: 138.22540
Epoch: 7600 Train: 0.03540 Test: 142.48524
Epoch: 7700 Train: 0.19446 Test: 144.34525
Epoch: 7800 Train: 0.11875 Test: 143.65190
Epoch: 7900 Train: 0.01606 Test: 147.09966
Epoch: 8000 Train: 0.29068 Test: 147.87608
Epoch: 8100 Train: 0.31257 Test: 147.45438
Epoch: 8200 Train: 0.07481 Test: 150.06358
Epoch: 8300 Train: 0.18134 Test: 151.72751
Epoch: 8400 Train: 0.53296 Test: 152.12877
Epoch: 8500 Train: 0.13312 Test: 154.49059
Epoch: 8600 Train: 0.03329 Test: 155.03850
Epoch: 8700 Train: 0.44833 Test: 156.78806
Epoch: 8800 Train: 0.34395 Test: 157.41139
Epoch: 8900 Train: 0.05898 Test: 159.06264
Epoch: 9000 Train: 0.04714 Test: 160.98929
Epoch: 9100 Train: 0.01163 Test: 161.52802
Epoch: 9200 Train: 0.02235 Test: 161.93343
Epoch: 9300 Train: 0.04992 Test: 163.59607
Epoch: 9400 Train: 0.53643 Test: 163.72192
Epoch: 9500 Train: 0.05263 Test: 165.08710
Epoch: 9600 Train: 0.04333 Test: 166.55565
Epoch: 9700 Train: 0.00404 Test: 167.53160
Epoch: 9800 Train: 0.04075 Test: 168.38211
Epoch: 9900 Train: 0.03812 Test: 169.19864
Epoch: 9999 Train: 0.00428 Test: 170.55878
Training Loss: tensor(0.0043)
Test Loss: tensor(170.5588)
Learned LE: [ 0.8677141  -0.04271962 -4.42718   ]
True LE: [ 8.5315758e-01  5.5142930e-03 -1.4527967e+01]
Relative Error: [1.1751727  1.3224599  1.4706298  1.5913593  1.6578013  1.6536965
 1.5762236  1.438641   1.294291   1.2827301  1.5643548  2.1229773
 2.8373103  3.6080585  4.344869   4.9452634  5.3286195  5.48083
 5.4489775  5.305266   5.117007   4.936119   4.7948813  4.7054334
 4.6634607  4.6587806  4.685824   4.7458267  4.8422666  4.9756374
 5.1378145  5.3053985  5.440467   5.505734   5.479078   5.3451877
 5.0933423  4.7552004  4.4088316  4.1062403  3.8268163  3.5138144
 3.1293414  2.6766262  2.1941414  1.7396997  1.376434   1.1563572
 1.0894223  1.1255629  1.1944873  1.2462482  1.2564201  1.2171773
 1.1309553  1.0088532  0.8711093  0.74693894 0.66959655 0.66073185
 0.71898514 0.83248717 0.99178714 1.1839561  1.3840503  1.5581275
 1.6763194  1.7240064  1.6995275  1.6081532  1.4747307  1.3870777
 1.5081819  1.9205438  2.5410104  3.2546911  3.9506202  4.505514
 4.8306355  4.9222517  4.8434744  4.673114   4.4745913  4.290893
 4.1485524  4.058464   4.0151215  4.0051556  4.0196986  4.059702
 4.1315055  4.241565   4.3913355  4.567746   4.7342105  4.843866
 4.868028   4.793537   4.5996203  4.2951145  3.9633653  3.6857028
 3.453254   3.1995568  2.8751454  2.4781427  2.0465133  1.6401182
 1.3244218  1.1474466  1.1066368  1.1466055  1.2037047  1.2365634
 1.2261953  1.1672909  1.0614433  0.91615945 0.74751884 0.58278674
 0.4608741  0.42145056 0.47415063 0.6001331  0.78582287 1.0170847
 1.2651408  1.4904119  1.6592714  1.7575688  1.785984   1.7455099
 1.6414933  1.5200528  1.5128165  1.7666351  2.2732882  2.91821
 3.5683367  4.0764027  4.3439584  4.3791127  4.261678   4.0771666
 3.8833048  3.7098918  3.5731237  3.482369   3.4353797  3.4184217
 3.4192972  3.4365795  3.4772203  3.551181   3.6691434  3.83352
 4.019127   4.1713104  4.245466   4.230611   4.1090574  3.857702
 3.5417483  3.2793074  3.0910726  2.9029188  2.648451   2.315596
 1.9377275  1.573412   1.2900357  1.1359494  1.1051555  1.1422408
 1.1878872  1.2055581  1.1809117  1.1115519  0.99835926 0.8449882
 0.661832   0.46978194 0.30035147 0.20084427 0.22573031 0.35292447
 0.55304784 0.81229836 1.1006566  1.3738557  1.5926458  1.7410877
 1.8219514  1.8355435  1.7752815  1.654787   1.5618113  1.6612496
 2.034397   2.596521   3.1963387  3.6614225  3.8804162  3.869511
 3.7217445  3.5291712  3.347031   3.1937025  3.0707836  2.9809983
 2.9269223  2.9009237  2.888995   2.8852916  2.8948362  2.9274035
 2.9971528  3.1218994  3.300846   3.4863875  3.6103148  3.6488354
 3.6033034  3.4332135  3.1467683  2.8841665  2.7276888  2.6077738
 2.4337733  2.1760905  1.8579189  1.5315527  1.2654614  1.115284
 1.0811343  1.1110381  1.1469142  1.1532997  1.1193627  1.046932
 0.9381523  0.79354495 0.6189706  0.4328831  0.2658698  0.14688209
 0.08273074 0.12604994 0.30466175 0.56962717 0.88323355 1.196997
 1.4642901  1.6628636  1.7953947  1.8643674  1.8585013  1.7684368
 1.6351613  1.6006114  1.8210801  2.281314   2.8261516  3.2626786
 3.4629793  3.4359138  3.2725432  3.0690813  2.8879375  2.7479362
 2.6406276  2.5557811  2.4928992  2.4532964  2.4291587  2.409826
 2.394829   2.3907692  2.4080284  2.469104   2.6013038  2.7912624
 2.962445   3.0503306  3.0651727  2.9962459  2.7807884  2.5088074
 2.3544028  2.2955627  2.2114322  2.0429716  1.7965496  1.5115234
 1.2530235  1.0894253  1.038451   1.0566893  1.084892   1.084004
 1.0440446  0.97227013 0.87546194 0.7536657  0.60925    0.46253935
 0.3555372  0.31307665 0.28929025 0.22570992 0.16265342 0.31873935
 0.62004006 0.95620817 1.2654679  1.5129474  1.6957333  1.8187115
 1.8729405  1.8378247  1.7128291  1.5840762  1.6363032  1.9629002
 2.4419692  2.8754282  3.116007   3.1305664  2.9802642  2.7614272
 2.5552802  2.3982513  2.2866483  2.2037263  2.1377447  2.0829656
 2.0414484  2.0100849  1.9802343  1.9522372  1.9295928  1.9243859
 1.9723481  2.1093473  2.3008394  2.4444711  2.5007958  2.5107982
 2.4226084  2.1787672  1.9795429  1.9494891  1.9577625  1.8936995
 1.7377384  1.51       1.2616739  1.0717764  0.98758954 0.98550075
 1.0072415  1.0046597  0.96190834 0.89014304 0.8051862  0.7119344
 0.60979426 0.5142215  0.46349823 0.47113505 0.48816985 0.4566339
 0.35450447 0.24419051 0.36400327 0.668057   0.99748635 1.2863007
 1.5155107  1.688236   1.8007574  1.8345554  1.7657776  1.6112279
 1.50633    1.6474634  2.0259743  2.4698431  2.8217843  2.9592364
 2.8699126  2.645269   2.3961506  2.189554   2.038769   1.927285
 1.8461678  1.793568   1.7431164  1.6919749 ]
