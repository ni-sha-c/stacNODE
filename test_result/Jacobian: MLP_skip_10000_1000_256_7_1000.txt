time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.31%, model saved.
Epoch: 0 Train: 60430.35156 Test: 3856.01587
Epoch 100: New minimal relative error: 91.32%, model saved.
Epoch: 100 Train: 17153.10156 Test: 1473.56641
Epoch: 200 Train: 16457.83594 Test: 1362.20740
Epoch: 300 Train: 17349.31641 Test: 2127.14209
Epoch 400: New minimal relative error: 61.89%, model saved.
Epoch: 400 Train: 13910.15625 Test: 1199.62524
Epoch: 500 Train: 15283.50781 Test: 1279.48804
Epoch: 600 Train: 14576.87207 Test: 1361.56189
Epoch: 700 Train: 14911.09961 Test: 1200.59070
Epoch: 800 Train: 14855.08594 Test: 1250.31995
Epoch: 900 Train: 14379.54785 Test: 1181.18921
Epoch: 1000 Train: 14375.82422 Test: 1306.80139
Epoch: 1100 Train: 13256.42969 Test: 1039.63489
Epoch: 1200 Train: 10354.22852 Test: 742.04572
Epoch 1300: New minimal relative error: 47.52%, model saved.
Epoch: 1300 Train: 10862.03711 Test: 791.52570
Epoch: 1400 Train: 9564.98535 Test: 523.76434
Epoch: 1500 Train: 7540.86621 Test: 389.58096
Epoch: 1600 Train: 5681.31934 Test: 218.48010
Epoch: 1700 Train: 4102.41602 Test: 126.99389
Epoch: 1800 Train: 2113.18701 Test: 65.87309
Epoch 1900: New minimal relative error: 18.98%, model saved.
Epoch: 1900 Train: 1704.20020 Test: 32.53728
Epoch: 2000 Train: 1494.56396 Test: 29.20872
Epoch 2100: New minimal relative error: 12.30%, model saved.
Epoch: 2100 Train: 1117.94849 Test: 17.56401
Epoch: 2200 Train: 913.84833 Test: 13.46529
Epoch 2300: New minimal relative error: 12.27%, model saved.
Epoch: 2300 Train: 843.73633 Test: 9.90140
Epoch: 2400 Train: 668.73999 Test: 7.57754
Epoch: 2500 Train: 621.33038 Test: 6.69133
Epoch: 2600 Train: 532.31958 Test: 5.83906
Epoch: 2700 Train: 504.76346 Test: 8.94043
Epoch: 2800 Train: 458.07263 Test: 9.90405
Epoch: 2900 Train: 392.45932 Test: 4.03029
Epoch: 3000 Train: 336.04028 Test: 3.19930
Epoch: 3100 Train: 321.99438 Test: 2.73492
Epoch: 3200 Train: 296.64490 Test: 2.27090
Epoch: 3300 Train: 325.96790 Test: 4.08388
Epoch: 3400 Train: 274.47763 Test: 2.53664
Epoch 3500: New minimal relative error: 10.32%, model saved.
Epoch: 3500 Train: 285.81171 Test: 3.43856
Epoch 3600: New minimal relative error: 7.05%, model saved.
Epoch: 3600 Train: 240.15904 Test: 2.29113
Epoch: 3700 Train: 227.20108 Test: 1.73074
Epoch: 3800 Train: 231.56715 Test: 1.85912
Epoch: 3900 Train: 266.15448 Test: 2.80873
Epoch: 4000 Train: 237.25774 Test: 1.73338
Epoch: 4100 Train: 213.95978 Test: 2.13433
Epoch 4200: New minimal relative error: 6.80%, model saved.
Epoch: 4200 Train: 214.30850 Test: 1.70765
Epoch: 4300 Train: 207.70090 Test: 1.52120
Epoch: 4400 Train: 179.83670 Test: 1.15981
Epoch: 4500 Train: 190.49814 Test: 1.25871
Epoch: 4600 Train: 202.25688 Test: 2.44674
Epoch: 4700 Train: 209.20921 Test: 2.13863
Epoch: 4800 Train: 192.69481 Test: 1.72415
Epoch 4900: New minimal relative error: 4.07%, model saved.
Epoch: 4900 Train: 189.34875 Test: 1.45450
Epoch: 5000 Train: 167.52954 Test: 1.14299
Epoch: 5100 Train: 159.67674 Test: 0.97773
Epoch: 5200 Train: 158.35732 Test: 0.99753
Epoch: 5300 Train: 153.23917 Test: 1.20789
Epoch: 5400 Train: 149.56798 Test: 1.35313
Epoch: 5500 Train: 140.71828 Test: 0.86485
Epoch: 5600 Train: 154.06543 Test: 1.76493
Epoch: 5700 Train: 138.61336 Test: 0.83142
Epoch: 5800 Train: 149.27248 Test: 1.92777
Epoch: 5900 Train: 126.14417 Test: 0.69692
Epoch: 6000 Train: 127.45405 Test: 0.91382
Epoch: 6100 Train: 118.92938 Test: 0.68277
Epoch: 6200 Train: 119.02858 Test: 0.73019
Epoch: 6300 Train: 115.96700 Test: 0.63270
Epoch: 6400 Train: 115.01339 Test: 0.65639
Epoch: 6500 Train: 124.32313 Test: 0.86983
Epoch: 6600 Train: 123.91489 Test: 1.02607
Epoch: 6700 Train: 135.08173 Test: 7.79940
Epoch: 6800 Train: 116.47771 Test: 0.63287
Epoch: 6900 Train: 113.43793 Test: 0.68315
Epoch: 7000 Train: 112.52291 Test: 0.63125
Epoch: 7100 Train: 116.38056 Test: 0.75676
Epoch: 7200 Train: 116.61369 Test: 0.69809
Epoch: 7300 Train: 113.18536 Test: 0.88641
Epoch: 7400 Train: 108.79819 Test: 0.61557
Epoch: 7500 Train: 107.64983 Test: 0.67406
Epoch: 7600 Train: 108.13580 Test: 1.83897
Epoch: 7700 Train: 102.79167 Test: 1.55476
Epoch: 7800 Train: 102.14060 Test: 0.55135
Epoch: 7900 Train: 105.18304 Test: 1.45641
Epoch: 8000 Train: 97.26318 Test: 0.58968
Epoch: 8100 Train: 97.21844 Test: 0.50413
Epoch: 8200 Train: 96.21920 Test: 0.64963
Epoch 8300: New minimal relative error: 2.17%, model saved.
Epoch: 8300 Train: 93.81766 Test: 0.50831
Epoch: 8400 Train: 88.85895 Test: 0.55884
Epoch: 8500 Train: 86.43095 Test: 0.42828
Epoch: 8600 Train: 81.95788 Test: 0.36803
Epoch: 8700 Train: 83.57458 Test: 0.38061
Epoch: 8800 Train: 85.27270 Test: 0.38813
Epoch: 8900 Train: 90.12486 Test: 0.38825
Epoch: 9000 Train: 87.54144 Test: 0.35187
Epoch: 9100 Train: 84.32007 Test: 0.39026
Epoch: 9200 Train: 87.91833 Test: 0.45619
Epoch: 9300 Train: 83.96654 Test: 0.41834
Epoch: 9400 Train: 82.78627 Test: 0.41027
Epoch: 9500 Train: 85.69230 Test: 0.47579
Epoch: 9600 Train: 83.34689 Test: 0.52013
Epoch: 9700 Train: 79.18933 Test: 0.45854
Epoch: 9800 Train: 82.38368 Test: 0.80546
Epoch: 9900 Train: 79.70332 Test: 0.39243
Epoch: 9999 Train: 79.72906 Test: 0.32524
Training Loss: tensor(79.7291)
Test Loss: tensor(0.3252)
Learned LE: [  0.83990145   0.02194031 -14.52643   ]
True LE: [ 8.70241880e-01  3.42352944e-03 -1.45431595e+01]
Relative Error: [12.298965  13.131296  13.87505   14.547159  14.670282  14.596266
 14.24836   13.935775  13.249536  12.834501  12.440348  12.283138
 12.3736315 12.404941  12.733708  13.16503   12.706221  12.500072
 11.790536  11.293355  10.977646  10.854663  10.912341  11.193052
 11.589621  11.970921  12.448868  12.880535  12.986776  13.032942
 13.179965  13.220189  13.001886  12.533554  12.085985  11.666562
 11.148775  10.712486  10.35954    9.699189   9.174318   8.891273
  8.883486   8.11074    7.0305     5.8138156  4.9734583  4.482281
  4.2679276  4.2450643  4.2431073  4.264603   4.4004984  4.5685296
  4.8282766  5.295336   6.0010204  6.882207   7.8706136  8.89328
  9.924087  10.916168  11.811693  12.568994  13.095577  13.614617
 13.876279  13.766386  13.464858  13.217895  12.635162  12.129166
 11.674946  11.468448  11.503851  11.537628  11.849496  12.401854
 11.928001  11.703043  10.984224  10.495326  10.225866  10.102685
 10.184229  10.423123  10.777179  11.203698  11.37266   11.351206
 11.342497  11.390347  11.523617  11.603356  11.568928  11.661098
 11.422282  10.955697  10.40243    9.890554   9.458683   8.754564
  8.304305   7.996629   8.014231   7.2977715  6.1953144  5.088705
  4.4082785  4.0112424  3.8689885  3.9055336  3.9088159  3.9317596
  4.0782213  4.207665   4.408919   4.8602896  5.5219946  6.388189
  7.4139304  8.480275   9.521394  10.539166  11.120482  11.653547
 12.120556  12.5753    13.004114  12.821035  12.42445   12.212244
 11.995311  11.529039  10.990927  10.712422  10.719795  10.709256
 10.939344  11.416784  11.066907  10.925481  10.2416725  9.764327
  9.456766   9.356916   9.4756975  9.770777  10.0104    10.116961
  9.938814   9.927876   9.8791485  9.931872  10.060414  10.193381
 10.198813  10.327197  10.446763  10.375478   9.780515   9.183238
  8.684571   7.9047017  7.4154234  7.2363005  7.240334   6.7144356
  5.8186     4.843808   4.1796284  3.8585386  3.813035   3.903228
  3.9317105  3.8015962  3.7855983  3.933235   4.078306   4.478561
  5.1414375  5.995825   6.9911017  8.112128   9.132772   9.779209
 10.3797865 10.87706   11.316926  11.7231    12.095962  12.09375
 11.5767355 11.310262  11.156453  10.745844  10.40841   10.057717
  9.992405   9.684958   9.818771  10.267046  10.058209   9.858147
  9.280905   8.892889   8.738229   8.662202   8.805686   9.158942
  9.502018   8.897114   8.679511   8.575265   8.562079   8.653319
  8.785514   8.8019085  8.865325   8.7777     8.771302   9.031202
  9.04546    8.539999   8.073747   7.1793556  6.6912875  6.5970974
  6.571836   6.429939   5.597079   4.628664   4.0731387  3.8850448
  3.8998451  4.0048714  4.1029134  3.9503727  3.8443758  3.789996
  3.8025439  4.1527457  4.785414   5.6554055  6.6690035  7.7549725
  8.209584   8.589712   9.109753   9.624224  10.138678  10.697996
 11.293713  11.615766  10.968069  10.599909  10.474493   9.967997
  9.50838    9.430722   9.085091   8.75871    8.771336   9.137444
  9.187778   8.895598   8.328057   7.871674   7.790354   8.002795
  8.18259    8.568684   8.680757   8.01278    7.660667   7.445229
  7.4042454  7.47715    7.3748374  7.402306   7.5486197  7.3861175
  7.2753086  7.4140935  7.7780366  7.6870346  7.3115015  6.6905184
  6.1496224  5.9285817  6.134643   6.1133137  5.426655   4.4340315
  4.0435934  3.9562032  4.0019355  4.1054354  4.2234106  4.1189013
  3.9680884  3.8999681  3.7941124  3.850521   4.455225   5.323519
  6.3706346  6.641487   6.821324   7.205804   7.7663755  8.302717
  8.781761   9.27115    9.777902  10.340918  10.4952965 10.083411
  9.900303   9.411652   8.793394   8.314667   8.216204   7.938985
  7.8013105  8.03895    8.442453   8.028815   7.5136704  6.9910674
  6.9115615  7.141184   7.620304   8.078027   8.027052   7.361086
  6.904213   6.5205836  6.479915   6.2775984  6.152685   6.221813
  6.4495997  6.2401066  6.054768   6.05353    6.294926   6.6975756
  6.628361   5.929232   5.4984426  5.450107   5.6116986  5.5656137
  5.3121977  4.3207593  3.959181   3.9833074  4.0813775  4.1759543
  4.2632833  4.2535367  4.074169   3.954219   3.8559365  3.8965385
  4.1459785  5.0214777  5.5082664  5.536145   5.7176847  6.0172615
  6.286358   6.65293    7.0875845  7.518661   8.047667   8.690607
  9.283745   9.284986   9.432292   9.070796   8.324979   7.453684
  7.1146154  6.9455185  6.89188    6.966678   7.449243   7.3061676
  6.941267   6.383027   6.203046   6.3828173  6.796956   7.4945173
  7.544712   7.0087366  6.4736075  5.916993 ]
