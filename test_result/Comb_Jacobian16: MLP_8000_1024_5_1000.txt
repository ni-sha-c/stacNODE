time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.99%, model saved.
Epoch: 0 Train: 59164.26172 Test: 3794.23022
Epoch: 80 Train: 5727.70166 Test: 900.41309
Epoch 160: New minimal relative error: 77.10%, model saved.
Epoch: 160 Train: 829.24359 Test: 125.72321
Epoch 240: New minimal relative error: 59.88%, model saved.
Epoch: 240 Train: 1654.78845 Test: 249.90495
Epoch 320: New minimal relative error: 49.27%, model saved.
Epoch: 320 Train: 194.97569 Test: 16.32779
Epoch 400: New minimal relative error: 30.22%, model saved.
Epoch: 400 Train: 114.92655 Test: 25.57165
Epoch 480: New minimal relative error: 29.90%, model saved.
Epoch: 480 Train: 118.47476 Test: 18.52492
Epoch: 560 Train: 62.08442 Test: 27.81193
Epoch 640: New minimal relative error: 9.03%, model saved.
Epoch: 640 Train: 32.51054 Test: 0.76061
Epoch: 720 Train: 55.16455 Test: 24.80272
Epoch 800: New minimal relative error: 6.76%, model saved.
Epoch: 800 Train: 7.08076 Test: 0.69456
Epoch: 880 Train: 4.56173 Test: 2.40489
Epoch: 960 Train: 2.68605 Test: 1.80485
Epoch: 1040 Train: 26.08417 Test: 1.67073
Epoch: 1120 Train: 56.16073 Test: 8.96517
Epoch: 1200 Train: 49.04639 Test: 11.92106
Epoch: 1280 Train: 27.87049 Test: 8.14732
Epoch: 1360 Train: 12.27910 Test: 0.51821
Epoch: 1440 Train: 34.92791 Test: 9.16818
Epoch: 1520 Train: 30.21947 Test: 9.96403
Epoch: 1600 Train: 14.97411 Test: 2.15863
Epoch 1680: New minimal relative error: 6.52%, model saved.
Epoch: 1680 Train: 10.02693 Test: 2.73530
Epoch: 1760 Train: 37.78917 Test: 6.67561
Epoch: 1840 Train: 8.83332 Test: 1.65005
Epoch 1920: New minimal relative error: 5.07%, model saved.
Epoch: 1920 Train: 8.96813 Test: 0.41249
Epoch: 2000 Train: 37.98004 Test: 8.95798
Epoch: 2080 Train: 5.15135 Test: 1.66458
Epoch 2160: New minimal relative error: 3.00%, model saved.
Epoch: 2160 Train: 1.64810 Test: 0.43827
Epoch: 2240 Train: 10.05666 Test: 0.78979
Epoch: 2320 Train: 11.89485 Test: 2.05299
Epoch: 2400 Train: 3.72888 Test: 0.31588
Epoch: 2480 Train: 6.28057 Test: 1.13994
Epoch: 2560 Train: 1.69874 Test: 0.24089
Epoch: 2640 Train: 1.35365 Test: 0.30756
Epoch: 2720 Train: 33.66287 Test: 2.49732
Epoch 2800: New minimal relative error: 1.24%, model saved.
Epoch: 2800 Train: 0.24395 Test: 0.01236
Epoch: 2880 Train: 9.96880 Test: 1.47590
Epoch: 2960 Train: 10.58515 Test: 2.74450
Epoch: 3040 Train: 2.91232 Test: 1.47147
Epoch: 3120 Train: 0.39041 Test: 0.06069
Epoch: 3200 Train: 3.37331 Test: 0.51650
Epoch: 3280 Train: 14.82734 Test: 3.13815
Epoch: 3360 Train: 18.75244 Test: 1.32532
Epoch: 3440 Train: 0.35232 Test: 0.03018
Epoch: 3520 Train: 1.27213 Test: 0.32907
Epoch: 3600 Train: 3.37303 Test: 0.16608
Epoch: 3680 Train: 1.43003 Test: 0.53041
Epoch: 3760 Train: 13.69428 Test: 2.37208
Epoch: 3840 Train: 0.50003 Test: 0.11069
Epoch: 3920 Train: 11.86561 Test: 3.13278
Epoch: 4000 Train: 1.76593 Test: 0.38989
Epoch: 4080 Train: 7.85090 Test: 0.78187
Epoch: 4160 Train: 0.68763 Test: 0.14811
Epoch: 4240 Train: 2.56674 Test: 0.68121
Epoch: 4320 Train: 7.00848 Test: 1.52176
Epoch: 4400 Train: 2.46927 Test: 0.46095
Epoch: 4480 Train: 0.48331 Test: 0.07861
Epoch: 4560 Train: 0.63014 Test: 0.12681
Epoch: 4640 Train: 0.29158 Test: 0.04507
Epoch: 4720 Train: 0.26738 Test: 0.04714
Epoch: 4800 Train: 0.47185 Test: 0.09544
Epoch: 4880 Train: 1.57086 Test: 0.44086
Epoch: 4960 Train: 6.21953 Test: 1.20032
Epoch: 5040 Train: 13.13003 Test: 3.10356
Epoch: 5120 Train: 4.52442 Test: 0.74159
Epoch: 5200 Train: 1.85634 Test: 0.26721
Epoch: 5280 Train: 9.16662 Test: 2.23130
Epoch: 5360 Train: 3.83150 Test: 0.99793
Epoch: 5440 Train: 1.23769 Test: 0.23282
Epoch: 5520 Train: 3.75583 Test: 1.28720
Epoch: 5600 Train: 0.19653 Test: 0.02804
Epoch: 5680 Train: 0.56314 Test: 0.02504
Epoch: 5760 Train: 2.56380 Test: 0.75105
Epoch: 5840 Train: 2.79300 Test: 0.55503
Epoch: 5920 Train: 1.07847 Test: 0.16729
Epoch: 6000 Train: 0.32368 Test: 0.05711
Epoch: 6080 Train: 3.50342 Test: 1.01733
Epoch: 6160 Train: 2.34951 Test: 0.35307
Epoch: 6240 Train: 4.46160 Test: 0.67040
Epoch: 6320 Train: 2.54771 Test: 0.99723
Epoch: 6400 Train: 0.17158 Test: 0.02731
Epoch: 6480 Train: 0.22206 Test: 0.03145
Epoch: 6560 Train: 3.09071 Test: 0.75081
Epoch: 6640 Train: 4.50324 Test: 1.17780
Epoch: 6720 Train: 0.39666 Test: 0.08291
Epoch: 6800 Train: 0.55927 Test: 0.11723
Epoch: 6880 Train: 0.99594 Test: 0.18789
Epoch: 6960 Train: 0.15380 Test: 0.03299
Epoch 7040: New minimal relative error: 1.11%, model saved.
Epoch: 7040 Train: 0.09653 Test: 0.01099
Epoch: 7120 Train: 0.10070 Test: 0.00505
Epoch: 7200 Train: 0.13699 Test: 0.01940
Epoch: 7280 Train: 0.25194 Test: 0.04384
Epoch: 7360 Train: 0.33372 Test: 0.04190
Epoch: 7440 Train: 0.25306 Test: 0.05739
Epoch: 7520 Train: 1.03137 Test: 0.30087
Epoch: 7600 Train: 0.57229 Test: 0.14901
Epoch: 7680 Train: 2.38496 Test: 0.69208
Epoch: 7760 Train: 1.34239 Test: 0.28772
Epoch: 7840 Train: 4.27171 Test: 0.51299
Epoch: 7920 Train: 0.12695 Test: 0.02544
Epoch: 7999 Train: 0.06011 Test: 0.00587
Training Loss: tensor(0.0601)
Test Loss: tensor(0.0059)
Learned LE: [  0.8909379   -0.02166022 -14.546773  ]
True LE: [ 8.6932725e-01  8.0312770e-03 -1.4548326e+01]
Relative Error: [1.0853126  1.0931515  1.1079462  1.1297823  1.1581115  1.1923193
 1.232293   1.2776575  1.3289194  1.3861444  1.4487547  1.5142026
 1.577623   1.6313579  1.6672572  1.6783204  1.6613698  1.6171372
 1.5507092  1.469932   1.3845961  1.3040366  1.2361925  1.1852952
 1.1517323  1.1322857  1.122278   1.1172119  1.1141993  1.1122783
 1.1121721  1.1154978  1.1242255  1.1393269  1.1619102  1.1917309
 1.2290069  1.2735543  1.3262917  1.3871466  1.4540203  1.5205334
 1.5759716  1.6072999  1.6041772  1.5631847  1.4902722  1.3984721
 1.3014706  1.209855   1.1299205  1.0655266  1.018859   0.9903567
 0.97667694 0.97133565 0.9677188  0.9614338  0.95198786 0.9411836
 0.9317842  0.92629355 0.9265138  0.93321913 0.94656646 0.96589214
 0.9909695  1.0207946  1.0550398  1.0935283  1.1366023  1.1851596
 1.2395129  1.2984314  1.3580806  1.4116452  1.4511178  1.4686898
 1.4601597  1.4252     1.3673879  1.2934244  1.2121152  1.1328943
 1.0648353  1.0132434  0.9795262  0.9604118  0.9510393  0.94618547
 0.9425507  0.9392173  0.93718207 0.9381508  0.9443281  0.9570198
 0.9766357  1.0031772  1.0366118  1.0767679  1.1244329  1.1808186
 1.2449999  1.3114951  1.3695618  1.4056633  1.4074451  1.370592
 1.3012518  1.2134578  1.1220113  1.0368377  0.96317184 0.9041021
 0.8622123  0.83826166 0.82875293 0.8265156  0.8242354  0.81796956
 0.8076784  0.79605496 0.7861006  0.7804442  0.7804273  0.78651726
 0.7985256  0.8158957  0.83770305 0.8633176  0.89201826 0.9236537
 0.95857155 0.9978478  1.0427337  1.0933169  1.1473451  1.1994395
 1.2415423  1.2658795  1.2668246  1.2426775  1.1957694  1.1306589
 1.0546662  0.97722197 0.90803385 0.8543428  0.8189337  0.79925984
 0.79004604 0.7854647  0.78149354 0.7769932  0.77311677 0.77204424
 0.77583075 0.78585863 0.8025502  0.82580245 0.8551758  0.89040965
 0.93250924 0.98330396 1.0432109  1.1086934  1.1695894  1.2112775
 1.2199843  1.1892821  1.1248914  1.0418677  0.95616066 0.87765026
 0.80993336 0.75539875 0.7170565  0.6965348  0.690778   0.6917069
 0.6912358  0.6853658  0.67492765 0.6632253  0.65366006 0.64853865
 0.64883435 0.6547294  0.6656837  0.68100166 0.69989085 0.7214705
 0.74509466 0.77026826 0.7974492  0.8274017  0.8619044  0.9024792
 0.9486353  0.99675703 1.0398481  1.069877   1.0803285  1.0682209
 1.0339632  0.9803997  0.912431   0.83809334 0.7678033  0.7108192
 0.67221916 0.6507268  0.6411745  0.6369199  0.6329676  0.6278328
 0.6225985  0.619653   0.62137556 0.62889403 0.6426837  0.6624607
 0.68755007 0.71746963 0.7531044  0.796703   0.8504081  0.91295505
 0.97550255 1.0234998  1.0410368  1.019014   0.96133155 0.8838607
 0.8041149  0.7321456  0.6704637  0.619948   0.5839993  0.5659647
 0.56364477 0.56825185 0.57017875 0.5655167  0.55567634 0.54462564
 0.53611153 0.53214383 0.53326863 0.5392632  0.549396   0.5628701
 0.57887346 0.5967897  0.6158824  0.63555884 0.6557271  0.6770437
 0.7012871  0.7306286  0.7663258  0.80745274 0.8486204  0.8821355
 0.90085006 0.9004666  0.8801308  0.8406194  0.7841049  0.7160158
 0.64595544 0.5853528  0.5421389  0.51743233 0.5067961  0.5029261
 0.4996501  0.49436185 0.4883091  0.4839509  0.48381588 0.48910373
 0.50012314 0.51643956 0.53720754 0.5616566  0.5901706  0.62502784
 0.6698386  0.7260822  0.7877679  0.84124523 0.8693072  0.8590744
 0.8107574  0.73974794 0.6660854  0.6011553  0.54574585 0.4990322
 0.46454012 0.44788307 0.4485426  0.45721814 0.46276116 0.46016905
 0.45155817 0.44172224 0.4345681  0.43194044 0.4341864  0.44058073
 0.45012832 0.4620309  0.47550434 0.49013644 0.5053762  0.5205904
 0.5352341  0.5495804  0.56458235 0.5826089  0.6063647  0.6369201
 0.67215425 0.70580447 0.7300808  0.73966587 0.7325229  0.70842874
 0.66694456 0.6099265  0.54405063 0.48118135 0.43247432 0.40282837
 0.38963374 0.38581172 0.38371986 0.3792663  0.37282246 0.36744997
 0.36586273 0.3692322  0.37776747 0.3909353  0.40775084 0.4270326
 0.4483792  0.47346628 0.50657576 0.55226916 0.60869086 0.66475236
 0.7032282  0.707747   0.6725839  0.6101265  0.54258704 0.4843121
 0.43598023 0.39399043 0.36058414 0.343597   0.34645963 0.3597888
 0.3700255  0.3706265  0.36369923 0.3551102  0.3492024  0.34786528
 0.3510911  0.35790423 0.36720276 0.37794447 0.38940153 0.401365
 0.41349152 0.42541832 0.4363646  0.44580632 0.45410877 0.46268502
 0.47445628 0.49223667 0.5171235  0.54599696 0.5718715  0.58822006
 0.59158856 0.58150613 0.5568488  0.5159748  0.46062228 0.39984548
 0.34692404 0.31098396 0.29321787 0.28805205]
