time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.32%, model saved.
Epoch: 0 Train: 59678.57031 Test: 3810.32520
Epoch: 80 Train: 16868.76953 Test: 1680.81555
Epoch: 160 Train: 15187.33691 Test: 1499.58167
Epoch 240: New minimal relative error: 86.84%, model saved.
Epoch: 240 Train: 14709.21387 Test: 1288.07825
Epoch 320: New minimal relative error: 65.42%, model saved.
Epoch: 320 Train: 14457.47363 Test: 1264.03748
Epoch: 400 Train: 13714.44141 Test: 1065.04285
Epoch: 480 Train: 12671.27148 Test: 1198.71509
Epoch 560: New minimal relative error: 62.43%, model saved.
Epoch: 560 Train: 12392.27832 Test: 1010.77820
Epoch 640: New minimal relative error: 59.02%, model saved.
Epoch: 640 Train: 11139.52930 Test: 900.28009
Epoch: 720 Train: 9729.61328 Test: 661.15088
Epoch: 800 Train: 8768.56543 Test: 519.25958
Epoch: 880 Train: 7420.98145 Test: 349.05054
Epoch: 960 Train: 7742.71045 Test: 406.71335
Epoch: 1040 Train: 6565.22070 Test: 360.12744
Epoch: 1120 Train: 4585.55713 Test: 257.12555
Epoch 1200: New minimal relative error: 51.06%, model saved.
Epoch: 1200 Train: 2844.60352 Test: 107.17709
Epoch 1280: New minimal relative error: 44.08%, model saved.
Epoch: 1280 Train: 1959.69153 Test: 37.68983
Epoch 1360: New minimal relative error: 43.68%, model saved.
Epoch: 1360 Train: 1429.85974 Test: 29.17154
Epoch: 1440 Train: 1606.83887 Test: 57.54702
Epoch 1520: New minimal relative error: 20.99%, model saved.
Epoch: 1520 Train: 1561.50525 Test: 41.91747
Epoch: 1600 Train: 1381.64941 Test: 48.30708
Epoch 1680: New minimal relative error: 11.07%, model saved.
Epoch: 1680 Train: 1046.35474 Test: 19.90438
Epoch: 1760 Train: 1010.12628 Test: 16.37508
Epoch: 1840 Train: 689.42877 Test: 23.97547
Epoch: 1920 Train: 563.00232 Test: 11.64641
Epoch 2000: New minimal relative error: 9.95%, model saved.
Epoch: 2000 Train: 496.59311 Test: 11.88536
Epoch: 2080 Train: 501.81360 Test: 24.41678
Epoch: 2160 Train: 434.36307 Test: 13.62036
Epoch 2240: New minimal relative error: 7.78%, model saved.
Epoch: 2240 Train: 424.97623 Test: 6.70102
Epoch: 2320 Train: 389.70120 Test: 18.97896
Epoch: 2400 Train: 325.66629 Test: 3.24054
Epoch: 2480 Train: 314.47641 Test: 2.54548
Epoch: 2560 Train: 307.81699 Test: 3.06911
Epoch: 2640 Train: 305.73196 Test: 2.91224
Epoch: 2720 Train: 301.61868 Test: 2.38273
Epoch 2800: New minimal relative error: 4.71%, model saved.
Epoch: 2800 Train: 270.08939 Test: 1.95438
Epoch: 2880 Train: 266.28427 Test: 4.37590
Epoch: 2960 Train: 249.11671 Test: 1.46063
Epoch: 3040 Train: 242.78471 Test: 1.69347
Epoch: 3120 Train: 231.70801 Test: 1.38573
Epoch 3200: New minimal relative error: 2.60%, model saved.
Epoch: 3200 Train: 215.29553 Test: 1.26282
Epoch: 3280 Train: 210.79236 Test: 1.19792
Epoch: 3360 Train: 214.67325 Test: 2.40416
Epoch: 3440 Train: 212.14966 Test: 1.29341
Epoch: 3520 Train: 221.07256 Test: 1.39050
Epoch: 3600 Train: 220.92656 Test: 1.48582
Epoch: 3680 Train: 214.70268 Test: 1.80388
Epoch: 3760 Train: 223.89493 Test: 2.01140
Epoch: 3840 Train: 200.71872 Test: 1.24922
Epoch: 3920 Train: 189.07762 Test: 1.31758
Epoch: 4000 Train: 173.25313 Test: 1.18795
Epoch: 4080 Train: 170.88756 Test: 1.19469
Epoch: 4160 Train: 156.05257 Test: 0.90734
Epoch: 4240 Train: 151.58540 Test: 0.98452
Epoch: 4320 Train: 148.60013 Test: 0.78868
Epoch: 4400 Train: 141.72188 Test: 0.86752
Epoch: 4480 Train: 136.32877 Test: 0.74431
Epoch: 4560 Train: 136.97704 Test: 0.81970
Epoch: 4640 Train: 132.79295 Test: 0.64275
Epoch: 4720 Train: 127.93472 Test: 0.63570
Epoch: 4800 Train: 118.86681 Test: 0.53509
Epoch: 4880 Train: 115.21753 Test: 0.58453
Epoch: 4960 Train: 120.05740 Test: 0.63742
Epoch: 5040 Train: 133.02637 Test: 2.95104
Epoch: 5120 Train: 127.37392 Test: 1.08570
Epoch: 5200 Train: 125.03465 Test: 0.87464
Epoch: 5280 Train: 123.61715 Test: 0.52809
Epoch 5360: New minimal relative error: 1.99%, model saved.
Epoch: 5360 Train: 117.62299 Test: 0.52400
Epoch: 5440 Train: 122.30291 Test: 0.88103
Epoch: 5520 Train: 119.23856 Test: 2.10760
Epoch: 5600 Train: 118.87077 Test: 0.68843
Epoch: 5680 Train: 136.67120 Test: 0.78310
Epoch: 5760 Train: 130.87633 Test: 0.77275
Epoch: 5840 Train: 126.81097 Test: 0.74130
Epoch: 5920 Train: 134.30841 Test: 0.87912
Epoch: 6000 Train: 131.29341 Test: 1.00998
Epoch: 6080 Train: 122.04891 Test: 0.69156
Epoch: 6160 Train: 127.19593 Test: 1.00740
Epoch: 6240 Train: 119.04119 Test: 0.69815
Epoch 6320: New minimal relative error: 1.89%, model saved.
Epoch: 6320 Train: 120.88055 Test: 0.70654
Epoch: 6400 Train: 118.09882 Test: 0.67441
Epoch: 6480 Train: 113.89335 Test: 0.78110
Epoch: 6560 Train: 110.83912 Test: 0.62554
Epoch: 6640 Train: 110.88898 Test: 0.77569
Epoch 6720: New minimal relative error: 1.71%, model saved.
Epoch: 6720 Train: 108.91901 Test: 0.64072
Epoch: 6800 Train: 114.76299 Test: 0.69950
Epoch: 6880 Train: 140.66174 Test: 1.21561
Epoch: 6960 Train: 138.38448 Test: 0.93034
Epoch: 7040 Train: 136.47299 Test: 1.04672
Epoch: 7120 Train: 137.44328 Test: 1.13482
Epoch: 7200 Train: 121.01084 Test: 0.69378
Epoch: 7280 Train: 114.85638 Test: 0.75766
Epoch: 7360 Train: 130.72130 Test: 0.80756
Epoch: 7440 Train: 119.65356 Test: 0.67464
Epoch: 7520 Train: 102.44264 Test: 0.49322
Epoch: 7600 Train: 102.35847 Test: 0.47113
Epoch: 7680 Train: 101.67547 Test: 0.52140
Epoch: 7760 Train: 104.26537 Test: 0.49187
Epoch: 7840 Train: 108.55328 Test: 0.54231
Epoch: 7920 Train: 111.22548 Test: 0.50718
Epoch: 7999 Train: 109.51688 Test: 0.48045
Training Loss: tensor(109.5169)
Test Loss: tensor(0.4804)
Learned LE: [  0.9373894   -0.08645988 -14.541833  ]
True LE: [ 8.70241880e-01  3.42352944e-03 -1.45431595e+01]
Relative Error: [3.4612796  3.247669   3.00714    2.9857116  3.0829825  3.2202172
 3.46644    3.8771088  4.111766   4.34648    4.470897   4.5095844
 4.5394716  4.464695   4.525335   4.2995367  3.9183416  3.385178
 2.8206596  3.0267923  3.170291   3.3237357  3.4495723  3.5857995
 3.734614   4.1447854  4.269588   4.4211493  4.622361   4.7803006
 5.38031    5.82564    6.0718656  6.8020344  7.3496675  7.718184
 7.958453   7.9439344  7.8109922  7.5277276  7.028752   6.948404
 7.021466   7.105709   7.127648   6.974551   6.7158093  6.3609853
 5.808499   5.2875423  4.70326    4.1036463  3.4965444  2.9101827
 2.3964047  1.8813215  1.6074935  1.7166097  1.925844   2.23034
 2.596961   2.9619365  2.9860384  2.7832747  2.499638   2.1483128
 1.6969824  1.8658584  2.3448067  2.8343167  3.1693656  3.471681
 3.6022956  3.8115163  3.9214053  3.9641123  4.097989   3.9534602
 3.580768   3.0572767  2.559246   2.8371196  3.0193276  3.2084863
 3.3422458  3.5508878  3.7644212  3.9940343  4.1555586  4.3307986
 4.495545   4.7010365  5.2352214  5.741804   5.9898224  6.4116464
 6.8545723  7.196305   7.5261006  7.535965   7.427226   7.1364603
 6.674557   6.4273486  6.4762635  6.533016   6.533072   6.4163556
 6.1433625  5.7924724  5.3014703  4.7712703  4.184461   3.5325396
 2.868228   2.2482135  1.7778722  1.5640967  1.4033542  1.2411577
 1.3488222  1.6980702  2.1079671  2.4043536  2.409743   2.3091712
 2.1412845  1.8932799  1.3046094  0.80202323 1.2737489  1.7993376
 2.2283256  2.5112796  2.7946062  3.1117954  3.3234913  3.4605014
 3.6435459  3.5854864  3.2952943  2.7900405  2.2805297  2.5671043
 2.8653116  3.0757616  3.226285   3.4675093  3.7095764  3.8731422
 3.994098   4.1893744  4.4291496  4.581812   5.0429416  5.6028924
 5.8607388  5.9636207  6.4133615  6.727764   7.052141   7.132437
 7.038813   6.765869   6.2881684  5.8777456  5.903914   5.9186
 5.912355   5.7939415  5.590008   5.366282   4.9766197  4.396886
 3.7338212  3.012179   2.3149111  1.7517328  1.5644122  1.5581551
 1.2767749  1.054849   1.0305862  1.1183157  1.5198091  1.707138
 1.7896689  1.796384   1.7511057  1.5706748  1.1188842  0.9102959
 1.0615338  1.1801184  1.4284272  1.6412361  1.9773626  2.3826866
 2.6990004  2.9380872  3.1937413  3.2008388  3.0014467  2.5815036
 1.9836264  2.258369   2.56787    2.8735204  3.0327563  3.387006
 3.6269267  3.7464287  3.7981536  4.0090117  4.289562   4.528891
 4.764484   5.3371944  5.6986313  5.694363   5.9748373  6.327204
 6.5672064  6.7149253  6.645147   6.3699403  5.8974257  5.367479
 5.3413143  5.3753843  5.3562717  5.22168    5.13989    5.044395
 4.6352725  4.059484   3.3395636  2.541498   1.8203903  1.4284387
 1.5485736  1.3951389  1.2493204  1.0687094  0.95343345 1.000609
 0.8667848  1.0010386  1.1837248  1.2640169  1.3368531  1.24483
 0.9883144  1.0310317  1.286377   1.3488805  1.3456581  1.2712932
 1.3112036  1.6654481  2.0455534  2.3781676  2.7226229  2.8129718
 2.7009132  2.4021187  1.8479456  1.9418424  2.1645021  2.4840517
 2.9167316  3.2964814  3.5709095  3.633377   3.7800498  3.9225326
 4.3239827  4.6951303  4.917693   5.17276    5.47446    5.5266867
 5.488355   5.893884   6.204749   6.2253428  6.17313    5.9898953
 5.5387473  4.985434   4.860784   4.840679   4.786494   4.793526
 4.8640885  4.766408   4.4150395  3.7705798  3.0016444  2.1660817
 1.4531523  1.3817949  1.2658955  1.0887041  1.1771213  1.2102379
 1.2073635  1.1556239  0.8724524  0.6517146  0.61885047 0.70125794
 0.8542574  0.88540965 0.90219283 1.1177729  1.4196525  1.5073415
 1.5790175  1.7603401  1.1087867  1.1181328  1.4868004  1.8218402
 2.1348515  2.377416   2.418538   2.2038574  1.8364674  1.7527952
 1.8908129  2.0865436  2.6613586  3.148369   3.435957   3.6392953
 3.8180397  4.061578   4.2872353  4.7588882  5.0682793  5.130811
 5.43833    5.467846   5.2114305  5.4311385  5.77658    5.842935
 5.717664   5.623762   5.2558036  4.8016934  4.4158945  4.2868724
 4.3247795  4.494113   4.668472   4.5413866  4.14874    3.5489645
 2.7346146  1.9226551  1.3362359  1.4015688  0.8521564  0.84148693
 1.1120526  1.3458365  1.4289308  1.3627199  1.0382723  0.75735176
 0.5832904  0.44991812 0.5088141  0.60739976 0.80296767 1.1083728
 1.4941281  1.6380627  1.6962286  1.9049343  1.8540112  1.1400126
 1.0976847  1.3459054  1.680916   2.0825408  2.2975187  2.266024
 1.8438265  1.4861633  1.7420158  2.0104437  2.4451709  2.9000428
 3.2623649  3.5665526  3.8554153  4.1576347 ]
