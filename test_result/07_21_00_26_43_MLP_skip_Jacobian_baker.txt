time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 500
num_test: 500
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 4
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 21.372064590 Test: 19.871545792
Epoch 0: New minimal relative error: 19.87%, model saved.
Epoch: 100 Train: 6.410405636 Test: 6.539670944
Epoch 100: New minimal relative error: 6.54%, model saved.
Epoch: 200 Train: 6.160745621 Test: 6.295225143
Epoch 200: New minimal relative error: 6.30%, model saved.
Epoch: 300 Train: 6.151919365 Test: 6.351859093
Epoch: 400 Train: 6.175983429 Test: 6.302292824
Epoch: 500 Train: 6.156837463 Test: 6.338914871
Epoch: 600 Train: 6.188720703 Test: 6.338751793
Epoch: 700 Train: 6.197453022 Test: 6.378499985
Epoch: 800 Train: 6.080569267 Test: 6.281098366
Epoch 800: New minimal relative error: 6.28%, model saved.
Epoch: 900 Train: 6.033150673 Test: 6.254783630
Epoch 900: New minimal relative error: 6.25%, model saved.
Epoch: 1000 Train: 6.014500618 Test: 6.316108227
Epoch: 1100 Train: 5.963840961 Test: 6.339164734
Epoch: 1200 Train: 6.035921097 Test: 6.455937386
Epoch: 1300 Train: 5.921747208 Test: 6.497727394
Epoch: 1400 Train: 6.005721092 Test: 6.658209324
Epoch: 1500 Train: 5.943961143 Test: 6.487306595
Epoch: 1600 Train: 5.964597702 Test: 6.470335960
Epoch: 1700 Train: 6.004768372 Test: 6.487089157
Epoch: 1800 Train: 6.055566788 Test: 6.752908707
Epoch: 1900 Train: 6.087404728 Test: 6.631601810
Epoch: 2000 Train: 6.083325386 Test: 6.490477085
Epoch: 2100 Train: 6.109735489 Test: 6.513448715
Epoch: 2200 Train: 6.092116356 Test: 6.501781464
Epoch: 2300 Train: 6.099186420 Test: 6.570110321
Epoch: 2400 Train: 6.102694511 Test: 6.597240448
Epoch: 2500 Train: 6.064618111 Test: 6.619790077
Epoch: 2600 Train: 6.126110077 Test: 6.611496449
Epoch: 2700 Train: 6.092385292 Test: 6.614302158
Epoch: 2800 Train: 6.136806488 Test: 6.765558243
Epoch: 2900 Train: 6.130764961 Test: 6.856214523
Epoch: 3000 Train: 6.054825783 Test: 6.677484512
Epoch: 3100 Train: 6.039761543 Test: 6.628644466
Epoch: 3200 Train: 6.062562943 Test: 6.664215088
Epoch: 3300 Train: 6.063919067 Test: 6.703616142
Epoch: 3400 Train: 6.073064327 Test: 6.696812153
Epoch: 3500 Train: 6.097665787 Test: 6.749674797
Epoch: 3600 Train: 6.125051498 Test: 6.815325737
Epoch: 3700 Train: 6.088752747 Test: 6.767265320
Epoch: 3800 Train: 6.097822189 Test: 6.783332825
Epoch: 3900 Train: 6.113695145 Test: 6.776186466
Epoch: 4000 Train: 6.110134602 Test: 6.697831631
Epoch: 4100 Train: 6.113034725 Test: 6.699330330
Epoch: 4200 Train: 6.115167618 Test: 6.690344810
Epoch: 4300 Train: 6.137959480 Test: 6.717378616
Epoch: 4400 Train: 6.131570816 Test: 6.735075951
Epoch: 4500 Train: 6.094257832 Test: 6.727610588
Epoch: 4600 Train: 6.063222885 Test: 6.727038860
Epoch: 4700 Train: 6.055488586 Test: 6.730213165
Epoch: 4800 Train: 6.035663128 Test: 6.700874329
Epoch: 4900 Train: 6.026543617 Test: 6.777450562
Epoch: 5000 Train: 6.030447960 Test: 6.738053322
Epoch: 5100 Train: 6.037036896 Test: 6.704244614
Epoch: 5200 Train: 6.053185463 Test: 6.692289829
Epoch: 5300 Train: 6.052707672 Test: 6.653827667
Epoch: 5400 Train: 6.066350460 Test: 6.617492199
Epoch: 5500 Train: 6.074621201 Test: 6.612024307
Epoch: 5600 Train: 6.086300373 Test: 6.739560127
Epoch: 5700 Train: 6.053190231 Test: 6.668152809
Epoch: 5800 Train: 6.070096970 Test: 6.607882023
Epoch: 5900 Train: 6.080314636 Test: 6.405485153
Epoch: 6000 Train: 6.074586868 Test: 6.449350357
Epoch: 6100 Train: 6.086835861 Test: 6.429662228
Epoch: 6200 Train: 6.096267223 Test: 6.444078445
Epoch: 6300 Train: 6.115702629 Test: 6.582733154
Epoch: 6400 Train: 6.119542122 Test: 6.756305695
Epoch: 6500 Train: 6.080625534 Test: 6.620922089
Epoch: 6600 Train: 6.086270332 Test: 6.510651588
Epoch: 6700 Train: 6.114212036 Test: 6.495018959
Epoch: 6800 Train: 6.111592293 Test: 6.473694801
Epoch: 6900 Train: 6.103237629 Test: 6.489680290
Epoch: 7000 Train: 6.113633633 Test: 6.485022545
Epoch: 7100 Train: 6.118266106 Test: 6.495777130
Epoch: 7200 Train: 6.119618416 Test: 6.479001522
Epoch: 7300 Train: 6.117821217 Test: 6.483491898
Epoch: 7400 Train: 6.126377106 Test: 6.476550102
Epoch: 7500 Train: 6.126430988 Test: 6.455997467
Epoch: 7600 Train: 6.131073952 Test: 6.435598373
Epoch: 7700 Train: 6.131788731 Test: 6.427937508
Epoch: 7800 Train: 6.129258156 Test: 6.431685448
Epoch: 7900 Train: 6.129729271 Test: 6.443924427
Epoch: 8000 Train: 6.131504059 Test: 6.466291428
Epoch: 8100 Train: 6.136229515 Test: 6.445460320
Epoch: 8200 Train: 6.135823727 Test: 6.454003334
Epoch: 8300 Train: 6.131943226 Test: 6.456334591
Epoch: 8400 Train: 6.129111767 Test: 6.454313755
Epoch: 8500 Train: 6.128267288 Test: 6.449151993
Epoch: 8600 Train: 6.131392479 Test: 6.447557449
Epoch: 8700 Train: 6.131915092 Test: 6.449224472
Epoch: 8800 Train: 6.131914139 Test: 6.457521439
Epoch: 8900 Train: 6.142015457 Test: 6.449422836
Epoch: 9000 Train: 6.141559601 Test: 6.440070629
Epoch: 9100 Train: 6.147975445 Test: 6.428937912
Epoch: 9200 Train: 6.146906376 Test: 6.418247223
Epoch: 9300 Train: 6.145690441 Test: 6.404621124
Epoch: 9400 Train: 6.150048256 Test: 6.411048889
Epoch: 9500 Train: 6.149114132 Test: 6.412220001
Epoch: 9600 Train: 6.147474289 Test: 6.406919956
Epoch: 9700 Train: 6.145975590 Test: 6.401980877
Epoch: 9800 Train: 6.144154549 Test: 6.405308723
Epoch: 9900 Train: 6.146188736 Test: 6.407798767
Epoch: 9999 Train: 6.145736694 Test: 6.413290977
Training Loss: tensor(6.1457)
Test Loss: tensor(6.4133)
True Mean x: tensor(3.1794, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.0130, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0057)
Jacobian term Test Loss: tensor(0.0059)
Learned LE: [10.190542   -0.14255649]
True LE: tensor([ 0.6932, -0.7457], dtype=torch.float64)
