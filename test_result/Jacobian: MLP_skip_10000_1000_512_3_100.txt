time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 98.48%, model saved.
Epoch: 0 Train: 9451.29688 Test: 4076.91162
Epoch: 100 Train: 2535.51660 Test: 1042.54138
Epoch: 200 Train: 1663.39844 Test: 570.27112
Epoch: 300 Train: 590.17932 Test: 132.21558
Epoch: 400 Train: 325.60236 Test: 65.11075
Epoch 500: New minimal relative error: 37.31%, model saved.
Epoch: 500 Train: 136.90642 Test: 17.32393
Epoch 600: New minimal relative error: 25.61%, model saved.
Epoch: 600 Train: 100.67638 Test: 15.63836
Epoch: 700 Train: 111.14584 Test: 46.80471
Epoch 800: New minimal relative error: 11.09%, model saved.
Epoch: 800 Train: 65.80261 Test: 5.78429
Epoch: 900 Train: 55.89808 Test: 3.31227
Epoch 1000: New minimal relative error: 3.69%, model saved.
Epoch: 1000 Train: 45.89896 Test: 2.18485
Epoch: 1100 Train: 41.98521 Test: 5.41908
Epoch 1200: New minimal relative error: 3.25%, model saved.
Epoch: 1200 Train: 35.13445 Test: 1.41905
Epoch: 1300 Train: 33.19128 Test: 1.34495
Epoch: 1400 Train: 33.22976 Test: 1.80696
Epoch: 1500 Train: 27.13284 Test: 1.00635
Epoch: 1600 Train: 27.36113 Test: 1.29898
Epoch: 1700 Train: 23.10117 Test: 1.43286
Epoch: 1800 Train: 23.37358 Test: 0.85612
Epoch: 1900 Train: 35.54070 Test: 2.01071
Epoch 2000: New minimal relative error: 2.46%, model saved.
Epoch: 2000 Train: 18.68640 Test: 0.50344
Epoch: 2100 Train: 26.01729 Test: 1.00463
Epoch: 2200 Train: 17.68816 Test: 0.45694
Epoch: 2300 Train: 27.10577 Test: 6.01980
Epoch: 2400 Train: 16.75453 Test: 3.35329
Epoch: 2500 Train: 14.46048 Test: 0.31029
Epoch: 2600 Train: 15.43559 Test: 0.38537
Epoch 2700: New minimal relative error: 2.15%, model saved.
Epoch: 2700 Train: 13.61417 Test: 0.28869
Epoch: 2800 Train: 13.73061 Test: 0.28324
Epoch: 2900 Train: 14.12250 Test: 0.39294
Epoch: 3000 Train: 13.93488 Test: 0.60372
Epoch: 3100 Train: 12.42570 Test: 0.32299
Epoch: 3200 Train: 11.81653 Test: 0.24242
Epoch: 3300 Train: 11.66086 Test: 0.25284
Epoch: 3400 Train: 11.63657 Test: 0.35263
Epoch: 3500 Train: 11.60486 Test: 0.38308
Epoch: 3600 Train: 11.29108 Test: 0.26931
Epoch: 3700 Train: 13.55651 Test: 1.30381
Epoch: 3800 Train: 9.99029 Test: 0.15526
Epoch: 3900 Train: 9.61976 Test: 0.20040
Epoch: 4000 Train: 11.13109 Test: 2.55934
Epoch: 4100 Train: 9.01246 Test: 0.12684
Epoch 4200: New minimal relative error: 1.93%, model saved.
Epoch: 4200 Train: 9.06537 Test: 0.13221
Epoch: 4300 Train: 8.95711 Test: 0.25195
Epoch: 4400 Train: 9.22157 Test: 0.14027
Epoch: 4500 Train: 16.98660 Test: 6.89240
Epoch: 4600 Train: 8.62445 Test: 0.40497
Epoch: 4700 Train: 8.60074 Test: 0.32021
Epoch: 4800 Train: 8.38148 Test: 0.17790
Epoch: 4900 Train: 10.36282 Test: 1.60472
Epoch: 5000 Train: 8.54897 Test: 0.31302
Epoch: 5100 Train: 8.32162 Test: 0.19026
Epoch: 5200 Train: 7.91791 Test: 0.14116
Epoch: 5300 Train: 7.83947 Test: 0.10912
Epoch: 5400 Train: 9.11332 Test: 1.16853
Epoch: 5500 Train: 7.34237 Test: 0.09738
Epoch: 5600 Train: 7.57472 Test: 0.25319
Epoch 5700: New minimal relative error: 1.48%, model saved.
Epoch: 5700 Train: 7.78925 Test: 0.11592
Epoch: 5800 Train: 8.06657 Test: 0.68524
Epoch: 5900 Train: 7.41853 Test: 0.08109
Epoch: 6000 Train: 7.45877 Test: 0.08514
Epoch: 6100 Train: 7.40787 Test: 0.25307
Epoch: 6200 Train: 7.95286 Test: 0.88233
Epoch: 6300 Train: 7.52643 Test: 0.47928
Epoch: 6400 Train: 6.79386 Test: 0.16352
Epoch: 6500 Train: 6.77596 Test: 0.11927
Epoch: 6600 Train: 6.54740 Test: 0.08829
Epoch: 6700 Train: 6.50951 Test: 0.08545
Epoch: 6800 Train: 6.32168 Test: 0.09571
Epoch: 6900 Train: 6.28904 Test: 0.09745
Epoch: 7000 Train: 6.22118 Test: 0.06915
Epoch: 7100 Train: 7.30432 Test: 1.20208
Epoch 7200: New minimal relative error: 1.04%, model saved.
Epoch: 7200 Train: 6.07370 Test: 0.06313
Epoch: 7300 Train: 6.08270 Test: 0.10849
Epoch: 7400 Train: 5.98472 Test: 0.08027
Epoch: 7500 Train: 6.20206 Test: 0.23152
Epoch: 7600 Train: 6.07683 Test: 0.17051
Epoch: 7700 Train: 5.98417 Test: 0.05805
Epoch: 7800 Train: 5.93261 Test: 0.14529
Epoch: 7900 Train: 6.96525 Test: 1.25427
Epoch: 8000 Train: 6.15122 Test: 0.35302
Epoch: 8100 Train: 5.99840 Test: 0.17053
Epoch: 8200 Train: 5.74909 Test: 0.09258
Epoch: 8300 Train: 5.73638 Test: 0.05807
Epoch: 8400 Train: 6.34181 Test: 0.39313
Epoch: 8500 Train: 5.84676 Test: 0.12930
Epoch: 8600 Train: 6.06858 Test: 0.31293
Epoch: 8700 Train: 5.69743 Test: 0.05571
Epoch: 8800 Train: 5.64041 Test: 0.05383
Epoch: 8900 Train: 5.78412 Test: 0.24069
Epoch: 9000 Train: 6.28640 Test: 0.75148
Epoch: 9100 Train: 5.54651 Test: 0.07594
Epoch: 9200 Train: 5.78572 Test: 0.36706
Epoch: 9300 Train: 5.55590 Test: 0.14231
Epoch: 9400 Train: 5.42088 Test: 0.11004
Epoch: 9500 Train: 5.33740 Test: 0.08668
Epoch: 9600 Train: 5.28843 Test: 0.18559
Epoch: 9700 Train: 5.08659 Test: 0.05235
Epoch: 9800 Train: 5.11304 Test: 0.04379
Epoch: 9900 Train: 5.09104 Test: 0.09062
Epoch: 9999 Train: 5.00823 Test: 0.06567
Training Loss: tensor(5.0082)
Test Loss: tensor(0.0657)
Learned LE: [ 9.0428686e-01 -1.0985811e-02 -1.4566009e+01]
True LE: [ 8.8240355e-01  1.2684742e-02 -1.4568612e+01]
Relative Error: [3.9000742  3.8390214  4.163137   4.6005955  5.270302   5.5486526
 5.6185694  5.4221177  4.9420652  4.3680887  3.9664946  3.6308877
 3.3365715  2.9921305  2.728271   2.567806   2.496355   2.4860754
 2.5415606  2.7365642  2.887864   2.9529243  2.8476107  2.7219825
 2.5106378  2.465193   2.176223   2.151721   2.0539727  1.7099485
 1.667335   1.5768257  1.5379909  1.8558443  2.3845396  3.2687519
 3.75378    3.7869174  3.7754872  3.6461499  3.5067077  3.3244212
 2.9642437  2.388106   1.9051691  1.6432924  1.48938    1.4144385
 1.1693616  0.940384   0.8548925  0.8653399  1.0217724  0.9461415
 0.86921716 0.9218421  1.1814933  1.614265   2.0248988  2.4554336
 2.9612186  3.19192    3.291535   3.194209   3.442716   3.870125
 4.5475793  4.9134364  4.972787   4.8774524  4.59184    4.135042
 3.4654539  3.2038317  2.9567935  2.6213543  2.434711   2.2394934
 2.1187518  2.046711   2.0049403  2.171186   2.231355   2.4090161
 2.582765   2.449743   2.2057476  2.1169388  1.9682575  1.7354889
 1.7371457  1.6063603  1.5390257  1.5642402  1.2667576  1.4242831
 1.8174121  2.7474635  3.3620498  3.4619968  3.492647   3.41407
 3.3689153  3.2118707  2.7593498  2.2215192  1.865273   1.5430218
 1.2854453  1.0780859  0.85042065 0.6383255  0.6505331  0.5961189
 0.5802664  0.5624663  0.51895666 0.5431358  0.71321166 1.1711365
 1.795915   2.198533   2.480078   2.6731052  2.7822335  2.7281313
 2.7496452  3.1853237  3.682271   4.148375   4.3136263  4.2623324
 4.053442   3.7949524  3.2015808  2.7743053  2.5924158  2.3043518
 2.0908022  1.961167   1.782106   1.6529852  1.5340816  1.5685711
 1.6018355  1.8332535  2.0446424  2.1266456  1.8953071  1.771366
 1.7412232  1.4818931  1.2793382  1.4192586  1.341498   1.4337984
 1.3153355  1.0457288  1.2331985  2.1356502  2.9169054  3.1275496
 3.1632214  3.1917024  3.2546175  3.1629648  2.7282732  2.196999
 1.8401699  1.5898123  1.2090878  0.8486905  0.63460886 0.4255246
 0.5481552  0.52404773 0.3880099  0.35972172 0.4256574  0.49594742
 0.5256418  0.81029385 1.4723507  1.820823   2.10669    2.2668545
 2.3927367  2.57935    2.3923051  2.4313438  2.8416467  3.3437066
 3.6561635  3.7090425  3.5612948  3.3602662  2.9465785  2.4341717
 2.2358944  2.0568352  1.8179278  1.7471075  1.4847933  1.2590188
 1.151787   1.0454895  1.1020007  1.318575   1.4158528  1.6962435
 1.5560743  1.401451   1.3540665  1.2836021  1.0527719  0.96405095
 1.1629009  1.2117451  1.2589008  0.9825251  0.8722552  1.3964171
 2.443951   2.7327733  2.8033864  2.940301   3.100814   3.1060162
 2.738247   2.2859855  1.8796407  1.5400693  1.2167314  0.77431023
 0.43191412 0.28573522 0.4608723  0.49146804 0.42816684 0.4729843
 0.59908044 0.6930442  0.6799492  0.7743276  1.0846566  1.3783219
 1.8238577  2.026597   2.1321213  2.3528073  2.4751182  2.1333077
 2.1315327  2.4991016  3.0357647  3.1750531  3.109764   2.9582412
 2.7098975  2.300816   1.8850273  1.8022077  1.6223292  1.5903642
 1.4290531  1.0937736  0.805231   0.68863755 0.85417134 0.9999156
 1.0850468  1.1541811  1.3437114  1.0207951  0.9139016  0.8880561
 0.851904   0.79490274 0.7935203  0.8955624  1.0256249  1.0310516
 0.7701913  0.7291789  1.6233768  2.2360573  2.3633897  2.6397548
 2.854911   2.972262   2.728878   2.4440367  1.9945867  1.5771203
 1.1898187  0.83590764 0.4362291  0.32556453 0.3094869  0.42432985
 0.4194064  0.52004975 0.5707362  0.6619774  0.75554466 0.8001134
 0.81322587 0.958245   1.3978784  1.855345   1.8956175  1.9382741
 2.0359972  2.1236348  1.8975053  1.942532   2.3391984  2.6837802
 2.743815   2.646108   2.4853566  2.156754   1.7865907  1.510793
 1.4096177  1.3556384  1.3945675  1.2488277  0.7857952  0.53108424
 0.5385989  0.85736394 1.002724   0.96557343 0.9522425  0.9774256
 0.6506473  0.5029071  0.5273092  0.6630792  0.7400118  0.7041943
 0.6865002  0.9314486  0.8733872  0.7276621  0.7433383  1.534992
 1.8036772  2.140267   2.4792345  2.7766328  2.7646246  2.5468075
 2.1657565  1.7745512  1.2423573  0.950553   0.593118   0.3970767
 0.1799849  0.3037402  0.40734917 0.42159972 0.49561393 0.5529859
 0.6093251  0.7324024  0.75541824 0.8105071  0.90878195 1.2365394
 1.4218191  1.5735807  1.77343    1.7157588  1.5599927  1.2771518
 1.3435096  1.7303324  2.0037966  2.309901   2.4268377  2.0590165
 1.7364357  1.4707797  1.1948166  1.1105156  1.0969394  1.2491527
 1.1258005  0.7349798  0.47054994 0.5220611  0.7265529  0.8819668
 0.8647962  0.8681329  0.7842261  0.35726815]
