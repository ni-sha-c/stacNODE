time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.46%, model saved.
Epoch: 0 Train: 3970.01953 Test: 4165.72998
Epoch 80: New minimal relative error: 64.97%, model saved.
Epoch: 80 Train: 82.10017 Test: 99.34103
Epoch 160: New minimal relative error: 31.28%, model saved.
Epoch: 160 Train: 16.04170 Test: 22.32381
Epoch 240: New minimal relative error: 16.72%, model saved.
Epoch: 240 Train: 5.70881 Test: 7.97224
Epoch: 320 Train: 3.87876 Test: 5.58313
Epoch: 400 Train: 2.81906 Test: 4.14912
Epoch: 480 Train: 1.75100 Test: 2.75881
Epoch: 560 Train: 1.22284 Test: 1.97750
Epoch: 640 Train: 1.12976 Test: 1.87075
Epoch: 720 Train: 4.09356 Test: 2.52506
Epoch: 800 Train: 1.32307 Test: 2.18704
Epoch: 880 Train: 0.58068 Test: 1.06442
Epoch: 960 Train: 0.50431 Test: 0.95638
Epoch: 1040 Train: 0.55821 Test: 1.21409
Epoch: 1120 Train: 0.36764 Test: 0.76504
Epoch 1200: New minimal relative error: 15.38%, model saved.
Epoch: 1200 Train: 0.34722 Test: 0.74697
Epoch: 1280 Train: 2.21039 Test: 2.31543
Epoch 1360: New minimal relative error: 14.90%, model saved.
Epoch: 1360 Train: 1.08347 Test: 0.91655
Epoch: 1440 Train: 1.88026 Test: 1.40575
Epoch: 1520 Train: 0.44228 Test: 0.69678
Epoch: 1600 Train: 0.33454 Test: 0.67584
Epoch: 1680 Train: 0.39301 Test: 0.75470
Epoch: 1760 Train: 0.50699 Test: 0.90460
Epoch: 1840 Train: 0.24389 Test: 0.57205
Epoch: 1920 Train: 0.19571 Test: 0.51528
Epoch: 2000 Train: 0.22433 Test: 0.56677
Epoch: 2080 Train: 1.65940 Test: 2.16576
Epoch: 2160 Train: 0.22541 Test: 0.53840
Epoch: 2240 Train: 0.15130 Test: 0.44684
Epoch: 2320 Train: 0.15171 Test: 0.44599
Epoch 2400: New minimal relative error: 13.90%, model saved.
Epoch: 2400 Train: 0.19011 Test: 0.48245
Epoch: 2480 Train: 0.14909 Test: 0.44383
Epoch: 2560 Train: 0.25869 Test: 0.57810
Epoch 2640: New minimal relative error: 13.03%, model saved.
Epoch: 2640 Train: 0.50826 Test: 0.77541
Epoch: 2720 Train: 0.65480 Test: 0.96278
Epoch 2800: New minimal relative error: 10.50%, model saved.
Epoch: 2800 Train: 0.20193 Test: 0.50843
Epoch: 2880 Train: 0.15933 Test: 0.43209
Epoch: 2960 Train: 0.53399 Test: 0.88616
Epoch: 3040 Train: 0.18781 Test: 0.46580
Epoch: 3120 Train: 0.42931 Test: 0.62951
Epoch: 3200 Train: 0.19425 Test: 0.49364
Epoch: 3280 Train: 0.14268 Test: 0.39122
Epoch: 3360 Train: 0.08971 Test: 0.39233
Epoch: 3440 Train: 0.14672 Test: 0.42805
Epoch: 3520 Train: 0.39697 Test: 0.59677
Epoch: 3600 Train: 0.07638 Test: 0.36271
Epoch 3680: New minimal relative error: 9.79%, model saved.
Epoch: 3680 Train: 0.09009 Test: 0.38561
Epoch: 3760 Train: 0.08492 Test: 0.36904
Epoch: 3840 Train: 0.11137 Test: 0.39041
Epoch: 3920 Train: 0.20247 Test: 0.49128
Epoch: 4000 Train: 0.10648 Test: 0.39809
Epoch: 4080 Train: 0.09146 Test: 0.37509
Epoch: 4160 Train: 0.06921 Test: 0.35040
Epoch: 4240 Train: 0.06683 Test: 0.34485
Epoch: 4320 Train: 0.12140 Test: 0.38759
Epoch: 4400 Train: 0.07036 Test: 0.35111
Epoch: 4480 Train: 0.06085 Test: 0.33806
Epoch: 4560 Train: 0.07107 Test: 0.34726
Epoch: 4640 Train: 0.08182 Test: 0.35323
Epoch: 4720 Train: 0.08776 Test: 0.34944
Epoch: 4800 Train: 0.05901 Test: 0.34072
Epoch: 4880 Train: 0.59422 Test: 1.00288
Epoch: 4960 Train: 0.05196 Test: 0.32409
Epoch: 5040 Train: 1.24126 Test: 1.43675
Epoch: 5120 Train: 0.05048 Test: 0.32238
Epoch: 5200 Train: 0.54998 Test: 1.00253
Epoch: 5280 Train: 0.04837 Test: 0.32011
Epoch: 5360 Train: 0.04684 Test: 0.31653
Epoch: 5440 Train: 0.12680 Test: 0.39926
Epoch: 5520 Train: 0.04540 Test: 0.31568
Epoch 5600: New minimal relative error: 7.80%, model saved.
Epoch: 5600 Train: 0.83258 Test: 1.22253
Epoch: 5680 Train: 0.04410 Test: 0.31300
Epoch: 5760 Train: 0.25197 Test: 0.48953
Epoch: 5840 Train: 0.04244 Test: 0.31102
Epoch: 5920 Train: 0.45520 Test: 0.52330
Epoch: 6000 Train: 0.04131 Test: 0.30944
Epoch: 6080 Train: 0.38585 Test: 0.70322
Epoch: 6160 Train: 0.05804 Test: 0.33070
Epoch: 6240 Train: 0.03921 Test: 0.30591
Epoch: 6320 Train: 0.06670 Test: 0.32834
Epoch: 6400 Train: 0.03806 Test: 0.30400
Epoch: 6480 Train: 0.04564 Test: 0.30968
Epoch: 6560 Train: 0.04839 Test: 0.31772
Epoch: 6640 Train: 0.03801 Test: 0.30389
Epoch: 6720 Train: 0.03655 Test: 0.30238
Epoch: 6800 Train: 0.07313 Test: 0.34979
Epoch: 6880 Train: 0.03503 Test: 0.30001
Epoch: 6960 Train: 0.05598 Test: 0.32873
Epoch: 7040 Train: 0.25518 Test: 0.47978
Epoch: 7120 Train: 0.03373 Test: 0.29858
Epoch: 7200 Train: 0.05738 Test: 0.31504
Epoch: 7280 Train: 0.03273 Test: 0.29772
Epoch: 7360 Train: 0.04950 Test: 0.31199
Epoch: 7440 Train: 0.03666 Test: 0.30017
Epoch: 7520 Train: 0.05855 Test: 0.32950
Epoch: 7600 Train: 0.03227 Test: 0.29791
Epoch: 7680 Train: 0.03089 Test: 0.29682
Epoch: 7760 Train: 0.16898 Test: 0.45604
Epoch: 7840 Train: 0.08789 Test: 0.37711
Epoch: 7920 Train: 0.02973 Test: 0.29656
Epoch: 7999 Train: 0.02936 Test: 0.29629
Training Loss: tensor(0.0294)
Test Loss: tensor(0.2963)
Learned LE: [ 0.8112249   0.00878824 -5.3111057 ]
True LE: [ 8.5514241e-01  8.6117024e-04 -1.4529507e+01]
Relative Error: [3.7608075 4.3953257 5.0703034 5.7345138 6.4083457 7.080526  7.681162
 8.124081  8.374191  8.488196  8.514863  8.44789   8.308755  8.096267
 7.846823  7.7949805 8.148365  8.710044  9.154762  9.316738  9.247765
 9.256181  9.440746  9.806081  9.996349  9.591722  9.191726  9.010654
 9.020618  9.025337  8.905076  8.692276  8.510991  8.466654  8.580307
 8.809367  8.954195  9.087268  9.382894  9.715203  9.869684  9.296842
 8.3709135 7.665258  7.18781   6.3794928 5.914144  6.06737   6.304051
 6.2741137 6.1976833 5.98372   5.5007205 4.8517084 4.2416697 3.816859
 3.4907217 3.1471312 2.8334656 2.5227833 2.325012  2.4821956 2.9191093
 3.526844  4.157663  4.7662086 5.3935385 6.0611653 6.7233453 7.2776446
 7.6238914 7.7518473 7.7566876 7.683631  7.538345  7.3395123 7.0550485
 6.812127  6.9260178 7.40801   7.8915997 8.164295  8.138761  8.006867
 8.061733  8.497113  8.8707    8.581059  8.311862  8.176464  8.181631
 8.22254   8.134142  7.924827  7.7013245 7.5628867 7.5031123 7.5635166
 7.769615  7.899847  8.009408  8.278439  8.547042  8.664085  7.9771414
 7.0329137 6.5156474 5.895263  5.204732  5.2926545 5.6411924 5.599489
 5.5715756 5.504381  5.1123343 4.441674  3.798806  3.3721924 3.103106
 2.8090796 2.5157018 2.2528288 1.933917  1.7860632 2.179588  2.7717927
 3.3639305 3.9128256 4.459743  5.0431447 5.66983   6.2849    6.7696333
 7.01039   7.0208573 6.9259415 6.7691216 6.572999  6.3347735 6.0079727
 5.824688  6.0841246 6.5748105 6.9793754 7.141615  7.037576  6.9250846
 7.202028  7.751755  7.7107215 7.5826316 7.4590826 7.3454237 7.364148
 7.353449  7.2072015 6.9740477 6.7919836 6.713386  6.697196  6.6692643
 6.7346573 6.8293734 6.8923473 7.1118007 7.3368573 7.4961343 6.7281046
 5.830599  5.401075  4.671155  4.495034  4.975596  5.055229  4.951087
 4.9971247 4.781921  4.1920047 3.5258656 3.0841074 2.8582718 2.6739013
 2.42552   2.1786213 1.9402101 1.6353507 1.5341932 2.0747452 2.676766
 3.1888497 3.665461  4.1459136 4.6493006 5.1882863 5.721793  6.1283035
 6.282919  6.207461  6.042432  5.8318496 5.6107907 5.355009  5.0084467
 4.870669  5.186237  5.6580844 6.054768  6.1792536 6.10057   6.165333
 6.626007  6.8560853 6.8484054 6.7103214 6.449371  6.3526406 6.447395
 6.4647765 6.3360443 6.156779  6.0275106 5.9606833 5.9650645 5.861757
 5.7101884 5.7539573 5.7653975 5.941735  6.1651645 6.420435  5.6339192
 4.7561584 4.323917  3.7239506 4.0744348 4.5105515 4.446063  4.434424
 4.4176373 4.020302  3.425318  2.9606252 2.7247689 2.6152802 2.485958
 2.2670383 2.0298707 1.7969159 1.5473503 1.4456348 1.9671897 2.5337317
 2.9855607 3.4044652 3.809472  4.203685  4.607176  5.0189233 5.358556
 5.4960523 5.40721   5.2196774 4.985783  4.7531834 4.5000734 4.136444
 3.955929  4.2213097 4.715615  5.1568394 5.300737  5.4152064 5.688354
 6.104789  6.018592  5.911794  5.665047  5.485082  5.5640926 5.7646284
 5.761413  5.55213   5.33517   5.2728543 5.312321  5.237742  5.0724015
 4.835983  4.7451625 4.7277956 4.8520703 5.1117377 5.40998   4.816245
 3.8814354 3.4028609 3.160637  3.5549502 3.9375517 3.8863091 3.9520237
 3.7966921 3.3597405 2.9477334 2.7086616 2.5839744 2.5359097 2.4410322
 2.2272189 1.9702256 1.7271338 1.5240693 1.4282465 1.7463018 2.3017116
 2.7281554 3.1049    3.4407027 3.7242146 3.9698403 4.23809   4.5369325
 4.7462907 4.7355204 4.5687904 4.326785  4.074711  3.8422785 3.4899116
 3.1899452 3.3177223 3.8171508 4.2440457 4.4920616 4.8649063 5.354348
 5.5792375 5.167401  4.961406  4.908525  4.916606  5.08843   5.255318
 5.1499834 4.875706  4.632525  4.5090904 4.523352  4.505247  4.2583084
 4.106771  3.9645486 3.8601966 3.8746696 4.133281  4.3481627 4.3264275
 3.3065948 2.8412356 2.755171  3.0222802 3.3445683 3.3286738 3.429405
 3.195695  2.8468173 2.6795456 2.6090157 2.5435379 2.529204  2.4564443
 2.2473693 1.9666778 1.7111484 1.5160632 1.4786025 1.5044852 1.9736019
 2.4042125 2.747726  3.020128  3.2222059 3.36922   3.5283988 3.7646818
 4.032148  4.1643276 4.0748596 3.845263  3.5521932 3.3171155 3.0481098
 2.6859186 2.6665545 3.104147  3.4124486 3.641531  4.2256927 4.8960257
 5.1703486]
