time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 102.62%, model saved.
Epoch: 0 Train: 3393.51904 Test: 4049.71558
Epoch 80: New minimal relative error: 42.20%, model saved.
Epoch: 80 Train: 96.52805 Test: 93.08696
Epoch 160: New minimal relative error: 34.69%, model saved.
Epoch: 160 Train: 21.33216 Test: 30.35674
Epoch: 240 Train: 21.54961 Test: 17.46002
Epoch 320: New minimal relative error: 9.33%, model saved.
Epoch: 320 Train: 6.50918 Test: 6.76847
Epoch: 400 Train: 6.38442 Test: 6.02838
Epoch: 480 Train: 11.77959 Test: 13.48616
Epoch: 560 Train: 14.06447 Test: 17.40406
Epoch: 640 Train: 11.31655 Test: 15.12435
Epoch: 720 Train: 10.27907 Test: 16.14688
Epoch: 800 Train: 22.87477 Test: 35.11526
Epoch: 880 Train: 22.89559 Test: 13.55236
Epoch: 960 Train: 6.59024 Test: 5.20290
Epoch: 1040 Train: 2.47841 Test: 1.93199
Epoch: 1120 Train: 1.69604 Test: 1.80687
Epoch: 1200 Train: 5.56933 Test: 3.89167
Epoch: 1280 Train: 1.17280 Test: 1.23995
Epoch: 1360 Train: 0.67306 Test: 0.56677
Epoch 1440: New minimal relative error: 8.72%, model saved.
Epoch: 1440 Train: 0.87098 Test: 0.90926
Epoch: 1520 Train: 1.43760 Test: 0.92376
Epoch: 1600 Train: 0.71873 Test: 0.92470
Epoch: 1680 Train: 0.65852 Test: 0.59662
Epoch 1760: New minimal relative error: 7.28%, model saved.
Epoch: 1760 Train: 0.50149 Test: 0.47787
Epoch: 1840 Train: 0.40669 Test: 0.37965
Epoch: 1920 Train: 0.42255 Test: 0.39982
Epoch 2000: New minimal relative error: 6.84%, model saved.
Epoch: 2000 Train: 0.47368 Test: 0.54528
Epoch: 2080 Train: 0.78407 Test: 0.89809
Epoch: 2160 Train: 0.97997 Test: 1.44853
Epoch: 2240 Train: 2.69904 Test: 1.98738
Epoch: 2320 Train: 2.92613 Test: 1.47242
Epoch: 2400 Train: 3.26043 Test: 2.63200
Epoch: 2480 Train: 0.81093 Test: 0.84123
Epoch: 2560 Train: 0.29797 Test: 0.36606
Epoch 2640: New minimal relative error: 4.55%, model saved.
Epoch: 2640 Train: 1.16388 Test: 1.60856
Epoch: 2720 Train: 1.64346 Test: 2.49447
Epoch: 2800 Train: 1.18578 Test: 1.40426
Epoch: 2880 Train: 0.76873 Test: 0.94095
Epoch: 2960 Train: 0.88816 Test: 1.07973
Epoch: 3040 Train: 0.49741 Test: 0.63189
Epoch: 3120 Train: 0.24448 Test: 0.29314
Epoch: 3200 Train: 2.81186 Test: 4.05801
Epoch: 3280 Train: 0.15777 Test: 0.16949
Epoch: 3360 Train: 0.39599 Test: 0.31571
Epoch: 3440 Train: 1.12380 Test: 0.94935
Epoch: 3520 Train: 0.54130 Test: 0.33203
Epoch: 3600 Train: 2.67474 Test: 3.07518
Epoch: 3680 Train: 0.37489 Test: 0.52754
Epoch: 3760 Train: 0.19916 Test: 0.21142
Epoch: 3840 Train: 0.19178 Test: 0.18255
Epoch: 3920 Train: 0.84656 Test: 1.22068
Epoch: 4000 Train: 0.12021 Test: 0.25512
Epoch: 4080 Train: 1.08892 Test: 0.87372
Epoch 4160: New minimal relative error: 3.16%, model saved.
Epoch: 4160 Train: 0.24988 Test: 0.31977
Epoch: 4240 Train: 0.23622 Test: 0.17531
Epoch: 4320 Train: 0.15074 Test: 0.18652
Epoch: 4400 Train: 0.58807 Test: 0.40672
Epoch: 4480 Train: 0.21317 Test: 0.24437
Epoch: 4560 Train: 0.27510 Test: 0.39385
Epoch: 4640 Train: 0.18917 Test: 0.25213
Epoch: 4720 Train: 0.38797 Test: 0.37209
Epoch: 4800 Train: 0.17885 Test: 0.13596
Epoch: 4880 Train: 0.27270 Test: 0.40768
Epoch: 4960 Train: 0.14545 Test: 0.15672
Epoch: 5040 Train: 0.60546 Test: 0.61525
Epoch: 5120 Train: 0.44878 Test: 0.39147
Epoch: 5200 Train: 0.09895 Test: 0.11622
Epoch 5280: New minimal relative error: 2.82%, model saved.
Epoch: 5280 Train: 0.08012 Test: 0.10200
Epoch: 5360 Train: 0.09788 Test: 0.10430
Epoch: 5440 Train: 0.08221 Test: 0.10138
Epoch: 5520 Train: 0.20812 Test: 0.27386
Epoch: 5600 Train: 0.36451 Test: 0.36372
Epoch: 5680 Train: 0.10799 Test: 0.12273
Epoch: 5760 Train: 0.51378 Test: 0.58089
Epoch: 5840 Train: 0.23106 Test: 0.20916
Epoch: 5920 Train: 0.06889 Test: 0.08906
Epoch: 6000 Train: 0.06588 Test: 0.08208
Epoch: 6080 Train: 0.09089 Test: 0.11786
Epoch: 6160 Train: 0.11727 Test: 0.13309
Epoch: 6240 Train: 0.11296 Test: 0.12448
Epoch: 6320 Train: 0.16225 Test: 0.17754
Epoch: 6400 Train: 0.49691 Test: 0.44553
Epoch: 6480 Train: 0.73276 Test: 0.89543
Epoch: 6560 Train: 0.15342 Test: 0.12051
Epoch: 6640 Train: 0.17020 Test: 0.25079
Epoch: 6720 Train: 0.13076 Test: 0.15144
Epoch: 6800 Train: 0.12743 Test: 0.12831
Epoch: 6880 Train: 0.18401 Test: 0.25217
Epoch: 6960 Train: 0.10597 Test: 0.12770
Epoch: 7040 Train: 0.07449 Test: 0.09349
Epoch: 7120 Train: 0.04923 Test: 0.06858
Epoch: 7200 Train: 0.05799 Test: 0.07960
Epoch: 7280 Train: 0.05361 Test: 0.07569
Epoch: 7360 Train: 0.08301 Test: 0.11081
Epoch: 7440 Train: 0.05623 Test: 0.07889
Epoch: 7520 Train: 0.04717 Test: 0.06507
Epoch: 7600 Train: 0.05274 Test: 0.07046
Epoch: 7680 Train: 0.08414 Test: 0.10470
Epoch: 7760 Train: 0.21971 Test: 0.24441
Epoch: 7840 Train: 0.04631 Test: 0.07377
Epoch: 7920 Train: 0.05718 Test: 0.08200
Epoch: 7999 Train: 0.05037 Test: 0.06157
Training Loss: tensor(0.0504)
Test Loss: tensor(0.0616)
Learned LE: [ 0.80754703  0.05641717 -3.2560413 ]
True LE: [ 8.8555712e-01  2.4050544e-03 -1.4563608e+01]
Relative Error: [1.2104859  1.1696382  1.3080922  1.503412   1.7235605  1.9003069
 1.931972   2.0524905  1.9534726  1.4790047  1.238016   1.3091785
 1.0535067  0.8152742  0.7321731  0.8564171  1.0531489  1.8178346
 2.3451169  2.6695955  3.4189243  3.8978171  4.4441447  5.1597824
 5.7560625  6.3269577  5.5577626  4.903345   4.2178106  3.425539
 2.463297   2.103542   2.0528448  1.9832492  2.1641371  2.3396602
 2.0313213  1.7569838  2.0415318  2.1629243  1.8790616  1.7476836
 2.0355926  2.0441835  1.7070383  1.4091907  1.1651995  0.7555277
 0.97538286 0.9887528  0.7716491  0.52033186 0.70914173 0.86374766
 0.8553257  1.0224125  1.268982   1.3377746  1.283238   1.1425202
 1.089917   1.0502914  0.96993184 0.900553   0.9470882  1.0292774
 1.105567   1.3472434  1.6824564  1.7960787  1.8069813  1.4338584
 0.9473974  0.82777005 0.8131369  0.7589954  0.7922646  0.8346109
 0.86119854 1.2824402  2.1452045  2.3898387  3.0691938  3.5542383
 4.079002   4.7823486  5.168349   5.5345836  4.991642   4.419301
 3.8219552  3.1577685  2.5704489  1.9908385  1.8877994  1.7005348
 1.4724498  1.7005373  1.7610676  1.4242282  1.1918572  1.477271
 1.3991513  1.216764   1.275477   1.4822583  1.359114   1.0453345
 0.9415147  0.7491579  0.8556558  0.96328604 0.9052461  0.6743936
 0.622025   0.85024714 0.88497645 0.9607158  1.1870021  0.9776089
 0.8965348  0.77035415 0.6320659  0.68176633 0.6525724  0.63551867
 0.7113561  0.82904017 0.92313814 0.9291482  0.98609114 1.3780824
 1.5659425  1.4321297  1.0122806  0.8256305  0.8018661  0.89033395
 1.0449356  0.9006994  0.8501387  0.89410925 1.9803133  2.2458546
 2.6860542  3.2369807  3.7148037  3.8304799  3.8326633  3.7770283
 4.017197   3.1285617  2.623789   2.1339326  1.4923916  1.585103
 1.5272274  1.5019425  1.182684   0.8693986  1.1503273  1.2001498
 0.7301234  0.6299651  0.7567987  0.7249703  0.5743426  0.6932643
 0.9444854  0.84667504 0.79572433 0.7549969  0.7595876  0.9397919
 1.0195366  1.0139691  0.7436346  0.71380806 0.78480315 0.88827115
 1.0003798  0.7932516  0.6973783  0.7112466  0.46105227 0.30065757
 0.33336347 0.373474   0.46330225 0.60584366 0.6681837  0.724583
 0.7965581  0.7502974  0.884536   1.2979378  1.1387469  0.96417516
 0.9084681  1.085511   1.1609068  1.0913626  0.92614675 0.69495434
 1.2629813  2.0156252  2.237802   2.8462832  3.317402   2.9950914
 2.5416892  2.2776344  2.266043   2.021845   1.5680788  1.1585573
 0.6463665  0.6355372  0.84240407 1.1664248  0.9499167  0.8321353
 0.50926924 0.7864191  0.7050666  0.44040686 0.33336776 0.35070452
 0.36517662 0.36664647 0.607864   0.766919   0.8024843  0.91227525
 0.57797915 0.4516163  0.5749397  0.8521564  0.75922185 0.8213345
 0.80103755 0.83720917 1.0088668  0.7718526  0.6205695  0.6990551
 0.702288   0.40224636 0.0975548  0.1134847  0.22291404 0.42673498
 0.61373997 0.61796856 0.61091495 0.73391026 0.70485073 0.43394062
 0.93153846 1.0687785  1.1597395  1.1064472  1.3181411  1.1287912
 0.9182524  0.7679283  0.62780935 1.6527634  1.9519663  2.410038
 2.7409606  2.410191   1.7768297  1.4165654  1.4186842  1.6635895
 1.4965886  0.9532786  0.56016713 0.6019116  0.5576782  0.49195188
 0.7477042  0.7058461  0.4136019  0.23794645 0.5040551  0.5000197
 0.4336478  0.35152894 0.5581179  0.51570654 0.48004007 0.6091409
 0.8499966  0.7673359  0.51690495 0.4242994  0.14455341 0.4716291
 0.55225337 0.48157695 0.6201832  0.82964927 1.0981666  0.80971074
 0.6143474  0.5228469  0.7318121  0.77571684 0.44006142 0.2548639
 0.21003115 0.13045187 0.27658623 0.56755346 0.7614569  0.6708209
 0.585594   0.7216641  0.3422295  0.36802146 0.6781741  1.3281536
 1.2584151  1.2736025  1.1242726  0.9244175  0.5805068  0.9559136
 1.8315676  2.0123236  2.4327826  2.47564    1.5208954  0.9018627
 0.908016   1.1439955  1.4721506  1.3822137  0.87250954 0.6243599
 0.59828097 0.5674574  0.43485686 0.5142549  0.5875113  0.43541557
 0.37771726 0.5889383  0.410012   0.5257096  0.45955622 0.6059193
 0.5122549  0.50765485 0.57595855 0.6434225  0.5248532  0.2987939
 0.25051114 0.18571146 0.32483628 0.30761182 0.339592   0.41465428
 0.59706706 0.7364087  0.6530767  0.47409168 0.36593184 0.47164744
 0.5588392  0.41400316 0.3996095  0.26440862 0.10887077 0.14015503
 0.37130746 0.5089577  0.57619673 0.40275183 0.41643274 0.5822913
 0.30371347 0.43425733 0.6493477  1.1090969  1.2505668  1.1218002
 0.91596746 0.8076643  1.2280484  1.9193295  2.0816908  2.254478
 2.2050154  1.2220318  0.6989082  0.66524804]
