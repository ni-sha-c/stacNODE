time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.34%, model saved.
Epoch: 0 Train: 32152.44141 Test: 4238.16797
Epoch 100: New minimal relative error: 103.63%, model saved.
Epoch: 100 Train: 8029.50049 Test: 1432.89026
Epoch 200: New minimal relative error: 59.91%, model saved.
Epoch: 200 Train: 6737.67969 Test: 1028.37866
Epoch: 300 Train: 6746.08838 Test: 1170.97217
Epoch 400: New minimal relative error: 54.07%, model saved.
Epoch: 400 Train: 6553.03125 Test: 1139.76440
Epoch: 500 Train: 4364.29297 Test: 472.86374
Epoch: 600 Train: 2758.88110 Test: 290.32938
Epoch: 700 Train: 1379.42432 Test: 82.39406
Epoch: 800 Train: 740.22852 Test: 59.92978
Epoch 900: New minimal relative error: 38.84%, model saved.
Epoch: 900 Train: 446.47989 Test: 46.15249
Epoch 1000: New minimal relative error: 19.69%, model saved.
Epoch: 1000 Train: 429.55341 Test: 14.80401
Epoch: 1100 Train: 263.40387 Test: 5.68665
Epoch: 1200 Train: 216.21193 Test: 4.85873
Epoch 1300: New minimal relative error: 8.34%, model saved.
Epoch: 1300 Train: 199.08469 Test: 4.79589
Epoch: 1400 Train: 197.38432 Test: 24.91211
Epoch: 1500 Train: 148.46533 Test: 9.82241
Epoch 1600: New minimal relative error: 4.58%, model saved.
Epoch: 1600 Train: 133.31017 Test: 1.63408
Epoch: 1700 Train: 143.02386 Test: 4.29978
Epoch: 1800 Train: 145.30710 Test: 1.53509
Epoch: 1900 Train: 116.20422 Test: 0.97881
Epoch: 2000 Train: 171.61478 Test: 32.35005
Epoch: 2100 Train: 99.55017 Test: 1.16483
Epoch: 2200 Train: 133.95589 Test: 7.69939
Epoch 2300: New minimal relative error: 3.42%, model saved.
Epoch: 2300 Train: 93.01875 Test: 0.58832
Epoch: 2400 Train: 90.99407 Test: 0.92944
Epoch: 2500 Train: 97.01883 Test: 2.43875
Epoch: 2600 Train: 85.15381 Test: 1.28194
Epoch: 2700 Train: 90.25123 Test: 2.63506
Epoch: 2800 Train: 67.08080 Test: 0.34477
Epoch: 2900 Train: 117.63994 Test: 14.45247
Epoch 3000: New minimal relative error: 2.40%, model saved.
Epoch: 3000 Train: 60.14878 Test: 0.23374
Epoch: 3100 Train: 60.16857 Test: 0.23914
Epoch 3200: New minimal relative error: 2.19%, model saved.
Epoch: 3200 Train: 59.09357 Test: 0.24928
Epoch: 3300 Train: 58.66705 Test: 0.58006
Epoch: 3400 Train: 59.16957 Test: 1.90156
Epoch: 3500 Train: 56.36521 Test: 0.94530
Epoch: 3600 Train: 57.33618 Test: 1.31054
Epoch: 3700 Train: 67.93008 Test: 2.35036
Epoch 3800: New minimal relative error: 2.12%, model saved.
Epoch: 3800 Train: 52.10990 Test: 0.21503
Epoch: 3900 Train: 51.81324 Test: 0.23782
Epoch: 4000 Train: 52.27444 Test: 1.92926
Epoch: 4100 Train: 53.59007 Test: 0.30435
Epoch: 4200 Train: 53.24497 Test: 2.35731
Epoch: 4300 Train: 47.95797 Test: 1.24293
Epoch: 4400 Train: 48.49203 Test: 2.53203
Epoch: 4500 Train: 45.65074 Test: 0.88429
Epoch: 4600 Train: 47.24427 Test: 0.17557
Epoch: 4700 Train: 45.54983 Test: 0.16474
Epoch: 4800 Train: 48.00464 Test: 0.28773
Epoch: 4900 Train: 45.89866 Test: 0.16755
Epoch: 5000 Train: 40.84207 Test: 0.15903
Epoch: 5100 Train: 40.74092 Test: 0.14981
Epoch: 5200 Train: 39.94973 Test: 0.18212
Epoch: 5300 Train: 39.96467 Test: 0.44767
Epoch: 5400 Train: 46.08932 Test: 0.75323
Epoch: 5500 Train: 38.69511 Test: 0.66074
Epoch: 5600 Train: 40.93351 Test: 1.04332
Epoch: 5700 Train: 36.05398 Test: 0.11271
Epoch: 5800 Train: 38.89489 Test: 0.15869
Epoch: 5900 Train: 39.10484 Test: 0.23618
Epoch: 6000 Train: 39.59421 Test: 0.73766
Epoch: 6100 Train: 37.32270 Test: 0.12503
Epoch 6200: New minimal relative error: 2.07%, model saved.
Epoch: 6200 Train: 39.48460 Test: 0.23336
Epoch: 6300 Train: 39.82867 Test: 0.19381
Epoch 6400: New minimal relative error: 1.50%, model saved.
Epoch: 6400 Train: 39.59721 Test: 0.34890
Epoch: 6500 Train: 37.17881 Test: 0.34602
Epoch: 6600 Train: 36.90324 Test: 0.19181
Epoch: 6700 Train: 35.61460 Test: 0.19670
Epoch: 6800 Train: 34.89321 Test: 0.46117
Epoch: 6900 Train: 33.88104 Test: 0.82243
Epoch: 7000 Train: 33.13433 Test: 0.11387
Epoch: 7100 Train: 32.57335 Test: 0.20836
Epoch: 7200 Train: 31.13034 Test: 0.23359
Epoch: 7300 Train: 29.98993 Test: 0.07094
Epoch: 7400 Train: 29.87033 Test: 0.16292
Epoch 7500: New minimal relative error: 1.43%, model saved.
Epoch: 7500 Train: 29.59715 Test: 0.12972
Epoch: 7600 Train: 29.75498 Test: 0.08324
Epoch: 7700 Train: 29.57171 Test: 0.39881
Epoch: 7800 Train: 28.21211 Test: 0.07639
Epoch: 7900 Train: 28.10845 Test: 0.27047
Epoch: 8000 Train: 30.80289 Test: 1.13613
Epoch: 8100 Train: 28.08077 Test: 0.06069
Epoch: 8200 Train: 28.00462 Test: 0.06043
Epoch 8300: New minimal relative error: 0.91%, model saved.
Epoch: 8300 Train: 28.70999 Test: 0.06496
Epoch: 8400 Train: 32.20627 Test: 1.17249
Epoch: 8500 Train: 29.27771 Test: 0.11762
Epoch: 8600 Train: 29.12223 Test: 0.08326
Epoch: 8700 Train: 29.98685 Test: 0.17307
Epoch: 8800 Train: 30.38123 Test: 0.21666
Epoch: 8900 Train: 29.87599 Test: 0.09033
Epoch: 9000 Train: 30.24747 Test: 0.59798
Epoch: 9100 Train: 28.98740 Test: 0.23996
Epoch: 9200 Train: 28.35006 Test: 0.07878
Epoch: 9300 Train: 27.59212 Test: 0.06901
Epoch: 9400 Train: 26.75167 Test: 0.12665
Epoch: 9500 Train: 27.34397 Test: 0.06765
Epoch: 9600 Train: 27.09118 Test: 0.08569
Epoch: 9700 Train: 28.12961 Test: 0.42457
Epoch: 9800 Train: 30.72540 Test: 0.21682
Epoch: 9900 Train: 30.37466 Test: 0.17410
Epoch: 9999 Train: 29.83248 Test: 0.12141
Training Loss: tensor(29.8325)
Test Loss: tensor(0.1214)
Learned LE: [ 8.6083680e-01  3.4407447e-03 -1.4534222e+01]
True LE: [ 8.6601180e-01 -6.5042251e-03 -1.4541409e+01]
Relative Error: [1.8395159  1.9961287  2.0432894  2.1331313  2.1890593  2.2650902
 2.1678972  1.9553797  1.6933311  1.6763523  1.6242759  1.625129
 1.668925   1.5487099  1.4925375  1.2616326  1.0038911  0.9597931
 1.2143745  1.5474088  1.8096155  1.9244691  1.8261297  1.6702868
 1.2986939  1.1641968  1.3266271  1.3806399  1.6556916  1.7978506
 1.7734578  1.7641925  1.6105853  1.5713668  1.6301788  1.655372
 1.3471695  0.92141765 0.63130957 0.5053168  0.47727755 0.5849663
 0.7460963  0.98219264 1.1400591  1.309896   1.3855963  1.365162
 0.9357166  0.588868   0.60130495 0.8624346  1.0800269  1.3680799
 1.295316   1.216509   1.2495034  1.2997924  1.3477169  1.425791
 1.4956844  1.6598477  1.7962555  1.9179649  2.0267043  2.120737
 2.1083605  2.1142464  2.0536394  1.8005334  1.6050726  1.5701253
 1.4399276  1.4427899  1.4911306  1.3631127  1.2702729  1.2390372
 0.94723815 0.9012374  1.106537   1.4088706  1.6467979  1.7116715
 1.6223632  1.5463198  1.2519431  1.0523462  1.0870984  1.2187992
 1.5937783  1.8372595  1.8115675  1.8301971  1.7313182  1.6370194
 1.7470387  1.8287711  1.5630255  1.1667191  0.71296686 0.5112304
 0.42677698 0.52827424 0.6652173  0.8521841  0.9619112  1.0937897
 1.1650671  1.1486632  0.9237791  0.5848539  0.57502353 0.68410426
 0.8819871  1.143786   1.063309   1.0221566  1.1585348  1.2677171
 1.293396   1.3904757  1.4864409  1.5623405  1.7139611  1.9145118
 2.0416818  2.1934872  2.1192622  2.0513065  1.9408513  1.7039518
 1.530719   1.506152   1.3116065  1.2749728  1.3075899  1.2216375
 1.1345676  1.0306066  0.87537104 0.7858402  0.957765   1.2784375
 1.5441455  1.5242794  1.4425845  1.3973659  1.1374054  1.0101674
 0.85085136 1.0328045  1.4475735  1.8360814  1.7897782  1.8529227
 1.8387065  1.701402   1.812465   1.9923893  1.8166151  1.3982947
 0.808783   0.47043636 0.39357346 0.4340574  0.59664345 0.7520394
 0.79830754 0.8435457  0.9306464  1.1248957  1.0086429  0.6345327
 0.51648754 0.47435313 0.6861961  0.9440669  0.8662543  0.87336266
 1.0716015  1.2387309  1.2840216  1.3803     1.4194403  1.46421
 1.5724168  1.7340763  1.960857   2.0880613  2.1265004  2.0864947
 1.9477345  1.5974786  1.5675439  1.4701618  1.272788   1.1243771
 1.1433501  1.1391488  1.0480335  0.832033   0.7400558  0.7094861
 0.82054895 1.123563   1.4178747  1.3488426  1.2306924  1.3558097
 1.0971788  0.8841776  0.75410855 0.9084211  1.3030297  1.7196901
 1.7957976  1.8033323  1.8589486  1.7290112  1.8767542  2.0821798
 2.0039601  1.5533321  0.9787679  0.6498363  0.38793644 0.39345202
 0.5062607  0.6521401  0.68643945 0.6440624  0.84907484 1.082916
 0.9752076  0.710698   0.48881656 0.36754835 0.5390408  0.76623803
 0.6825091  0.7028438  0.91782606 1.0893321  1.1485417  1.2736028
 1.4059439  1.4474734  1.4255965  1.4827044  1.659537   1.926281
 2.0022879  1.9231448  1.8560519  1.5533214  1.56969    1.5443479
 1.2406495  1.0959946  1.0592991  1.0006354  0.9515391  0.70083773
 0.5447019  0.57521105 0.7134602  0.9261047  1.2146114  1.164567
 0.93765056 1.1163968  1.1299803  0.8472654  0.65773946 0.796247
 1.1934094  1.5593153  1.7585533  1.6585972  1.7311279  1.7165674
 1.8224134  2.1002696  2.1130776  1.6270306  1.1483     0.77453804
 0.50173765 0.33882138 0.4357982  0.532228   0.6047663  0.5807361
 0.7336435  1.0249386  0.95927185 0.8594973  0.58321476 0.42556456
 0.49504218 0.645909   0.47710755 0.48856264 0.7530707  0.93411636
 1.0233098  1.1239157  1.2948439  1.3600166  1.3134184  1.3829793
 1.504862   1.6844951  1.8963497  1.8891821  1.7706834  1.4709734
 1.3742635  1.5866048  1.3376081  1.0854367  1.0851831  0.91327494
 0.8338972  0.5578353  0.3418913  0.456896   0.4721605  0.74073565
 1.0110855  1.0715778  0.75573295 0.7459319  1.0176156  0.8219216
 0.6135286  0.7225187  0.9805304  1.3283842  1.5774103  1.6426299
 1.4826822  1.6505767  1.7076405  1.9736929  2.117488   1.5867412
 1.217079   0.86106414 0.6165426  0.37940574 0.30284637 0.38533998
 0.5559097  0.5949192  0.69043833 0.9525471  0.9971291  0.9604986
 0.6440295  0.48748603 0.48722187 0.58216465 0.42457026 0.36402577
 0.5670051  0.7735633  0.880022   1.0111556  1.0993236  1.235205
 1.337574   1.2558254  1.4002081  1.5730033  1.7495632  1.9103928
 1.8088583  1.5639534  1.1929529  1.3468066  1.4053161  1.1486117
 1.056524   0.94976723 0.7811228  0.4462234  0.24752624 0.21103176
 0.39459896 0.5134401  0.86594176 1.1044552  0.7347425  0.6018287
 0.841252   0.80857605 0.5827021  0.6222026 ]
