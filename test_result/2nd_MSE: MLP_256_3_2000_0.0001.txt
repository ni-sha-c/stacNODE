time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.44%, model saved.
Epoch: 0 Train: 3688.79712 Test: 4214.39453
Epoch 100: New minimal relative error: 25.50%, model saved.
Epoch: 100 Train: 116.84987 Test: 127.61930
Epoch 200: New minimal relative error: 13.29%, model saved.
Epoch: 200 Train: 14.31403 Test: 17.07301
Epoch: 300 Train: 7.08884 Test: 8.81694
Epoch: 400 Train: 4.40950 Test: 8.31286
Epoch: 500 Train: 6.57263 Test: 4.64736
Epoch: 600 Train: 1.96436 Test: 2.49482
Epoch 700: New minimal relative error: 10.25%, model saved.
Epoch: 700 Train: 1.69294 Test: 2.21314
Epoch: 800 Train: 3.18417 Test: 3.21446
Epoch: 900 Train: 1.12663 Test: 1.46492
Epoch: 1000 Train: 1.64145 Test: 1.58104
Epoch: 1100 Train: 0.78194 Test: 1.03744
Epoch: 1200 Train: 4.76517 Test: 4.12790
Epoch: 1300 Train: 0.95579 Test: 1.00558
Epoch: 1400 Train: 0.71159 Test: 0.83096
Epoch: 1500 Train: 0.44579 Test: 0.57871
Epoch: 1600 Train: 0.47686 Test: 0.56398
Epoch: 1700 Train: 0.49357 Test: 0.65177
Epoch: 1800 Train: 0.79849 Test: 1.09159
Epoch: 1900 Train: 0.63992 Test: 0.47074
Epoch: 2000 Train: 0.43806 Test: 0.54510
Epoch: 2100 Train: 0.94001 Test: 1.17264
Epoch 2200: New minimal relative error: 6.93%, model saved.
Epoch: 2200 Train: 0.34473 Test: 0.38741
Epoch 2300: New minimal relative error: 6.17%, model saved.
Epoch: 2300 Train: 0.23599 Test: 0.29299
Epoch: 2400 Train: 2.71946 Test: 3.15669
Epoch: 2500 Train: 0.30092 Test: 0.30249
Epoch: 2600 Train: 0.26292 Test: 0.27030
Epoch: 2700 Train: 0.19832 Test: 0.25338
Epoch: 2800 Train: 0.36101 Test: 0.58403
Epoch: 2900 Train: 0.39412 Test: 0.49193
Epoch: 3000 Train: 1.01821 Test: 0.97401
Epoch: 3100 Train: 0.31658 Test: 0.32881
Epoch: 3200 Train: 0.24160 Test: 0.31680
Epoch: 3300 Train: 0.61599 Test: 0.47267
Epoch: 3400 Train: 0.23644 Test: 0.26459
Epoch: 3500 Train: 0.13843 Test: 0.17950
Epoch: 3600 Train: 0.32653 Test: 0.39389
Epoch: 3700 Train: 0.14443 Test: 0.21012
Epoch: 3800 Train: 0.14711 Test: 0.18053
Epoch: 3900 Train: 0.12436 Test: 0.16700
Epoch: 4000 Train: 0.15739 Test: 0.19721
Epoch: 4100 Train: 0.12592 Test: 0.16998
Epoch: 4200 Train: 0.11104 Test: 0.15003
Epoch: 4300 Train: 0.12656 Test: 0.16338
Epoch: 4400 Train: 1.90567 Test: 1.78267
Epoch: 4500 Train: 0.10322 Test: 0.13926
Epoch: 4600 Train: 0.10048 Test: 0.13633
Epoch: 4700 Train: 0.10193 Test: 0.13549
Epoch: 4800 Train: 0.09625 Test: 0.13204
Epoch: 4900 Train: 0.09854 Test: 0.13424
Epoch: 5000 Train: 0.11875 Test: 0.16304
Epoch: 5100 Train: 1.38258 Test: 1.50313
Epoch: 5200 Train: 0.10137 Test: 0.12901
Epoch: 5300 Train: 0.09058 Test: 0.12922
Epoch: 5400 Train: 0.30857 Test: 0.33308
Epoch: 5500 Train: 0.64424 Test: 0.57000
Epoch: 5600 Train: 0.10371 Test: 0.12756
Epoch: 5700 Train: 0.08076 Test: 0.11349
Epoch: 5800 Train: 0.07766 Test: 0.11069
Epoch: 5900 Train: 0.07673 Test: 0.11003
Epoch: 6000 Train: 0.08447 Test: 0.12930
Epoch: 6100 Train: 0.15661 Test: 0.22375
Epoch: 6200 Train: 0.07232 Test: 0.10489
Epoch: 6300 Train: 0.23973 Test: 0.20269
Epoch: 6400 Train: 0.20281 Test: 0.27894
Epoch: 6500 Train: 0.06837 Test: 0.10020
Epoch: 6600 Train: 0.06766 Test: 0.10056
Epoch: 6700 Train: 0.07001 Test: 0.10413
Epoch: 6800 Train: 0.06526 Test: 0.09707
Epoch: 6900 Train: 0.08689 Test: 0.12638
Epoch: 7000 Train: 0.06288 Test: 0.09280
Epoch: 7100 Train: 0.06251 Test: 0.09332
Epoch: 7200 Train: 0.07110 Test: 0.10778
Epoch: 7300 Train: 0.06004 Test: 0.09131
Epoch: 7400 Train: 0.05903 Test: 0.09043
Epoch: 7500 Train: 0.05817 Test: 0.08975
Epoch: 7600 Train: 0.14456 Test: 0.22299
Epoch: 7700 Train: 0.06322 Test: 0.09556
Epoch: 7800 Train: 0.19273 Test: 0.17757
Epoch: 7900 Train: 0.17152 Test: 0.24011
Epoch: 8000 Train: 0.05411 Test: 0.08495
Epoch: 8100 Train: 0.05321 Test: 0.08433
Epoch: 8200 Train: 0.05918 Test: 0.09015
Epoch: 8300 Train: 0.05141 Test: 0.08180
Epoch: 8400 Train: 0.05090 Test: 0.08183
Epoch: 8500 Train: 0.05005 Test: 0.08024
Epoch: 8600 Train: 0.04950 Test: 0.08002
Epoch: 8700 Train: 0.17685 Test: 0.15425
Epoch: 8800 Train: 0.12253 Test: 0.10418
Epoch: 8900 Train: 0.04758 Test: 0.07753
Epoch: 9000 Train: 0.04873 Test: 0.08031
Epoch: 9100 Train: 0.04643 Test: 0.07621
Epoch: 9200 Train: 0.05756 Test: 0.09011
Epoch: 9300 Train: 0.04569 Test: 0.07563
Epoch: 9400 Train: 0.04486 Test: 0.07462
Epoch: 9500 Train: 0.04579 Test: 0.07550
Epoch: 9600 Train: 0.04396 Test: 0.07364
Epoch: 9700 Train: 0.04335 Test: 0.07269
Epoch: 9800 Train: 0.05767 Test: 0.07925
Epoch: 9900 Train: 0.04252 Test: 0.07186
Epoch: 9999 Train: 0.04202 Test: 0.07127
Training Loss: tensor(0.0420)
Test Loss: tensor(0.0713)
Learned LE: [ 8.6803615e-01  3.1075811e-03 -4.2265563e+00]
True LE: [ 8.5384184e-01  5.9057334e-03 -1.4544742e+01]
Relative Error: [0.8594455  0.91081697 0.95356023 0.9591376  0.91740304 0.8513674
 0.81743777 0.86029583 0.9561395  1.0318017  1.0229214  0.9232615
 0.7952639  0.7267008  0.79323965 0.9084249  0.89251995 0.7227756
 0.52964145 0.36777797 0.20697771 0.21165968 0.4141674  0.62222576
 0.79283285 0.91720307 0.99992454 1.0585055  1.1178219  1.1882337
 1.2489183  1.2683247  1.2234606  1.0899459  0.89455533 0.7477952
 0.68261015 0.60628355 0.43993717 0.22505191 0.21079414 0.4112074
 0.6614946  0.89146477 0.9805608  0.8933672  0.72713006 0.6277551
 0.7292445  0.8584862  0.75194037 0.56617296 0.56067026 0.64830655
 0.7383156  0.8241403  0.8938975  0.92652774 0.91247976 0.86119026
 0.7965004  0.7499598  0.7489202  0.79759645 0.86979324 0.92727935
 0.94240457 0.9179857  0.8978896  0.93590283 1.0264943  1.0974096
 1.0714422  0.9360282  0.7718773  0.6847223  0.73809576 0.84559965
 0.85003424 0.72519076 0.6141951  0.55083364 0.42244744 0.27283207
 0.32590407 0.52942985 0.7287433  0.88056344 0.971008   1.0051789
 1.0085754  1.0250938  1.0799872  1.1409693  1.1650453  1.1097269
 0.9285992  0.7334821  0.6460555  0.596682   0.46329474 0.23225057
 0.20327826 0.42298022 0.68183154 0.92995286 1.0375588  0.95356864
 0.7658997  0.5622481  0.49846783 0.6063142  0.53466904 0.42517477
 0.53225785 0.6628325  0.7575396  0.84553874 0.9282629  0.97939587
 0.9797909  0.93113947 0.850504   0.7645062  0.70828855 0.71335506
 0.7802477  0.8741571  0.9522342  0.9921677  1.0109419  1.0562149
 1.143347   1.2131845  1.1775602  1.0068972  0.7941114  0.6763281
 0.7062645  0.78725255 0.79562634 0.726713   0.7095539  0.7500638
 0.6823594  0.51296234 0.4010681  0.4845249  0.6644993  0.8376327
 0.9592715  1.0120459  0.9996126  0.944742   0.9104153  0.95576596
 1.0398868  1.0874388  0.9907868  0.7626653  0.64489007 0.6094116
 0.5217011  0.2923824  0.17910929 0.40266976 0.66209143 0.9252686
 1.0671643  1.0090164  0.8441217  0.6375324  0.42037874 0.40509716
 0.4484506  0.52339286 0.68059766 0.7889415  0.84503007 0.9004742
 0.97643507 1.0435054  1.068488   1.0419104  0.97201616 0.87619406
 0.7812321  0.7248478  0.74096143 0.82812595 0.9477837  1.0579084
 1.1405903  1.2128155  1.2969584  1.3623133  1.3247108  1.1285374
 0.85411334 0.6815011  0.68614733 0.75765854 0.7713896  0.74431926
 0.781551   0.8997633  0.92847776 0.81145203 0.6495304  0.5772096
 0.6414125  0.7750282  0.9067539  0.9889068  1.0083336  0.966807
 0.86764294 0.77679574 0.8260564  0.9679299  1.0187569  0.8433726
 0.6709228  0.63380605 0.58393747 0.39762655 0.15076096 0.3424165
 0.607651   0.8783276  1.0695752  1.0587465  0.93619937 0.802223
 0.63342005 0.47474843 0.5287828  0.73186296 0.91445726 0.9862656
 0.9862799  0.98250943 1.0256811  1.0953249  1.1440338  1.1491714
 1.1099261  1.0344576  0.938752   0.85117966 0.81078804 0.8486892
 0.9609093  1.1125636  1.2642581  1.3897736  1.4822111  1.5332724
 1.4944602  1.2972711  0.9707342  0.7136565  0.67604214 0.7611099
 0.81184477 0.8241115  0.8593584  0.98346406 1.1190007  1.0984197
 0.9276335  0.7378151  0.65822375 0.70617616 0.8245117  0.9376602
 0.97651255 0.95288986 0.89530593 0.7798957  0.63146937 0.7034831
 0.9157411  0.92152727 0.7277337  0.6645918  0.63091534 0.5274494
 0.24358031 0.22503029 0.5183865  0.7936336  1.0406854  1.1130375
 1.0378745  0.9642222  0.8930347  0.73779327 0.6408311  0.8223935
 1.0709924  1.1696036  1.1431062  1.0793576  1.0684221  1.124947
 1.1887943  1.2206421  1.2158029  1.177173   1.1098514  1.0297095
 0.9675056  0.960664   1.0324223  1.1765417  1.3642205  1.5510789
 1.6833606  1.725593   1.6702024  1.4904704  1.1608628  0.81212914
 0.68393004 0.77220106 0.8752631  0.94615555 1.0005455  1.0534427
 1.1751701  1.2134031  1.0288825  0.7390749  0.5300755  0.49110624
 0.5501598  0.6923676  0.8845836  0.96519905 0.88690275 0.7864169
 0.67475563 0.4886931  0.5863956  0.8454129  0.8224058  0.6854607
 0.6691115  0.6227736  0.46431315 0.14244635 0.36162114 0.667828
 0.9543311  1.1518958  1.1469545  1.0726876  1.0170493  0.89978206
 0.6729321  0.6485866  0.9223037  1.1524723  1.2117426  1.1562867
 1.095682   1.1171501  1.1908387  1.2455381  1.2630535  1.2539438
 1.2249523  1.1788771  1.1299967  1.1090086  1.1482875  1.2620203
 1.4420767  1.6589682  1.8493844  1.9294534  1.8604     1.6742002
 1.3854468  1.0075307  0.7377532  0.75379515 0.9046685  1.0229135
 1.1435492  1.2156625  1.1819515  1.104792   0.9446406  0.67410064
 0.4218065  0.344477   0.37832767 0.3763329 ]
