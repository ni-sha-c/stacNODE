time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 103.52%, model saved.
Epoch: 0 Train: 4116.21729 Test: 4057.13818
Epoch: 80 Train: 76.72287 Test: 90.14806
Epoch 160: New minimal relative error: 44.99%, model saved.
Epoch: 160 Train: 26.72733 Test: 19.01148
Epoch 240: New minimal relative error: 12.51%, model saved.
Epoch: 240 Train: 12.74422 Test: 17.36692
Epoch: 320 Train: 4.78524 Test: 6.90670
Epoch: 400 Train: 8.54636 Test: 27.25784
Epoch: 480 Train: 7.00218 Test: 5.00906
Epoch: 560 Train: 7.93817 Test: 6.96575
Epoch: 640 Train: 2.13265 Test: 5.26950
Epoch: 720 Train: 1.51148 Test: 3.14692
Epoch: 800 Train: 4.39341 Test: 5.34807
Epoch: 880 Train: 2.24861 Test: 3.75564
Epoch: 960 Train: 4.40255 Test: 4.73965
Epoch: 1040 Train: 0.85242 Test: 2.36643
Epoch: 1120 Train: 10.46930 Test: 11.62092
Epoch: 1200 Train: 3.18051 Test: 5.93871
Epoch: 1280 Train: 1.21781 Test: 2.10765
Epoch 1360: New minimal relative error: 10.72%, model saved.
Epoch: 1360 Train: 1.01262 Test: 1.81455
Epoch: 1440 Train: 0.73524 Test: 1.53587
Epoch 1520: New minimal relative error: 7.63%, model saved.
Epoch: 1520 Train: 3.24341 Test: 4.71163
Epoch: 1600 Train: 2.50309 Test: 4.53893
Epoch: 1680 Train: 1.30137 Test: 2.84094
Epoch: 1760 Train: 0.29851 Test: 1.94862
Epoch 1840: New minimal relative error: 6.05%, model saved.
Epoch: 1840 Train: 1.40279 Test: 1.51597
Epoch 1920: New minimal relative error: 5.94%, model saved.
Epoch: 1920 Train: 1.38330 Test: 2.32616
Epoch: 2000 Train: 1.45452 Test: 2.88044
Epoch: 2080 Train: 2.22438 Test: 3.27581
Epoch: 2160 Train: 1.12834 Test: 1.82990
Epoch: 2240 Train: 3.93925 Test: 4.99043
Epoch: 2320 Train: 1.13199 Test: 1.26956
Epoch: 2400 Train: 0.18566 Test: 1.04699
Epoch: 2480 Train: 1.09289 Test: 2.46861
Epoch: 2560 Train: 1.09219 Test: 2.21475
Epoch: 2640 Train: 0.70698 Test: 1.39797
Epoch 2720: New minimal relative error: 4.40%, model saved.
Epoch: 2720 Train: 2.33598 Test: 2.41818
Epoch: 2800 Train: 4.28684 Test: 6.56896
Epoch 2880: New minimal relative error: 3.76%, model saved.
Epoch: 2880 Train: 0.21402 Test: 1.08836
Epoch: 2960 Train: 0.09240 Test: 0.77525
Epoch: 3040 Train: 0.08161 Test: 0.75154
Epoch: 3120 Train: 0.17714 Test: 0.84635
Epoch: 3200 Train: 1.47568 Test: 1.97601
Epoch: 3280 Train: 1.17058 Test: 1.23451
Epoch: 3360 Train: 0.35471 Test: 1.16335
Epoch: 3440 Train: 0.23579 Test: 0.91045
Epoch: 3520 Train: 0.09225 Test: 0.82879
Epoch: 3600 Train: 0.65898 Test: 1.21589
Epoch: 3680 Train: 0.06678 Test: 0.73334
Epoch 3760: New minimal relative error: 2.83%, model saved.
Epoch: 3760 Train: 0.05004 Test: 0.71035
Epoch: 3840 Train: 0.11102 Test: 0.71988
Epoch: 3920 Train: 0.07866 Test: 0.71188
Epoch: 4000 Train: 2.84764 Test: 4.49749
Epoch: 4080 Train: 0.17415 Test: 0.78460
Epoch: 4160 Train: 0.08657 Test: 0.77285
Epoch: 4240 Train: 0.06664 Test: 0.71191
Epoch: 4320 Train: 0.10206 Test: 0.67565
Epoch: 4400 Train: 0.25070 Test: 0.90726
Epoch: 4480 Train: 0.64393 Test: 1.96241
Epoch: 4560 Train: 0.32648 Test: 0.83538
Epoch: 4640 Train: 1.02075 Test: 1.81534
Epoch: 4720 Train: 0.13913 Test: 0.70401
Epoch 4800: New minimal relative error: 2.45%, model saved.
Epoch: 4800 Train: 0.03551 Test: 0.60522
Epoch: 4880 Train: 0.06209 Test: 0.60500
Epoch: 4960 Train: 0.29987 Test: 1.00242
Epoch: 5040 Train: 0.10074 Test: 0.65613
Epoch: 5120 Train: 0.26795 Test: 1.09372
Epoch: 5200 Train: 0.14327 Test: 0.70216
Epoch: 5280 Train: 0.04752 Test: 0.59029
Epoch: 5360 Train: 0.05397 Test: 0.62333
Epoch: 5440 Train: 0.46832 Test: 0.88139
Epoch: 5520 Train: 0.27556 Test: 0.99533
Epoch: 5600 Train: 0.05604 Test: 0.56748
Epoch: 5680 Train: 0.05415 Test: 0.61761
Epoch: 5760 Train: 0.07121 Test: 0.64048
Epoch: 5840 Train: 0.03466 Test: 0.54770
Epoch: 5920 Train: 0.57615 Test: 1.36550
Epoch: 6000 Train: 0.02655 Test: 0.55922
Epoch: 6080 Train: 0.03138 Test: 0.53311
Epoch: 6160 Train: 0.03069 Test: 0.55275
Epoch: 6240 Train: 0.04194 Test: 0.58979
Epoch: 6320 Train: 0.05373 Test: 0.55620
Epoch: 6400 Train: 0.19749 Test: 0.64070
Epoch: 6480 Train: 0.36993 Test: 1.08879
Epoch: 6560 Train: 0.74966 Test: 1.43834
Epoch: 6640 Train: 1.33150 Test: 1.75994
Epoch: 6720 Train: 0.15897 Test: 0.76362
Epoch: 6800 Train: 0.02120 Test: 0.51546
Epoch: 6880 Train: 0.02699 Test: 0.51231
Epoch: 6960 Train: 0.04309 Test: 0.51427
Epoch: 7040 Train: 0.05817 Test: 0.59768
Epoch: 7120 Train: 0.54035 Test: 1.00519
Epoch 7200: New minimal relative error: 2.36%, model saved.
Epoch: 7200 Train: 0.02990 Test: 0.55314
Epoch: 7280 Train: 0.10428 Test: 0.62343
Epoch: 7360 Train: 0.02490 Test: 0.48814
Epoch: 7440 Train: 0.01951 Test: 0.50616
Epoch: 7520 Train: 0.04053 Test: 0.52561
Epoch 7600: New minimal relative error: 2.05%, model saved.
Epoch: 7600 Train: 0.04508 Test: 0.48994
Epoch: 7680 Train: 0.03515 Test: 0.50536
Epoch: 7760 Train: 0.07883 Test: 0.54488
Epoch: 7840 Train: 0.42752 Test: 0.76750
Epoch: 7920 Train: 0.09859 Test: 0.64357
Epoch: 7999 Train: 0.17113 Test: 0.72748
Training Loss: tensor(0.1711)
Test Loss: tensor(0.7275)
Learned LE: [ 0.79983443  0.01485078 -3.957467  ]
True LE: [ 8.3489048e-01  9.6018186e-05 -1.4518081e+01]
Relative Error: [0.7045362  0.5869353  0.55806905 0.62579423 0.69212973 0.88965756
 1.0024805  1.0637143  0.7127287  0.65467435 0.57143426 0.6417168
 0.4192893  0.6145172  0.73720187 0.64575267 0.40983823 0.618385
 1.2993091  0.9163583  1.2776808  1.4280317  3.1925826  3.440237
 3.1635933  3.0710814  3.709303   4.6612406  4.632557   4.166642
 3.499004   2.8344865  2.7555928  2.7768474  2.8215184  2.6843097
 2.2245617  2.1027536  2.0405767  1.9087543  1.9767497  1.9879931
 2.1668131  1.8950738  1.7775787  1.6374797  1.6928679  1.6036419
 1.2640122  1.3838077  1.2860619  1.1850212  1.088416   1.1705214
 0.94704676 0.978844   0.88337594 0.7464655  0.7737533  0.88707095
 0.95113015 0.8970214  0.93139935 0.76261675 0.7120011  0.7470404
 0.7093679  0.8065394  0.8874971  1.1759194  0.6392126  0.64118844
 0.7427565  0.79698396 0.6906129  1.0311328  0.96914953 0.5391022
 0.6979563  1.0228661  0.8073671  0.84622455 1.7901129  3.0193062
 1.4696839  1.2180774  1.7416718  1.8033361  1.9628326  2.0995476
 2.390867   2.0525153  1.9017402  1.4499577  1.4878155  1.5971562
 1.761315   1.7615643  1.3738302  1.319713   1.2588742  1.421309
 1.4419805  1.387905   1.5463697  1.3682235  1.1830857  1.11025
 1.3567088  1.1210428  1.0447663  1.2730274  1.0249896  0.9205944
 0.9426675  0.905386   0.91747594 1.0012999  0.9497584  0.796355
 0.741708   0.72316855 0.7456261  0.77499676 0.83144194 0.79332674
 0.78900075 0.8291933  0.7265411  0.76551807 0.6955548  1.1604817
 0.4856319  0.6046418  0.716225   0.8792446  0.90441716 1.0667313
 0.81257683 0.45483238 1.334472   0.73350865 0.70763814 1.6912764
 1.8875883  0.8554048  0.8788731  0.93047655 1.2710145  1.2299092
 1.041498   0.71436036 1.2739446  1.1098073  1.0855489  0.87267673
 0.9014438  0.97301614 1.0808939  1.2791564  1.0660702  0.9704224
 0.94658184 1.082988   1.2728232  1.0722569  1.3806857  1.1376556
 0.87065995 0.77788144 0.9816476  0.8589456  1.0397182  0.995383
 0.8313083  0.9101967  0.78214043 0.7460506  0.7934978  0.90265155
 0.88389087 0.81011784 0.8080895  0.7702322  0.52429897 0.5306494
 0.6772128  0.8029259  0.67446667 0.7024479  0.6670808  0.6894113
 0.619001   1.0761174  0.56457126 0.5143997  0.804207   0.71609455
 0.9717448  1.0989127  0.49614775 0.8166519  0.47288132 1.0951228
 1.2206773  1.7116208  1.4849764  1.214195   1.0457293  0.7371985
 0.80585706 0.97679484 0.64843273 0.32722542 0.6989035  0.59348375
 0.65716094 0.52686226 0.5315586  0.55113035 0.7646683  1.0810976
 1.0111474  0.80557346 0.51362354 0.812841   1.0693569  0.77740526
 1.1164052  0.9638521  0.6800035  0.7078968  0.78595394 0.8575002
 0.8538259  0.87122947 0.78240216 0.88240653 0.6186561  0.61008793
 0.7017509  0.8337683  0.7760029  0.8174059  0.8185299  0.7846248
 0.6302803  0.48840633 0.5318451  0.7563382  0.71591353 0.64776504
 0.52330434 0.66761917 0.57641244 0.84917045 0.589259   0.5368306
 0.7796839  0.652175   1.0486326  0.7387542  0.3757236  1.0794312
 0.5593133  1.044027   1.7070165  1.245499   1.0552659  0.9547722
 0.998799   0.83041775 0.48431894 0.8102478  0.56118894 0.34536886
 0.32298768 0.31919262 0.55808884 0.50402    0.3732636  0.32552353
 0.5986162  0.96435463 1.0700593  0.77922183 0.34278372 0.57894504
 0.9111876  0.7532886  0.92521936 0.7180356  0.64171326 0.7043927
 0.80336297 0.7942266  0.66920847 0.8670151  0.7444768  0.85754377
 0.53544533 0.48972234 0.66379833 0.6910336  0.5981031  0.677559
 0.85174495 0.78798217 0.6547014  0.6572923  0.54624367 0.53508294
 0.7445665  0.67178094 0.5056686  0.5773895  0.5668341  0.6520111
 0.5542445  0.7973721  0.57020813 0.6599971  0.7993178  0.550141
 0.4827866  0.61996186 0.20020299 1.7067286  0.85338366 0.7716311
 0.6575605  0.57983947 0.45281914 0.6521234  0.7477305  0.6578838
 0.4936542  0.3924686  0.05038533 0.2854988  0.46240482 0.6106242
 0.5402647  0.36655617 0.4597724  0.8439357  1.1568681  0.90266097
 0.4652757  0.40944394 0.7386055  0.694652   0.7874522  0.66055745
 0.5735277  0.6686647  0.81552184 0.80278563 0.6985582  0.70692736
 0.76184195 0.87562984 0.5587758  0.33638042 0.61810267 0.5687735
 0.4303755  0.56851745 0.6967278  0.592703   0.5611866  0.59679276
 0.68707335 0.49346685 0.48601708 0.6143705  0.49818242 0.47146618
 0.6775184  0.52171826 0.5736317  0.7217227  0.5387957  0.56649107
 0.7249575  0.6070771  0.42315185 0.36624405 1.0472717  0.77301764
 0.6607212  0.80397016 0.54582196 0.46368822 0.37854648 0.37272036
 0.7788653  0.74290377 0.44261682 0.45700395]
