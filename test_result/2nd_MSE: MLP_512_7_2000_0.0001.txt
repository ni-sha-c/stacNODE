time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.97%, model saved.
Epoch: 0 Train: 4042.34668 Test: 4065.13086
Epoch 100: New minimal relative error: 55.95%, model saved.
Epoch: 100 Train: 69.00358 Test: 40.85091
Epoch: 200 Train: 7.63373 Test: 17.82612
Epoch: 300 Train: 14.06432 Test: 15.93501
Epoch 400: New minimal relative error: 51.95%, model saved.
Epoch: 400 Train: 21.43207 Test: 28.64381
Epoch 500: New minimal relative error: 33.71%, model saved.
Epoch: 500 Train: 4.29851 Test: 1.45904
Epoch 600: New minimal relative error: 27.95%, model saved.
Epoch: 600 Train: 5.74604 Test: 9.68269
Epoch: 700 Train: 1.71834 Test: 2.86604
Epoch: 800 Train: 4.56245 Test: 7.09377
Epoch: 900 Train: 1.18246 Test: 2.02032
Epoch: 1000 Train: 2.40751 Test: 3.43926
Epoch: 1100 Train: 3.26645 Test: 1.35903
Epoch: 1200 Train: 1.24500 Test: 1.12367
Epoch: 1300 Train: 1.20559 Test: 1.35768
Epoch: 1400 Train: 3.34639 Test: 3.24693
Epoch: 1500 Train: 1.01058 Test: 1.04283
Epoch: 1600 Train: 0.55625 Test: 0.56998
Epoch: 1700 Train: 0.35009 Test: 0.91770
Epoch 1800: New minimal relative error: 22.81%, model saved.
Epoch: 1800 Train: 5.38657 Test: 4.31679
Epoch: 1900 Train: 4.22160 Test: 3.15869
Epoch 2000: New minimal relative error: 11.94%, model saved.
Epoch: 2000 Train: 0.19255 Test: 0.35602
Epoch: 2100 Train: 1.74405 Test: 2.43632
Epoch: 2200 Train: 1.10294 Test: 1.32689
Epoch: 2300 Train: 2.64854 Test: 2.94327
Epoch: 2400 Train: 1.08756 Test: 1.45951
Epoch: 2500 Train: 1.87437 Test: 1.41549
Epoch: 2600 Train: 0.16509 Test: 0.26859
Epoch: 2700 Train: 2.54736 Test: 1.34507
Epoch: 2800 Train: 0.36418 Test: 0.33044
Epoch: 2900 Train: 1.81959 Test: 2.00236
Epoch: 3000 Train: 0.21134 Test: 0.17814
Epoch: 3100 Train: 0.79601 Test: 1.27250
Epoch: 3200 Train: 0.17931 Test: 0.26400
Epoch: 3300 Train: 1.83475 Test: 1.49445
Epoch: 3400 Train: 0.45473 Test: 0.64038
Epoch: 3500 Train: 0.75542 Test: 0.91962
Epoch: 3600 Train: 0.98840 Test: 1.46254
Epoch: 3700 Train: 0.49648 Test: 0.59810
Epoch: 3800 Train: 1.73699 Test: 1.92987
Epoch: 3900 Train: 0.30772 Test: 0.40051
Epoch: 4000 Train: 0.29914 Test: 0.39151
Epoch: 4100 Train: 0.16967 Test: 0.47543
Epoch: 4200 Train: 0.24462 Test: 0.35295
Epoch: 4300 Train: 0.11601 Test: 0.26402
Epoch: 4400 Train: 0.23138 Test: 0.26414
Epoch: 4500 Train: 0.52674 Test: 0.79855
Epoch: 4600 Train: 0.21246 Test: 0.26814
Epoch: 4700 Train: 0.02267 Test: 0.08590
Epoch: 4800 Train: 0.23113 Test: 0.16106
Epoch: 4900 Train: 0.69865 Test: 0.80491
Epoch: 5000 Train: 0.74559 Test: 0.77961
Epoch: 5100 Train: 1.19799 Test: 1.44838
Epoch: 5200 Train: 0.08491 Test: 0.11219
Epoch: 5300 Train: 0.02807 Test: 0.09909
Epoch: 5400 Train: 0.73727 Test: 0.87656
Epoch: 5500 Train: 0.40228 Test: 0.44104
Epoch: 5600 Train: 0.02237 Test: 0.08469
Epoch: 5700 Train: 0.46785 Test: 0.52553
Epoch: 5800 Train: 0.34177 Test: 0.34545
Epoch: 5900 Train: 0.39186 Test: 0.54909
Epoch: 6000 Train: 0.19981 Test: 0.28471
Epoch: 6100 Train: 0.12755 Test: 0.21785
Epoch: 6200 Train: 0.74522 Test: 0.79609
Epoch: 6300 Train: 0.04687 Test: 0.11669
Epoch: 6400 Train: 0.12203 Test: 0.18079
Epoch: 6500 Train: 0.03512 Test: 0.12826
Epoch: 6600 Train: 0.06368 Test: 0.15278
Epoch: 6700 Train: 0.48152 Test: 0.62758
Epoch: 6800 Train: 0.04839 Test: 0.12042
Epoch: 6900 Train: 0.03359 Test: 0.09946
Epoch: 7000 Train: 0.02364 Test: 0.08865
Epoch: 7100 Train: 1.01218 Test: 0.91986
Epoch: 7200 Train: 0.01677 Test: 0.08212
Epoch: 7300 Train: 0.04512 Test: 0.12483
Epoch: 7400 Train: 0.01025 Test: 0.07578
Epoch: 7500 Train: 0.00831 Test: 0.07151
Epoch: 7600 Train: 0.38286 Test: 0.43394
Epoch: 7700 Train: 0.12854 Test: 0.17215
Epoch: 7800 Train: 0.20607 Test: 0.25991
Epoch: 7900 Train: 0.05890 Test: 0.12403
Epoch: 8000 Train: 0.04575 Test: 0.10387
Epoch: 8100 Train: 0.02230 Test: 0.07883
Epoch: 8200 Train: 0.58316 Test: 0.69396
Epoch: 8300 Train: 0.15310 Test: 0.25283
Epoch: 8400 Train: 0.01121 Test: 0.07696
Epoch: 8500 Train: 0.04478 Test: 0.08369
Epoch: 8600 Train: 0.01532 Test: 0.07753
Epoch: 8700 Train: 0.05088 Test: 0.13642
Epoch: 8800 Train: 0.09087 Test: 0.11070
Epoch: 8900 Train: 0.01572 Test: 0.08215
Epoch: 9000 Train: 0.00608 Test: 0.06861
Epoch: 9100 Train: 0.00665 Test: 0.06861
Epoch: 9200 Train: 0.00813 Test: 0.07193
Epoch: 9300 Train: 0.01584 Test: 0.07587
Epoch: 9400 Train: 0.08935 Test: 0.11664
Epoch: 9500 Train: 0.13861 Test: 0.10932
Epoch: 9600 Train: 0.09465 Test: 0.14213
Epoch: 9700 Train: 0.00528 Test: 0.06788
Epoch: 9800 Train: 0.00681 Test: 0.06859
Epoch: 9900 Train: 0.00553 Test: 0.06848
Epoch: 9999 Train: 0.00945 Test: 0.06996
Training Loss: tensor(0.0095)
Test Loss: tensor(0.0700)
Learned LE: [ 0.91991127 -0.01406484 -4.4505777 ]
True LE: [ 8.6913520e-01 -1.8331523e-03 -1.4547618e+01]
Relative Error: [3.8475435  3.8282824  3.8332186  3.8964636  3.9799638  4.066256
 4.1452394  4.1598907  4.0733447  3.9017904  3.6832378  3.44841
 3.2149866  2.9994426  2.8379695  2.8017564  2.9737158  3.3680677
 3.8792994  4.3597655  4.747268   5.0956297  5.485401   5.9412155
 6.432357   6.9197774  7.38777    7.8341618  8.241391   8.572276
 8.76335    8.717364   8.326541   7.534574   6.445583   5.3205566
 4.3758464  3.6713018  3.1753638  2.831017   2.569355   2.316476
 2.0197158  1.6430054  1.2139981  1.120722   1.5457597  2.0535707
 2.422069   2.6189618  2.6755698  2.6354914  2.5381355  2.4192276
 2.314981   2.2641711  2.300594   2.4365065  2.6510863  2.8983045
 3.125796   3.2901366  3.3673546  3.3685372  3.352391   3.3911803
 3.4780848  3.5573256  3.6262143  3.6577277  3.6053069  3.4720585
 3.2917473  3.0915635  2.8793204  2.6515813  2.4146369  2.2146938
 2.1614225  2.3660002  2.7769606  3.199643   3.5165792  3.778661
 4.0915933  4.4944572  4.9510517  5.4042096  5.826273   6.225728
 6.6062794  6.9452705  7.2051487  7.3084517  7.1361237  6.580651
 5.6615343  4.608362   3.6862724  2.9935846  2.5086596  2.1871085
 1.9821603  1.8396567  1.7115229  1.5321805  1.1999292  0.9280718
 1.1800414  1.6900321  2.1142848  2.3654385  2.460511   2.4424067
 2.3502436  2.2160177  2.0696473  1.9454144  1.883968   1.9204649
 2.0610173  2.2735193  2.5054967  2.7076926  2.8445547  2.9000301
 2.8962498  2.9029     2.9787512  3.078226   3.1478364  3.1939886
 3.183876   3.1008935  2.9714494  2.823606   2.6639814  2.4788787
 2.2493494  1.9745015  1.7096376  1.6150644  1.8269162  2.1790757
 2.4424505  2.6202767  2.8396661  3.165403   3.5731819  3.9952815
 4.377479   4.7186236  5.0489397  5.368402   5.6541805  5.870275
 5.916542   5.6523542  4.995908   4.0737147  3.179266   2.4855297
 1.9906273  1.6537541  1.4459102  1.3357106  1.2973036  1.2974024
 1.17373    0.89939356 0.867116   1.2700666  1.7423805  2.0765412
 2.2428236  2.2757308  2.2156165  2.0938997  1.9358205  1.7665333
 1.6189748  1.5362697  1.5562721  1.6822077  1.8750688  2.0822983
 2.2634416  2.3893394  2.4444807  2.4533236  2.4876628  2.5926635
 2.6970696  2.7558336  2.782036   2.752531   2.6751394  2.5785918
 2.476542   2.3582387  2.1969626  1.9663455  1.6567247  1.3190565
 1.1630001  1.3406552  1.5663468  1.6801658  1.7998576  2.0177422
 2.3371317  2.7130105  3.0718377  3.367142   3.6229742  3.8867478
 4.1525903  4.4052277  4.608821   4.6369796  4.335138   3.6660273
 2.845016   2.1467853  1.6362019  1.270128   1.0256507  0.89234275
 0.85302675 0.9210634  0.99945396 0.91010684 0.76726556 0.87744325
 1.2855763  1.7006714  1.9720923  2.0909941  2.0932252  2.0130303
 1.8765621  1.7045537  1.5184639  1.3494148  1.23961    1.2277945
 1.3186166  1.4731463  1.6460989  1.811501   1.9444826  2.0187094
 2.049143   2.1031964  2.2247062  2.333331   2.3834717  2.3986294
 2.3693104  2.3120923  2.2494247  2.1842785  2.0973432  1.9589864
 1.7410699  1.422454   1.0339268  0.8088667  0.9135101  1.007151
 1.054213   1.1688677  1.35261    1.6118973  1.9250557  2.2131207
 2.420224   2.5858958  2.7793474  2.995221   3.2286193  3.4477706
 3.50677    3.24076    2.643906   1.9714346  1.4428822  1.0534151
 0.76045114 0.5729661  0.49645767 0.52963746 0.68123585 0.76266986
 0.74745256 0.7227336  0.8498369  1.205196   1.5751023  1.817836
 1.9212633  1.9151913  1.8300397  1.6902221  1.5145668  1.323257
 1.1460261  1.0189109  0.97292215 1.0129595  1.1068677  1.2247854
 1.3672438  1.5182401  1.6281577  1.6858239  1.7455882  1.8652385
 1.9825226  2.0280983  2.0379286  2.0130587  1.9683067  1.9219067
 1.8751001  1.8089497  1.6967078  1.5110164  1.2196156  0.825753
 0.55021816 0.58358055 0.6186072  0.69897956 0.8091733  0.9013413
 1.0346096  1.2549877  1.4821391  1.6156538  1.6792508  1.7821232
 1.9381514  2.146007   2.3935359  2.5346115  2.3853092  1.9247432
 1.3996576  0.995829   0.6811038  0.42429772 0.26837802 0.25619146
 0.37977785 0.5496375  0.59609383 0.6354631  0.67948395 0.77110267
 1.034177   1.3651334  1.6088654  1.7245656  1.7294192  1.6521163
 1.5188552  1.349205   1.1645103  0.9972958  0.87990946 0.82588977
 0.8269446  0.851025   0.87778354 0.9522499  1.105946   1.2643749
 1.3619707  1.4226826  1.5102135  1.6333395  1.6841218  1.6871915
 1.6606702  1.6114271  1.5582     1.5100483  1.4555199  1.3714795
 1.2322159  1.0009882  0.65070105 0.35911712 0.36507344 0.42112273
 0.57419765 0.68739736 0.71921825 0.7066234 ]
