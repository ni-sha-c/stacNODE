time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 6000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 512
n_layers: 4
reg_param: 200.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 294.957946777 Test: 11.034546852
Epoch 0: New minimal relative error: 11.03%, model saved.
Epoch: 60 Train: 9.987968445 Test: 6.681630611
Epoch 60: New minimal relative error: 6.68%, model saved.
Epoch: 120 Train: 9.283308029 Test: 6.024581432
Epoch 120: New minimal relative error: 6.02%, model saved.
Epoch: 180 Train: 7.771654129 Test: 5.702879429
Epoch 180: New minimal relative error: 5.70%, model saved.
Epoch: 240 Train: 7.332700729 Test: 5.633868217
Epoch 240: New minimal relative error: 5.63%, model saved.
Epoch: 300 Train: 7.103875160 Test: 5.702875614
Epoch: 360 Train: 6.974319935 Test: 5.495428562
Epoch 360: New minimal relative error: 5.50%, model saved.
Epoch: 420 Train: 7.026803970 Test: 5.746490955
Epoch: 480 Train: 6.925293922 Test: 5.696776390
Epoch: 540 Train: 7.216735363 Test: 5.654889584
Epoch: 600 Train: 7.004262447 Test: 5.627245903
Epoch: 660 Train: 6.997713089 Test: 5.710943699
Epoch: 720 Train: 6.953974724 Test: 5.659497738
Epoch: 780 Train: 6.824824333 Test: 5.648425579
Epoch: 840 Train: 6.826491833 Test: 5.674132824
Epoch: 900 Train: 6.808858395 Test: 5.654777050
Epoch: 960 Train: 6.768449783 Test: 5.703474045
Epoch: 1020 Train: 6.917996407 Test: 5.669032574
Epoch: 1080 Train: 6.908460617 Test: 5.689795017
Epoch: 1140 Train: 6.918775558 Test: 5.731212139
Epoch: 1200 Train: 6.820598125 Test: 5.654002190
Epoch: 1260 Train: 6.831210613 Test: 5.753892422
Epoch: 1320 Train: 6.843286514 Test: 5.708212376
Epoch: 1380 Train: 6.775781155 Test: 5.674349308
Epoch: 1440 Train: 6.865350246 Test: 5.696683407
Epoch: 1500 Train: 6.804783344 Test: 5.698351383
Epoch: 1560 Train: 6.774734020 Test: 5.698183537
Epoch: 1620 Train: 6.774590492 Test: 5.727293015
Epoch: 1680 Train: 6.735947609 Test: 5.648244858
Epoch: 1740 Train: 6.790998459 Test: 5.834956646
Epoch: 1800 Train: 6.744221687 Test: 5.693245411
Epoch: 1860 Train: 6.761533737 Test: 5.716286182
Epoch: 1920 Train: 6.765723228 Test: 5.715040684
Epoch: 1980 Train: 6.735451221 Test: 5.683948040
Epoch: 2040 Train: 6.708027840 Test: 5.711687565
Epoch: 2100 Train: 6.838471413 Test: 5.702967644
Epoch: 2160 Train: 6.734264851 Test: 5.711879730
Epoch: 2220 Train: 6.725728035 Test: 5.711775303
Epoch: 2280 Train: 6.777921200 Test: 5.698662758
Epoch: 2340 Train: 6.713077545 Test: 5.710335255
Epoch: 2400 Train: 6.706917286 Test: 5.718891144
Epoch: 2460 Train: 6.752607822 Test: 5.737121582
Epoch: 2520 Train: 6.725118160 Test: 5.694776058
Epoch: 2580 Train: 6.706464291 Test: 5.654616356
Epoch: 2640 Train: 6.756911278 Test: 5.626970768
Epoch: 2700 Train: 6.837711334 Test: 5.680304527
Epoch: 2760 Train: 6.740487099 Test: 5.750303745
Epoch: 2820 Train: 6.696416378 Test: 5.719084740
Epoch: 2880 Train: 6.696955204 Test: 5.702715874
Epoch: 2940 Train: 6.705872059 Test: 5.721987724
Epoch: 3000 Train: 6.667617321 Test: 5.691658497
Epoch: 3060 Train: 6.713723660 Test: 5.731985092
Epoch: 3120 Train: 6.691710949 Test: 5.714117050
Epoch: 3180 Train: 6.713973045 Test: 5.715652466
Epoch: 3240 Train: 6.722049713 Test: 5.701999187
Epoch: 3300 Train: 6.709153652 Test: 5.720481873
Epoch: 3360 Train: 6.751632690 Test: 5.717680931
Epoch: 3420 Train: 6.721028328 Test: 5.713868618
Epoch: 3480 Train: 6.717875481 Test: 5.705127716
Epoch: 3540 Train: 6.708911896 Test: 5.703313828
Epoch: 3600 Train: 6.711777687 Test: 5.700346470
Epoch: 3660 Train: 6.694577217 Test: 5.698590279
Epoch: 3720 Train: 6.696423531 Test: 5.705804348
Epoch: 3780 Train: 6.718478680 Test: 5.704761028
Epoch: 3840 Train: 6.691818237 Test: 5.703878880
Epoch: 3900 Train: 6.682216644 Test: 5.695216179
Epoch: 3960 Train: 6.678502560 Test: 5.700282097
Epoch: 4020 Train: 6.720810890 Test: 5.701088428
Epoch: 4080 Train: 6.706535816 Test: 5.696378708
Epoch: 4140 Train: 6.678123951 Test: 5.706590176
Epoch: 4200 Train: 6.734135151 Test: 5.692498684
Epoch: 4260 Train: 6.704056263 Test: 5.712773323
Epoch: 4320 Train: 6.693029404 Test: 5.712779522
Epoch: 4380 Train: 6.697199821 Test: 5.717349052
Epoch: 4440 Train: 6.704628468 Test: 5.710089207
Epoch: 4500 Train: 6.738400936 Test: 5.717363358
Epoch: 4560 Train: 6.748012066 Test: 5.723561287
Epoch: 4620 Train: 6.730121613 Test: 5.720390797
Epoch: 4680 Train: 6.715152264 Test: 5.710846901
Epoch: 4740 Train: 6.710781097 Test: 5.711386681
Epoch: 4800 Train: 6.707024097 Test: 5.708546162
Epoch: 4860 Train: 6.688780785 Test: 5.700059891
Epoch: 4920 Train: 6.680561066 Test: 5.720186234
Epoch: 4980 Train: 6.699839592 Test: 5.717694759
Epoch: 5040 Train: 6.694427013 Test: 5.707370281
Epoch: 5100 Train: 6.716761589 Test: 5.700965881
Epoch: 5160 Train: 6.687374115 Test: 5.705827713
Epoch: 5220 Train: 6.690083981 Test: 5.709484100
Epoch: 5280 Train: 6.704182625 Test: 5.702254772
Epoch: 5340 Train: 6.701039314 Test: 5.703485489
Epoch: 5400 Train: 6.697710037 Test: 5.709836006
Epoch: 5460 Train: 6.694481373 Test: 5.704584599
Epoch: 5520 Train: 6.739741325 Test: 5.712229252
Epoch: 5580 Train: 6.744587898 Test: 5.710312366
Epoch: 5640 Train: 6.730630398 Test: 5.711119652
Epoch: 5700 Train: 6.723539352 Test: 5.717687607
Epoch: 5760 Train: 6.726859093 Test: 5.704617500
Epoch: 5820 Train: 6.738550663 Test: 5.716430187
Epoch: 5880 Train: 6.711794853 Test: 5.709737301
Epoch: 5940 Train: 6.700198174 Test: 5.712888718
Epoch: 5999 Train: 6.706330776 Test: 5.710240841
Training Loss: tensor(6.7063)
Test Loss: tensor(5.7102)
True Mean x: tensor(3.4447, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(5.2684e+21, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.5065, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(inf, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0051)
Jacobian term Test Loss: tensor(9.4871e-05)
Learned LE: [1.6730697  0.41748676]
True LE: tensor([ 0.6932, -0.7446], dtype=torch.float64)
