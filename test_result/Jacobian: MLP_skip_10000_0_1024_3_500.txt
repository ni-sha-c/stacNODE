time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.74%, model saved.
Epoch: 0 Train: 32290.82422 Test: 4097.68506
Epoch: 100 Train: 7516.03711 Test: 1153.84680
Epoch: 200 Train: 7076.43115 Test: 1353.89160
Epoch 300: New minimal relative error: 86.06%, model saved.
Epoch: 300 Train: 5231.72070 Test: 715.70129
Epoch: 400 Train: 4270.18018 Test: 608.48010
Epoch 500: New minimal relative error: 54.73%, model saved.
Epoch: 500 Train: 3145.79980 Test: 287.70529
Epoch 600: New minimal relative error: 46.25%, model saved.
Epoch: 600 Train: 1844.14026 Test: 120.37975
Epoch: 700 Train: 1040.41516 Test: 367.48666
Epoch: 800 Train: 444.65482 Test: 10.39030
Epoch 900: New minimal relative error: 34.38%, model saved.
Epoch: 900 Train: 308.62082 Test: 7.03590
Epoch 1000: New minimal relative error: 17.67%, model saved.
Epoch: 1000 Train: 291.06564 Test: 13.18415
Epoch 1100: New minimal relative error: 8.87%, model saved.
Epoch: 1100 Train: 232.16110 Test: 6.74088
Epoch: 1200 Train: 195.42426 Test: 9.25955
Epoch: 1300 Train: 441.22766 Test: 100.78777
Epoch 1400: New minimal relative error: 8.18%, model saved.
Epoch: 1400 Train: 151.30029 Test: 1.89785
Epoch: 1500 Train: 144.83746 Test: 1.69200
Epoch: 1600 Train: 123.50801 Test: 3.11118
Epoch: 1700 Train: 137.28029 Test: 1.78487
Epoch: 1800 Train: 122.70686 Test: 2.31079
Epoch: 1900 Train: 126.30190 Test: 8.48725
Epoch: 2000 Train: 106.87080 Test: 20.59014
Epoch: 2100 Train: 83.24173 Test: 0.71203
Epoch: 2200 Train: 79.48923 Test: 1.89049
Epoch: 2300 Train: 83.61005 Test: 2.43750
Epoch: 2400 Train: 81.06202 Test: 1.06308
Epoch 2500: New minimal relative error: 2.93%, model saved.
Epoch: 2500 Train: 64.79653 Test: 0.50268
Epoch 2600: New minimal relative error: 2.13%, model saved.
Epoch: 2600 Train: 70.77469 Test: 0.55711
Epoch: 2700 Train: 59.73247 Test: 0.34702
Epoch: 2800 Train: 61.60276 Test: 2.82136
Epoch: 2900 Train: 63.91228 Test: 0.83763
Epoch: 3000 Train: 64.69609 Test: 9.10153
Epoch: 3100 Train: 54.80736 Test: 3.71315
Epoch: 3200 Train: 51.03664 Test: 0.24899
Epoch: 3300 Train: 50.02287 Test: 0.28398
Epoch: 3400 Train: 53.29015 Test: 2.64027
Epoch: 3500 Train: 52.06373 Test: 2.32073
Epoch: 3600 Train: 43.32642 Test: 0.26731
Epoch: 3700 Train: 62.43982 Test: 15.00980
Epoch: 3800 Train: 60.94959 Test: 9.01909
Epoch 3900: New minimal relative error: 0.87%, model saved.
Epoch: 3900 Train: 41.37550 Test: 0.18126
Epoch: 4000 Train: 43.13423 Test: 0.21411
Epoch: 4100 Train: 38.86982 Test: 0.16901
Epoch: 4200 Train: 39.21948 Test: 0.21257
Epoch: 4300 Train: 36.05265 Test: 1.11647
Epoch: 4400 Train: 38.41461 Test: 1.26057
Epoch: 4500 Train: 35.24105 Test: 0.63221
Epoch: 4600 Train: 38.94307 Test: 3.09408
Epoch: 4700 Train: 32.23323 Test: 0.31609
Epoch: 4800 Train: 31.10501 Test: 0.13071
Epoch: 4900 Train: 30.71574 Test: 0.09704
Epoch: 5000 Train: 38.07609 Test: 0.65411
Epoch: 5100 Train: 29.29315 Test: 0.09043
Epoch: 5200 Train: 28.65831 Test: 0.09655
Epoch: 5300 Train: 28.08780 Test: 0.10708
Epoch: 5400 Train: 27.38704 Test: 0.07805
Epoch: 5500 Train: 28.24996 Test: 0.30447
Epoch: 5600 Train: 36.53265 Test: 2.57942
Epoch: 5700 Train: 26.57116 Test: 0.08556
Epoch: 5800 Train: 25.72511 Test: 0.06788
Epoch: 5900 Train: 30.39991 Test: 0.16959
Epoch: 6000 Train: 26.42801 Test: 0.07667
Epoch: 6100 Train: 26.60359 Test: 0.27804
Epoch: 6200 Train: 25.45774 Test: 0.06723
Epoch: 6300 Train: 25.85028 Test: 0.08737
Epoch: 6400 Train: 25.11183 Test: 0.13957
Epoch: 6500 Train: 23.96776 Test: 0.06402
Epoch: 6600 Train: 23.84768 Test: 0.08227
Epoch: 6700 Train: 23.36661 Test: 0.10489
Epoch: 6800 Train: 25.46926 Test: 0.83344
Epoch: 6900 Train: 22.85334 Test: 0.13257
Epoch: 7000 Train: 22.36402 Test: 1.10487
Epoch: 7100 Train: 20.96742 Test: 0.16083
Epoch: 7200 Train: 20.45420 Test: 0.05955
Epoch: 7300 Train: 20.74204 Test: 0.07405
Epoch: 7400 Train: 19.40655 Test: 0.04701
Epoch: 7500 Train: 19.88100 Test: 0.05292
Epoch: 7600 Train: 20.51925 Test: 0.32539
Epoch: 7700 Train: 19.75103 Test: 0.09020
Epoch: 7800 Train: 19.63140 Test: 0.18271
Epoch: 7900 Train: 20.15183 Test: 0.77672
Epoch: 8000 Train: 20.26159 Test: 0.71999
Epoch: 8100 Train: 19.45365 Test: 0.12831
Epoch: 8200 Train: 18.85635 Test: 0.05436
Epoch: 8300 Train: 20.08794 Test: 0.06491
Epoch: 8400 Train: 20.98178 Test: 0.12065
Epoch: 8500 Train: 22.16080 Test: 0.10588
Epoch: 8600 Train: 20.42975 Test: 0.07002
Epoch: 8700 Train: 20.29266 Test: 0.21604
Epoch: 8800 Train: 19.97262 Test: 0.05411
Epoch: 8900 Train: 20.31875 Test: 0.27266
Epoch: 9000 Train: 19.76169 Test: 0.10474
Epoch: 9100 Train: 19.38401 Test: 0.05033
Epoch: 9200 Train: 18.94218 Test: 0.05379
Epoch: 9300 Train: 19.15478 Test: 0.05041
Epoch: 9400 Train: 22.50125 Test: 0.36848
Epoch: 9500 Train: 22.84832 Test: 0.48786
Epoch: 9600 Train: 22.96287 Test: 0.12197
Epoch: 9700 Train: 20.79629 Test: 0.06864
Epoch: 9800 Train: 19.84129 Test: 0.06778
Epoch: 9900 Train: 19.51931 Test: 0.07097
Epoch: 9999 Train: 19.09309 Test: 0.05159
Training Loss: tensor(19.0931)
Test Loss: tensor(0.0516)
Learned LE: [ 8.99123192e-01 -1.25611145e-02 -1.45569057e+01]
True LE: [ 8.7465066e-01  4.0821671e-03 -1.4560904e+01]
Relative Error: [4.915744   4.8643684  4.834763   4.993402   5.126569   5.1463885
 5.1554914  5.1192384  4.4822035  4.550482   4.5413365  3.9380848
 3.663861   3.1886961  2.878076   2.8271198  2.853133   2.9444005
 3.1224494  2.75731    2.4052935  2.089437   1.9095542  2.2025707
 2.8982267  3.7544224  4.3180575  4.505054   4.629511   4.968756
 4.2342615  4.094751   3.9503534  3.6574025  3.3100884  3.3410766
 3.0285184  2.315595   1.6224378  1.1985061  1.0580068  1.0247676
 1.144449   1.360212   1.6061797  1.8309038  2.1679475  2.4630744
 2.6736789  2.7381372  2.8718662  2.9830816  3.22352    3.541018
 3.9608908  4.688698   5.3260007  5.087132   4.9425855  4.734885
 4.359912   4.230197   4.228998   4.12757    4.07704    4.032277
 4.387972   4.3964515  4.2781672  4.4615507  4.479474   4.838244
 4.5585346  3.7575848  3.4898717  3.2196596  2.8913238  2.8038516
 2.8582144  2.9157348  3.0481956  2.7789664  2.3785195  1.9714166
 1.8338127  1.8919868  2.4289665  3.2328498  3.975331   4.538038
 4.2804136  4.3836465  4.254244   3.8329298  3.5991364  3.229042
 2.834706   3.0784297  2.913467   2.1998878  1.4425166  0.9626056
 0.8205843  1.009486   1.1816814  1.2786301  1.4236119  1.5768611
 1.9098948  2.1719553  2.296655   2.4021156  2.6423292  2.7285042
 2.984019   3.4451735  4.0364223  4.753205   5.4307065  5.2836065
 4.9229236  4.286918   3.7728508  3.4885314  3.4781396  3.4905634
 3.478313   3.360273   3.710289   3.8525124  3.6158283  3.6954954
 3.7193377  4.3159     4.5606127  3.760212   3.3101547  3.275625
 2.8905456  2.7567468  2.7679908  2.7914095  2.9095995  2.7827523
 2.3663068  1.927509   1.7197833  1.7412993  2.136119   2.644394
 3.4640055  4.285499   4.274223   3.909624   4.2121224  3.628228
 3.4032252  2.9020176  2.4740047  2.5571008  2.631266   2.2471662
 1.4044529  0.8143371  0.6935308  1.0379536  1.149126   1.241457
 1.167626   1.2464024  1.6069336  1.896535   2.0129926  2.0760477
 2.2043226  2.3963501  2.614596   3.188996   3.9592617  4.66849
 5.306207   5.3870416  4.710193   4.0161204  3.5045073  2.998046
 2.8656793  2.9480758  2.979102   2.8677914  2.9371965  3.3294227
 3.1510174  3.1205225  3.1042368  3.598106   3.9040961  3.558857
 3.3040802  3.115198   3.0384521  2.6860604  2.6071792  2.6524553
 2.784404   2.788504   2.4524567  2.0258527  1.7295964  1.6972369
 1.898402   2.1959488  2.7144704  3.466437   3.955123   3.7812278
 3.6546245  3.8620055  3.3262584  2.8192434  2.253416   2.0553758
 2.2186706  2.0738838  1.6488526  0.8764389  0.6882681  0.99787956
 1.3137201  1.025454   0.9383739  0.8745612  1.1498469  1.585985
 1.8083483  1.7625846  1.773061   2.0183554  2.2277598  2.6769018
 3.5540526  4.3861074  5.071159   5.0389123  4.410012   3.8621798
 3.2566586  2.8445234  2.3869135  2.219538   2.2932787  2.3466122
 2.3347194  2.6550283  2.8962126  2.6753285  2.6163144  2.8193295
 3.4245987  3.1125891  2.9777634  3.013457   3.0437696  2.7589798
 2.5169878  2.5579243  2.6376972  2.6689262  2.5090384  2.0865874
 1.7975695  1.7125762  1.8492032  2.0544431  2.18994    2.5223775
 3.0473378  3.7219472  3.4768906  3.3932824  3.4062414  2.9864478
 2.2066603  1.8102133  1.8934511  1.7380419  1.5215801  1.281334
 0.8777487  1.0370699  1.3118857  0.9959999  0.5259449  0.5429201
 0.65853393 1.186334   1.5057898  1.5639052  1.4263992  1.5558571
 1.8959969  2.2757068  2.9895046  3.802814   4.617879   4.7420363
 4.028772   3.512525   3.095959   2.6980617  2.1680923  1.6469545
 1.4963654  1.5947912  1.5932328  1.6752478  2.1874619  2.27485
 2.2832055  2.224819   2.7661986  3.0153522  2.7316923  2.6909726
 2.7310708  2.8478317  2.5297697  2.3558962  2.4134068  2.5125215
 2.4598072  2.1621776  1.8282255  1.6715562  1.7713668  2.0646431
 2.0879138  2.0470674  1.9341074  2.6374588  3.3036199  2.9336402
 2.9395275  3.0366466  2.597191   2.0271502  1.7381968  1.5558504
 1.3751637  1.0505221  1.1618439  1.2665123  1.4090067  1.0628976
 0.43463403 0.20277305 0.26492804 0.5756191  1.057618   1.2045304
 1.159844   1.0529928  1.3462099  1.9177105  2.5441086  3.3324018
 3.977742   4.109343   3.739429   3.1786387  2.7903953  2.435738
 2.0358021  1.6186142  1.1998476  1.1043173  1.1363178  1.1050792
 1.1816895  1.6017122  1.6451061  1.7686275  2.0775864  2.685905
 2.7740815  2.604004   2.499279   2.4380314  2.4903276  2.3231833
 2.1301868  2.2356231  2.2729151  2.1251838  1.8632793  1.6286528
 1.6389579  1.9232727  2.0535538  2.2110105 ]
