time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.85%, model saved.
Epoch: 0 Train: 31604.97070 Test: 3597.42407
Epoch 100: New minimal relative error: 89.33%, model saved.
Epoch: 100 Train: 8226.30762 Test: 1187.10828
Epoch: 200 Train: 7200.08691 Test: 984.51654
Epoch 300: New minimal relative error: 88.21%, model saved.
Epoch: 300 Train: 7125.45264 Test: 886.99774
Epoch 400: New minimal relative error: 82.95%, model saved.
Epoch: 400 Train: 5479.76270 Test: 773.68555
Epoch: 500 Train: 3405.63452 Test: 310.05713
Epoch 600: New minimal relative error: 52.22%, model saved.
Epoch: 600 Train: 1209.57422 Test: 74.69235
Epoch 700: New minimal relative error: 23.77%, model saved.
Epoch: 700 Train: 728.56171 Test: 34.87260
Epoch 800: New minimal relative error: 19.12%, model saved.
Epoch: 800 Train: 454.58099 Test: 15.37464
Epoch 900: New minimal relative error: 16.99%, model saved.
Epoch: 900 Train: 341.14139 Test: 12.81070
Epoch: 1000 Train: 282.33026 Test: 6.66524
Epoch 1100: New minimal relative error: 12.76%, model saved.
Epoch: 1100 Train: 249.89505 Test: 6.52068
Epoch: 1200 Train: 228.79169 Test: 8.45779
Epoch: 1300 Train: 192.12640 Test: 3.82500
Epoch: 1400 Train: 183.46964 Test: 3.81519
Epoch: 1500 Train: 181.40749 Test: 5.93553
Epoch: 1600 Train: 162.28421 Test: 7.29632
Epoch: 1700 Train: 160.34697 Test: 6.23650
Epoch: 1800 Train: 150.44099 Test: 3.96507
Epoch 1900: New minimal relative error: 10.58%, model saved.
Epoch: 1900 Train: 143.75925 Test: 2.92151
Epoch: 2000 Train: 168.21722 Test: 24.40494
Epoch: 2100 Train: 135.85965 Test: 4.28682
Epoch 2200: New minimal relative error: 4.58%, model saved.
Epoch: 2200 Train: 128.80345 Test: 2.15952
Epoch: 2300 Train: 142.05530 Test: 28.80012
Epoch: 2400 Train: 116.11685 Test: 1.54862
Epoch: 2500 Train: 116.23100 Test: 1.62483
Epoch 2600: New minimal relative error: 4.32%, model saved.
Epoch: 2600 Train: 110.98574 Test: 1.62432
Epoch: 2700 Train: 106.79357 Test: 1.41892
Epoch: 2800 Train: 148.18558 Test: 28.03834
Epoch: 2900 Train: 105.37857 Test: 4.04201
Epoch: 3000 Train: 102.02732 Test: 1.68552
Epoch: 3100 Train: 100.08578 Test: 3.20210
Epoch: 3200 Train: 97.46310 Test: 4.64998
Epoch: 3300 Train: 91.20779 Test: 2.23375
Epoch: 3400 Train: 100.82233 Test: 8.44131
Epoch: 3500 Train: 87.65932 Test: 1.70441
Epoch: 3600 Train: 89.87617 Test: 6.17802
Epoch: 3700 Train: 83.29200 Test: 1.57518
Epoch: 3800 Train: 84.47390 Test: 2.57053
Epoch: 3900 Train: 89.59411 Test: 11.16625
Epoch: 4000 Train: 79.28448 Test: 1.35179
Epoch 4100: New minimal relative error: 1.36%, model saved.
Epoch: 4100 Train: 78.03406 Test: 0.87379
Epoch: 4200 Train: 75.75538 Test: 1.67329
Epoch: 4300 Train: 74.45773 Test: 0.76554
Epoch: 4400 Train: 72.75998 Test: 0.96360
Epoch: 4500 Train: 72.31552 Test: 0.91402
Epoch: 4600 Train: 70.58487 Test: 0.65501
Epoch: 4700 Train: 69.39714 Test: 0.65205
Epoch: 4800 Train: 68.46802 Test: 0.78116
Epoch: 4900 Train: 65.55878 Test: 0.64213
Epoch: 5000 Train: 64.46633 Test: 0.75045
Epoch: 5100 Train: 63.05132 Test: 0.60136
Epoch: 5200 Train: 61.25133 Test: 0.91115
Epoch: 5300 Train: 59.90529 Test: 0.53597
Epoch: 5400 Train: 57.95026 Test: 0.43617
Epoch: 5500 Train: 57.33205 Test: 0.42231
Epoch: 5600 Train: 57.27944 Test: 0.43120
Epoch: 5700 Train: 56.23034 Test: 0.83495
Epoch: 5800 Train: 57.10506 Test: 0.80184
Epoch: 5900 Train: 54.27833 Test: 0.40009
Epoch: 6000 Train: 52.85526 Test: 1.10407
Epoch: 6100 Train: 50.91726 Test: 0.42134
Epoch: 6200 Train: 51.89484 Test: 0.39168
Epoch: 6300 Train: 51.88136 Test: 0.46396
Epoch: 6400 Train: 51.29781 Test: 0.55140
Epoch: 6500 Train: 51.50150 Test: 0.39499
Epoch: 6600 Train: 51.07471 Test: 0.57062
Epoch: 6700 Train: 50.31983 Test: 0.45368
Epoch: 6800 Train: 50.24268 Test: 0.36710
Epoch: 6900 Train: 51.53821 Test: 0.57853
Epoch: 7000 Train: 50.29793 Test: 0.33898
Epoch: 7100 Train: 51.17167 Test: 0.44452
Epoch: 7200 Train: 50.70797 Test: 0.38298
Epoch: 7300 Train: 55.52057 Test: 0.48441
Epoch: 7400 Train: 52.90182 Test: 0.39094
Epoch: 7500 Train: 53.75138 Test: 1.48420
Epoch: 7600 Train: 52.42048 Test: 0.60303
Epoch: 7700 Train: 53.47833 Test: 0.51716
Epoch: 7800 Train: 53.07603 Test: 0.54958
Epoch: 7900 Train: 49.90038 Test: 0.37710
Epoch: 8000 Train: 49.42717 Test: 0.39943
Epoch: 8100 Train: 48.06266 Test: 0.35874
Epoch: 8200 Train: 50.15911 Test: 0.66057
Epoch 8300: New minimal relative error: 0.93%, model saved.
Epoch: 8300 Train: 47.50984 Test: 0.34835
Epoch: 8400 Train: 45.86455 Test: 0.34417
Epoch: 8500 Train: 46.31429 Test: 0.36005
Epoch: 8600 Train: 45.39037 Test: 0.35289
Epoch: 8700 Train: 45.00009 Test: 0.32975
Epoch: 8800 Train: 45.25497 Test: 0.46619
Epoch: 8900 Train: 45.10698 Test: 0.32968
Epoch: 9000 Train: 47.32353 Test: 0.75404
Epoch: 9100 Train: 45.91989 Test: 0.97252
Epoch: 9200 Train: 45.49268 Test: 0.34239
Epoch: 9300 Train: 44.49371 Test: 0.35037
Epoch: 9400 Train: 45.17415 Test: 1.03971
Epoch: 9500 Train: 43.76449 Test: 0.31863
Epoch: 9600 Train: 44.13603 Test: 0.45168
Epoch: 9700 Train: 43.34201 Test: 0.30399
Epoch: 9800 Train: 43.45709 Test: 0.46245
Epoch: 9900 Train: 42.70541 Test: 0.29892
Epoch: 9999 Train: 42.65676 Test: 0.39443
Training Loss: tensor(42.6568)
Test Loss: tensor(0.3944)
Learned LE: [  0.9119282   -0.04470276 -14.549294  ]
True LE: [ 8.7361157e-01 -1.4430428e-03 -1.4543825e+01]
Relative Error: [0.744253   0.81068546 0.9690224  0.96468806 0.79157734 0.754482
 0.64457375 0.39949763 0.20784491 0.5980512  1.0547367  1.3058503
 1.4951522  1.7669148  1.9266136  1.604217   1.2221345  1.2087288
 1.4268402  1.2756106  0.96308523 0.5499866  0.9746457  1.6223433
 1.7097292  1.6313298  0.8792655  0.12499756 0.63334095 0.8132905
 0.7668456  0.6874462  0.5554341  0.95013344 0.93045115 1.0683057
 1.0889477  1.1677749  1.4397726  1.720296   1.7875258  1.5400404
 1.1732566  1.0128866  1.3446532  1.3023652  1.001907   1.0842983
 1.4109533  1.5982137  1.4778373  1.2940007  1.2700721  1.4434922
 1.4005051  1.483301   1.2983766  1.0652149  0.9873862  1.0673879
 0.9881814  0.7472182  0.77569    0.7740967  0.89911425 0.9713852
 0.86922085 0.7566293  0.7197362  0.789331   0.6816214  0.5515964
 0.25481007 0.49416363 0.92116094 1.1554546  1.2706683  1.3579873
 1.5090102  1.3535732  0.9500434  0.98467994 1.1005052  1.0649292
 0.64285505 0.49585533 1.1999108  1.3733817  1.5438777  0.89832854
 0.08962741 0.6191684  0.82641727 0.67630976 0.4787622  0.18132082
 0.6649196  0.7381789  0.83139867 1.0258783  1.2422752  1.4543195
 1.9080832  2.210958   1.9688566  1.4461392  0.76320165 0.7923562
 1.1469251  1.0646775  0.94759876 1.3457063  1.6614543  1.5742676
 1.4303694  1.3686861  1.5559535  1.2131675  1.3069947  1.1291927
 0.92172694 0.9684018  1.2102847  1.2166252  0.9720085  0.6951182
 0.67501664 0.7419288  0.8365628  0.54015607 0.27360535 0.14843869
 0.16827442 0.29090747 0.52150357 0.4903676  0.40395647 0.20678487
 0.51034766 0.71956414 0.85160506 0.9011875  1.0648453  1.1219312
 0.7621529  0.7319166  0.9116138  0.8762914  0.6842875  0.40865803
 1.0334066  1.1284177  1.2346809  0.28463322 0.4665745  0.7742363
 0.6599491  0.30617917 0.39695287 0.48037356 0.5499429  0.52469164
 0.7771256  1.1319184  1.4544923  1.7390115  2.3703384  2.4018934
 2.1869698  1.3798273  0.72078097 0.5883023  0.9211174  0.98685175
 1.0030625  1.5037943  1.5213875  1.5352426  1.575901   1.7873013
 1.4145283  1.0766453  0.93043745 0.7639075  0.7938033  1.1436939
 1.582966   1.3036568  0.92851555 0.47361174 0.33528423 0.3132012
 0.42497373 0.49661148 0.3840316  0.34890074 0.5245315  0.4549189
 0.29617396 0.1220907  0.14335153 0.12348542 0.26237    0.24939576
 0.28062314 0.3908308  0.4662652  0.98289496 0.77382463 0.60603887
 0.6856563  0.79244494 0.7269862  0.31596226 0.6258571  1.0762211
 1.1458235  0.11675747 0.41633373 0.6570614  0.5338058  0.35813653
 0.8100773  0.5193626  0.4507699  0.41385746 0.47481167 0.86098176
 1.2867961  1.7189888  2.2354147  2.0433881  1.7999986  1.3491997
 0.5951906  0.53079987 0.597835   0.81305575 0.9612076  1.1045604
 1.2869147  1.5450648  1.8807414  2.1806576  1.3944111  0.98658985
 0.6552057  0.58911633 0.8003323  1.2957046  1.5957955  1.1387004
 0.5141443  0.21430804 0.18135265 0.24087101 0.43605125 0.63869816
 0.70764667 0.8979715  0.93120354 0.84387225 0.77910054 0.61704266
 0.46912703 0.4639778  0.19961753 0.1621513  0.2995638  0.13627024
 0.13607216 0.24337243 0.74916786 0.71129346 0.5816384  0.5924815
 0.4923534  0.5060799  0.12719038 0.6040883  1.1544728  1.0215026
 0.35959405 0.44863963 0.39519775 0.44757134 0.67355543 0.37949696
 0.3254526  0.20668234 0.36182985 0.58055025 0.9199288  1.4727981
 1.3098011  1.2246585  1.3291866  1.2696842  0.98264265 0.35347435
 0.5854612  0.66348255 0.50792617 0.45526022 0.88206476 1.350491
 1.9780778  2.0442388  1.507315   0.82711494 0.6384244  0.42398396
 0.71054184 1.2346185  1.3551768  0.95276004 0.37628794 0.17125285
 0.15412395 0.3645231  0.59195054 0.74418366 0.87574816 0.86749995
 0.79502326 0.90275425 0.759023   0.67894995 0.7201315  0.64037925
 0.63309216 0.61184174 0.4755819  0.2899283  0.2734106  0.31640488
 0.36189145 0.53797716 0.9282986  0.84169686 0.6806086  0.27188852
 0.3197895  0.3100232  0.3778711  0.53919977 1.120123   0.6623652
 0.24683388 0.17578457 0.37523252 0.31032455 0.26413974 0.22105984
 0.28178027 0.32229605 0.67342716 0.97057277 1.083069   0.7157999
 0.58978295 0.8048793  0.74154013 0.5648687  0.51072466 0.3463402
 0.539904   0.69814014 0.3346126  0.5384816  1.2130802  1.5702181
 1.5491738  1.1299969  0.71160126 0.6509767  0.503083   0.83832973
 1.1459419  0.96367    0.67933583 0.31075612 0.14723568 0.1568447
 0.24105707 0.42457852 0.67570937 0.8516513  0.8390743  0.691694
 0.4534781  0.48253754 0.5223341  0.49986356 0.48743436 0.41412845
 0.35457435 0.423929   0.56057554 0.6884778 ]
