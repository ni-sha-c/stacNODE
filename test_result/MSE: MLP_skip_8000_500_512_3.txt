time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.50%, model saved.
Epoch: 0 Train: 3736.07227 Test: 4043.88745
Epoch 80: New minimal relative error: 88.55%, model saved.
Epoch: 80 Train: 135.84932 Test: 156.83199
Epoch 160: New minimal relative error: 22.99%, model saved.
Epoch: 160 Train: 16.55288 Test: 20.64112
Epoch 240: New minimal relative error: 18.95%, model saved.
Epoch: 240 Train: 7.82591 Test: 10.26837
Epoch 320: New minimal relative error: 16.52%, model saved.
Epoch: 320 Train: 6.06228 Test: 7.51037
Epoch 400: New minimal relative error: 12.18%, model saved.
Epoch: 400 Train: 4.44289 Test: 5.80331
Epoch 480: New minimal relative error: 11.75%, model saved.
Epoch: 480 Train: 3.81804 Test: 4.99079
Epoch 560: New minimal relative error: 9.82%, model saved.
Epoch: 560 Train: 4.51859 Test: 4.95826
Epoch: 640 Train: 3.06983 Test: 4.05662
Epoch: 720 Train: 2.78275 Test: 3.66309
Epoch: 800 Train: 2.56771 Test: 3.35990
Epoch: 880 Train: 2.21837 Test: 2.84746
Epoch: 960 Train: 2.01507 Test: 2.51611
Epoch: 1040 Train: 1.94775 Test: 2.36700
Epoch: 1120 Train: 1.69350 Test: 2.24527
Epoch: 1200 Train: 2.46563 Test: 3.53719
Epoch: 1280 Train: 2.29286 Test: 2.88912
Epoch: 1360 Train: 1.93526 Test: 1.89220
Epoch: 1440 Train: 8.21215 Test: 8.89459
Epoch: 1520 Train: 3.53706 Test: 2.53781
Epoch: 1600 Train: 4.65823 Test: 5.34252
Epoch: 1680 Train: 4.05198 Test: 5.35035
Epoch 1760: New minimal relative error: 9.36%, model saved.
Epoch: 1760 Train: 1.16864 Test: 1.50302
Epoch: 1840 Train: 0.87616 Test: 1.06746
Epoch: 1920 Train: 0.89583 Test: 1.12128
Epoch: 2000 Train: 0.78384 Test: 0.97696
Epoch: 2080 Train: 0.78109 Test: 1.09312
Epoch: 2160 Train: 0.95161 Test: 1.24731
Epoch: 2240 Train: 0.67332 Test: 0.83834
Epoch: 2320 Train: 0.66322 Test: 0.79949
Epoch: 2400 Train: 0.56965 Test: 0.72632
Epoch 2480: New minimal relative error: 7.18%, model saved.
Epoch: 2480 Train: 0.54195 Test: 0.69116
Epoch 2560: New minimal relative error: 7.14%, model saved.
Epoch: 2560 Train: 0.70255 Test: 0.78205
Epoch: 2640 Train: 1.21177 Test: 1.68346
Epoch: 2720 Train: 0.97788 Test: 1.35392
Epoch: 2800 Train: 0.43055 Test: 0.56278
Epoch: 2880 Train: 0.41131 Test: 0.54823
Epoch: 2960 Train: 0.43241 Test: 0.55964
Epoch: 3040 Train: 0.44924 Test: 0.59141
Epoch: 3120 Train: 0.47913 Test: 0.54692
Epoch: 3200 Train: 1.58493 Test: 1.63341
Epoch: 3280 Train: 0.66947 Test: 0.79950
Epoch: 3360 Train: 0.33702 Test: 0.49691
Epoch: 3440 Train: 0.39689 Test: 0.54065
Epoch: 3520 Train: 0.31294 Test: 0.44183
Epoch: 3600 Train: 0.32476 Test: 0.44661
Epoch: 3680 Train: 0.63844 Test: 0.75156
Epoch: 3760 Train: 1.27202 Test: 1.48848
Epoch: 3840 Train: 0.26985 Test: 0.38638
Epoch: 3920 Train: 0.25937 Test: 0.37175
Epoch: 4000 Train: 0.73432 Test: 0.72753
Epoch: 4080 Train: 0.24330 Test: 0.34907
Epoch: 4160 Train: 0.26881 Test: 0.37776
Epoch: 4240 Train: 0.27424 Test: 0.42321
Epoch: 4320 Train: 0.91859 Test: 0.83649
Epoch: 4400 Train: 0.21803 Test: 0.31803
Epoch: 4480 Train: 0.24878 Test: 0.33231
Epoch: 4560 Train: 0.20868 Test: 0.30569
Epoch: 4640 Train: 0.20763 Test: 0.30448
Epoch: 4720 Train: 0.20586 Test: 0.31467
Epoch: 4800 Train: 0.19975 Test: 0.29290
Epoch: 4880 Train: 0.19268 Test: 0.28516
Epoch: 4960 Train: 0.58206 Test: 0.77166
Epoch: 5040 Train: 0.18524 Test: 0.27562
Epoch: 5120 Train: 0.49536 Test: 0.65444
Epoch: 5200 Train: 0.19055 Test: 0.27942
Epoch: 5280 Train: 0.17609 Test: 0.26363
Epoch: 5360 Train: 0.21750 Test: 0.36678
Epoch: 5440 Train: 0.16869 Test: 0.25626
Epoch: 5520 Train: 0.20336 Test: 0.34490
Epoch 5600: New minimal relative error: 7.07%, model saved.
Epoch: 5600 Train: 0.16287 Test: 0.24868
Epoch: 5680 Train: 0.16444 Test: 0.25677
Epoch: 5760 Train: 0.16173 Test: 0.24560
Epoch: 5840 Train: 0.17179 Test: 0.24188
Epoch: 5920 Train: 0.15271 Test: 0.23493
Epoch: 6000 Train: 0.15565 Test: 0.25135
Epoch: 6080 Train: 0.14803 Test: 0.22953
Epoch 6160: New minimal relative error: 4.62%, model saved.
Epoch: 6160 Train: 0.15931 Test: 0.23575
Epoch: 6240 Train: 0.14906 Test: 0.22495
Epoch: 6320 Train: 0.14737 Test: 0.23045
Epoch: 6400 Train: 0.13965 Test: 0.21837
Epoch: 6480 Train: 0.15824 Test: 0.24967
Epoch: 6560 Train: 0.13638 Test: 0.21454
Epoch: 6640 Train: 0.57984 Test: 0.56274
Epoch 6720: New minimal relative error: 4.22%, model saved.
Epoch: 6720 Train: 0.13245 Test: 0.20913
Epoch: 6800 Train: 0.13399 Test: 0.20947
Epoch: 6880 Train: 0.12966 Test: 0.20434
Epoch: 6960 Train: 0.12746 Test: 0.20213
Epoch: 7040 Train: 0.17470 Test: 0.26229
Epoch: 7120 Train: 0.14497 Test: 0.22682
Epoch: 7200 Train: 0.12399 Test: 0.19665
Epoch: 7280 Train: 0.12190 Test: 0.19594
Epoch: 7360 Train: 0.25942 Test: 0.35665
Epoch: 7440 Train: 0.11880 Test: 0.19078
Epoch: 7520 Train: 0.32094 Test: 0.36603
Epoch: 7600 Train: 0.11616 Test: 0.18714
Epoch: 7680 Train: 0.22249 Test: 0.26566
Epoch: 7760 Train: 0.11363 Test: 0.18385
Epoch: 7840 Train: 0.11235 Test: 0.18195
Epoch: 7920 Train: 0.12681 Test: 0.19664
Epoch: 7999 Train: 0.11004 Test: 0.17892
Training Loss: tensor(0.1100)
Test Loss: tensor(0.1789)
Learned LE: [ 0.8205594   0.03566007 -4.735329  ]
True LE: [ 8.7173051e-01 -4.4633062e-03 -1.4558794e+01]
Relative Error: [ 8.906569   8.954997   9.181055   9.409887   9.665291  10.111778
 10.494398  10.819816  10.601345  10.515002  10.528935  10.690119
 10.597977  10.148997   9.835381   9.498162   9.352315   9.340399
  9.519555   9.647326   9.7356205  9.754747   9.708862   9.616629
  9.532002   9.492547   9.809471  10.111173  10.003955   9.926229
 10.1742735 10.28383    9.82068    9.3515     8.359943   7.5307384
  7.277436   7.314416   7.2104583  7.0849643  6.7913933  6.557247
  6.2035685  5.8858867  5.603195   5.2116046  4.923679   4.6452684
  4.423773   4.221915   4.043784   3.9271183  3.8692105  4.0028114
  4.3279395  4.8508244  5.565188   6.3213787  7.0431337  7.7313466
  8.348073   8.808539   9.046006   8.956808   9.063123   9.1568165
  9.319603   9.647868  10.02823   10.331691  10.079321   9.962051
  9.999707  10.137547   9.779128   9.351701   8.823142   8.592818
  8.550789   8.756317   8.992842   9.137552   9.16851    9.091027
  8.97252    8.871095   8.797858   8.800431   9.1378     9.496782
  9.528012   9.431145   9.560558   9.718652   9.218578   8.701999
  7.677547   6.8746104  6.720195   6.771298   6.6806846  6.51009
  6.282114   6.084779   5.730069   5.462559   5.1958656  4.917588
  4.6174417  4.3969965  4.241195   4.063345   3.8751035  3.7027352
  3.640429   3.798798   4.2025695  4.7150035  5.4232993  6.1870446
  6.9681005  7.685983   8.323414   8.7866745  9.044697   8.933622
  8.983677   8.98641    9.037339   9.269172   9.664191   9.864729
  9.56607    9.450215   9.488976   9.540037   9.00037    8.345446
  7.8941145  7.7708287  7.961192   8.286334   8.544565   8.6561775
  8.601404   8.439185   8.300615   8.193371   8.050755   8.088237
  8.385275   8.746969   8.853639   8.920888   8.982268   9.129446
  8.6469965  8.117117   7.0287957  6.266059   6.176771   6.3021293
  6.17696    6.0200195  5.8733335  5.6400833  5.2913637  5.090254
  4.8729653  4.6301727  4.4020033  4.2701373  4.1341248  3.961803
  3.7704036  3.5816298  3.4946356  3.6577384  3.9586148  4.4007034
  5.1200423  5.9200864  6.7663164  7.59082    8.240832   8.644215
  8.878624   8.948751   9.00391    8.917813   8.841134   9.014724
  9.321676   9.424617   9.08451    9.000197   8.989519   8.837836
  8.214546   7.414059   7.0379057  7.1107173  7.5741487  8.022098
  8.267889   8.301086   8.181684   7.942863   7.7535667  7.569652
  7.3619885  7.4772916  7.721522   8.022987   8.307865   8.471463
  8.418678   8.491102   8.080968   7.559808   6.4298925  5.7164483
  5.6490293  5.8699164  5.7694836  5.645426   5.482183   5.221982
  4.9067583  4.7790904  4.5994596  4.3609023  4.245158   4.1808834
  4.0702267  3.8918507  3.7083247  3.5301342  3.426446   3.39384
  3.6031048  4.091538   4.8444705  5.7145324  6.601152   7.39839
  7.9892726  8.465871   8.821969   8.904313   9.034272   8.969245
  8.792563   8.821201   9.012044   9.055282   8.656428   8.576662
  8.502592   8.163107   7.373933   6.5634685  6.3388147  6.6689534
  7.2843747  7.7713704  8.013423   8.05209    7.8918233  7.7065835
  7.3812733  7.0360513  6.841645   6.954311   7.103145   7.293949
  7.589651   7.8689775  7.906479   7.8792377  7.526742   6.9565825
  5.912931   5.206627   5.156289   5.512398   5.3896623  5.2956448
  5.0742664  4.825297   4.574918   4.492241   4.311079   4.1425996
  4.1325426  4.126995   4.047944   3.8385377  3.5880313  3.387905
  3.204637   3.1478002  3.3167138  3.8337831  4.6256795  5.572225
  6.497795   7.1894197  7.6216655  7.9972653  8.262452   8.4620695
  8.666803   8.751902   8.734166   8.723579   8.761135   8.77259
  8.366778   8.183633   8.033656   7.532058   6.6379266  5.814366
  5.708079   6.273496   7.034053   7.5555243  7.855296   7.8869076
  7.8375444  7.687702   7.358225   6.948792   6.521059   6.499067
  6.515155   6.5949745  6.7657104  7.1408606  7.3896704  7.2815075
  7.192752   6.5554366  5.5995936  4.794339   4.7330866  5.154986
  5.0567985  5.0046816  4.6863294  4.4782968  4.2545686  4.2291427
  4.0496407  3.9768946  4.047257   4.1118593  3.9695907  3.7333767
  3.4725156  3.2442465  3.0324771  2.9641223  3.1322625  3.6097753
  4.475429   5.487988   6.231059   6.6981     7.1518064  7.5453906
  7.842233   7.9892373  8.325451   8.421919   8.456149   8.406484
  8.488291   8.618754   8.176138   7.8511147  7.5989323  6.8666553
  6.0039415  5.1805916  5.161777   5.9480653  6.8409147  7.441216
  7.7688665  7.9549155  8.007338   7.818552   7.622996   7.151565
  6.56472    6.1980104  6.054077   5.949331 ]
