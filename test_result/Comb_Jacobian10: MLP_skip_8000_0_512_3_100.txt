time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 98.62%, model saved.
Epoch: 0 Train: 9207.76172 Test: 3921.07104
Epoch: 80 Train: 2474.48242 Test: 1063.81592
Epoch 160: New minimal relative error: 86.55%, model saved.
Epoch: 160 Train: 1699.28210 Test: 599.54565
Epoch: 240 Train: 845.73914 Test: 219.64363
Epoch: 320 Train: 516.06152 Test: 99.55534
Epoch 400: New minimal relative error: 33.83%, model saved.
Epoch: 400 Train: 216.23505 Test: 30.64876
Epoch 480: New minimal relative error: 24.88%, model saved.
Epoch: 480 Train: 160.13678 Test: 22.75169
Epoch 560: New minimal relative error: 18.62%, model saved.
Epoch: 560 Train: 111.92676 Test: 23.65658
Epoch 640: New minimal relative error: 11.26%, model saved.
Epoch: 640 Train: 84.01960 Test: 8.90646
Epoch: 720 Train: 71.95661 Test: 11.40067
Epoch: 800 Train: 71.75177 Test: 21.99084
Epoch: 880 Train: 140.87448 Test: 80.57058
Epoch 960: New minimal relative error: 2.23%, model saved.
Epoch: 960 Train: 44.86537 Test: 2.19041
Epoch: 1040 Train: 40.08374 Test: 1.86670
Epoch: 1120 Train: 41.14701 Test: 1.93889
Epoch: 1200 Train: 36.10722 Test: 8.17243
Epoch 1280: New minimal relative error: 2.02%, model saved.
Epoch: 1280 Train: 33.82407 Test: 1.31207
Epoch: 1360 Train: 39.63949 Test: 13.09538
Epoch 1440: New minimal relative error: 1.63%, model saved.
Epoch: 1440 Train: 28.36417 Test: 0.87575
Epoch: 1520 Train: 29.24718 Test: 1.71423
Epoch: 1600 Train: 26.09686 Test: 0.86941
Epoch: 1680 Train: 24.67496 Test: 0.73977
Epoch: 1760 Train: 27.37366 Test: 1.71196
Epoch: 1840 Train: 21.84223 Test: 0.60956
Epoch: 1920 Train: 21.71587 Test: 0.70243
Epoch: 2000 Train: 25.21431 Test: 2.04412
Epoch: 2080 Train: 19.76438 Test: 0.52025
Epoch 2160: New minimal relative error: 1.41%, model saved.
Epoch: 2160 Train: 19.52504 Test: 0.50970
Epoch: 2240 Train: 18.98622 Test: 0.64618
Epoch: 2320 Train: 20.49494 Test: 1.21307
Epoch: 2400 Train: 16.98215 Test: 0.37631
Epoch: 2480 Train: 17.35372 Test: 0.43115
Epoch 2560: New minimal relative error: 1.07%, model saved.
Epoch: 2560 Train: 15.68267 Test: 0.34066
Epoch: 2640 Train: 16.73287 Test: 0.56380
Epoch: 2720 Train: 16.26516 Test: 0.38956
Epoch 2800: New minimal relative error: 0.93%, model saved.
Epoch: 2800 Train: 14.19464 Test: 0.27325
Epoch: 2880 Train: 13.58332 Test: 0.28120
Epoch: 2960 Train: 13.46554 Test: 0.37650
Epoch: 3040 Train: 13.69408 Test: 0.81386
Epoch: 3120 Train: 12.23088 Test: 0.28629
Epoch: 3200 Train: 11.79158 Test: 0.21825
Epoch: 3280 Train: 13.08765 Test: 0.35388
Epoch 3360: New minimal relative error: 0.71%, model saved.
Epoch: 3360 Train: 11.15100 Test: 0.18022
Epoch: 3440 Train: 12.21465 Test: 0.45555
Epoch: 3520 Train: 10.85841 Test: 0.17201
Epoch: 3600 Train: 11.71253 Test: 0.24516
Epoch: 3680 Train: 10.84197 Test: 0.17436
Epoch: 3760 Train: 10.57228 Test: 0.23986
Epoch: 3840 Train: 12.07440 Test: 1.94273
Epoch: 3920 Train: 10.03016 Test: 0.17634
Epoch: 4000 Train: 9.78929 Test: 0.15372
Epoch: 4080 Train: 9.69746 Test: 0.23483
Epoch: 4160 Train: 9.67028 Test: 0.24478
Epoch: 4240 Train: 9.93990 Test: 1.01533
Epoch: 4320 Train: 9.22594 Test: 0.12963
Epoch: 4400 Train: 9.05572 Test: 0.16655
Epoch: 4480 Train: 10.07235 Test: 0.83840
Epoch: 4560 Train: 11.83128 Test: 3.41787
Epoch: 4640 Train: 8.46337 Test: 0.10984
Epoch: 4720 Train: 8.36158 Test: 0.11567
Epoch: 4800 Train: 8.84182 Test: 0.34889
Epoch: 4880 Train: 8.34003 Test: 0.10533
Epoch: 4960 Train: 8.32264 Test: 0.12483
Epoch: 5040 Train: 9.08206 Test: 0.45429
Epoch: 5120 Train: 9.07313 Test: 1.10809
Epoch: 5200 Train: 8.08835 Test: 0.10461
Epoch: 5280 Train: 8.39843 Test: 0.18471
Epoch: 5360 Train: 7.93647 Test: 0.09555
Epoch: 5440 Train: 7.99940 Test: 0.11285
Epoch: 5520 Train: 7.89527 Test: 0.10487
Epoch: 5600 Train: 7.67891 Test: 0.09144
Epoch: 5680 Train: 7.51075 Test: 0.11980
Epoch 5760: New minimal relative error: 0.59%, model saved.
Epoch: 5760 Train: 7.38450 Test: 0.08579
Epoch: 5840 Train: 7.45550 Test: 0.12301
Epoch: 5920 Train: 8.38956 Test: 1.81966
Epoch: 6000 Train: 7.26030 Test: 0.08081
Epoch: 6080 Train: 7.16006 Test: 0.07582
Epoch: 6160 Train: 8.02994 Test: 1.22211
Epoch: 6240 Train: 7.00197 Test: 0.07342
Epoch: 6320 Train: 7.06082 Test: 0.11976
Epoch: 6400 Train: 6.79219 Test: 0.16371
Epoch: 6480 Train: 6.59574 Test: 0.06585
Epoch: 6560 Train: 6.57785 Test: 0.07093
Epoch: 6640 Train: 6.51368 Test: 0.08072
Epoch: 6720 Train: 6.61120 Test: 0.15609
Epoch: 6800 Train: 6.58225 Test: 0.10405
Epoch: 6880 Train: 6.47922 Test: 0.09723
Epoch: 6960 Train: 6.45210 Test: 0.10127
Epoch: 7040 Train: 6.32499 Test: 0.06795
Epoch: 7120 Train: 6.26352 Test: 0.05670
Epoch: 7200 Train: 6.25102 Test: 0.05641
Epoch: 7280 Train: 6.24340 Test: 0.06363
Epoch: 7360 Train: 6.31588 Test: 0.13614
Epoch: 7440 Train: 6.16369 Test: 0.06471
Epoch: 7520 Train: 6.24797 Test: 0.05673
Epoch: 7600 Train: 6.29864 Test: 0.09519
Epoch: 7680 Train: 6.47974 Test: 0.30014
Epoch: 7760 Train: 6.30685 Test: 0.14151
Epoch: 7840 Train: 6.20521 Test: 0.09066
Epoch: 7920 Train: 6.43859 Test: 0.36609
Epoch: 7999 Train: 6.10721 Test: 0.06969
Training Loss: tensor(6.1072)
Test Loss: tensor(0.0697)
Learned LE: [  0.92098594  -0.03942015 -14.561847  ]
True LE: [ 8.8240355e-01  1.2684742e-02 -1.4568612e+01]
Relative Error: [3.0957017  2.9359689  2.4903777  2.0497656  1.9191753  2.226639
 2.6571193  2.8520095  2.8103042  2.8780158  2.766942   2.6169646
 2.2966652  1.9658579  1.6253746  1.1778476  1.3551073  1.415552
 1.544708   1.6072905  1.6568118  1.6517226  2.0023992  2.4103374
 2.696267   3.2973285  4.02792    4.677998   4.914933   4.8089137
 4.3866777  3.7818937  3.2440107  2.8793688  2.720558   2.7346992
 2.6152866  2.6305356  2.5897286  2.575213   2.299666   1.9124644
 1.5260646  1.3289405  1.2638674  1.5042717  2.051038   2.4422066
 2.6812186  2.7281241  2.6080747  2.5160475  2.3490162  2.0405862
 1.7413968  1.7428937  1.6732945  1.8516997  1.9593652  2.0207837
 2.234615   2.5489242  2.7917337  2.8067822  2.3815875  1.9677656
 1.8103272  2.186872   2.6285415  2.859138   2.7865818  2.8101006
 2.7625854  2.4642062  2.1992438  1.8599436  1.4545687  0.9313423
 0.759888   0.77367777 0.80849564 1.0060623  1.183409   1.2808417
 1.6230325  1.9989761  2.3001194  2.9139233  3.6254022  4.309611
 4.7625713  4.837877   4.4792876  3.7246506  3.2322555  2.720229
 2.4839325  2.4053354  2.322121   2.3216264  2.3821664  2.3820324
 2.1429164  1.760287   1.3552811  1.1593723  1.0989089  1.1345029
 1.5303652  1.9405649  2.173331   2.2081473  2.1279318  2.0124755
 1.8841183  1.6344392  1.4046723  1.3433177  1.3590026  1.5358473
 1.6146458  1.758082   1.8901534  2.0929904  2.3976693  2.5159085
 2.225141   1.8778079  1.6934518  2.0082345  2.6318464  2.8986259
 2.8531876  2.7700138  2.744532   2.2931187  2.018399   1.659933
 1.1920773  0.77597195 0.48930383 0.37657273 0.41263258 0.51108354
 0.69721437 0.85466766 1.1656296  1.6208715  1.9392394  2.4745128
 3.1315486  3.804383   4.321268   4.591715   4.4230165  3.7896085
 3.1505103  2.587846   2.293363   2.1085443  2.0438704  1.9692074
 2.1234245  2.13397    1.9652488  1.589341   1.2181288  1.0102108
 0.9687357  0.90575516 1.0239273  1.3951226  1.6586397  1.7350478
 1.7366925  1.6541954  1.517469   1.2859073  1.066555   1.0365134
 1.0379322  1.1125978  1.3387065  1.4982882  1.56326    1.6245835
 1.8574758  2.1417716  2.133058   1.755698   1.5097488  1.6979071
 2.4127183  2.813071   2.8691492  2.7315528  2.7042897  2.384674
 1.863517   1.5379887  1.0504762  0.71290433 0.5214801  0.8572949
 0.5672487  0.41690814 0.35062212 0.46093798 0.7230256  1.2629043
 1.6590327  2.2112944  2.75142    3.2086327  3.7305741  4.0938025
 4.216149   3.8808022  3.0840256  2.5380595  2.0683482  1.8823942
 1.7905056  1.7025249  1.7852602  1.8558923  1.7935997  1.4238635
 1.0542297  0.8545177  0.8352733  0.76013595 0.6848652  0.8796377
 1.1715665  1.3281108  1.4404707  1.4463573  1.3281053  1.0750662
 0.81469697 0.79942274 0.75076395 0.8368712  0.9811681  1.107538
 1.2015206  1.2771661  1.3740648  1.5081744  1.6569926  1.5877035
 1.2940397  1.4061183  2.020459   2.6268222  2.8051543  2.6177094
 2.6374338  2.5712469  1.9753203  1.4779737  0.9334063  0.6709033
 0.5647759  0.8544092  0.8716365  0.6099361  0.39577657 0.31756517
 0.34600762 0.9000801  1.3812829  1.9562792  2.4677174  2.955392
 3.2054012  3.4408107  3.6628225  3.7292943  3.1686108  2.3928611
 1.8557522  1.5919814  1.5365591  1.566036   1.5127705  1.6602595
 1.6288956  1.3236747  0.85610235 0.68553895 0.7441254  0.7044637
 0.7132197  0.611513   0.7692417  1.018254   1.282165   1.3325175
 1.2119054  0.9254899  0.57432234 0.58454436 0.55359846 0.6095015
 0.7385441  0.7760858  0.82762176 0.8845183  0.8781334  0.9799124
 0.99706966 1.0966802  1.0273113  1.052957   1.6169058  2.1974142
 2.5496726  2.6173604  2.3965316  2.5872066  2.374791   1.7318865
 1.1974974  0.801343   0.7492681  0.83706677 0.82773745 0.7955692
 0.5499385  0.43738824 0.32894066 0.41749686 1.0395085  1.5861236
 2.1613271  2.6828353  3.003921   3.081169   2.9837613  3.1081243
 3.1203237  2.4432328  1.6883777  1.2494028  1.1982646  1.3714092
 1.4401667  1.4629285  1.5843679  1.36733    0.8145314  0.5476537
 0.5348177  0.7320282  0.7543811  0.9457482  0.6725042  0.81356746
 1.1526021  1.3139124  1.2169784  0.9058898  0.48073134 0.36037597
 0.39086398 0.51022476 0.6148537  0.6413133  0.62518895 0.59257776
 0.5676593  0.6176886  0.74923456 0.8271832  0.84191614 0.8193005
 1.1680845  1.7517471  2.036768   2.3461668  2.3121767  2.1508868
 2.2354958  2.040936   1.638374   1.3237844  1.1317629  1.0919728
 0.966804   0.82199156 0.7512481  0.5454861  0.50609094 0.37721515
 0.45817095 1.0344031  1.6886611  2.2762554 ]
