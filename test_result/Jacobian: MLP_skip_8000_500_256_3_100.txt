time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.57%, model saved.
Epoch: 0 Train: 9632.83984 Test: 3796.07373
Epoch: 80 Train: 2759.68311 Test: 1110.55005
Epoch: 160 Train: 2656.72510 Test: 1058.28296
Epoch: 240 Train: 2389.61865 Test: 994.97058
Epoch: 320 Train: 1989.26807 Test: 801.04596
Epoch: 400 Train: 1002.41449 Test: 327.95007
Epoch 480: New minimal relative error: 67.02%, model saved.
Epoch: 480 Train: 453.12274 Test: 112.37516
Epoch: 560 Train: 253.53903 Test: 52.46984
Epoch 640: New minimal relative error: 32.51%, model saved.
Epoch: 640 Train: 166.60886 Test: 27.48166
Epoch 720: New minimal relative error: 28.95%, model saved.
Epoch: 720 Train: 125.09216 Test: 16.24610
Epoch 800: New minimal relative error: 10.45%, model saved.
Epoch: 800 Train: 92.21590 Test: 10.55182
Epoch: 880 Train: 76.15550 Test: 8.42390
Epoch: 960 Train: 65.49688 Test: 6.68674
Epoch: 1040 Train: 57.92526 Test: 6.23360
Epoch: 1120 Train: 50.33696 Test: 4.75723
Epoch: 1200 Train: 47.05178 Test: 4.50001
Epoch: 1280 Train: 42.22083 Test: 3.84721
Epoch 1360: New minimal relative error: 6.48%, model saved.
Epoch: 1360 Train: 39.41825 Test: 3.74132
Epoch: 1440 Train: 37.90268 Test: 5.38880
Epoch: 1520 Train: 34.08438 Test: 2.73002
Epoch: 1600 Train: 32.95386 Test: 3.37383
Epoch: 1680 Train: 30.54256 Test: 2.49577
Epoch 1760: New minimal relative error: 4.32%, model saved.
Epoch: 1760 Train: 29.42773 Test: 2.14747
Epoch: 1840 Train: 28.07715 Test: 1.99109
Epoch: 1920 Train: 27.06262 Test: 2.17951
Epoch: 2000 Train: 25.89355 Test: 1.86532
Epoch: 2080 Train: 50.01708 Test: 20.55042
Epoch: 2160 Train: 23.73334 Test: 1.82359
Epoch: 2240 Train: 22.92868 Test: 1.62295
Epoch: 2320 Train: 24.46487 Test: 2.17258
Epoch: 2400 Train: 21.72051 Test: 1.53948
Epoch 2480: New minimal relative error: 3.78%, model saved.
Epoch: 2480 Train: 21.29884 Test: 1.45349
Epoch: 2560 Train: 21.59486 Test: 1.75028
Epoch: 2640 Train: 20.27146 Test: 1.41398
Epoch: 2720 Train: 19.39335 Test: 1.29815
Epoch: 2800 Train: 18.81031 Test: 1.21672
Epoch: 2880 Train: 18.25522 Test: 1.22930
Epoch: 2960 Train: 17.78033 Test: 1.15123
Epoch: 3040 Train: 26.51971 Test: 6.41301
Epoch: 3120 Train: 17.11628 Test: 1.13777
Epoch: 3200 Train: 16.68670 Test: 1.09930
Epoch: 3280 Train: 18.60398 Test: 3.77835
Epoch: 3360 Train: 16.10792 Test: 1.05006
Epoch: 3440 Train: 15.73116 Test: 0.99216
Epoch: 3520 Train: 15.75943 Test: 1.38954
Epoch: 3600 Train: 15.19466 Test: 0.97825
Epoch: 3680 Train: 14.80177 Test: 0.92463
Epoch: 3760 Train: 14.76060 Test: 1.08839
Epoch: 3840 Train: 14.16545 Test: 0.88282
Epoch: 3920 Train: 13.99438 Test: 0.85481
Epoch: 4000 Train: 13.73139 Test: 0.83014
Epoch: 4080 Train: 13.62292 Test: 0.82762
Epoch 4160: New minimal relative error: 2.83%, model saved.
Epoch: 4160 Train: 13.37897 Test: 0.81192
Epoch: 4240 Train: 13.24385 Test: 0.78587
Epoch: 4320 Train: 13.14606 Test: 0.79567
Epoch: 4400 Train: 12.95603 Test: 0.74200
Epoch: 4480 Train: 13.00421 Test: 1.05526
Epoch: 4560 Train: 12.55870 Test: 0.73201
Epoch: 4640 Train: 12.51701 Test: 0.71279
Epoch: 4720 Train: 12.32142 Test: 0.72273
Epoch: 4800 Train: 12.26685 Test: 0.76203
Epoch: 4880 Train: 11.98097 Test: 0.73064
Epoch: 4960 Train: 11.85950 Test: 0.73239
Epoch: 5040 Train: 13.56244 Test: 1.96893
Epoch 5120: New minimal relative error: 2.26%, model saved.
Epoch: 5120 Train: 11.75428 Test: 0.73814
Epoch: 5200 Train: 11.77457 Test: 0.74417
Epoch: 5280 Train: 11.89707 Test: 0.80940
Epoch: 5360 Train: 11.72035 Test: 0.69703
Epoch: 5440 Train: 11.86037 Test: 0.69903
Epoch: 5520 Train: 12.22216 Test: 1.28002
Epoch: 5600 Train: 11.56692 Test: 0.70161
Epoch: 5680 Train: 11.55967 Test: 0.70428
Epoch: 5760 Train: 11.31033 Test: 0.70709
Epoch: 5840 Train: 11.21108 Test: 0.70660
Epoch: 5920 Train: 11.27960 Test: 0.71142
Epoch: 6000 Train: 11.30592 Test: 0.70682
Epoch: 6080 Train: 11.41644 Test: 0.72257
Epoch: 6160 Train: 11.91849 Test: 0.81065
Epoch: 6240 Train: 12.95219 Test: 0.94497
Epoch: 6320 Train: 12.47628 Test: 0.98256
Epoch: 6400 Train: 11.95661 Test: 0.85573
Epoch: 6480 Train: 11.47343 Test: 0.80459
Epoch: 6560 Train: 11.38146 Test: 0.88658
Epoch: 6640 Train: 11.20987 Test: 0.78572
Epoch: 6720 Train: 10.99084 Test: 0.76417
Epoch: 6800 Train: 10.99053 Test: 0.77576
Epoch: 6880 Train: 10.85639 Test: 0.76684
Epoch: 6960 Train: 10.81453 Test: 0.76791
Epoch: 7040 Train: 10.70834 Test: 0.74455
Epoch: 7120 Train: 10.70002 Test: 0.77937
Epoch: 7200 Train: 10.49843 Test: 0.72405
Epoch: 7280 Train: 11.26604 Test: 1.42803
Epoch 7360: New minimal relative error: 1.97%, model saved.
Epoch: 7360 Train: 10.41487 Test: 0.71670
Epoch: 7440 Train: 10.38336 Test: 0.70939
Epoch: 7520 Train: 10.51973 Test: 0.82385
Epoch: 7600 Train: 10.40179 Test: 0.68975
Epoch: 7680 Train: 10.30449 Test: 0.67006
Epoch: 7760 Train: 10.28689 Test: 0.65952
Epoch: 7840 Train: 10.09992 Test: 0.65671
Epoch: 7920 Train: 10.03905 Test: 0.67000
Epoch: 7999 Train: 9.99851 Test: 0.67435
Training Loss: tensor(9.9985)
Test Loss: tensor(0.6744)
Learned LE: [  0.80800337   0.04162087 -14.508897  ]
True LE: [ 8.6252856e-01 -3.5234098e-03 -1.4536466e+01]
Relative Error: [1.3960555  1.4787251  1.2721035  1.2953645  1.356965   1.1653707
 0.9834463  1.1941439  1.8431764  3.0203292  4.530082   5.5155444
 4.8367715  4.23189    3.0762517  2.0871062  1.4995394  1.4124519
 1.6742861  2.0370378  2.2992797  2.3897002  2.3417199  1.9590927
 1.4174192  0.8870696  0.599791   0.61583805 0.9303512  1.4211098
 2.1397212  2.8274915  3.3212214  3.0185916  2.060052   1.2387321
 0.78614587 0.56904036 0.9572259  1.6862365  2.0043895  2.0367856
 1.892657   1.733993   1.8154452  2.3744993  2.8896947  3.166396
 2.0548615  1.1842383  0.62433904 0.44930542 0.47205245 0.50110805
 0.49360487 0.44083205 0.33640516 0.45007378 0.742444   1.0090392
 1.1803705  1.1962653  1.2673923  1.3909856  1.2325292  1.3240874
 1.3885013  1.1505892  0.92328477 0.90994847 1.6002089  2.8386452
 4.237386   5.395937   4.56378    3.8769624  2.6141684  1.686131
 1.2268478  1.2989529  1.625527   1.9806061  2.3217404  2.4755926
 2.427897   2.2348096  1.6971074  1.4215442  1.1156813  0.9585552
 0.94232494 1.1457396  1.4927288  2.2882974  2.7336125  2.7321296
 2.2355697  1.4542214  1.101778   0.99634045 0.9228861  1.5724299
 1.8589814  1.880616   1.7087235  1.441689   1.397833   1.9171739
 2.8691285  3.326436   2.186307   1.2342956  0.56846607 0.45318547
 0.57096666 0.626623   0.6887716  0.67270654 0.5739358  0.45015594
 0.43414325 0.75570244 0.9832089  1.0062045  1.0625625  1.2477082
 1.1609373  1.306917   1.3699327  1.1489207  0.864802   0.63618195
 1.3457408  2.5886595  3.846887   5.035035   4.366837   3.5072737
 2.2686205  1.3956764  1.0675706  1.2622653  1.6024587  1.9115471
 2.175503   2.400602   2.429007   2.2724402  1.9772639  1.8213743
 1.5720603  1.3141711  1.1791916  1.1224537  1.1239607  1.6956196
 2.2865915  2.551      2.3872728  1.6698986  1.3434099  1.3332316
 1.2931916  1.3445941  1.6741425  1.7548796  1.6051899  1.2967893
 1.0768174  1.4042076  2.354641   3.4028716  2.355441   1.4133663
 0.6685882  0.41043568 0.6723863  0.79186076 0.82899845 0.9030663
 0.927611   0.823648   0.61519754 0.64390165 0.8911166  0.8649715
 0.8331688  1.0498691  1.0562096  1.2184796  1.2685585  1.1397705
 0.91363686 0.5054805  1.0857167  2.2672646  3.3942823  4.452129
 4.2415032  3.2632258  2.0307808  1.1980482  0.97751516 1.243256
 1.5760909  1.8325374  2.002386   2.1472912  2.24258    2.1697614
 2.1941776  2.0935407  1.8651528  1.5719358  1.408664   1.2728281
 1.0902559  1.3404256  1.836842   2.2163215  2.3424067  1.9047621
 1.4957451  1.5460542  1.5273118  1.1780128  1.4382652  1.6346056
 1.5698043  1.3139588  0.9090246  0.967263   1.7505059  2.884323
 2.5428338  1.6030709  0.87779117 0.4652486  0.64059365 0.7812305
 0.7688465  0.8119808  1.0050123  1.2768662  1.1014712  0.8908974
 1.04187    0.94985926 0.69161284 0.82557267 0.87165916 0.9832209
 1.0733787  1.0661446  0.8734147  0.45129746 0.89763206 1.9244801
 2.880955   3.8221128  4.1891403  3.1512585  1.8992158  1.0864792
 0.94973314 1.246064   1.5411928  1.7317792  1.8256369  1.8472078
 1.8616123  1.8732319  2.1444314  2.2097824  1.9923983  1.7388252
 1.5269343  1.3960718  1.2345755  1.2216394  1.5143678  1.8645612
 2.1769826  2.0269017  1.5806783  1.5936278  1.6209692  1.4073306
 1.1041222  1.4656808  1.5632921  1.4019562  0.9872627  0.75858295
 1.2136611  2.15646    2.618577   1.8991742  1.0860021  0.6768448
 0.45197862 0.59935033 0.6623624  0.6544244  0.7981794  1.155717
 1.2411073  0.8847798  0.6647345  0.705157   0.6866608  0.6822291
 0.61004734 0.6230406  0.7712351  0.90833974 0.77304274 0.39829278
 0.6614183  1.5524122  2.3959026  3.1789932  4.1318855  3.1697001
 1.8530965  1.015237   0.93956625 1.2780162  1.5472764  1.6718569
 1.6603911  1.5591385  1.4406707  1.4693129  1.8019725  2.00507
 1.984181   1.7216136  1.5114323  1.4234298  1.5135756  1.4683145
 1.2625135  1.3938813  1.8284814  1.9904256  1.6738186  1.4636114
 1.5494745  1.5003625  1.2439336  1.1667448  1.4601256  1.4638247
 1.1668408  0.85070235 0.83646894 1.4060886  2.519925   2.0525916
 1.4577515  0.7991557  0.41348514 0.47148815 0.5681864  0.54512763
 0.69954413 1.0399772  0.7884898  0.5005223  0.32031184 0.4165446
 0.6204013  0.6452401  0.5928832  0.33329695 0.3344238  0.6235688
 0.6064654  0.34538704 0.39313263 1.1728106  1.8551489  2.5511184
 3.3002377  3.3551939  1.9227244  0.97465503 0.880373   1.267301
 1.5709823  1.6891012  1.6189995  1.3981464  1.0839815  1.0093516
 1.2315001  1.4554862  1.6177164  1.5427681 ]
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.57%, model saved.
Epoch: 0 Train: 9632.83984 Test: 3796.07373
Epoch: 80 Train: 2759.68311 Test: 1110.55005
Epoch: 160 Train: 2656.72510 Test: 1058.28296
Epoch: 240 Train: 2389.61865 Test: 994.97058
Epoch: 320 Train: 1989.26807 Test: 801.04596
Epoch: 400 Train: 1002.41449 Test: 327.95007
Epoch 480: New minimal relative error: 67.02%, model saved.
Epoch: 480 Train: 453.12274 Test: 112.37516
Epoch: 560 Train: 253.53903 Test: 52.46984
Epoch 640: New minimal relative error: 32.51%, model saved.
Epoch: 640 Train: 166.60886 Test: 27.48166
Epoch 720: New minimal relative error: 28.95%, model saved.
Epoch: 720 Train: 125.09216 Test: 16.24610
Epoch 800: New minimal relative error: 10.45%, model saved.
Epoch: 800 Train: 92.21590 Test: 10.55182
Epoch: 880 Train: 76.15550 Test: 8.42390
Epoch: 960 Train: 65.49688 Test: 6.68674
Epoch: 1040 Train: 57.92526 Test: 6.23360
Epoch: 1120 Train: 50.33696 Test: 4.75723
Epoch: 1200 Train: 47.05178 Test: 4.50001
Epoch: 1280 Train: 42.22083 Test: 3.84721
Epoch 1360: New minimal relative error: 6.48%, model saved.
Epoch: 1360 Train: 39.41825 Test: 3.74132
Epoch: 1440 Train: 37.90268 Test: 5.38880
Epoch: 1520 Train: 34.08438 Test: 2.73002
Epoch: 1600 Train: 32.95386 Test: 3.37383
Epoch: 1680 Train: 30.54256 Test: 2.49577
Epoch 1760: New minimal relative error: 4.32%, model saved.
Epoch: 1760 Train: 29.42773 Test: 2.14747
Epoch: 1840 Train: 28.07715 Test: 1.99109
Epoch: 1920 Train: 27.06262 Test: 2.17951
Epoch: 2000 Train: 25.89355 Test: 1.86532
Epoch: 2080 Train: 50.01708 Test: 20.55042
Epoch: 2160 Train: 23.73334 Test: 1.82359
Epoch: 2240 Train: 22.92868 Test: 1.62295
Epoch: 2320 Train: 24.46487 Test: 2.17258
Epoch: 2400 Train: 21.72051 Test: 1.53948
Epoch 2480: New minimal relative error: 3.78%, model saved.
Epoch: 2480 Train: 21.29884 Test: 1.45349
Epoch: 2560 Train: 21.59486 Test: 1.75028
Epoch: 2640 Train: 20.27146 Test: 1.41398
Epoch: 2720 Train: 19.39335 Test: 1.29815
Epoch: 2800 Train: 18.81031 Test: 1.21672
Epoch: 2880 Train: 18.25522 Test: 1.22930
Epoch: 2960 Train: 17.78033 Test: 1.15123
Epoch: 3040 Train: 26.51971 Test: 6.41301
Epoch: 3120 Train: 17.11628 Test: 1.13777
Epoch: 3200 Train: 16.68670 Test: 1.09930
Epoch: 3280 Train: 18.60398 Test: 3.77835
Epoch: 3360 Train: 16.10792 Test: 1.05006
Epoch: 3440 Train: 15.73116 Test: 0.99216
Epoch: 3520 Train: 15.75943 Test: 1.38954
Epoch: 3600 Train: 15.19466 Test: 0.97825
Epoch: 3680 Train: 14.80177 Test: 0.92463
Epoch: 3760 Train: 14.76060 Test: 1.08839
Epoch: 3840 Train: 14.16545 Test: 0.88282
Epoch: 3920 Train: 13.99438 Test: 0.85481
Epoch: 4000 Train: 13.73139 Test: 0.83014
Epoch: 4080 Train: 13.62292 Test: 0.82762
Epoch 4160: New minimal relative error: 2.83%, model saved.
Epoch: 4160 Train: 13.37897 Test: 0.81192
Epoch: 4240 Train: 13.24385 Test: 0.78587
Epoch: 4320 Train: 13.14606 Test: 0.79567
Epoch: 4400 Train: 12.95603 Test: 0.74200
Epoch: 4480 Train: 13.00421 Test: 1.05526
Epoch: 4560 Train: 12.55870 Test: 0.73201
Epoch: 4640 Train: 12.51701 Test: 0.71279
Epoch: 4720 Train: 12.32142 Test: 0.72273
Epoch: 4800 Train: 12.26685 Test: 0.76203
Epoch: 4880 Train: 11.98097 Test: 0.73064
Epoch: 4960 Train: 11.85950 Test: 0.73239
Epoch: 5040 Train: 13.56244 Test: 1.96893
Epoch 5120: New minimal relative error: 2.26%, model saved.
Epoch: 5120 Train: 11.75428 Test: 0.73814
Epoch: 5200 Train: 11.77457 Test: 0.74417
Epoch: 5280 Train: 11.89707 Test: 0.80940
Epoch: 5360 Train: 11.72035 Test: 0.69703
Epoch: 5440 Train: 11.86037 Test: 0.69903
Epoch: 5520 Train: 12.22216 Test: 1.28002
Epoch: 5600 Train: 11.56692 Test: 0.70161
Epoch: 5680 Train: 11.55967 Test: 0.70428
Epoch: 5760 Train: 11.31033 Test: 0.70709
Epoch: 5840 Train: 11.21108 Test: 0.70660
Epoch: 5920 Train: 11.27960 Test: 0.71142
Epoch: 6000 Train: 11.30592 Test: 0.70682
Epoch: 6080 Train: 11.41644 Test: 0.72257
Epoch: 6160 Train: 11.91849 Test: 0.81065
Epoch: 6240 Train: 12.95219 Test: 0.94497
Epoch: 6320 Train: 12.47628 Test: 0.98256
Epoch: 6400 Train: 11.95661 Test: 0.85573
Epoch: 6480 Train: 11.47343 Test: 0.80459
Epoch: 6560 Train: 11.38146 Test: 0.88658
Epoch: 6640 Train: 11.20987 Test: 0.78572
Epoch: 6720 Train: 10.99084 Test: 0.76417
Epoch: 6800 Train: 10.99053 Test: 0.77576
Epoch: 6880 Train: 10.85639 Test: 0.76684
Epoch: 6960 Train: 10.81453 Test: 0.76791
Epoch: 7040 Train: 10.70834 Test: 0.74455
Epoch: 7120 Train: 10.70002 Test: 0.77937
Epoch: 7200 Train: 10.49843 Test: 0.72405
Epoch: 7280 Train: 11.26604 Test: 1.42803
Epoch 7360: New minimal relative error: 1.97%, model saved.
Epoch: 7360 Train: 10.41487 Test: 0.71670
Epoch: 7440 Train: 10.38336 Test: 0.70939
Epoch: 7520 Train: 10.51973 Test: 0.82385
Epoch: 7600 Train: 10.40179 Test: 0.68975
Epoch: 7680 Train: 10.30449 Test: 0.67006
Epoch: 7760 Train: 10.28689 Test: 0.65952
Epoch: 7840 Train: 10.09992 Test: 0.65671
Epoch: 7920 Train: 10.03905 Test: 0.67000
Epoch: 7999 Train: 9.99851 Test: 0.67435
time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.57%, model saved.
Epoch: 0 Train: 9632.83984 Test: 3796.07373
Epoch: 80 Train: 2759.68311 Test: 1110.55005
Epoch: 160 Train: 2656.72510 Test: 1058.28296
Epoch: 240 Train: 2389.61865 Test: 994.97058
Epoch: 320 Train: 1989.26807 Test: 801.04596
Epoch: 400 Train: 1002.41449 Test: 327.95007
Epoch 480: New minimal relative error: 67.02%, model saved.
Epoch: 480 Train: 453.12274 Test: 112.37516
Epoch: 560 Train: 253.53903 Test: 52.46984
Epoch 640: New minimal relative error: 32.51%, model saved.
Epoch: 640 Train: 166.60886 Test: 27.48166
Epoch 720: New minimal relative error: 28.95%, model saved.
Epoch: 720 Train: 125.09216 Test: 16.24610
Epoch 800: New minimal relative error: 10.45%, model saved.
Epoch: 800 Train: 92.21590 Test: 10.55182
Epoch: 880 Train: 76.15550 Test: 8.42390
Epoch: 960 Train: 65.49688 Test: 6.68674
Epoch: 1040 Train: 57.92526 Test: 6.23360
Epoch: 1120 Train: 50.33696 Test: 4.75723
Epoch: 1200 Train: 47.05178 Test: 4.50001
Epoch: 1280 Train: 42.22083 Test: 3.84721
Epoch 1360: New minimal relative error: 6.48%, model saved.
Epoch: 1360 Train: 39.41825 Test: 3.74132
Epoch: 1440 Train: 37.90268 Test: 5.38880
Epoch: 1520 Train: 34.08438 Test: 2.73002
Epoch: 1600 Train: 32.95386 Test: 3.37383
Epoch: 1680 Train: 30.54256 Test: 2.49577
Epoch 1760: New minimal relative error: 4.32%, model saved.
Epoch: 1760 Train: 29.42773 Test: 2.14747
Epoch: 1840 Train: 28.07715 Test: 1.99109
Epoch: 1920 Train: 27.06262 Test: 2.17951
Epoch: 2000 Train: 25.89355 Test: 1.86532
Epoch: 2080 Train: 50.01708 Test: 20.55042
Epoch: 2160 Train: 23.73334 Test: 1.82359
Epoch: 2240 Train: 22.92868 Test: 1.62295
Epoch: 2320 Train: 24.46487 Test: 2.17258
Epoch: 2400 Train: 21.72051 Test: 1.53948
Epoch 2480: New minimal relative error: 3.78%, model saved.
Epoch: 2480 Train: 21.29884 Test: 1.45349
Epoch: 2560 Train: 21.59486 Test: 1.75028
Epoch: 2640 Train: 20.27146 Test: 1.41398
Epoch: 2720 Train: 19.39335 Test: 1.29815
Epoch: 2800 Train: 18.81031 Test: 1.21672
Epoch: 2880 Train: 18.25522 Test: 1.22930
Epoch: 2960 Train: 17.78033 Test: 1.15123
Epoch: 3040 Train: 26.51971 Test: 6.41301
Epoch: 3120 Train: 17.11628 Test: 1.13777
Epoch: 3200 Train: 16.68670 Test: 1.09930
Epoch: 3280 Train: 18.60398 Test: 3.77835
Epoch: 3360 Train: 16.10792 Test: 1.05006
Epoch: 3440 Train: 15.73116 Test: 0.99216
Epoch: 3520 Train: 15.75943 Test: 1.38954
Epoch: 3600 Train: 15.19466 Test: 0.97825
Epoch: 3680 Train: 14.80177 Test: 0.92463
Epoch: 3760 Train: 14.76060 Test: 1.08839
Epoch: 3840 Train: 14.16545 Test: 0.88282
Epoch: 3920 Train: 13.99438 Test: 0.85481
Epoch: 4000 Train: 13.73139 Test: 0.83014
Epoch: 4080 Train: 13.62292 Test: 0.82762
Epoch 4160: New minimal relative error: 2.83%, model saved.
Epoch: 4160 Train: 13.37897 Test: 0.81192
Epoch: 4240 Train: 13.24385 Test: 0.78587
Epoch: 4320 Train: 13.14606 Test: 0.79567
Epoch: 4400 Train: 12.95603 Test: 0.74200
Epoch: 4480 Train: 13.00421 Test: 1.05526
Epoch: 4560 Train: 12.55870 Test: 0.73201
Epoch: 4640 Train: 12.51701 Test: 0.71279
Epoch: 4720 Train: 12.32142 Test: 0.72273
Epoch: 4800 Train: 12.26685 Test: 0.76203
Epoch: 4880 Train: 11.98097 Test: 0.73064
Epoch: 4960 Train: 11.85950 Test: 0.73239
Epoch: 5040 Train: 13.56244 Test: 1.96893
Epoch 5120: New minimal relative error: 2.26%, model saved.
Epoch: 5120 Train: 11.75428 Test: 0.73814
Epoch: 5200 Train: 11.77457 Test: 0.74417
Epoch: 5280 Train: 11.89707 Test: 0.80940
Epoch: 5360 Train: 11.72035 Test: 0.69703
Epoch: 5440 Train: 11.86037 Test: 0.69903
Epoch: 5520 Train: 12.22216 Test: 1.28002
Epoch: 5600 Train: 11.56692 Test: 0.70161
Epoch: 5680 Train: 11.55967 Test: 0.70428
Epoch: 5760 Train: 11.31033 Test: 0.70709
Epoch: 5840 Train: 11.21108 Test: 0.70660
Epoch: 5920 Train: 11.27960 Test: 0.71142
Epoch: 6000 Train: 11.30592 Test: 0.70682
Epoch: 6080 Train: 11.41644 Test: 0.72257
Epoch: 6160 Train: 11.91849 Test: 0.81065
Epoch: 6240 Train: 12.95219 Test: 0.94497
Epoch: 6320 Train: 12.47628 Test: 0.98256
Epoch: 6400 Train: 11.95661 Test: 0.85573
Epoch: 6480 Train: 11.47343 Test: 0.80459
Epoch: 6560 Train: 11.38146 Test: 0.88658
Epoch: 6640 Train: 11.20987 Test: 0.78572
Epoch: 6720 Train: 10.99084 Test: 0.76417
Epoch: 6800 Train: 10.99053 Test: 0.77576
Epoch: 6880 Train: 10.85639 Test: 0.76684
Epoch: 6960 Train: 10.81453 Test: 0.76791
Epoch: 7040 Train: 10.70834 Test: 0.74455
Epoch: 7120 Train: 10.70002 Test: 0.77937
Epoch: 7200 Train: 10.49843 Test: 0.72405
Epoch: 7280 Train: 11.26604 Test: 1.42803
Epoch 7360: New minimal relative error: 1.97%, model saved.
Epoch: 7360 Train: 10.41487 Test: 0.71670
Epoch: 7440 Train: 10.38336 Test: 0.70939
Epoch: 7520 Train: 10.51973 Test: 0.82385
Epoch: 7600 Train: 10.40179 Test: 0.68975
Epoch: 7680 Train: 10.30449 Test: 0.67006
Epoch: 7760 Train: 10.28689 Test: 0.65952
Epoch: 7840 Train: 10.09992 Test: 0.65671
Epoch: 7920 Train: 10.03905 Test: 0.67000
Epoch: 7999 Train: 9.99851 Test: 0.67435
Training Loss: tensor(9.9985)
Test Loss: tensor(0.6744)
Learned LE: [  0.80800337   0.04162087 -14.508897  ]
True LE: [ 8.6252856e-01 -3.5234098e-03 -1.4536466e+01]
Relative Error: [1.3960555  1.4787251  1.2721035  1.2953645  1.356965   1.1653707
 0.9834463  1.1941439  1.8431764  3.0203292  4.530082   5.5155444
 4.8367715  4.23189    3.0762517  2.0871062  1.4995394  1.4124519
 1.6742861  2.0370378  2.2992797  2.3897002  2.3417199  1.9590927
 1.4174192  0.8870696  0.599791   0.61583805 0.9303512  1.4211098
 2.1397212  2.8274915  3.3212214  3.0185916  2.060052   1.2387321
 0.78614587 0.56904036 0.9572259  1.6862365  2.0043895  2.0367856
 1.892657   1.733993   1.8154452  2.3744993  2.8896947  3.166396
 2.0548615  1.1842383  0.62433904 0.44930542 0.47205245 0.50110805
 0.49360487 0.44083205 0.33640516 0.45007378 0.742444   1.0090392
 1.1803705  1.1962653  1.2673923  1.3909856  1.2325292  1.3240874
 1.3885013  1.1505892  0.92328477 0.90994847 1.6002089  2.8386452
 4.237386   5.395937   4.56378    3.8769624  2.6141684  1.686131
 1.2268478  1.2989529  1.625527   1.9806061  2.3217404  2.4755926
 2.427897   2.2348096  1.6971074  1.4215442  1.1156813  0.9585552
 0.94232494 1.1457396  1.4927288  2.2882974  2.7336125  2.7321296
 2.2355697  1.4542214  1.101778   0.99634045 0.9228861  1.5724299
 1.8589814  1.880616   1.7087235  1.441689   1.397833   1.9171739
 2.8691285  3.326436   2.186307   1.2342956  0.56846607 0.45318547
 0.57096666 0.626623   0.6887716  0.67270654 0.5739358  0.45015594
 0.43414325 0.75570244 0.9832089  1.0062045  1.0625625  1.2477082
 1.1609373  1.306917   1.3699327  1.1489207  0.864802   0.63618195
 1.3457408  2.5886595  3.846887   5.035035   4.366837   3.5072737
 2.2686205  1.3956764  1.0675706  1.2622653  1.6024587  1.9115471
 2.175503   2.400602   2.429007   2.2724402  1.9772639  1.8213743
 1.5720603  1.3141711  1.1791916  1.1224537  1.1239607  1.6956196
 2.2865915  2.551      2.3872728  1.6698986  1.3434099  1.3332316
 1.2931916  1.3445941  1.6741425  1.7548796  1.6051899  1.2967893
 1.0768174  1.4042076  2.354641   3.4028716  2.355441   1.4133663
 0.6685882  0.41043568 0.6723863  0.79186076 0.82899845 0.9030663
 0.927611   0.823648   0.61519754 0.64390165 0.8911166  0.8649715
 0.8331688  1.0498691  1.0562096  1.2184796  1.2685585  1.1397705
 0.91363686 0.5054805  1.0857167  2.2672646  3.3942823  4.452129
 4.2415032  3.2632258  2.0307808  1.1980482  0.97751516 1.243256
 1.5760909  1.8325374  2.002386   2.1472912  2.24258    2.1697614
 2.1941776  2.0935407  1.8651528  1.5719358  1.408664   1.2728281
 1.0902559  1.3404256  1.836842   2.2163215  2.3424067  1.9047621
 1.4957451  1.5460542  1.5273118  1.1780128  1.4382652  1.6346056
 1.5698043  1.3139588  0.9090246  0.967263   1.7505059  2.884323
 2.5428338  1.6030709  0.87779117 0.4652486  0.64059365 0.7812305
 0.7688465  0.8119808  1.0050123  1.2768662  1.1014712  0.8908974
 1.04187    0.94985926 0.69161284 0.82557267 0.87165916 0.9832209
 1.0733787  1.0661446  0.8734147  0.45129746 0.89763206 1.9244801
 2.880955   3.8221128  4.1891403  3.1512585  1.8992158  1.0864792
 0.94973314 1.246064   1.5411928  1.7317792  1.8256369  1.8472078
 1.8616123  1.8732319  2.1444314  2.2097824  1.9923983  1.7388252
 1.5269343  1.3960718  1.2345755  1.2216394  1.5143678  1.8645612
 2.1769826  2.0269017  1.5806783  1.5936278  1.6209692  1.4073306
 1.1041222  1.4656808  1.5632921  1.4019562  0.9872627  0.75858295
 1.2136611  2.15646    2.618577   1.8991742  1.0860021  0.6768448
 0.45197862 0.59935033 0.6623624  0.6544244  0.7981794  1.155717
 1.2411073  0.8847798  0.6647345  0.705157   0.6866608  0.6822291
 0.61004734 0.6230406  0.7712351  0.90833974 0.77304274 0.39829278
 0.6614183  1.5524122  2.3959026  3.1789932  4.1318855  3.1697001
 1.8530965  1.015237   0.93956625 1.2780162  1.5472764  1.6718569
 1.6603911  1.5591385  1.4406707  1.4693129  1.8019725  2.00507
 1.984181   1.7216136  1.5114323  1.4234298  1.5135756  1.4683145
 1.2625135  1.3938813  1.8284814  1.9904256  1.6738186  1.4636114
 1.5494745  1.5003625  1.2439336  1.1667448  1.4601256  1.4638247
 1.1668408  0.85070235 0.83646894 1.4060886  2.519925   2.0525916
 1.4577515  0.7991557  0.41348514 0.47148815 0.5681864  0.54512763
 0.69954413 1.0399772  0.7884898  0.5005223  0.32031184 0.4165446
 0.6204013  0.6452401  0.5928832  0.33329695 0.3344238  0.6235688
 0.6064654  0.34538704 0.39313263 1.1728106  1.8551489  2.5511184
 3.3002377  3.3551939  1.9227244  0.97465503 0.880373   1.267301
 1.5709823  1.6891012  1.6189995  1.3981464  1.0839815  1.0093516
 1.2315001  1.4554862  1.6177164  1.5427681 ]
