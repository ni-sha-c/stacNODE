time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 101.46%, model saved.
Epoch: 0 Train: 173418.06250 Test: 3919.15869
Epoch: 80 Train: 44868.24219 Test: 1780.23193
Epoch: 160 Train: 40439.14062 Test: 1422.53125
Epoch 240: New minimal relative error: 99.89%, model saved.
Epoch: 240 Train: 41715.51562 Test: 1350.82568
Epoch 320: New minimal relative error: 97.61%, model saved.
Epoch: 320 Train: 40174.04688 Test: 1275.75684
Epoch 400: New minimal relative error: 70.11%, model saved.
Epoch: 400 Train: 41054.64453 Test: 1368.84387
Epoch: 480 Train: 48167.57812 Test: 1393.47803
Epoch 560: New minimal relative error: 65.04%, model saved.
Epoch: 560 Train: 35054.81250 Test: 1302.76343
Epoch: 640 Train: 40042.65234 Test: 1321.17932
Epoch: 720 Train: 44125.28125 Test: 1462.47949
Epoch: 800 Train: 34246.85156 Test: 1108.38232
Epoch: 880 Train: 46238.64453 Test: 1464.97424
Epoch 960: New minimal relative error: 62.50%, model saved.
Epoch: 960 Train: 42008.34766 Test: 1510.08289
Epoch: 1040 Train: 37734.64844 Test: 1327.91138
Epoch 1120: New minimal relative error: 61.54%, model saved.
Epoch: 1120 Train: 45548.94531 Test: 1418.58032
Epoch: 1200 Train: 38755.61328 Test: 1377.61694
Epoch: 1280 Train: 41259.35547 Test: 1395.07642
Epoch: 1360 Train: 40080.77344 Test: 1356.50989
Epoch: 1440 Train: 39684.76953 Test: 1330.71411
Epoch 1520: New minimal relative error: 61.18%, model saved.
Epoch: 1520 Train: 37557.25000 Test: 1237.25085
Epoch: 1600 Train: 40308.33203 Test: 1325.50964
Epoch: 1680 Train: 38437.86719 Test: 1304.68237
Epoch: 1760 Train: 37636.60156 Test: 1304.05481
Epoch: 1840 Train: 38689.25391 Test: 1335.63965
Epoch: 1920 Train: 34193.04297 Test: 1115.95801
Epoch: 2000 Train: 36145.26172 Test: 1109.53625
Epoch: 2080 Train: 36556.91406 Test: 1209.31213
Epoch: 2160 Train: 32813.19141 Test: 1094.47791
Epoch: 2240 Train: 33181.01172 Test: 940.99628
Epoch 2320: New minimal relative error: 54.56%, model saved.
Epoch: 2320 Train: 35492.32031 Test: 1081.26868
Epoch: 2400 Train: 32847.47266 Test: 987.33636
Epoch: 2480 Train: 31341.96094 Test: 869.49872
Epoch: 2560 Train: 32204.93359 Test: 892.61169
Epoch: 2640 Train: 32163.08594 Test: 943.93109
Epoch: 2720 Train: 29682.24805 Test: 854.10284
Epoch: 2800 Train: 28215.58789 Test: 765.32715
Epoch: 2880 Train: 28314.08398 Test: 730.62701
Epoch: 2960 Train: 25697.62500 Test: 598.49835
Epoch: 3040 Train: 23417.74414 Test: 538.25818
Epoch: 3120 Train: 22199.23633 Test: 496.21664
Epoch: 3200 Train: 21797.39258 Test: 473.85077
Epoch: 3280 Train: 20388.00977 Test: 393.16690
Epoch: 3360 Train: 19127.26953 Test: 360.17990
Epoch: 3440 Train: 16709.58594 Test: 272.41443
Epoch: 3520 Train: 14347.46094 Test: 200.34726
Epoch: 3600 Train: 11170.02930 Test: 138.82660
Epoch: 3680 Train: 6143.64453 Test: 93.50038
Epoch: 3760 Train: 4800.80420 Test: 75.59772
Epoch: 3840 Train: 3670.41797 Test: 38.61996
Epoch: 3920 Train: 3266.67773 Test: 66.50591
Epoch 4000: New minimal relative error: 42.34%, model saved.
Epoch: 4000 Train: 3424.50635 Test: 45.23309
Epoch 4080: New minimal relative error: 27.35%, model saved.
Epoch: 4080 Train: 2791.92334 Test: 19.59808
Epoch 4160: New minimal relative error: 21.45%, model saved.
Epoch: 4160 Train: 2362.00000 Test: 16.52670
Epoch 4240: New minimal relative error: 12.36%, model saved.
Epoch: 4240 Train: 2144.98853 Test: 12.58124
Epoch: 4320 Train: 1990.48608 Test: 10.09467
Epoch: 4400 Train: 2089.25049 Test: 12.43976
Epoch: 4480 Train: 1803.94263 Test: 11.30503
Epoch: 4560 Train: 1814.04419 Test: 8.67172
Epoch: 4640 Train: 1800.56152 Test: 10.03302
Epoch: 4720 Train: 1580.21912 Test: 8.01069
Epoch: 4800 Train: 1528.94800 Test: 8.64301
Epoch: 4880 Train: 1573.78699 Test: 10.72746
Epoch: 4960 Train: 1530.41370 Test: 10.05078
Epoch: 5040 Train: 1376.11328 Test: 6.01823
Epoch: 5120 Train: 1445.61426 Test: 6.97699
Epoch: 5200 Train: 1425.67517 Test: 13.50696
Epoch 5280: New minimal relative error: 9.90%, model saved.
Epoch: 5280 Train: 1319.98206 Test: 4.88093
Epoch: 5360 Train: 1311.95203 Test: 5.71437
Epoch: 5440 Train: 1263.92200 Test: 6.58374
Epoch: 5520 Train: 1266.39624 Test: 6.50433
Epoch: 5600 Train: 1210.69739 Test: 6.50387
Epoch: 5680 Train: 1183.30798 Test: 4.86682
Epoch: 5760 Train: 1209.30737 Test: 7.79552
Epoch: 5840 Train: 1177.37854 Test: 4.37309
Epoch: 5920 Train: 1152.40393 Test: 4.50456
Epoch: 6000 Train: 1115.13245 Test: 5.05171
Epoch: 6080 Train: 1102.12036 Test: 4.16143
Epoch: 6160 Train: 1078.63586 Test: 4.23951
Epoch: 6240 Train: 1048.63782 Test: 4.04801
Epoch: 6320 Train: 1045.95020 Test: 3.42619
Epoch: 6400 Train: 1084.41284 Test: 4.22590
Epoch: 6480 Train: 1074.28906 Test: 4.62906
Epoch: 6560 Train: 1066.92004 Test: 3.75564
Epoch: 6640 Train: 1035.92737 Test: 3.55346
Epoch: 6720 Train: 1063.07605 Test: 4.18262
Epoch: 6800 Train: 1077.33447 Test: 4.59119
Epoch: 6880 Train: 1030.73120 Test: 3.59755
Epoch: 6960 Train: 1042.47729 Test: 4.92212
Epoch: 7040 Train: 1031.01477 Test: 3.58098
Epoch 7120: New minimal relative error: 5.06%, model saved.
Epoch: 7120 Train: 978.35944 Test: 3.52942
Epoch: 7200 Train: 1004.07928 Test: 3.62396
Epoch: 7280 Train: 909.86121 Test: 3.01682
Epoch: 7360 Train: 895.41724 Test: 2.88814
Epoch: 7440 Train: 844.35156 Test: 2.83426
Epoch: 7520 Train: 863.76385 Test: 2.89821
Epoch: 7600 Train: 832.19220 Test: 2.66150
Epoch: 7680 Train: 829.73779 Test: 2.45348
Epoch: 7760 Train: 883.31262 Test: 2.95986
Epoch: 7840 Train: 813.93060 Test: 2.53079
Epoch: 7920 Train: 761.58820 Test: 2.51313
Epoch: 7999 Train: 796.87115 Test: 3.18394
Training Loss: tensor(796.8712)
Test Loss: tensor(3.1839)
Learned LE: [  0.97571963  -0.10028506 -14.496963  ]
True LE: [ 8.5112417e-01  4.3851924e-03 -1.4529404e+01]
Relative Error: [7.660836   6.9871016  6.3781223  5.848799   5.3565373  4.8547516
 4.3105664  3.6240385  3.0487387  2.4332485  1.7805759  1.5971512
 1.4236045  1.1630422  0.8559595  0.6814281  0.6684892  0.7621023
 1.1696312  1.8909411  2.7465305  3.6132069  4.4871473  4.832659
 4.8669686  5.016907   5.479957   6.136785   6.712843   7.0423865
 7.3912034  7.80788    8.189914   8.468134   8.59557    8.566661
 8.407424   8.419914   8.533648   8.617418   8.52659    7.729865
 6.4561462  5.316855   4.6245193  4.26241    4.1498127  4.2368445
 4.4678593  4.779469   5.1079607  5.1419506  5.346076   5.7523494
 6.3742914  7.2362967  8.277062   9.12883    9.136786   9.047871
 8.36625    7.6586976  6.9530435  6.3007092  5.7252235  5.249509
 4.8221936  4.401491   3.8741891  3.3193157  2.8897805  2.408613
 1.8436515  1.4981021  1.415553   1.0910152  0.7193041  0.5987288
 0.5686114  0.5680335  0.81677467 1.5061806  2.4083748  3.3634412
 4.065133   4.3414145  4.1235485  4.2872286  4.8513675  5.5515914
 6.0826406  6.4729915  6.886521   7.35769    7.793577   8.11453
 8.268426   8.171894   7.871723   7.81587    7.889159   7.7991204
 7.7905574  7.158987   5.752815   4.5893536  3.8898258  3.485846
 3.3333979  3.385552   3.7227657  4.2690268  4.7293825  4.86883
 5.075123   5.514391   6.1918387  7.0845113  8.190707   8.801303
 8.709136   8.32815    7.743426   7.0752196  6.3892803  5.7577167
 5.2070637  4.774294   4.420739   4.096465   3.6158879  3.2549016
 3.0229666  2.742945   2.2973065  1.7098297  1.3370597  1.1651101
 0.8517833  0.5083215  0.51865757 0.45702443 0.56610703 1.1060092
 2.011898   3.0022764  3.5323672  3.6605113  3.464714   3.6215105
 4.194395   4.998891   5.5223346  5.949984   6.4332776  6.9556365
 7.453388   7.828058   7.9948335  7.756      7.4466953  7.2779446
 7.135412   6.980742   6.838249   6.4317226  5.179447   4.00822
 3.223159   2.744454   2.544664   2.7767425  3.228299   3.7793648
 4.3598084  4.683879   4.850002   5.2612643  5.98835    6.9615073
 8.087099   8.372394   8.290864   7.743808   7.262002   6.646538
 5.987962   5.35968    4.8154254  4.4105935  4.12702    3.8764157
 3.4932957  3.3448071  3.3179555  3.202682   2.6454947  2.0924063
 1.5836947  1.3640304  1.1258653  0.7155917  0.46782666 0.4822162
 0.65033233 1.1089622  1.804381   2.6433635  3.0148315  2.9403849
 2.8581262  3.003796   3.5552173  4.461504   5.019845   5.4557095
 5.924267   6.32416    6.830545   7.306756   7.4767947  7.4350195
 7.127341   6.6561418  6.476931   6.189759   5.9431133  5.8252597
 4.784853   3.534618   2.629409   2.1277652  2.1045291  2.4257078
 2.833644   3.3395045  3.8585074  4.3850107  4.780994   5.059069
 5.7522306  6.803931   7.690034   8.013771   7.7538333  7.283506
 6.8660126  6.4229584  5.7540693  5.116833   4.558889   4.152886
 3.9155939  3.7131872  3.448942   3.4872174  3.6475282  3.3955865
 3.0025387  2.5596647  2.1565259  1.4851936  1.3647289  1.0101861
 0.5438725  0.5533842  0.7762605  1.2508823  1.9410207  2.5965266
 2.6683345  2.3069065  2.219314   2.4520793  2.9359114  3.8551266
 4.548274   4.82041    5.086936   5.5397     6.136423   6.663458
 6.7926817  6.8080254  6.603943   6.140717   5.9228983  5.4646425
 5.1516523  5.1164594  4.5652685  3.1742465  2.1951058  1.680496
 1.7350472  2.033457   2.4090183  2.8426454  3.3073328  3.8033805
 4.4977183  4.9623437  5.563895   6.556435   7.226866   7.7212753
 7.325917   6.9330463  6.604016   6.252493   5.747166   5.0642943
 4.4678335  4.017675   3.7735965  3.6023781  3.4267843  3.5949662
 3.734833   3.5574841  3.322343   3.0313957  2.5840707  2.0399697
 1.5593998  1.3012677  0.73869896 0.595557   0.86886567 1.3508637
 2.0344625  2.800746   2.6268196  2.0229228  1.6561078  1.8503722
 2.3836153  3.238886   4.1559153  4.248635   4.552034   5.0236073
 5.598286   5.9741316  6.1959558  6.239305   5.912605   5.6228757
 5.3924875  4.8786297  4.480215   4.346295   4.2016635  3.0207794
 1.9381125  1.3443831  1.4653358  1.7795007  2.0759983  2.396369
 2.786844   3.212902   3.8910325  4.8310905  5.4116473  6.190069
 6.738063   7.237482   7.03973    6.687673   6.4523354  6.1924243
 5.8701735  5.273292   4.5851765  4.0379205  3.7211933  3.5391514
 3.393711   3.5541437  3.606309   3.6025412  3.5564404  3.3650942
 2.923025   2.5267174  2.032948   1.4667286  0.99255794 0.5491639
 0.8153747  1.2271211  1.8872608  2.7684908  2.809633   2.0417788
 1.517108   1.320306   1.7447276  2.6615076 ]
