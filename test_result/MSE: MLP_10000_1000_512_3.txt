time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.11%, model saved.
Epoch: 0 Train: 4088.67993 Test: 3962.43213
Epoch 100: New minimal relative error: 49.27%, model saved.
Epoch: 100 Train: 40.74245 Test: 44.03544
Epoch 200: New minimal relative error: 29.86%, model saved.
Epoch: 200 Train: 7.96105 Test: 10.38191
Epoch: 300 Train: 4.32778 Test: 4.77926
Epoch 400: New minimal relative error: 17.73%, model saved.
Epoch: 400 Train: 2.79406 Test: 3.11158
Epoch: 500 Train: 1.98848 Test: 2.25323
Epoch 600: New minimal relative error: 15.11%, model saved.
Epoch: 600 Train: 3.55633 Test: 1.97628
Epoch 700: New minimal relative error: 12.87%, model saved.
Epoch: 700 Train: 1.65170 Test: 1.56272
Epoch: 800 Train: 1.14472 Test: 1.42135
Epoch: 900 Train: 0.92980 Test: 1.11696
Epoch: 1000 Train: 4.18464 Test: 5.12331
Epoch: 1100 Train: 1.37720 Test: 0.67818
Epoch: 1200 Train: 0.45871 Test: 0.64462
Epoch: 1300 Train: 0.79032 Test: 0.93766
Epoch: 1400 Train: 0.39991 Test: 0.47703
Epoch: 1500 Train: 6.31088 Test: 8.59860
Epoch: 1600 Train: 4.80177 Test: 5.16896
Epoch: 1700 Train: 0.42701 Test: 0.50804
Epoch: 1800 Train: 0.73151 Test: 0.81514
Epoch: 1900 Train: 1.34220 Test: 1.02905
Epoch: 2000 Train: 0.35330 Test: 0.42660
Epoch: 2100 Train: 2.05483 Test: 1.09581
Epoch: 2200 Train: 1.08725 Test: 1.16187
Epoch: 2300 Train: 0.62194 Test: 0.73881
Epoch: 2400 Train: 1.44654 Test: 1.45107
Epoch: 2500 Train: 0.19814 Test: 0.23341
Epoch: 2600 Train: 1.18544 Test: 1.18574
Epoch: 2700 Train: 0.61675 Test: 0.93716
Epoch: 2800 Train: 0.31683 Test: 0.32102
Epoch: 2900 Train: 0.15287 Test: 0.18562
Epoch: 3000 Train: 1.79781 Test: 2.12627
Epoch: 3100 Train: 0.36178 Test: 0.22433
Epoch: 3200 Train: 1.12383 Test: 0.93271
Epoch: 3300 Train: 0.20712 Test: 0.23224
Epoch: 3400 Train: 0.62230 Test: 0.72099
Epoch: 3500 Train: 0.16451 Test: 0.47739
Epoch 3600: New minimal relative error: 9.02%, model saved.
Epoch: 3600 Train: 0.09835 Test: 0.12698
Epoch: 3700 Train: 0.58984 Test: 0.66071
Epoch: 3800 Train: 0.12775 Test: 0.12840
Epoch: 3900 Train: 2.00294 Test: 1.93481
Epoch: 4000 Train: 1.76953 Test: 1.46025
Epoch: 4100 Train: 0.86897 Test: 0.56187
Epoch: 4200 Train: 0.26223 Test: 0.18547
Epoch: 4300 Train: 0.11128 Test: 0.18344
Epoch: 4400 Train: 0.35332 Test: 0.31296
Epoch: 4500 Train: 0.10558 Test: 0.12998
Epoch: 4600 Train: 0.80014 Test: 0.94817
Epoch: 4700 Train: 0.14157 Test: 0.22242
Epoch: 4800 Train: 0.08805 Test: 0.12198
Epoch: 4900 Train: 0.08378 Test: 0.10823
Epoch: 5000 Train: 0.07431 Test: 0.09895
Epoch: 5100 Train: 0.06552 Test: 0.08986
Epoch: 5200 Train: 0.08042 Test: 0.11051
Epoch: 5300 Train: 0.41756 Test: 0.48198
Epoch: 5400 Train: 0.12985 Test: 0.14878
Epoch: 5500 Train: 0.29920 Test: 0.37996
Epoch: 5600 Train: 0.15820 Test: 0.19745
Epoch: 5700 Train: 0.16092 Test: 0.16219
Epoch: 5800 Train: 0.05975 Test: 0.07991
Epoch: 5900 Train: 0.05939 Test: 0.08141
Epoch: 6000 Train: 0.62810 Test: 0.75609
Epoch: 6100 Train: 0.07419 Test: 0.09684
Epoch: 6200 Train: 0.05127 Test: 0.07533
Epoch: 6300 Train: 0.17921 Test: 0.15702
Epoch: 6400 Train: 0.07505 Test: 0.09592
Epoch: 6500 Train: 0.04513 Test: 0.06560
Epoch: 6600 Train: 0.04487 Test: 0.06513
Epoch: 6700 Train: 0.05968 Test: 0.06552
Epoch: 6800 Train: 0.05431 Test: 0.06599
Epoch: 6900 Train: 0.04353 Test: 0.06644
Epoch: 7000 Train: 0.06319 Test: 0.07330
Epoch: 7100 Train: 0.04894 Test: 0.06621
Epoch: 7200 Train: 0.03944 Test: 0.05913
Epoch: 7300 Train: 0.49246 Test: 0.51436
Epoch: 7400 Train: 0.32443 Test: 0.39552
Epoch: 7500 Train: 0.04760 Test: 0.07196
Epoch: 7600 Train: 0.09282 Test: 0.13219
Epoch: 7700 Train: 0.06134 Test: 0.07092
Epoch: 7800 Train: 0.28223 Test: 0.28072
Epoch: 7900 Train: 0.03484 Test: 0.05341
Epoch: 8000 Train: 0.34408 Test: 0.35095
Epoch: 8100 Train: 0.37897 Test: 0.42556
Epoch: 8200 Train: 0.03342 Test: 0.05209
Epoch: 8300 Train: 0.03333 Test: 0.05106
Epoch: 8400 Train: 0.10038 Test: 0.09122
Epoch: 8500 Train: 0.03596 Test: 0.05503
Epoch: 8600 Train: 0.06535 Test: 0.06416
Epoch: 8700 Train: 0.03657 Test: 0.05477
Epoch: 8800 Train: 0.03054 Test: 0.04843
Epoch: 8900 Train: 0.03157 Test: 0.04877
Epoch: 9000 Train: 0.02967 Test: 0.04706
Epoch: 9100 Train: 0.17189 Test: 0.07821
Epoch: 9200 Train: 0.02877 Test: 0.04634
Epoch: 9300 Train: 0.59387 Test: 0.60272
Epoch: 9400 Train: 0.02804 Test: 0.04541
Epoch: 9500 Train: 0.03084 Test: 0.04905
Epoch: 9600 Train: 0.02726 Test: 0.04456
Epoch: 9700 Train: 0.02720 Test: 0.04424
Epoch: 9800 Train: 0.02661 Test: 0.04375
Epoch: 9900 Train: 0.02683 Test: 0.04379
Epoch: 9999 Train: 0.11092 Test: 0.06007
Training Loss: tensor(0.1109)
Test Loss: tensor(0.0601)
Learned LE: [ 0.8744755  -0.02141963 -5.122442  ]
True LE: [ 8.5714769e-01  1.0620058e-02 -1.4541451e+01]
Relative Error: [ 9.454675   9.771373  10.153275  10.589632  11.054793  11.50974
 11.906396  12.194672  12.328057  12.262321  11.950398  11.353243
 10.485672   9.4514885  8.401241   7.4389925  6.6291094  6.0707455
  5.749723   5.522424   5.325613   5.1505966  4.9565625  4.7239156
  4.4915495  4.314864   4.2069902  4.1303234  4.0370345  3.919873
  3.82282    3.8005087  3.8726728  4.012932   4.1929636  4.4540243
  4.840799   5.2470617  5.5696273  5.8669133  6.13171    6.296803
  6.5387616  6.7879534  6.807168   6.640519   6.3849115  6.1458707
  6.0156565  6.0498366  6.2616014  6.600745   6.950867   7.202702
  7.347263   7.4609356  7.599914   7.75267    7.8920474  8.02292
  8.170213   8.357593   8.604945   8.927063   9.328312   9.7979555
 10.308739  10.817999  11.271192  11.610638  11.783792  11.743999
 11.442282  10.835667   9.940005   8.877348   7.8205547  6.8685517
  6.0692434  5.535184   5.241354   5.033608   4.8613067  4.6982093
  4.4898195  4.218847   3.9397168  3.728772   3.6054604  3.520098
  3.4104233  3.2678823  3.1446164  3.1027508  3.1700675  3.324271
  3.500449   3.7037354  4.047586   4.484576   4.848169   5.1657414
  5.462476   5.636891   5.8547025  6.108539   6.1223063  5.9323387
  5.64451    5.376819   5.223517   5.240377   5.449221   5.8071966
  6.1878963  6.461292   6.611359   6.7231474  6.857999   6.9998927
  7.119943   7.227854   7.3528967  7.522621   7.761135   8.086332
  8.504249   9.005065   9.561719  10.12903   10.646754  11.049048
 11.276417  11.278627  11.004768  10.405601   9.493373   8.406614
  7.3423715  6.3882246  5.5728817  5.043856   4.7852597  4.621281
  4.478586   4.311137   4.069648   3.7467365  3.425445   3.2129855
  3.1158686  3.0574846  2.9624698  2.8282497  2.715658   2.673454
  2.7013006  2.7904565  2.9153826  3.04985    3.314241   3.7557755
  4.165646   4.4971557  4.816154   5.009294   5.192561   5.4606414
  5.4940734  5.301273   4.991749   4.6954064  4.512568   4.50368
  4.700706   5.071598   5.4838734  5.7836385  5.940357   6.0444465
  6.165062   6.285682   6.3771234  6.4538665  6.5509186  6.699861
  6.9268956  7.2504835  7.677866   8.202006   8.798114   9.420304
 10.004532  10.478489  10.775261  10.839491  10.617195  10.050713
  9.139783   8.034663   6.958609   5.988248   5.1292863  4.588088
  4.382182   4.279773   4.1560473  3.9626057  3.6734977  3.3017137
  2.9781055  2.8126495  2.7625422  2.7307994  2.6416605  2.5139809
  2.428976   2.4279888  2.4723115  2.5029974  2.516194   2.5386415
  2.683625   3.0696497  3.5213068  3.865929   4.1885657  4.407247
  4.5430083  4.821464   4.906659   4.7443247  4.432123   4.107725
  3.8855314  3.8409882  4.0168858  4.3914905  4.833528   5.163934
  5.329292   5.421001   5.5186615  5.609233   5.663736   5.7014165
  5.765306   5.8917065  6.1052465  6.4210234  6.846442   7.3792205
  7.9991007  8.661849   9.303438   9.850269  10.230471  10.381353
 10.2437     9.748553   8.871087   7.758159   6.662506   5.6610966
  4.731459   4.1550846  4.0135403  3.9775298  3.8603134  3.6279316
  3.2880702  2.9011526  2.6392522  2.5472696  2.5341175  2.5080712
  2.4026499  2.2590344  2.1798694  2.2053604  2.2956996  2.36326
  2.3359466  2.2393887  2.223259   2.4594123  2.905514   3.2704074
  3.5737712  3.8137143  3.9028664  4.1582575  4.3239245  4.235483
  3.9572506  3.6159782  3.3458512  3.2528183  3.3963456  3.7609935
  4.222462   4.583989   4.762024   4.8411283  4.9105587  4.965549
  4.977115   4.968094   4.9928427  5.0946684  5.2939186  5.596966
  6.0080156  6.528971   7.145956   7.820078   8.493902   9.101301
  9.571086   9.832774   9.819111   9.450415   8.665274   7.576291
  6.459583   5.4135823  4.3874426  3.7298706  3.641123   3.6701055
  3.560787   3.2919674  2.9130752  2.5705812  2.4243898  2.4075036
  2.4117923  2.3802526  2.2620964  2.0990303  2.0034242  2.017139
  2.0942051  2.1971393  2.2312577  2.1188307  1.9931101  2.014784
  2.3186824  2.7007368  2.972955   3.2106276  3.2879875  3.4525168
  3.7035272  3.724834   3.530152   3.2063353  2.8936048  2.7388067
  2.8358011  3.1710672  3.628393   4.016143   4.2143183  4.2856865
  4.326311   4.3449473  4.3131423  4.2519774  4.2291107  4.3005137
  4.482936   4.770902   5.159686   5.648878   6.2282267  6.8691435
  7.5312243  8.166417   8.713265   9.096471   9.238319   9.054177
  8.44971    7.4628644  6.360989   5.278217   4.142212   3.3183787
  3.2253158  3.315482   3.231851   2.9495397  2.5610993  2.309999
  2.2840998  2.3209045  2.3267663  2.2927752]
