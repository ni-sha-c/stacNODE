time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.02%, model saved.
Epoch: 0 Train: 32467.77344 Test: 4264.41064
Epoch 100: New minimal relative error: 78.53%, model saved.
Epoch: 100 Train: 8757.62402 Test: 1551.83386
Epoch: 200 Train: 7971.19580 Test: 1331.28772
Epoch 300: New minimal relative error: 65.33%, model saved.
Epoch: 300 Train: 6963.94727 Test: 1117.53589
Epoch: 400 Train: 6248.56543 Test: 754.81274
Epoch: 500 Train: 5020.01221 Test: 501.80121
Epoch: 600 Train: 3944.52734 Test: 392.51630
Epoch: 700 Train: 3105.30591 Test: 217.33177
Epoch 800: New minimal relative error: 23.69%, model saved.
Epoch: 800 Train: 1388.56824 Test: 77.61265
Epoch 900: New minimal relative error: 15.93%, model saved.
Epoch: 900 Train: 674.13739 Test: 20.57377
Epoch: 1000 Train: 465.00635 Test: 11.33793
Epoch 1100: New minimal relative error: 11.82%, model saved.
Epoch: 1100 Train: 325.03568 Test: 6.79647
Epoch: 1200 Train: 280.19336 Test: 3.85714
Epoch: 1300 Train: 246.66386 Test: 4.07687
Epoch: 1400 Train: 236.99936 Test: 8.25556
Epoch: 1500 Train: 214.72903 Test: 2.25085
Epoch: 1600 Train: 203.31708 Test: 3.59239
Epoch: 1700 Train: 212.89171 Test: 11.91406
Epoch: 1800 Train: 181.20358 Test: 1.65271
Epoch: 1900 Train: 176.93723 Test: 2.09083
Epoch: 2000 Train: 163.99606 Test: 1.61895
Epoch 2100: New minimal relative error: 4.23%, model saved.
Epoch: 2100 Train: 162.88445 Test: 1.72286
Epoch: 2200 Train: 170.98904 Test: 11.10326
Epoch: 2300 Train: 150.28516 Test: 1.22921
Epoch: 2400 Train: 146.27385 Test: 1.53591
Epoch: 2500 Train: 144.64890 Test: 1.46590
Epoch: 2600 Train: 149.53310 Test: 1.93930
Epoch: 2700 Train: 143.67525 Test: 1.09633
Epoch: 2800 Train: 137.95120 Test: 1.28745
Epoch: 2900 Train: 130.20551 Test: 2.49027
Epoch 3000: New minimal relative error: 4.04%, model saved.
Epoch: 3000 Train: 124.45667 Test: 0.86095
Epoch: 3100 Train: 124.27469 Test: 0.78958
Epoch: 3200 Train: 118.44098 Test: 1.07894
Epoch: 3300 Train: 114.43835 Test: 0.70353
Epoch: 3400 Train: 110.72945 Test: 0.69304
Epoch: 3500 Train: 104.53505 Test: 0.72834
Epoch: 3600 Train: 103.65224 Test: 0.59145
Epoch: 3700 Train: 101.94042 Test: 0.54575
Epoch: 3800 Train: 98.28024 Test: 0.57432
Epoch: 3900 Train: 96.39718 Test: 0.51637
Epoch: 4000 Train: 93.49211 Test: 0.52588
Epoch: 4100 Train: 92.01927 Test: 0.52072
Epoch: 4200 Train: 87.70020 Test: 0.58082
Epoch: 4300 Train: 92.94193 Test: 0.68633
Epoch: 4400 Train: 85.53811 Test: 0.46029
Epoch: 4500 Train: 85.95586 Test: 0.59837
Epoch: 4600 Train: 85.03584 Test: 0.48863
Epoch: 4700 Train: 81.15988 Test: 0.69026
Epoch: 4800 Train: 86.19053 Test: 1.90558
Epoch: 4900 Train: 78.07479 Test: 0.47175
Epoch: 5000 Train: 75.71994 Test: 0.34389
Epoch: 5100 Train: 72.53071 Test: 0.30759
Epoch 5200: New minimal relative error: 2.33%, model saved.
Epoch: 5200 Train: 72.04683 Test: 0.33268
Epoch: 5300 Train: 75.12844 Test: 1.32146
Epoch 5400: New minimal relative error: 2.31%, model saved.
Epoch: 5400 Train: 70.55093 Test: 0.34641
Epoch: 5500 Train: 66.93645 Test: 0.28769
Epoch: 5600 Train: 65.04589 Test: 0.69676
Epoch: 5700 Train: 64.99985 Test: 0.29209
Epoch: 5800 Train: 63.17381 Test: 0.27576
Epoch: 5900 Train: 62.38954 Test: 0.25388
Epoch: 6000 Train: 62.54086 Test: 0.49995
Epoch: 6100 Train: 60.98532 Test: 0.53869
Epoch: 6200 Train: 59.13142 Test: 0.25872
Epoch 6300: New minimal relative error: 1.82%, model saved.
Epoch: 6300 Train: 55.56793 Test: 0.22355
Epoch: 6400 Train: 55.81042 Test: 0.21311
Epoch: 6500 Train: 56.68031 Test: 0.23486
Epoch: 6600 Train: 56.76814 Test: 0.40078
Epoch: 6700 Train: 56.45157 Test: 0.24828
Epoch: 6800 Train: 59.53759 Test: 1.92623
Epoch: 6900 Train: 57.24855 Test: 0.22049
Epoch: 7000 Train: 56.56669 Test: 0.38426
Epoch: 7100 Train: 56.79691 Test: 0.28949
Epoch: 7200 Train: 55.00360 Test: 0.33555
Epoch: 7300 Train: 52.02927 Test: 0.19476
Epoch: 7400 Train: 52.18394 Test: 0.49853
Epoch: 7500 Train: 50.42823 Test: 0.26346
Epoch: 7600 Train: 51.12403 Test: 0.40284
Epoch: 7700 Train: 49.01825 Test: 0.19629
Epoch: 7800 Train: 51.10571 Test: 0.24339
Epoch: 7900 Train: 51.02604 Test: 0.25182
Epoch: 8000 Train: 52.52828 Test: 0.22445
Epoch: 8100 Train: 53.33709 Test: 0.23143
Epoch: 8200 Train: 53.67974 Test: 0.23441
Epoch: 8300 Train: 50.98803 Test: 0.39583
Epoch: 8400 Train: 50.16562 Test: 0.24388
Epoch: 8500 Train: 51.43572 Test: 0.23362
Epoch: 8600 Train: 50.70476 Test: 0.22600
Epoch: 8700 Train: 48.90415 Test: 0.21169
Epoch: 8800 Train: 47.13239 Test: 0.20439
Epoch: 8900 Train: 45.96322 Test: 0.20010
Epoch 9000: New minimal relative error: 1.80%, model saved.
Epoch: 9000 Train: 46.06288 Test: 0.18341
Epoch 9100: New minimal relative error: 1.67%, model saved.
Epoch: 9100 Train: 45.60473 Test: 0.19605
Epoch: 9200 Train: 43.35854 Test: 0.16016
Epoch: 9300 Train: 41.32609 Test: 0.15466
Epoch: 9400 Train: 42.39010 Test: 0.15753
Epoch: 9500 Train: 45.18678 Test: 0.31143
Epoch: 9600 Train: 44.26504 Test: 0.25193
Epoch: 9700 Train: 46.58289 Test: 0.24725
Epoch: 9800 Train: 47.16815 Test: 0.27089
Epoch 9900: New minimal relative error: 1.42%, model saved.
Epoch: 9900 Train: 46.59954 Test: 0.28420
Epoch: 9999 Train: 43.12871 Test: 0.19520
Training Loss: tensor(43.1287)
Test Loss: tensor(0.1952)
Learned LE: [ 8.2106394e-01  6.5184422e-03 -1.4493712e+01]
True LE: [ 8.4026521e-01  5.7190098e-03 -1.4515159e+01]
Relative Error: [1.7461915  1.752521   1.814881   1.7122918  1.4892069  1.3091601
 1.1984934  1.3014895  1.437682   1.7478714  1.9036382  1.68767
 1.2380505  0.7299461  0.35051703 0.63992625 0.8377928  0.9050485
 0.8107394  0.662653   0.8726054  0.88391584 0.5585818  0.32611662
 0.32562968 0.36388507 0.4708368  0.61831945 0.8384426  0.9425599
 0.9970623  0.8844583  0.99863726 0.96302915 1.2123253  1.5965117
 1.8465564  1.8667284  1.661592   1.2466207  0.80921555 0.8676939
 1.175213   1.4605     1.2876147  0.75501525 0.53446376 0.40411064
 0.27895153 0.17481251 0.24302807 0.36862865 0.7347522  1.0135324
 1.2308186  1.4229261  1.5656421  1.7198522  1.7669752  1.7161832
 1.6052363  1.4584475  1.4965196  1.4333153  1.398481   1.4352465
 1.3788884  1.193388   1.0865979  0.9831749  1.1405747  1.4243039
 1.6966281  1.7321646  1.41683    0.8058968  0.359412   0.43009654
 0.6843123  0.8523267  0.84962153 0.67622906 0.8206876  0.8793483
 0.74056906 0.543838   0.5663569  0.57410717 0.6249555  0.69957405
 0.8414532  0.9201724  0.9489657  0.8963158  0.78894866 0.81547415
 0.76487225 1.2137759  1.5999945  1.7706765  1.7083344  1.4702568
 1.0561776  0.65387493 1.0142031  1.3038589  1.3785102  0.8489875
 0.654502   0.57209337 0.3361217  0.20713425 0.16013722 0.5114698
 0.8037115  0.9951042  1.134592   1.2725816  1.4571482  1.6379887
 1.6922405  1.7034223  1.641253   1.5550219  1.2518923  0.9821554
 0.90592885 0.9941025  1.0817721  1.0806992  0.9126655  0.8861513
 0.82004964 1.0573306  1.3273784  1.5619326  1.4961219  1.1050823
 0.41523352 0.25073603 0.49336758 0.7697093  0.8838039  0.8023831
 0.8431544  0.941805   0.88116974 0.78791827 0.90716845 0.9325701
 0.88962775 0.91334265 0.80882    0.83020943 0.7375861  0.71405566
 0.72487104 0.76949906 0.5683521  0.6702871  1.1876172  1.5284752
 1.6303308  1.5801897  1.3262821  0.75503874 0.6828715  1.1546617
 1.2928361  0.88070196 0.6722452  0.69178975 0.5716409  0.50909895
 0.4397499  0.636021   0.9157197  1.03234    1.0098364  0.99716157
 1.144109   1.3659176  1.501965   1.5567169  1.5700722  1.5797689
 1.3449736  0.96893    0.72968316 0.67352587 0.733841   0.833964
 0.8246594  0.66336495 0.7153744  0.7307898  0.88191164 1.1944234
 1.3757627  1.2463869  0.81612086 0.23497814 0.35794148 0.69146043
 0.9879005  1.007354   0.8584582  1.0083307  0.96996343 0.86953574
 1.0464834  1.2166054  1.3086462  1.1332855  0.93046856 0.76544154
 0.6857346  0.5045793  0.5018041  0.6193823  0.7200983  0.47000322
 0.603952   1.0763686  1.3926319  1.5194794  1.4725044  1.095211
 0.54197705 1.0095922  1.1552827  0.91373974 0.6331924  0.71571255
 0.77276456 0.7561276  0.7300394  0.75609994 0.93277127 0.9832392
 0.9171973  0.80181134 0.7412126  0.95334435 1.181971   1.3386724
 1.4409951  1.5657481  1.5126785  1.272146   1.1652256  1.1216826
 1.0007946  0.79482925 0.7609626  0.6937048  0.5244399  0.5464061
 0.5666197  0.6797794  0.98463196 1.1849382  1.0838238  0.80385095
 0.36292994 0.67143184 0.96944886 1.2111092  1.1297207  0.953672
 1.0282357  0.96394306 1.0201132  1.2561538  1.4552838  1.3618892
 1.1624044  0.9336167  0.72377354 0.57484555 0.4069031  0.48469463
 0.61959934 0.6428598  0.54654366 0.3693545  0.9434743  1.2544272
 1.4240441  1.3247924  0.88751304 0.5696428  1.0675955  0.7732804
 0.58439666 0.6197677  0.8318648  0.8887859  1.0036253  0.8248968
 1.0743222  1.1231056  1.0035596  0.7633631  0.5741915  0.74508727
 0.861611   1.1327933  1.3070362  1.4162185  1.4741862  1.4900702
 1.3734113  1.4772085  1.654798   1.7203556  1.3537431  1.0543165
 0.8001102  0.487951   0.31434715 0.4369966  0.4771086  0.71477574
 1.0150205  1.0635109  1.0699451  0.6058452  0.7739675  1.0809789
 1.2164948  1.0436201  0.96053016 1.0909544  1.0797673  1.075649
 1.3996893  1.4819502  1.3261027  1.1349704  0.9972243  0.85422117
 0.7445286  0.5340265  0.47036105 0.56790483 0.47526067 0.57029384
 0.32252568 0.70944417 1.1110764  1.3013046  1.1832818  0.7404871
 0.64667183 0.9062063  0.5619572  0.44281837 0.632585   0.84559387
 1.0592976  1.1606095  1.1805849  1.2637691  1.1710112  0.9259653
 0.60651124 0.67139596 0.86994755 0.8809149  0.98323756 1.0556015
 1.1132902  1.260816   1.3243759  1.2509023  1.2815187  1.4418107
 1.7209629  2.09513    1.7194927  1.304367   0.689262   0.11948317
 0.27397835 0.32452813 0.4348374  0.7658079  1.0870557  1.1157502
 0.73663336 0.71964604 1.1260793  1.1631669  1.0231973  0.86741215
 1.0657887  1.1269169  1.1154326  1.3329737 ]
