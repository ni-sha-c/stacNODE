time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.79%, model saved.
Epoch: 0 Train: 170147.17188 Test: 3907.55493
Epoch: 80 Train: 32910.16797 Test: 1577.80408
Epoch: 160 Train: 32059.24023 Test: 1203.37634
Epoch: 240 Train: 33849.55469 Test: 2181.61011
Epoch: 320 Train: 34940.21484 Test: 1103.63159
Epoch: 400 Train: 33838.21484 Test: 1107.61340
Epoch: 480 Train: 32656.80078 Test: 1275.85938
Epoch 560: New minimal relative error: 56.14%, model saved.
Epoch: 560 Train: 32286.57227 Test: 1276.53638
Epoch: 640 Train: 31382.39258 Test: 1148.98181
Epoch: 720 Train: 35668.16016 Test: 1303.59155
Epoch: 800 Train: 32545.45508 Test: 1386.56531
Epoch: 880 Train: 33906.23828 Test: 1248.37427
Epoch: 960 Train: 35308.06250 Test: 1382.38806
Epoch: 1040 Train: 35622.54297 Test: 1339.57190
Epoch: 1120 Train: 34864.95312 Test: 1102.28760
Epoch: 1200 Train: 35098.01953 Test: 1412.43652
Epoch: 1280 Train: 35261.06641 Test: 1289.14783
Epoch: 1360 Train: 36873.16797 Test: 1512.34558
Epoch 1440: New minimal relative error: 54.35%, model saved.
Epoch: 1440 Train: 32446.91406 Test: 1264.01587
Epoch 1520: New minimal relative error: 54.35%, model saved.
Epoch: 1520 Train: 34305.27734 Test: 1231.10522
Epoch: 1600 Train: 36959.12500 Test: 1385.74609
Epoch: 1680 Train: 34752.14844 Test: 1312.93506
Epoch: 1760 Train: 33092.25391 Test: 1222.17908
Epoch: 1840 Train: 36771.25000 Test: 1318.48096
Epoch: 1920 Train: 32416.76562 Test: 1227.96252
Epoch: 2000 Train: 30648.82227 Test: 1037.36890
Epoch: 2080 Train: 32027.46484 Test: 1038.61206
Epoch: 2160 Train: 30944.15625 Test: 1072.03955
Epoch: 2240 Train: 28169.17578 Test: 981.01086
Epoch: 2320 Train: 27215.72070 Test: 862.62817
Epoch: 2400 Train: 28592.44141 Test: 790.81329
Epoch: 2480 Train: 24299.36133 Test: 763.12213
Epoch: 2560 Train: 21437.85742 Test: 588.12250
Epoch: 2640 Train: 19832.93164 Test: 486.86597
Epoch: 2720 Train: 17816.96680 Test: 393.54938
Epoch: 2800 Train: 15551.93359 Test: 281.51132
Epoch: 2880 Train: 14127.80859 Test: 234.69460
Epoch: 2960 Train: 11738.56445 Test: 196.96608
Epoch 3040: New minimal relative error: 53.73%, model saved.
Epoch: 3040 Train: 9507.79004 Test: 114.54108
Epoch: 3120 Train: 8490.97656 Test: 114.09650
Epoch 3200: New minimal relative error: 20.39%, model saved.
Epoch: 3200 Train: 7888.47656 Test: 124.21449
Epoch 3280: New minimal relative error: 19.44%, model saved.
Epoch: 3280 Train: 7546.47168 Test: 131.64383
Epoch: 3360 Train: 7001.39990 Test: 95.41990
Epoch: 3440 Train: 6816.09912 Test: 91.83701
Epoch: 3520 Train: 6976.39453 Test: 115.62579
Epoch: 3600 Train: 6422.45312 Test: 85.55715
Epoch: 3680 Train: 5918.23438 Test: 76.81870
Epoch: 3760 Train: 5770.82812 Test: 73.33764
Epoch: 3840 Train: 5534.25488 Test: 64.19086
Epoch: 3920 Train: 5476.94678 Test: 50.39023
Epoch: 4000 Train: 5770.08496 Test: 64.22722
Epoch: 4080 Train: 4962.35107 Test: 51.16672
Epoch: 4160 Train: 5596.10107 Test: 74.33992
Epoch: 4240 Train: 4707.78174 Test: 56.20589
Epoch: 4320 Train: 4621.95166 Test: 47.61927
Epoch: 4400 Train: 4495.98730 Test: 44.53350
Epoch: 4480 Train: 5143.49756 Test: 57.70000
Epoch 4560: New minimal relative error: 13.22%, model saved.
Epoch: 4560 Train: 4873.74512 Test: 44.06482
Epoch: 4640 Train: 4285.71436 Test: 55.27148
Epoch: 4720 Train: 5106.85400 Test: 55.83481
Epoch: 4800 Train: 4672.65723 Test: 43.75243
Epoch: 4880 Train: 4203.29736 Test: 75.32088
Epoch: 4960 Train: 4814.31543 Test: 55.77910
Epoch: 5040 Train: 5043.16895 Test: 66.63467
Epoch: 5120 Train: 4268.75342 Test: 30.85361
Epoch: 5200 Train: 4424.02002 Test: 41.21000
Epoch: 5280 Train: 4835.49121 Test: 53.85216
Epoch: 5360 Train: 4240.27051 Test: 47.66166
Epoch: 5440 Train: 3927.95825 Test: 39.50992
Epoch: 5520 Train: 3787.25562 Test: 35.34924
Epoch: 5600 Train: 3542.24805 Test: 26.55347
Epoch: 5680 Train: 4201.74023 Test: 81.59747
Epoch: 5760 Train: 3552.55054 Test: 26.78140
Epoch: 5840 Train: 3695.70996 Test: 29.85271
Epoch: 5920 Train: 3507.87061 Test: 45.31683
Epoch: 6000 Train: 3430.88501 Test: 28.52000
Epoch: 6080 Train: 3375.31592 Test: 25.84430
Epoch: 6160 Train: 3198.08398 Test: 22.87539
Epoch: 6240 Train: 2839.15039 Test: 25.62204
Epoch: 6320 Train: 2737.81982 Test: 23.09612
Epoch: 6400 Train: 2567.10498 Test: 30.98166
Epoch: 6480 Train: 2739.59741 Test: 51.38549
Epoch 6560: New minimal relative error: 11.18%, model saved.
Epoch: 6560 Train: 2398.91846 Test: 14.60863
Epoch: 6640 Train: 2649.29810 Test: 31.49134
Epoch: 6720 Train: 2420.53198 Test: 24.07308
Epoch: 6800 Train: 2255.23096 Test: 14.26471
Epoch: 6880 Train: 2207.04565 Test: 14.02112
Epoch: 6960 Train: 2138.16406 Test: 15.84580
Epoch: 7040 Train: 2006.21069 Test: 13.26017
Epoch: 7120 Train: 1982.43005 Test: 11.04084
Epoch: 7200 Train: 2610.49048 Test: 20.78729
Epoch: 7280 Train: 2124.50415 Test: 25.04793
Epoch: 7360 Train: 2111.63770 Test: 14.38694
Epoch: 7440 Train: 1923.98621 Test: 10.99188
Epoch: 7520 Train: 2523.18384 Test: 37.15678
Epoch: 7600 Train: 2009.28259 Test: 15.13044
Epoch: 7680 Train: 1967.70142 Test: 11.64768
Epoch: 7760 Train: 2002.29810 Test: 16.65784
Epoch: 7840 Train: 1735.22595 Test: 12.11121
Epoch: 7920 Train: 1732.02124 Test: 14.82272
Epoch: 7999 Train: 1922.25842 Test: 13.16520
Training Loss: tensor(1922.2584)
Test Loss: tensor(13.1652)
Learned LE: [  1.0730274   -0.33935195 -14.472143  ]
True LE: [ 8.5843372e-01  2.2930421e-03 -1.4534534e+01]
Relative Error: [21.614592  21.614065  21.451117  21.002666  20.388937  19.922735
 19.623016  19.49461   19.29846   19.190117  19.236837  19.302204
 19.167217  18.93533   18.57147   17.98037   17.16866   16.12488
 14.973791  13.670678  12.156229  10.497715   8.594794   6.9284415
  5.541288   4.5687327  4.2064743  4.4922104  5.1466136  6.231412
  7.6289186  9.184944  10.314606  10.992571  11.164237  11.191448
 11.07168   10.826961  10.50338   10.330042  10.2030735 10.162034
 10.199154  10.300296  10.402565  10.578767  10.835687  11.16259
 11.550553  12.024607  12.501137  12.948743  13.563318  14.42271
 15.347374  16.237865  17.276497  18.527046  19.57797   20.253923
 20.819883  21.342428  21.610218  21.517048  21.23935   20.704605
 20.006638  19.44786   19.192423  18.976929  18.802036  18.843575
 19.048553  19.09541   19.142666  18.983185  18.591013  17.963415
 17.10705   16.027634  14.721568  13.229518  11.734379  10.011515
  8.034945   6.08248    4.6347356  3.8348646  3.8925192  4.683887
  5.6157837  6.913515   8.472167  10.074544  11.085314  11.663091
 11.690131  11.6943245 11.52442   11.145699  10.906706  10.711368
 10.606246  10.589661  10.598092  10.61184   10.716065  10.924485
 11.236543  11.628663  12.087357  12.59651   13.180983  13.732929
 14.2767935 15.032289  16.00809   16.870148  17.846746  19.056067
 20.186848  20.827759  21.294271  21.6936    21.72875   21.558811
 21.166363  20.530663  19.752277  19.108858  18.80357   18.520645
 18.485102  18.687885  18.896957  19.12754   19.169876  18.964184
 18.546919  17.911884  17.057198  15.968015  14.619431  13.060824
 11.31566    9.608226   7.6529374  5.3699107  3.777753   3.19264
  3.771647   5.045212   6.202687   7.6240234  9.298404  10.895063
 11.817342  12.259947  12.23698   12.178278  11.893301  11.571785
 11.297897  11.104026  11.014954  10.912522  10.822627  10.83522
 10.997868  11.266584  11.632753  12.081551  12.597847  13.181092
 13.807779  14.480016  15.094076  15.741245  16.696724  17.61371
 18.471453  19.64896   20.841883  21.497494  21.887945  22.145704
 22.05358   21.758495  21.261518  20.518627  19.6381    18.905146
 18.482704  18.181404  18.234327  18.516716  18.896235  19.10915
 19.122707  18.931963  18.505215  17.843952  16.960484  15.862082
 14.558098  12.98715   11.186181   9.233837   7.3384113  5.0489283
  3.0009425  2.5888498  3.7269871  5.4736767  6.821868   8.323356
 10.083436  11.663634  12.5338125 12.870905  12.788001  12.686261
 12.348146  12.002067  11.696514  11.512786  11.314337  11.096228
 11.043178  11.12648   11.360126  11.698786  12.112429  12.612645
 13.181128  13.7982025 14.45956   15.183314  15.924762  16.573004
 17.44133   18.410807  19.242126  20.338184  21.483217  22.288292
 22.617132  22.76433   22.596914  22.149559  21.55574   20.706629
 19.711536  18.83765   18.276386  18.017452  18.103771  18.49924
 18.860207  19.08748   19.075512  18.912945  18.540287  17.91612
 17.004656  15.819487  14.474294  12.938571  11.171619   9.171251
  7.171088   5.0292253  2.6162152  2.0463016  3.7168286  5.8255997
  7.421296   9.000501  10.814569  12.404218  13.246222  13.489099
 13.382024  13.246984  12.8601465 12.45479   12.143967  11.8916855
 11.531167  11.346456  11.336621  11.48078   11.732593  12.095346
 12.560989  13.122684  13.762718  14.459599  15.174192  15.912559
 16.714312  17.50219   18.273314  19.23821   20.148087  21.065275
 22.198944  23.211708  23.509525  23.595984  23.35871   22.836716
 22.088394  21.146635  20.020529  18.996616  18.234165  17.889605
 18.038113  18.457027  18.75157   18.917006  19.033836  18.934376
 18.648703  18.084131  17.237278  16.13902   14.690772  12.943204
 11.400003   9.6517515  7.564706   5.3991685  3.1037557  1.9791186
  3.7055726  6.0678287  7.949411   9.6222725 11.465003  13.106209
 13.951876  14.151612  14.003071  13.881815  13.421443  12.968302
 12.627737  12.18802   11.82836   11.683324  11.718857  11.857075
 12.133984  12.537442  13.050125  13.652947  14.321809  15.086271
 15.906278  16.72681   17.506786  18.389328  19.204288  20.137383
 21.098415  21.963848  23.034256  24.070707  24.603106  24.646933
 24.370222  23.780298  22.947792  21.89405   20.620922  19.420013
 18.391245  17.912216  18.082947  18.26957   18.424072  18.705744
 18.841228  18.89802   18.710669  18.271015  17.571587  16.482561
 15.099841  13.641429  11.979413  10.1214905  8.152839   5.8925195
  3.6294856  2.0529556  3.5771987  5.953533 ]
