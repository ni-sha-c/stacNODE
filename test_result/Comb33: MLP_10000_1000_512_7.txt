time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.00%, model saved.
Epoch: 0 Train: 3667.50806 Test: 3924.45410
Epoch 100: New minimal relative error: 39.36%, model saved.
Epoch: 100 Train: 41.77020 Test: 43.00130
Epoch: 200 Train: 55.89965 Test: 80.28435
Epoch: 300 Train: 15.43327 Test: 20.65413
Epoch 400: New minimal relative error: 30.58%, model saved.
Epoch: 400 Train: 15.18163 Test: 16.27993
Epoch: 500 Train: 1.58492 Test: 3.41331
Epoch 600: New minimal relative error: 17.24%, model saved.
Epoch: 600 Train: 0.90184 Test: 1.15252
Epoch: 700 Train: 0.50468 Test: 1.51086
Epoch: 800 Train: 3.20202 Test: 2.75893
Epoch: 900 Train: 1.17918 Test: 0.50431
Epoch: 1000 Train: 0.54119 Test: 0.62383
Epoch: 1100 Train: 0.41119 Test: 0.43876
Epoch: 1200 Train: 0.75808 Test: 0.78590
Epoch: 1300 Train: 6.64652 Test: 5.20617
Epoch: 1400 Train: 1.08552 Test: 1.61197
Epoch: 1500 Train: 2.02222 Test: 1.51673
Epoch: 1600 Train: 0.12085 Test: 0.21303
Epoch: 1700 Train: 3.58260 Test: 3.40753
Epoch: 1800 Train: 1.33475 Test: 1.00443
Epoch: 1900 Train: 3.53106 Test: 3.92635
Epoch: 2000 Train: 1.05324 Test: 1.58195
Epoch: 2100 Train: 0.85657 Test: 0.84640
Epoch: 2200 Train: 0.39946 Test: 0.29844
Epoch: 2300 Train: 0.48332 Test: 0.41985
Epoch: 2400 Train: 1.46071 Test: 1.72010
Epoch: 2500 Train: 0.53792 Test: 0.76346
Epoch: 2600 Train: 0.12383 Test: 0.18527
Epoch: 2700 Train: 2.07185 Test: 3.07287
Epoch: 2800 Train: 0.47088 Test: 0.53606
Epoch: 2900 Train: 1.75839 Test: 1.97702
Epoch: 3000 Train: 1.75126 Test: 2.05891
Epoch: 3100 Train: 0.99967 Test: 1.17843
Epoch: 3200 Train: 2.62710 Test: 3.35790
Epoch: 3300 Train: 0.63795 Test: 0.74585
Epoch: 3400 Train: 0.37275 Test: 0.44080
Epoch: 3500 Train: 0.36884 Test: 0.36588
Epoch: 3600 Train: 0.60261 Test: 0.56537
Epoch: 3700 Train: 0.54210 Test: 0.55422
Epoch: 3800 Train: 0.10417 Test: 0.11993
Epoch: 3900 Train: 1.07905 Test: 0.85844
Epoch: 4000 Train: 0.30267 Test: 0.29101
Epoch: 4100 Train: 0.22868 Test: 0.31435
Epoch: 4200 Train: 1.90041 Test: 2.09496
Epoch: 4300 Train: 0.15272 Test: 0.22256
Epoch: 4400 Train: 0.05703 Test: 0.07903
Epoch: 4500 Train: 0.62320 Test: 0.73843
Epoch: 4600 Train: 0.09760 Test: 0.09069
Epoch: 4700 Train: 0.05193 Test: 0.08523
Epoch: 4800 Train: 0.74474 Test: 0.78116
Epoch: 4900 Train: 0.15432 Test: 0.16169
Epoch: 5000 Train: 0.27160 Test: 0.33701
Epoch: 5100 Train: 0.27701 Test: 0.25064
Epoch: 5200 Train: 0.15653 Test: 0.07667
Epoch 5300: New minimal relative error: 15.72%, model saved.
Epoch: 5300 Train: 0.90409 Test: 0.91912
Epoch: 5400 Train: 0.86269 Test: 0.97775
Epoch: 5500 Train: 0.03209 Test: 0.06479
Epoch: 5600 Train: 0.33934 Test: 0.33731
Epoch: 5700 Train: 0.75378 Test: 0.81563
Epoch: 5800 Train: 1.55058 Test: 1.14409
Epoch: 5900 Train: 0.03205 Test: 0.04956
Epoch: 6000 Train: 0.01639 Test: 0.03865
Epoch: 6100 Train: 0.43233 Test: 0.43710
Epoch: 6200 Train: 0.01155 Test: 0.03262
Epoch: 6300 Train: 0.05303 Test: 0.08806
Epoch: 6400 Train: 0.15409 Test: 0.21733
Epoch: 6500 Train: 0.11922 Test: 0.18291
Epoch: 6600 Train: 0.04255 Test: 0.05138
Epoch: 6700 Train: 0.33654 Test: 0.43577
Epoch: 6800 Train: 0.42792 Test: 0.31667
Epoch: 6900 Train: 0.07576 Test: 0.11363
Epoch: 7000 Train: 0.15928 Test: 0.14793
Epoch: 7100 Train: 0.12629 Test: 0.17418
Epoch: 7200 Train: 0.54798 Test: 0.67033
Epoch: 7300 Train: 0.01645 Test: 0.04066
Epoch: 7400 Train: 0.45200 Test: 0.46730
Epoch 7500: New minimal relative error: 13.79%, model saved.
Epoch: 7500 Train: 0.06426 Test: 0.10028
Epoch: 7600 Train: 0.54575 Test: 0.70794
Epoch: 7700 Train: 0.06070 Test: 0.08867
Epoch: 7800 Train: 0.01104 Test: 0.03324
Epoch: 7900 Train: 0.00940 Test: 0.02556
Epoch: 8000 Train: 0.04887 Test: 0.07598
Epoch: 8100 Train: 0.04282 Test: 0.06394
Epoch: 8200 Train: 0.10706 Test: 0.04468
Epoch: 8300 Train: 0.07026 Test: 0.09663
Epoch: 8400 Train: 0.36498 Test: 0.33098
Epoch: 8500 Train: 0.04085 Test: 0.06600
Epoch: 8600 Train: 0.04594 Test: 0.07124
Epoch: 8700 Train: 0.08799 Test: 0.14194
Epoch: 8800 Train: 0.01074 Test: 0.02409
Epoch: 8900 Train: 0.18121 Test: 0.17154
Epoch: 9000 Train: 0.01179 Test: 0.02893
Epoch: 9100 Train: 0.20715 Test: 0.32341
Epoch: 9200 Train: 0.46290 Test: 0.56193
Epoch: 9300 Train: 0.03512 Test: 0.06182
Epoch: 9400 Train: 0.06532 Test: 0.10894
Epoch: 9500 Train: 0.13680 Test: 0.13314
Epoch: 9600 Train: 0.09864 Test: 0.14173
Epoch: 9700 Train: 0.00583 Test: 0.01962
Epoch: 9800 Train: 0.02041 Test: 0.03242
Epoch: 9900 Train: 0.02359 Test: 0.03867
Epoch: 9999 Train: 0.04637 Test: 0.07668
Training Loss: tensor(0.0464)
Test Loss: tensor(0.0767)
Learned LE: [ 0.9335955  -0.04676986 -6.315235  ]
True LE: [ 8.7161195e-01  1.1620533e-03 -1.4545065e+01]
Relative Error: [2.0301816  2.0271533  2.0286384  2.045724   2.0485046  1.9963863
 1.8786576  1.7154323  1.53572    1.3631896  1.2146348  1.1004752
 1.0245737  0.98429173 0.9713845  0.9753817  0.9860628  0.9963166
 1.0030663  1.0099635  1.0287853  1.0727267  1.1436191  1.2249953
 1.2951878  1.3502954  1.4111954  1.4950362  1.5862316  1.6495477
 1.6584548  1.6051723  1.4953822  1.3380693  1.1425174  0.9191458
 0.67918557 0.4382651  0.24081248 0.23342279 0.38674244 0.54635066
 0.67973226 0.7840692  0.862086   0.9177726  0.955558   0.98092926
 1.0003856  1.0197288  1.0432483  1.0734897  1.1109132  1.1533859
 1.1979247  1.2458161  1.3054062  1.3876959  1.4952629  1.6154585
 1.7230176  1.791516   1.8099651  1.794889   1.7824166  1.7884245
 1.78273    1.7240199  1.6050478  1.4483839  1.2791091  1.1149817
 0.96943575 0.8530799  0.7716437  0.725011   0.7075824  0.71029973
 0.72337955 0.73809934 0.74775696 0.75152785 0.7595007  0.78862625
 0.8471032  0.9187801  0.97013783 0.98338455 0.98279595 1.0103573
 1.0683972  1.1170825  1.1212184  1.0699607  0.9691678  0.829738
 0.6633695  0.4857874  0.3157197  0.20638779 0.26651892 0.41805753
 0.5682789  0.69137007 0.78235006 0.8441453  0.88216263 0.9023191
 0.9103777  0.91241467 0.9142399  0.92028195 0.9329933  0.953466
 0.9812552  1.0138829  1.0472435  1.0811194  1.1243615  1.1906111
 1.286249   1.4003092  1.5065929  1.5748858  1.5889575  1.5629758
 1.5373036  1.535586   1.5289078  1.4743191  1.3675009  1.2317679
 1.0858388  0.94018495 0.8055992  0.6932828  0.6110296  0.5613921
 0.5422846  0.5482716  0.5713949  0.60177463 0.62809676 0.64288205
 0.6504464  0.66789186 0.71425235 0.7835408  0.83426464 0.824319
 0.75691956 0.68806016 0.6663628  0.67080945 0.6547544  0.5972939
 0.501441   0.3788672  0.2516751  0.18473019 0.23576505 0.34601158
 0.47480157 0.6032387  0.7146391  0.79840475 0.852146   0.8789981
 0.8846884  0.87603796 0.8600884  0.8435531  0.8318123  0.827834
 0.8323411  0.8448761  0.864489   0.88894016 0.9135036  0.93585753
 0.9630703  1.0107105  1.0892652  1.1920455  1.2938918  1.3620185
 1.3742138  1.3393337  1.3005182  1.2925977  1.290937   1.249662
 1.166254   1.064018   0.9528367  0.83505344 0.7200702  0.6210393
 0.5474609  0.5026916  0.48588222 0.49554563 0.528014   0.574181
 0.6196534  0.6514444  0.66666865 0.67790794 0.7126183  0.7827813
 0.85306096 0.86392367 0.78562933 0.64592    0.5082376  0.40671238
 0.32101923 0.22414969 0.11170694 0.02183894 0.16183488 0.3160143
 0.4513848  0.5620429  0.65817547 0.7442244  0.8148861  0.86356515
 0.8877946  0.8888307  0.87098706 0.8412375  0.8076645  0.7777682
 0.75665444 0.74578243 0.7441842  0.75026464 0.7629235  0.78063214
 0.7994173  0.81459314 0.82940274 0.8580688  0.9149825  0.99960554
 1.0918671  1.1589776  1.1723064  1.1318622  1.079399   1.0636693
 1.0702897  1.0486362  0.9957989  0.9355005  0.86695254 0.78293586
 0.69271356 0.6129852  0.55520284 0.52169234 0.509308   0.51816416
 0.5503325  0.60110146 0.65666556 0.7008343  0.7245166  0.7325989
 0.7554214  0.8231204  0.9169433  0.9746003  0.94292796 0.81815237
 0.64738625 0.48421547 0.34821132 0.25806904 0.2584478  0.33424896
 0.43836492 0.55747724 0.6636999  0.7409793  0.7964302  0.8393816
 0.87128    0.8898573  0.89225703 0.8767651  0.84456664 0.8014542
 0.75648063 0.71844876 0.69213456 0.6773713  0.67151487 0.6722673
 0.67831844 0.68963164 0.70461375 0.7178562  0.72706735 0.74126
 0.77532244 0.835175   0.9099323  0.97197706 0.9886924  0.9479536
 0.88218784 0.853216   0.867011   0.8674684  0.8463392  0.8305104
 0.808138   0.7598335  0.69450957 0.6335321  0.59141076 0.5701285
 0.562707   0.5668462  0.5899508  0.6344703  0.68991923 0.7399003
 0.7704059  0.7766097  0.7842271  0.8390188  0.94464433 1.0480917
 1.0842081  1.0193465  0.8750801  0.71350694 0.58170784 0.51014686
 0.52027714 0.5859248  0.66488653 0.7514986  0.8278034  0.8732289
 0.8915499  0.8945414  0.890249   0.8824913  0.8696366  0.8461944
 0.8084036  0.7592632  0.70845926 0.66667455 0.6394256  0.6244748
 0.61668414 0.6132241  0.61303574 0.61716163 0.62755543 0.64153194
 0.65269315 0.66176367 0.6788242  0.7122255  0.7607963  0.80919874
 0.8279269  0.7928037  0.717855   0.6672909  0.68040884 0.7022088
 0.7074027  0.7306225  0.7547149  0.7431753  0.7009608  0.6546999
 0.6238708  0.6128677  0.61192864 0.61201495 0.6213142  0.6516547
 0.6990674  0.74886584 0.785528   0.79488593 0.7863095  0.81395245
 0.90945184 1.0389943  1.1385072  1.1538371 ]
