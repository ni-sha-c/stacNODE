time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 101.91%, model saved.
Epoch: 0 Train: 3934.12793 Test: 3908.85327
Epoch 100: New minimal relative error: 30.63%, model saved.
Epoch: 100 Train: 29.61564 Test: 25.34624
Epoch 200: New minimal relative error: 20.68%, model saved.
Epoch: 200 Train: 13.25978 Test: 11.44850
Epoch 300: New minimal relative error: 20.52%, model saved.
Epoch: 300 Train: 3.57068 Test: 3.43646
Epoch 400: New minimal relative error: 9.63%, model saved.
Epoch: 400 Train: 2.58644 Test: 2.52083
Epoch: 500 Train: 4.78479 Test: 4.46581
Epoch: 600 Train: 1.55007 Test: 1.44508
Epoch: 700 Train: 2.07636 Test: 5.22338
Epoch: 800 Train: 0.85403 Test: 0.87286
Epoch: 900 Train: 0.71754 Test: 0.77095
Epoch: 1000 Train: 1.17644 Test: 1.20933
Epoch: 1100 Train: 1.35803 Test: 2.30256
Epoch: 1200 Train: 1.53438 Test: 2.89255
Epoch: 1300 Train: 17.97948 Test: 18.15370
Epoch: 1400 Train: 0.33866 Test: 0.47724
Epoch: 1500 Train: 0.39404 Test: 0.49951
Epoch: 1600 Train: 0.39385 Test: 0.55571
Epoch: 1700 Train: 0.31899 Test: 0.49626
Epoch: 1800 Train: 3.79740 Test: 4.64345
Epoch: 1900 Train: 8.47831 Test: 7.85457
Epoch: 2000 Train: 6.05337 Test: 6.62671
Epoch: 2100 Train: 0.31863 Test: 0.41366
Epoch: 2200 Train: 1.81940 Test: 2.48207
Epoch: 2300 Train: 0.36268 Test: 0.53215
Epoch: 2400 Train: 1.41131 Test: 1.92543
Epoch: 2500 Train: 0.19003 Test: 0.38831
Epoch: 2600 Train: 1.71226 Test: 0.81961
Epoch: 2700 Train: 0.20950 Test: 0.42244
Epoch: 2800 Train: 0.13011 Test: 0.32165
Epoch: 2900 Train: 0.13031 Test: 0.32268
Epoch: 3000 Train: 0.13214 Test: 0.33328
Epoch: 3100 Train: 0.11521 Test: 0.31539
Epoch: 3200 Train: 0.14243 Test: 0.36636
Epoch 3300: New minimal relative error: 8.16%, model saved.
Epoch: 3300 Train: 0.11059 Test: 0.31221
Epoch 3400: New minimal relative error: 6.77%, model saved.
Epoch: 3400 Train: 0.10224 Test: 0.30432
Epoch: 3500 Train: 0.16030 Test: 0.36058
Epoch: 3600 Train: 0.13555 Test: 0.34999
Epoch: 3700 Train: 0.27173 Test: 0.53746
Epoch: 3800 Train: 0.11949 Test: 0.31107
Epoch 3900: New minimal relative error: 6.40%, model saved.
Epoch: 3900 Train: 0.08938 Test: 0.29932
Epoch: 4000 Train: 0.08148 Test: 0.28836
Epoch: 4100 Train: 0.14877 Test: 0.31176
Epoch: 4200 Train: 0.08290 Test: 0.29856
Epoch: 4300 Train: 0.11699 Test: 0.57336
Epoch: 4400 Train: 0.06839 Test: 0.27657
Epoch: 4500 Train: 0.08286 Test: 0.29367
Epoch: 4600 Train: 0.07512 Test: 0.29996
Epoch: 4700 Train: 1.61056 Test: 1.09041
Epoch: 4800 Train: 0.06227 Test: 0.27432
Epoch: 4900 Train: 0.07689 Test: 0.29008
Epoch: 5000 Train: 0.07684 Test: 0.28611
Epoch: 5100 Train: 0.10503 Test: 0.33164
Epoch: 5200 Train: 3.75217 Test: 5.16372
Epoch: 5300 Train: 0.05468 Test: 0.26932
Epoch: 5400 Train: 0.28147 Test: 0.59322
Epoch: 5500 Train: 0.09395 Test: 0.31481
Epoch: 5600 Train: 0.04984 Test: 0.26071
Epoch: 5700 Train: 0.05270 Test: 0.27878
Epoch: 5800 Train: 1.09780 Test: 1.55618
Epoch: 5900 Train: 0.06621 Test: 0.28801
Epoch: 6000 Train: 0.04558 Test: 0.25604
Epoch: 6100 Train: 0.56745 Test: 0.97528
Epoch: 6200 Train: 0.69017 Test: 1.13782
Epoch: 6300 Train: 0.04137 Test: 0.25026
Epoch 6400: New minimal relative error: 3.98%, model saved.
Epoch: 6400 Train: 0.08811 Test: 0.31895
Epoch: 6500 Train: 0.13804 Test: 0.39043
Epoch: 6600 Train: 0.08157 Test: 0.31323
Epoch: 6700 Train: 0.03831 Test: 0.24763
Epoch: 6800 Train: 0.43131 Test: 0.84539
Epoch: 6900 Train: 0.03663 Test: 0.24780
Epoch: 7000 Train: 0.03833 Test: 0.24847
Epoch: 7100 Train: 0.33837 Test: 0.35961
Epoch: 7200 Train: 0.03394 Test: 0.24441
Epoch 7300: New minimal relative error: 3.18%, model saved.
Epoch: 7300 Train: 0.04015 Test: 0.24748
Epoch: 7400 Train: 0.03881 Test: 0.24818
Epoch: 7500 Train: 0.03246 Test: 0.24284
Epoch: 7600 Train: 0.03169 Test: 0.24362
Epoch: 7700 Train: 0.06858 Test: 0.26938
Epoch: 7800 Train: 0.22778 Test: 0.52947
Epoch: 7900 Train: 0.02973 Test: 0.24189
Epoch: 8000 Train: 1.10241 Test: 1.58594
Epoch: 8100 Train: 0.02865 Test: 0.24185
Epoch: 8200 Train: 0.04452 Test: 0.40457
Epoch: 8300 Train: 0.28711 Test: 0.59953
Epoch: 8400 Train: 0.02723 Test: 0.24088
Epoch: 8500 Train: 0.02646 Test: 0.23879
Epoch: 8600 Train: 0.02745 Test: 0.24321
Epoch 8700: New minimal relative error: 3.12%, model saved.
Epoch: 8700 Train: 0.02567 Test: 0.23896
Epoch: 8800 Train: 0.02613 Test: 0.24027
Epoch: 8900 Train: 0.23777 Test: 0.38732
Epoch: 9000 Train: 0.02456 Test: 0.23909
Epoch: 9100 Train: 0.02401 Test: 0.23726
Epoch: 9200 Train: 0.04691 Test: 0.25224
Epoch: 9300 Train: 0.02344 Test: 0.23873
Epoch: 9400 Train: 0.02321 Test: 0.24434
Epoch: 9500 Train: 0.02262 Test: 0.23697
Epoch: 9600 Train: 0.02311 Test: 0.25807
Epoch: 9700 Train: 0.02209 Test: 0.23779
Epoch: 9800 Train: 0.02166 Test: 0.23571
Epoch: 9900 Train: 0.02152 Test: 0.23675
Epoch: 9999 Train: 0.05308 Test: 0.24246
Training Loss: tensor(0.0531)
Test Loss: tensor(0.2425)
Learned LE: [ 0.9418057  -0.00966098 -6.167437  ]
True LE: [ 8.9260888e-01  6.1167665e-03 -1.4572832e+01]
Relative Error: [1.572901   1.894438   2.2708097  2.5766017  2.726774   2.6711435
 2.3891008  1.9585295  1.5671719  1.3319874  1.1809807  1.0980828
 1.2209138  1.5095737  1.9064356  2.3871777  2.8559475  3.1856225
 3.3138177  3.335904   3.395994   3.468656   3.424265   3.2536554
 3.0331862  2.753938   2.408147   2.1198993  1.9847475  2.0260317
 2.1658952  2.257952   2.2738736  2.2010052  2.0200038  1.8835981
 1.7847672  1.6925814  1.5936794  1.4629047  1.2895186  1.1367742
 1.019387   0.8873125  0.77202797 0.7364869  0.7312035  0.68560004
 0.5822133  0.45531607 0.3756817  0.29191494 0.17712952 0.18127516
 0.25445792 0.45093843 0.6432166  0.7924992  0.90558016 0.97959244
 1.0287932  1.0832137  1.191643   1.4210988  1.751818   2.0519872
 2.2112772  2.171331   1.9103844  1.5178654  1.203381   1.0830383
 1.0302899  0.9860857  1.151866   1.5304794  1.9910988  2.4929814
 2.9553902  3.259552   3.332615   3.270854   3.2441902  3.2528474
 3.1696715  2.976315   2.7493637  2.5583582  2.2981248  1.9726015
 1.7391887  1.6884083  1.8128176  1.9519349  2.000706   1.9168818
 1.7450078  1.6291387  1.5090144  1.377975   1.2725401  1.1805657
 1.0426098  0.9063065  0.82562697 0.7244412  0.5810341  0.50868917
 0.53396946 0.53908086 0.4848632  0.4017956  0.35544842 0.29888836
 0.212889   0.19435957 0.23834719 0.32160947 0.49989307 0.6514691
 0.7535685  0.79870224 0.79968023 0.79046583 0.8143575  0.93559325
 1.1930196  1.4810354  1.6562531  1.6429514  1.4165244  1.0712876
 0.8304829  0.8044304  0.8369958  0.87429124 1.0977834  1.5407907
 2.0401187  2.5066342  2.9026184  3.1546428  3.1829967  3.056313
 2.9593468  2.9169962  2.7991965  2.595391   2.3638363  2.182772
 2.0771346  1.8400024  1.5163456  1.2952968  1.306275   1.490127
 1.6395748  1.6206784  1.4711075  1.3673286  1.231747   1.0832738
 0.96736413 0.8932526  0.7919509  0.65157133 0.5823411  0.5396558
 0.44938368 0.3330493  0.3420173  0.4006242  0.38640556 0.31763342
 0.2763408  0.24179353 0.18420489 0.19262365 0.24028921 0.27078965
 0.34778437 0.48296565 0.5654651  0.5895337  0.566293   0.51670945
 0.47964385 0.49670157 0.6407726  0.89312994 1.0895973  1.1134077
 0.92851293 0.63015354 0.48068544 0.58693653 0.7431406  0.9098175
 1.1638     1.5102477  1.9140638  2.2643485  2.5260823  2.6954415
 2.7103956  2.552268   2.3994534  2.3652415  2.3042228  2.1152446
 1.8852353  1.6777585  1.574253   1.5426234  1.312201   0.95924085
 0.7425472  0.84111893 1.1128458  1.2567639  1.1631249  1.0550048
 0.91176516 0.7970638  0.7116418  0.63688534 0.5549827  0.4126264
 0.325086   0.29949433 0.2667339  0.21574816 0.17823996 0.24370758
 0.28932616 0.24315105 0.17222206 0.14573608 0.09479865 0.1092523
 0.18415914 0.24370211 0.26931974 0.31287333 0.36304387 0.36742777
 0.34290153 0.28927234 0.22754508 0.19264428 0.19442397 0.34845376
 0.56280756 0.644225   0.5202767  0.3072497  0.38002232 0.5760952
 0.81837076 1.0933214  1.3483176  1.4813938  1.5944064  1.7507896
 1.8633293  1.9225433  1.9295043  1.8396305  1.6969357  1.612884
 1.599944   1.544728   1.3806549  1.1612972  0.99686927 0.92847633
 0.9460629  0.756515   0.41880846 0.22875443 0.452183   0.79445016
 0.82524186 0.73236793 0.5760555  0.45893523 0.43319547 0.41481912
 0.35128745 0.23018225 0.09590445 0.07200088 0.06917987 0.06929959
 0.08418535 0.10549629 0.1340698  0.15918995 0.08943872 0.04494739
 0.02572423 0.00787631 0.0715386  0.12132153 0.21075577 0.22854015
 0.18498032 0.16237402 0.14506152 0.12003534 0.08550303 0.12868762
 0.17156272 0.11834124 0.15588135 0.29683575 0.2789089  0.26549605
 0.5677715  0.77845246 0.987137   1.2773783  1.5034384  1.4976772
 1.3727959  1.2881799  1.2596285  1.2310736  1.1601967  1.0671606
 0.94586724 0.8657836  0.8881703  0.88179225 0.8038688  0.72947335
 0.58333004 0.45287105 0.3468941  0.44129965 0.41688898 0.24245054
 0.10227019 0.27014163 0.52508825 0.4409159  0.30904913 0.21739733
 0.16659495 0.15464321 0.18797244 0.18156506 0.14208792 0.1376569
 0.14468679 0.15183064 0.14343472 0.16407318 0.17351325 0.16330977
 0.12504494 0.04857125 0.06811626 0.08075471 0.04519524 0.06208465
 0.03324595 0.16293289 0.19937406 0.08676208 0.04693371 0.04252452
 0.0612881  0.12404265 0.2234629  0.28381634 0.25509554 0.19738826
 0.19420196 0.23430386 0.5810662  0.92029774 1.1122473  1.3861798
 1.5860701  1.5473616  1.314659   1.1104311  0.95640373 0.8377115
 0.72766525 0.5663878  0.44304532 0.3087033  0.18515708 0.22255985
 0.3531904  0.31430954 0.27071476 0.2748021 ]
