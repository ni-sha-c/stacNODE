time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 106.54%, model saved.
Epoch: 0 Train: 3761.27930 Test: 4151.63916
Epoch: 80 Train: 599.89105 Test: 650.38318
Epoch 160: New minimal relative error: 59.68%, model saved.
Epoch: 160 Train: 61.67979 Test: 59.18153
Epoch 240: New minimal relative error: 29.57%, model saved.
Epoch: 240 Train: 32.50454 Test: 20.23475
Epoch 320: New minimal relative error: 27.36%, model saved.
Epoch: 320 Train: 27.57850 Test: 22.53757
Epoch 400: New minimal relative error: 10.74%, model saved.
Epoch: 400 Train: 8.91777 Test: 6.99591
Epoch: 480 Train: 9.17911 Test: 7.36622
Epoch: 560 Train: 11.95011 Test: 13.31874
Epoch: 640 Train: 6.06449 Test: 5.63012
Epoch: 720 Train: 12.85493 Test: 9.89226
Epoch: 800 Train: 19.65022 Test: 13.01637
Epoch: 880 Train: 5.04457 Test: 5.57179
Epoch: 960 Train: 7.04463 Test: 7.71074
Epoch: 1040 Train: 15.20187 Test: 19.22114
Epoch: 1120 Train: 8.33815 Test: 11.95186
Epoch: 1200 Train: 6.74115 Test: 3.17805
Epoch: 1280 Train: 4.62754 Test: 4.82508
Epoch: 1360 Train: 15.03167 Test: 19.45745
Epoch: 1440 Train: 2.96071 Test: 3.22494
Epoch 1520: New minimal relative error: 8.97%, model saved.
Epoch: 1520 Train: 4.00447 Test: 4.02896
Epoch: 1600 Train: 3.08916 Test: 3.01276
Epoch: 1680 Train: 3.48992 Test: 3.30098
Epoch: 1760 Train: 1.51350 Test: 1.14331
Epoch 1840: New minimal relative error: 6.44%, model saved.
Epoch: 1840 Train: 1.35745 Test: 1.08699
Epoch: 1920 Train: 15.87395 Test: 14.54936
Epoch: 2000 Train: 5.20567 Test: 6.49812
Epoch: 2080 Train: 1.18292 Test: 1.00453
Epoch: 2160 Train: 0.84055 Test: 0.68428
Epoch: 2240 Train: 0.71991 Test: 0.63149
Epoch: 2320 Train: 0.97104 Test: 0.81683
Epoch: 2400 Train: 0.82764 Test: 0.77727
Epoch: 2480 Train: 1.04562 Test: 0.97364
Epoch: 2560 Train: 0.71791 Test: 0.75577
Epoch: 2640 Train: 2.00215 Test: 2.65424
Epoch: 2720 Train: 0.70136 Test: 0.74348
Epoch: 2800 Train: 2.73559 Test: 3.61207
Epoch: 2880 Train: 1.42559 Test: 1.70496
Epoch: 2960 Train: 4.61519 Test: 4.46462
Epoch 3040: New minimal relative error: 6.26%, model saved.
Epoch: 3040 Train: 0.83559 Test: 0.87808
Epoch: 3120 Train: 0.60235 Test: 0.67019
Epoch: 3200 Train: 2.11764 Test: 2.25878
Epoch: 3280 Train: 1.13197 Test: 0.99303
Epoch 3360: New minimal relative error: 3.61%, model saved.
Epoch: 3360 Train: 0.40961 Test: 0.46486
Epoch: 3440 Train: 0.94451 Test: 1.16070
Epoch: 3520 Train: 1.73922 Test: 1.89696
Epoch: 3600 Train: 0.50814 Test: 0.53182
Epoch: 3680 Train: 0.86311 Test: 1.14144
Epoch: 3760 Train: 1.83722 Test: 1.71606
Epoch: 3840 Train: 0.51918 Test: 0.65678
Epoch: 3920 Train: 0.71538 Test: 0.73392
Epoch: 4000 Train: 0.27223 Test: 0.33596
Epoch: 4080 Train: 1.45818 Test: 1.85324
Epoch: 4160 Train: 0.27165 Test: 0.33538
Epoch: 4240 Train: 0.25138 Test: 0.32504
Epoch: 4320 Train: 0.30460 Test: 0.39273
Epoch: 4400 Train: 0.94160 Test: 1.13809
Epoch: 4480 Train: 1.28734 Test: 1.28868
Epoch: 4560 Train: 1.49185 Test: 1.68351
Epoch: 4640 Train: 0.48640 Test: 0.48117
Epoch: 4720 Train: 1.60691 Test: 1.65421
Epoch: 4800 Train: 0.21453 Test: 0.29218
Epoch: 4880 Train: 0.23882 Test: 0.30119
Epoch: 4960 Train: 0.41864 Test: 0.57406
Epoch: 5040 Train: 0.95595 Test: 0.95985
Epoch: 5120 Train: 0.41281 Test: 0.43178
Epoch 5200: New minimal relative error: 3.34%, model saved.
Epoch: 5200 Train: 0.35593 Test: 0.46666
Epoch: 5280 Train: 1.08780 Test: 1.33540
Epoch: 5360 Train: 1.65894 Test: 2.00121
Epoch: 5440 Train: 0.18764 Test: 0.27019
Epoch: 5520 Train: 0.21859 Test: 0.30701
Epoch: 5600 Train: 0.18520 Test: 0.26429
Epoch: 5680 Train: 0.18585 Test: 0.26435
Epoch: 5760 Train: 0.27281 Test: 0.34801
Epoch 5840: New minimal relative error: 2.91%, model saved.
Epoch: 5840 Train: 0.19105 Test: 0.28077
Epoch: 5920 Train: 0.17414 Test: 0.25855
Epoch: 6000 Train: 0.18758 Test: 0.28175
Epoch: 6080 Train: 0.19117 Test: 0.26461
Epoch: 6160 Train: 0.22386 Test: 0.30717
Epoch: 6240 Train: 0.22488 Test: 0.31316
Epoch: 6320 Train: 0.36511 Test: 0.42353
Epoch: 6400 Train: 0.30854 Test: 0.48178
Epoch: 6480 Train: 0.38972 Test: 0.38611
Epoch: 6560 Train: 2.09410 Test: 2.70757
Epoch: 6640 Train: 0.15143 Test: 0.23527
Epoch: 6720 Train: 0.16382 Test: 0.25375
Epoch 6800: New minimal relative error: 2.48%, model saved.
Epoch: 6800 Train: 0.17333 Test: 0.25736
Epoch: 6880 Train: 0.30937 Test: 0.40749
Epoch: 6960 Train: 0.14993 Test: 0.23553
Epoch 7040: New minimal relative error: 2.40%, model saved.
Epoch: 7040 Train: 0.14380 Test: 0.22926
Epoch: 7120 Train: 0.17220 Test: 0.25176
Epoch: 7200 Train: 0.13832 Test: 0.22135
Epoch: 7280 Train: 0.17021 Test: 0.24231
Epoch: 7360 Train: 0.19630 Test: 0.30081
Epoch: 7440 Train: 0.20870 Test: 0.32621
Epoch: 7520 Train: 0.13371 Test: 0.21711
Epoch: 7600 Train: 0.13714 Test: 0.21741
Epoch: 7680 Train: 0.30795 Test: 0.37615
Epoch: 7760 Train: 0.59292 Test: 0.63365
Epoch: 7840 Train: 0.21250 Test: 0.31857
Epoch: 7920 Train: 0.32010 Test: 0.31242
Epoch: 7999 Train: 0.13427 Test: 0.21868
Training Loss: tensor(0.1343)
Test Loss: tensor(0.2187)
Learned LE: [ 7.2355115e-01  1.8661265e-03 -3.2407191e+00]
True LE: [ 8.7169605e-01  4.1620247e-04 -1.4544998e+01]
Relative Error: [5.0969014  4.918282   4.8260903  4.760657   4.663185   4.518547
 4.3230696  3.9807558  3.5846598  2.9906814  2.574768   2.1217694
 1.7201304  1.5049869  1.4820764  1.5385727  2.008736   2.0847406
 2.2579453  2.5673177  3.1913168  3.8830132  4.5211596  4.924838
 5.175476   5.1275177  4.7825704  4.200508   3.4437034  2.8297908
 2.42563    2.4589486  2.8385506  4.003902   5.7078347  6.1061015
 6.6473975  6.839971   5.718523   4.8199935  4.162112   2.6407433
 1.734224   2.5995119  3.4569218  4.1471868  4.267886   4.3376775
 4.354724   4.3330774  4.3594556  4.060042   3.7667687  3.5648837
 3.4288208  3.2499948  3.364899   3.6879077  4.071989   4.2925596
 4.6412773  4.600616   4.329454   4.12899    4.048074   3.972251
 3.9495013  3.8834379  3.7405155  3.5739136  3.1963143  2.672338
 2.335757   2.022357   1.6352478  1.3810321  1.353344   1.492658
 1.7455492  1.832711   2.066315   2.5359733  2.9468086  3.385191
 4.207663   4.752663   4.9668446  4.9433994  4.650414   4.3232384
 3.894792   3.2564082  2.4516733  1.8516891  2.1014616  2.5149512
 3.4725196  5.065866   5.358394   6.059086   5.856251   4.7477255
 4.0770335  2.9679446  1.7255995  1.8079364  2.6872902  3.341928
 3.7119615  3.5946517  3.51468    3.3695002  3.24227    3.0998821
 2.7608247  2.534933   2.4266183  2.2471466  2.412125   2.7734141
 3.1343594  3.414897   3.7518308  3.7944696  3.6570542  3.4932506
 3.306042   3.2184818  3.1818013  3.221508   3.1886704  3.035389
 2.8543317  2.4247239  2.0221696  1.866503   1.5992142  1.3353922
 1.261481   1.4635037  1.5651917  1.4751484  1.8208768  2.319945
 2.8801215  3.049873   3.6781986  4.2503     4.5965176  4.706113
 4.6388345  4.5793037  4.1867914  3.6746328  2.9561493  1.9907333
 1.322434   1.9724332  2.227032   2.9631472  4.1557417  4.6193705
 5.3357205  4.8596215  3.9056644  3.429438   2.0595937  1.2319905
 1.9741589  2.5217123  2.9118302  2.9997604  2.786883   2.5051777
 2.194429   1.988765   2.0064886  1.7026795  1.4691505  1.2759068
 1.3222013  1.6690602  2.0609367  2.4116502  2.881146   3.060318
 2.9511335  2.8045332  2.7274523  2.648148   2.582943   2.5196958
 2.5317333  2.434373   2.3030694  2.1670296  1.8177373  1.8213117
 1.6501535  1.3547024  1.2638763  1.2985488  1.4764608  1.2117239
 1.4612247  1.9629276  2.6464186  2.938062   3.2420955  3.6549573
 4.21635    4.494874   4.798451   4.925651   4.652097   4.1772914
 3.5094767  2.5463727  1.5509646  1.0236516  1.5601795  1.7665111
 2.4020443  3.2659452  3.708425   4.438523   3.9233162  3.1750221
 2.5034525  1.3825271  0.99304664 1.7861602  2.1514099  2.1431246
 2.1647701  1.8151644  1.6386119  0.98886657 0.9768849  0.8348356
 0.7511745  0.62759453 0.59926724 0.6937173  1.0495652  1.347759
 1.7990781  2.2778814  2.2695057  2.216333   2.1729355  2.12484
 2.1168299  2.0323663  1.9434469  1.9470582  1.8897884  1.801516
 1.7449309  1.6079088  2.0001965  1.6446465  1.2104332  1.2527775
 1.5383182  1.2188122  0.99427867 1.5028605  2.1902728  2.8733988
 2.9095821  3.5079994  3.9389982  3.849401   3.9167516  3.9552822
 3.901305   4.240774   4.605343   3.8716629  2.7933242  1.8967758
 0.9795716  1.1159196  1.4265565  1.9763621  2.8995636  2.7398484
 3.3911412  3.117008   2.497751   1.772572   0.89042133 0.7879163
 1.379045   1.6020695  1.4381706  1.2885923  1.4044336  0.77239317
 0.3033924  0.5887608  0.7767411  0.862359   0.96457106 0.6936896
 0.3982361  0.4805333  0.7348317  1.2256098  1.5312849  1.6404748
 1.7361356  1.6903987  1.6666824  1.5895251  1.5636322  1.6151419
 1.5913429  1.5625236  1.4888133  1.5263249  1.6234974  2.0311718
 1.6320513  1.1772004  1.163519   1.5790701  1.221388   0.8193046
 1.5534416  2.3091824  3.2187476  4.0807986  3.1650014  3.0116658
 3.1450999  3.4466035  3.575347   3.100263   2.762448   2.9726093
 3.177633   3.3940353  2.330026   1.2903965  0.91524476 1.2309915
 1.29777    2.5165882  2.0021381  2.3330073  2.2365885  1.9257629
 1.490671   0.7262302  0.42772764 0.83985144 1.009174   0.87360114
 1.1240369  1.4193485  0.8720747  1.3118337  1.886044   1.7894609
 1.7345624  1.5524757  1.1137407  0.6497317  0.3092599  0.26557973
 0.75514454 1.0203654  1.2518812  1.3815116  1.4563648  1.2977898
 1.1588942  1.2007972  1.2831444  1.2956927  1.3257163  1.2989511
 1.3682829  1.5708568  1.783561   1.6095121  1.24149    1.050885
 1.3771596  1.4380934  0.54701895 1.4661802  2.1698496  4.262615
 3.0904589  2.6322174  2.7297752  2.898957  ]
