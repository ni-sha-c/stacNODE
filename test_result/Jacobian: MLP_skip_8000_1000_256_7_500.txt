time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.47%, model saved.
Epoch: 0 Train: 32399.63672 Test: 3874.70337
Epoch: 80 Train: 9864.13477 Test: 1606.70239
Epoch: 160 Train: 8523.12402 Test: 1229.41199
Epoch 240: New minimal relative error: 89.22%, model saved.
Epoch: 240 Train: 8520.77148 Test: 1290.83301
Epoch: 320 Train: 8168.29980 Test: 1052.83655
Epoch 400: New minimal relative error: 76.16%, model saved.
Epoch: 400 Train: 7608.38330 Test: 1088.45142
Epoch 480: New minimal relative error: 67.70%, model saved.
Epoch: 480 Train: 7277.89795 Test: 890.96191
Epoch: 560 Train: 5656.92871 Test: 702.18005
Epoch: 640 Train: 4854.12207 Test: 474.87717
Epoch: 720 Train: 4006.84033 Test: 412.77979
Epoch: 800 Train: 2495.95264 Test: 172.76530
Epoch: 880 Train: 1152.96594 Test: 55.16938
Epoch 960: New minimal relative error: 23.30%, model saved.
Epoch: 960 Train: 648.10284 Test: 16.40863
Epoch 1040: New minimal relative error: 16.47%, model saved.
Epoch: 1040 Train: 477.11203 Test: 10.32885
Epoch: 1120 Train: 384.87097 Test: 7.92413
Epoch: 1200 Train: 323.79404 Test: 5.72453
Epoch: 1280 Train: 332.35858 Test: 13.02688
Epoch 1360: New minimal relative error: 12.10%, model saved.
Epoch: 1360 Train: 266.47583 Test: 4.12715
Epoch: 1440 Train: 240.39865 Test: 6.06943
Epoch 1520: New minimal relative error: 12.02%, model saved.
Epoch: 1520 Train: 213.83693 Test: 3.28096
Epoch: 1600 Train: 201.06885 Test: 5.81009
Epoch: 1680 Train: 210.50606 Test: 11.37240
Epoch: 1760 Train: 164.80202 Test: 4.76911
Epoch 1840: New minimal relative error: 11.54%, model saved.
Epoch: 1840 Train: 140.80020 Test: 1.31776
Epoch: 1920 Train: 133.69733 Test: 1.70901
Epoch 2000: New minimal relative error: 11.26%, model saved.
Epoch: 2000 Train: 132.49461 Test: 10.31005
Epoch 2080: New minimal relative error: 10.45%, model saved.
Epoch: 2080 Train: 115.84421 Test: 1.03872
Epoch 2160: New minimal relative error: 5.67%, model saved.
Epoch: 2160 Train: 113.54095 Test: 1.01719
Epoch: 2240 Train: 108.53654 Test: 1.14133
Epoch: 2320 Train: 147.92108 Test: 21.58940
Epoch: 2400 Train: 109.30247 Test: 0.94908
Epoch: 2480 Train: 102.50439 Test: 0.84234
Epoch: 2560 Train: 94.44597 Test: 0.77682
Epoch: 2640 Train: 89.01973 Test: 0.81940
Epoch: 2720 Train: 94.14492 Test: 3.25943
Epoch: 2800 Train: 85.66113 Test: 1.87310
Epoch: 2880 Train: 76.10392 Test: 0.80663
Epoch: 2960 Train: 76.04234 Test: 0.70747
Epoch: 3040 Train: 74.74461 Test: 5.29535
Epoch: 3120 Train: 71.72314 Test: 6.21526
Epoch: 3200 Train: 64.52618 Test: 0.42148
Epoch: 3280 Train: 66.21796 Test: 0.49751
Epoch: 3360 Train: 65.26924 Test: 0.44094
Epoch: 3440 Train: 62.23375 Test: 0.38419
Epoch: 3520 Train: 61.88115 Test: 0.64961
Epoch: 3600 Train: 64.73354 Test: 1.36891
Epoch 3680: New minimal relative error: 3.95%, model saved.
Epoch: 3680 Train: 59.80519 Test: 0.46403
Epoch: 3760 Train: 66.89404 Test: 3.84733
Epoch: 3840 Train: 54.89559 Test: 0.35341
Epoch: 3920 Train: 56.24841 Test: 0.38618
Epoch: 4000 Train: 55.41252 Test: 0.34552
Epoch: 4080 Train: 56.59446 Test: 0.44134
Epoch: 4160 Train: 54.45376 Test: 0.34410
Epoch: 4240 Train: 53.85963 Test: 0.41067
Epoch: 4320 Train: 54.26134 Test: 0.41706
Epoch 4400: New minimal relative error: 3.59%, model saved.
Epoch: 4400 Train: 59.05146 Test: 0.67861
Epoch: 4480 Train: 54.97267 Test: 0.43935
Epoch: 4560 Train: 52.65137 Test: 0.52193
Epoch: 4640 Train: 55.31613 Test: 0.68679
Epoch: 4720 Train: 52.05452 Test: 0.39214
Epoch: 4800 Train: 53.93454 Test: 0.39611
Epoch: 4880 Train: 51.22550 Test: 0.87080
Epoch: 4960 Train: 49.79918 Test: 0.56629
Epoch: 5040 Train: 49.14674 Test: 0.56751
Epoch: 5120 Train: 46.06160 Test: 0.34349
Epoch: 5200 Train: 47.66411 Test: 0.30713
Epoch: 5280 Train: 47.26936 Test: 0.30684
Epoch: 5360 Train: 50.40057 Test: 1.26184
Epoch: 5440 Train: 47.05046 Test: 0.41736
Epoch: 5520 Train: 48.13327 Test: 0.30476
Epoch: 5600 Train: 49.23393 Test: 0.30690
Epoch: 5680 Train: 45.26225 Test: 0.30552
Epoch: 5760 Train: 44.48523 Test: 0.29728
Epoch: 5840 Train: 40.64215 Test: 0.23694
Epoch 5920: New minimal relative error: 3.46%, model saved.
Epoch: 5920 Train: 38.77785 Test: 0.21760
Epoch: 6000 Train: 41.28634 Test: 1.28319
Epoch: 6080 Train: 38.90744 Test: 0.25811
Epoch: 6160 Train: 38.94663 Test: 1.14263
Epoch: 6240 Train: 36.57529 Test: 0.28603
Epoch: 6320 Train: 37.48794 Test: 0.45341
Epoch: 6400 Train: 39.57554 Test: 0.31658
Epoch: 6480 Train: 42.40022 Test: 0.25841
Epoch: 6560 Train: 44.66911 Test: 0.78732
Epoch: 6640 Train: 43.58624 Test: 0.23290
Epoch: 6720 Train: 43.51999 Test: 0.44433
Epoch: 6800 Train: 43.94450 Test: 0.89548
Epoch: 6880 Train: 42.82729 Test: 0.46448
Epoch: 6960 Train: 41.59448 Test: 0.20734
Epoch: 7040 Train: 39.38122 Test: 0.17874
Epoch: 7120 Train: 39.11621 Test: 0.39429
Epoch: 7200 Train: 38.09873 Test: 0.57837
Epoch 7280: New minimal relative error: 3.03%, model saved.
Epoch: 7280 Train: 37.76455 Test: 0.19431
Epoch: 7360 Train: 37.15664 Test: 0.20456
Epoch: 7440 Train: 35.40512 Test: 0.17871
Epoch: 7520 Train: 34.19088 Test: 0.16371
Epoch: 7600 Train: 33.84293 Test: 0.18339
Epoch: 7680 Train: 34.66628 Test: 0.21222
Epoch: 7760 Train: 34.63976 Test: 0.17636
Epoch: 7840 Train: 35.59510 Test: 0.24640
Epoch: 7920 Train: 37.74613 Test: 0.30097
Epoch: 7999 Train: 35.85928 Test: 0.33133
Training Loss: tensor(35.8593)
Test Loss: tensor(0.3313)
Learned LE: [ 8.44473958e-01  5.88040799e-03 -1.44877825e+01]
True LE: [ 8.5023195e-01 -2.0244617e-03 -1.4525001e+01]
Relative Error: [11.458562   11.201043   10.909791   10.578156   10.205054    9.818585
  9.544018    9.276248    8.962931    8.442517    7.9247246   7.413137
  6.8920956   6.402129    5.9674644   5.5376067   5.25448     4.843182
  4.5957794   4.551512    4.736312    5.046234    5.5219965   5.8917427
  6.3344636   6.847176    7.4227915   7.969434    8.536823    9.084134
  9.579891    9.894237   10.178984   10.402338   10.534206   10.58205
 10.562864   10.503893   10.450397   10.397239   10.406574   10.401675
 10.482467   10.634672   10.790909   10.549929   10.390596   10.287405
 10.287898   10.327417   10.109939   10.002169    9.908395    9.855595
  9.882106    9.987171   10.191161   10.426876   10.744606   10.823904
 10.655784   10.479115   10.328447   10.123216    9.883765    9.598955
  9.252576    8.849749    8.619855    8.386914    8.101086    7.5885715
  7.0608892   6.533281    6.004647    5.4836354   5.025895    4.5853233
  4.2253585   3.8075678   3.5693672   3.5387306   3.7886512   4.141438
  4.6723604   5.1048036   5.6053557   6.1594477   6.7461424   7.30177
  7.875101    8.42959     8.766779    9.103881    9.390541    9.610704
  9.732664    9.7667055   9.744161    9.7006      9.638496    9.58403
  9.574644    9.622639    9.731993    9.93167    10.018257    9.791579
  9.605525    9.506551    9.503717    9.30245     9.096655    9.005381
  8.95827     8.895438    8.941654    9.072516    9.251054    9.508587
  9.729877    9.756589    9.5195265   9.373042    9.272563    9.127162
  8.944008    8.699726    8.3822365   8.005627    7.7768364   7.5517035
  7.2722034   6.7960987   6.257803    5.7102623   5.1629467   4.611944
  4.10769     3.6584682   3.2254868   2.8215003   2.70623     2.7720747
  3.0567436   3.478007    4.036638    4.497657    5.027194    5.60217
  6.183152    6.73329     7.2970257   7.721035    8.032944    8.375797
  8.674861    8.883352    8.988317    9.009794    8.996458    8.949663
  8.866789    8.80636     8.779421    8.870899    8.994673    9.207874
  9.279648    9.031272    8.84101     8.752055    8.653355    8.317377
  8.117341    8.041156    8.006323    7.9636755   8.030692    8.185659
  8.382614    8.569005    8.739266    8.722918    8.531758    8.34241
  8.283181    8.200064    8.075821    7.875713    7.5927377   7.2523437
  7.0029535   6.7867727   6.5077953   6.053791    5.481728    4.901467
  4.3294454   3.7741172   3.2439475   2.7673798   2.3485057   2.065557
  1.9430466   2.060793    2.3893135   2.8815134   3.5076017   4.054612
  4.6243796   5.209117    5.7554226   6.278652    6.7462482   7.0662217
  7.3739557   7.650418    7.906556    8.084122    8.188159    8.269193
  8.311237    8.233176    8.144435    8.052944    8.049089    8.118049
  8.268337    8.498681    8.5735655   8.285241    8.098616    8.023789
  7.7170796   7.3805017   7.178954    7.112255    7.0783906   7.0616217
  7.1413307   7.310795    7.5104504   7.60244     7.7733135   7.7299914
  7.5998797   7.443992    7.351556    7.3412037   7.2734475   7.121167
  6.888467    6.5728297   6.2987366   6.089544    5.7406645   5.2976675
  4.7131915   4.1191964   3.5157576   2.9305913   2.371983    2.0946684
  1.7347157   1.4159609   1.3155785   1.4467182   1.8006417   2.3657265
  3.067463    3.7149172   4.3477983   4.9665437   5.4764876   5.923842
  6.2422643   6.445126    6.6763625   6.910164    7.109397    7.250758
  7.3638043   7.466234    7.535891    7.516886    7.4279013   7.34485
  7.3209844   7.385151    7.5759354   7.8703866   7.910062    7.615377
  7.4165845   7.2533035   6.8033814   6.4786515   6.276369    6.2242312
  6.186606    6.1666      6.2651234   6.456787    6.664676    6.6938767
  6.822052    6.785462    6.724988    6.6087885   6.5164433   6.5318017
  6.526497    6.428753    6.2543006   5.9673843   5.6177554   5.330821
  4.9782286   4.5692897   3.9745073   3.360793    2.745265    2.1656063
  1.7697629   1.5645664   1.1639794   0.84097785  0.7053282   0.9172068
  1.3909069   1.9724084   2.693347    3.3996806   4.076767    4.759449
  5.3197393   5.660746    5.7991004   5.8773766   6.016246    6.194829
  6.355377    6.4723773   6.578579    6.657484    6.682735    6.7020473
  6.7160554   6.683334    6.646799    6.6893125   6.8874784   7.2368793
  7.2476587   6.9830284   6.782268    6.3863316   5.874037    5.5304813
  5.349855    5.2913246   5.3185215   5.439054    5.6272655   5.854758
  6.1178813   6.1517487   6.1485405   5.9944515   5.88809     5.834017
  5.776104    5.760497    5.823144    5.801871    5.671354    5.3715477
  4.899828    4.637704    4.292678    3.8739786   3.30584     2.6791246
  2.054352    1.5800184   1.336178    1.0755804   0.681658    0.34469944
  0.15216494  0.6097788   1.0832841   1.6428543   2.3599477   3.1095037
  3.8241706   4.5482006   5.110574    5.4270787 ]
