time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 105.54%, model saved.
Epoch: 0 Train: 4257.95850 Test: 4294.61963
Epoch: 100 Train: 157.71066 Test: 256.70801
Epoch 200: New minimal relative error: 58.28%, model saved.
Epoch: 200 Train: 38.15134 Test: 52.01749
Epoch 300: New minimal relative error: 31.98%, model saved.
Epoch: 300 Train: 11.60498 Test: 13.50712
Epoch: 400 Train: 18.82882 Test: 29.19684
Epoch: 500 Train: 7.30805 Test: 8.05937
Epoch 600: New minimal relative error: 22.89%, model saved.
Epoch: 600 Train: 36.62556 Test: 32.70577
Epoch 700: New minimal relative error: 21.95%, model saved.
Epoch: 700 Train: 11.80271 Test: 12.32155
Epoch: 800 Train: 12.33861 Test: 9.02127
Epoch: 900 Train: 4.05080 Test: 4.96963
Epoch 1000: New minimal relative error: 20.87%, model saved.
Epoch: 1000 Train: 2.65827 Test: 3.07971
Epoch: 1100 Train: 2.87722 Test: 2.67266
Epoch 1200: New minimal relative error: 19.47%, model saved.
Epoch: 1200 Train: 4.29669 Test: 4.12388
Epoch 1300: New minimal relative error: 18.60%, model saved.
Epoch: 1300 Train: 7.51952 Test: 6.69627
Epoch: 1400 Train: 6.93040 Test: 8.68469
Epoch: 1500 Train: 8.88611 Test: 2.66304
Epoch: 1600 Train: 2.00428 Test: 2.31895
Epoch: 1700 Train: 1.23933 Test: 1.33360
Epoch: 1800 Train: 4.40964 Test: 3.48679
Epoch 1900: New minimal relative error: 17.13%, model saved.
Epoch: 1900 Train: 1.53494 Test: 1.92441
Epoch: 2000 Train: 1.10465 Test: 1.18116
Epoch: 2100 Train: 1.32135 Test: 1.53179
Epoch: 2200 Train: 1.42576 Test: 1.27891
Epoch: 2300 Train: 2.55914 Test: 3.07747
Epoch: 2400 Train: 1.95617 Test: 2.30143
Epoch: 2500 Train: 2.63411 Test: 3.34990
Epoch 2600: New minimal relative error: 16.89%, model saved.
Epoch: 2600 Train: 4.68426 Test: 3.88978
Epoch: 2700 Train: 5.45259 Test: 4.38088
Epoch: 2800 Train: 1.88096 Test: 2.29861
Epoch 2900: New minimal relative error: 13.71%, model saved.
Epoch: 2900 Train: 0.71382 Test: 0.78022
Epoch: 3000 Train: 1.96740 Test: 1.86134
Epoch: 3100 Train: 2.27223 Test: 1.44233
Epoch: 3200 Train: 2.13122 Test: 2.07128
Epoch: 3300 Train: 0.77150 Test: 0.95877
Epoch: 3400 Train: 1.45168 Test: 1.93118
Epoch: 3500 Train: 3.62359 Test: 3.27750
Epoch: 3600 Train: 0.46689 Test: 0.61809
Epoch: 3700 Train: 1.60592 Test: 1.41114
Epoch: 3800 Train: 0.71695 Test: 0.59209
Epoch: 3900 Train: 7.27297 Test: 5.29671
Epoch: 4000 Train: 0.94627 Test: 1.16716
Epoch: 4100 Train: 0.39811 Test: 0.45227
Epoch: 4200 Train: 0.70639 Test: 0.87315
Epoch: 4300 Train: 1.22214 Test: 1.11403
Epoch: 4400 Train: 0.64147 Test: 0.52479
Epoch: 4500 Train: 2.15944 Test: 2.60111
Epoch: 4600 Train: 1.58553 Test: 1.83584
Epoch: 4700 Train: 0.65300 Test: 0.75649
Epoch: 4800 Train: 0.42440 Test: 0.53545
Epoch: 4900 Train: 1.90178 Test: 2.31090
Epoch: 5000 Train: 0.24839 Test: 0.31212
Epoch: 5100 Train: 0.82453 Test: 0.66921
Epoch: 5200 Train: 0.34956 Test: 0.46295
Epoch: 5300 Train: 0.49942 Test: 0.56431
Epoch 5400: New minimal relative error: 9.65%, model saved.
Epoch: 5400 Train: 0.34143 Test: 0.31340
Epoch: 5500 Train: 0.19725 Test: 0.25942
Epoch: 5600 Train: 0.19222 Test: 0.25417
Epoch: 5700 Train: 0.20459 Test: 0.26381
Epoch: 5800 Train: 0.17982 Test: 0.24034
Epoch: 5900 Train: 0.22915 Test: 0.35291
Epoch: 6000 Train: 0.60259 Test: 0.60131
Epoch: 6100 Train: 0.56396 Test: 0.42646
Epoch: 6200 Train: 3.55046 Test: 3.21247
Epoch: 6300 Train: 0.44866 Test: 0.61498
Epoch: 6400 Train: 0.24763 Test: 0.39990
Epoch: 6500 Train: 0.57535 Test: 0.44953
Epoch: 6600 Train: 0.15886 Test: 0.21237
Epoch: 6700 Train: 0.19126 Test: 0.25990
Epoch: 6800 Train: 0.34231 Test: 0.28858
Epoch: 6900 Train: 0.23280 Test: 0.28146
Epoch: 7000 Train: 0.13892 Test: 0.18714
Epoch: 7100 Train: 0.13838 Test: 0.18545
Epoch: 7200 Train: 0.13685 Test: 0.18093
Epoch: 7300 Train: 0.13840 Test: 0.18504
Epoch: 7400 Train: 0.13644 Test: 0.19077
Epoch: 7500 Train: 0.24157 Test: 0.34384
Epoch: 7600 Train: 0.17015 Test: 0.28389
Epoch: 7700 Train: 0.26869 Test: 0.26019
Epoch: 7800 Train: 0.78500 Test: 1.01759
Epoch: 7900 Train: 0.14570 Test: 0.19680
Epoch: 8000 Train: 0.27970 Test: 0.35774
Epoch: 8100 Train: 0.13110 Test: 0.17620
Epoch: 8200 Train: 0.12251 Test: 0.16463
Epoch: 8300 Train: 0.12370 Test: 0.16075
Epoch: 8400 Train: 0.15126 Test: 0.16290
Epoch: 8500 Train: 0.48726 Test: 0.31016
Epoch: 8600 Train: 0.10710 Test: 0.14666
Epoch: 8700 Train: 0.11172 Test: 0.15118
Epoch: 8800 Train: 0.10429 Test: 0.14309
Epoch: 8900 Train: 0.10554 Test: 0.14415
Epoch: 9000 Train: 0.10167 Test: 0.13950
Epoch: 9100 Train: 0.10073 Test: 0.13848
Epoch: 9200 Train: 0.25023 Test: 0.28898
Epoch: 9300 Train: 0.27672 Test: 0.37769
Epoch: 9400 Train: 0.09688 Test: 0.13342
Epoch: 9500 Train: 0.10890 Test: 0.14779
Epoch: 9600 Train: 0.11820 Test: 0.16577
Epoch: 9700 Train: 0.09364 Test: 0.12915
Epoch: 9800 Train: 0.09592 Test: 0.13228
Epoch: 9900 Train: 0.09336 Test: 0.12883
Epoch: 9999 Train: 0.09102 Test: 0.12547
Training Loss: tensor(0.0910)
Test Loss: tensor(0.1255)
Learned LE: [ 0.7593595   0.01610933 -2.5609338 ]
True LE: [ 8.7897575e-01 -3.8395475e-03 -1.4552708e+01]
Relative Error: [20.095135  19.996214  19.78405   19.530981  19.222393  18.758215
 17.929817  16.868483  16.456537  16.000633  15.602641  15.24982
 14.8077545 14.683214  14.786831  15.247644  15.667836  16.14476
 16.803902  17.576065  18.256607  18.818922  19.34877   19.860495
 20.319613  20.619879  20.813112  20.916864  20.858875  20.928932
 21.039894  20.651814  19.823458  18.584143  18.160625  18.05698
 18.18215   18.122663  17.840021  17.928782  17.657526  17.18172
 16.813673  16.844501  17.095257  17.087328  17.014507  16.99396
 17.007536  16.980703  17.039797  17.140785  17.259941  17.453295
 17.580046  17.657822  17.719816  17.805288  18.090662  18.336132
 18.567753  18.775906  18.94803   18.895565  18.61224   18.237942
 17.906977  17.377722  16.443817  15.456     15.026421  14.527401
 14.337817  13.972185  13.678832  13.70236   14.056866  14.6560755
 15.131131  15.6829405 16.389084  17.176678  17.8672    18.489466
 19.063793  19.646889  20.04124   20.313417  20.565685  20.603367
 20.487597  20.361717  20.387922  19.88604   18.977896  17.81032
 17.384335  17.122847  17.203112  16.9316    16.980587  16.907797
 16.523022  16.153925  15.929598  16.085596  16.202938  16.177132
 16.061235  16.149742  16.171997  16.233952  16.361183  16.44044
 16.486368  16.702673  16.889172  16.99777   17.042082  16.956871
 17.112495  17.320337  17.536526  17.723637  17.857468  17.85532
 17.514994  17.018833  16.65981   16.074123  15.0229645 14.128401
 13.612366  13.241232  13.022574  12.710011  12.663672  12.834122
 13.497345  14.129055  14.635187  15.102313  15.834679  16.580122
 17.235634  17.967497  18.69543   19.392982  19.777887  20.083023
 20.251814  20.279821  20.076971  19.827845  19.729713  19.135515
 18.151167  16.978743  16.627493  16.227846  16.248348  15.949653
 16.187454  15.927151  15.507862  15.232023  15.169823  15.3628435
 15.39219   15.19956   15.199547  15.357595  15.420206  15.565075
 15.689248  15.813483  15.8644705 16.046406  16.294233  16.182442
 16.014008  15.8264265 15.909383  16.255621  16.471478  16.720564
 16.848104  16.776564  16.495798  15.890184  15.484598  14.822298
 13.6902075 12.867172  12.25133   12.054201  11.820845  11.7064085
 11.857954  12.422844  13.070251  13.657869  13.968004  14.471245
 15.225634  15.872732  16.619255  17.339458  18.131151  18.776043
 19.372974  19.778921  19.935305  19.874052  19.660898  19.358461
 19.06395   18.41008   17.345242  16.156588  15.841222  15.450237
 15.269022  15.200493  15.314035  14.903424  14.6150055 14.367443
 14.44655   14.520478  14.410128  14.298349  14.396403  14.599789
 14.736776  14.857453  15.04053   15.1712    15.2705555 15.119529
 14.998902  14.833715  14.605008  14.316046  14.214084  14.569855
 15.084018  15.508668  15.742366  15.791922  15.531063  14.892528
 14.390519  13.645756  12.452492  11.670238  11.110295  10.913509
 10.887242  10.897773  11.203253  11.933827  12.629071  12.9942875
 13.363354  13.784387  14.398638  14.924896  15.714534  16.59516
 17.493145  18.162512  18.800337  19.346401  19.5119    19.465666
 19.202131  18.781277  18.391048  17.715366  16.562235  15.342179
 14.984359  14.69862   14.451594  14.458887  14.428349  13.939468
 13.730915  13.573991  13.697526  13.7020855 13.480936  13.461609
 13.625712  13.841699  13.977898  14.15502   14.393987  14.523794
 14.175564  13.8136835 13.726372  13.458721  13.125703  12.812945
 12.618772  12.9035845 13.386902  14.02434   14.527817  14.587946
 14.603848  13.992999  13.366501  12.563694  11.346875  10.523557
 10.121559  10.00459    9.999413  10.121499  10.70199   11.4337435
 11.986193  12.34266   12.53606   12.601595  12.86091   13.399898
 14.145318  15.079339  16.11346   17.065304  18.099005  18.678934
 19.038708  18.964924  18.702694  18.241238  17.805119  16.994589
 15.807072  14.564131  14.145279  13.972296  13.703428  13.720819
 13.576135  13.099662  12.888678  12.854653  12.96471   12.767837
 12.609395  12.641849  12.83345   13.093654  13.236801  13.395853
 13.61594   13.248132  12.82019   12.53434   12.293429  12.000079
 11.735401  11.4356365 11.207966  11.362554  11.8037815 12.378643
 13.111141  13.434993  13.390878  13.120694  12.395742  11.59104
 10.364735   9.567429   9.340042   9.136368   9.115355   9.364393
 10.139031  10.789553  11.26922   11.597094  11.276315  11.342431
 11.607958  12.301937  13.047731  13.970394  14.826647  15.667227
 16.810797  17.772943  18.309858  18.40918  ]
