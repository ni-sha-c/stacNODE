time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.03%, model saved.
Epoch: 0 Train: 60549.35156 Test: 4211.57910
Epoch 80: New minimal relative error: 86.46%, model saved.
Epoch: 80 Train: 16750.48438 Test: 1698.40271
Epoch 160: New minimal relative error: 79.16%, model saved.
Epoch: 160 Train: 16159.98926 Test: 1596.16455
Epoch: 240 Train: 14890.65918 Test: 1414.92126
Epoch: 320 Train: 15688.95312 Test: 1389.50110
Epoch 400: New minimal relative error: 72.00%, model saved.
Epoch: 400 Train: 14387.64160 Test: 1318.59644
Epoch: 480 Train: 15881.82227 Test: 1373.78540
Epoch: 560 Train: 15289.39648 Test: 1375.90857
Epoch: 640 Train: 15046.48145 Test: 1490.05176
Epoch: 720 Train: 14213.00391 Test: 1296.07947
Epoch 800: New minimal relative error: 68.10%, model saved.
Epoch: 800 Train: 12847.60645 Test: 1124.54224
Epoch: 880 Train: 12024.91309 Test: 896.97723
Epoch: 960 Train: 9166.09668 Test: 608.39655
Epoch: 1040 Train: 6851.21973 Test: 343.82309
Epoch: 1120 Train: 4523.17822 Test: 144.19727
Epoch 1200: New minimal relative error: 63.93%, model saved.
Epoch: 1200 Train: 2431.42456 Test: 53.42364
Epoch 1280: New minimal relative error: 20.26%, model saved.
Epoch: 1280 Train: 1754.84290 Test: 26.63882
Epoch: 1360 Train: 1203.07129 Test: 15.83078
Epoch 1440: New minimal relative error: 18.60%, model saved.
Epoch: 1440 Train: 926.20837 Test: 11.87126
Epoch: 1520 Train: 745.49646 Test: 6.06888
Epoch: 1600 Train: 630.25647 Test: 4.82030
Epoch 1680: New minimal relative error: 13.29%, model saved.
Epoch: 1680 Train: 545.32489 Test: 4.28816
Epoch 1760: New minimal relative error: 11.81%, model saved.
Epoch: 1760 Train: 480.02057 Test: 3.42691
Epoch 1840: New minimal relative error: 10.60%, model saved.
Epoch: 1840 Train: 431.89658 Test: 2.83611
Epoch 1920: New minimal relative error: 9.23%, model saved.
Epoch: 1920 Train: 372.20917 Test: 1.99189
Epoch: 2000 Train: 349.32193 Test: 2.24587
Epoch: 2080 Train: 332.95499 Test: 1.92829
Epoch: 2160 Train: 344.22189 Test: 1.85940
Epoch: 2240 Train: 332.79434 Test: 2.57729
Epoch: 2320 Train: 363.34195 Test: 4.23767
Epoch: 2400 Train: 321.75189 Test: 2.49819
Epoch: 2480 Train: 308.12341 Test: 2.07852
Epoch 2560: New minimal relative error: 7.70%, model saved.
Epoch: 2560 Train: 283.34741 Test: 1.57811
Epoch 2640: New minimal relative error: 6.88%, model saved.
Epoch: 2640 Train: 277.33792 Test: 1.70998
Epoch: 2720 Train: 283.11166 Test: 1.76197
Epoch: 2800 Train: 268.51669 Test: 1.17175
Epoch: 2880 Train: 256.62375 Test: 1.11178
Epoch: 2960 Train: 237.79918 Test: 1.64366
Epoch: 3040 Train: 233.29581 Test: 1.39419
Epoch: 3120 Train: 221.03784 Test: 1.69052
Epoch: 3200 Train: 214.54045 Test: 1.28845
Epoch: 3280 Train: 210.50055 Test: 1.30261
Epoch: 3360 Train: 203.46481 Test: 1.04331
Epoch: 3440 Train: 199.51550 Test: 0.99993
Epoch: 3520 Train: 198.09970 Test: 1.20541
Epoch: 3600 Train: 202.32411 Test: 1.16400
Epoch 3680: New minimal relative error: 6.32%, model saved.
Epoch: 3680 Train: 202.19736 Test: 1.00014
Epoch: 3760 Train: 193.47903 Test: 0.99483
Epoch: 3840 Train: 183.48338 Test: 0.73728
Epoch: 3920 Train: 166.83121 Test: 0.40890
Epoch: 4000 Train: 161.66090 Test: 0.45553
Epoch: 4080 Train: 176.64014 Test: 1.20921
Epoch 4160: New minimal relative error: 6.28%, model saved.
Epoch: 4160 Train: 176.28661 Test: 0.77803
Epoch: 4240 Train: 166.13420 Test: 0.77563
Epoch 4320: New minimal relative error: 3.81%, model saved.
Epoch: 4320 Train: 151.36006 Test: 0.49644
Epoch: 4400 Train: 156.00723 Test: 0.54985
Epoch: 4480 Train: 158.70517 Test: 0.54284
Epoch: 4560 Train: 155.52829 Test: 0.59464
Epoch: 4640 Train: 153.89761 Test: 0.56341
Epoch: 4720 Train: 154.92529 Test: 0.59632
Epoch: 4800 Train: 155.17198 Test: 0.60970
Epoch: 4880 Train: 149.14211 Test: 0.56634
Epoch: 4960 Train: 145.94984 Test: 0.51608
Epoch: 5040 Train: 146.32224 Test: 0.47712
Epoch: 5120 Train: 148.58231 Test: 0.50416
Epoch: 5200 Train: 153.81258 Test: 0.74886
Epoch: 5280 Train: 149.89767 Test: 0.75031
Epoch: 5360 Train: 159.43713 Test: 0.70045
Epoch: 5440 Train: 153.43387 Test: 0.48020
Epoch: 5520 Train: 159.88620 Test: 0.49052
Epoch: 5600 Train: 150.71736 Test: 0.47945
Epoch: 5680 Train: 155.66734 Test: 0.56622
Epoch: 5760 Train: 159.25276 Test: 0.85196
Epoch: 5840 Train: 163.01662 Test: 0.82991
Epoch: 5920 Train: 154.32315 Test: 0.56402
Epoch: 6000 Train: 151.46399 Test: 0.49901
Epoch: 6080 Train: 145.89746 Test: 0.43355
Epoch: 6160 Train: 151.36247 Test: 0.56306
Epoch: 6240 Train: 155.16682 Test: 0.77275
Epoch: 6320 Train: 156.89149 Test: 0.73769
Epoch: 6400 Train: 150.62970 Test: 0.78632
Epoch: 6480 Train: 161.00287 Test: 0.60940
Epoch: 6560 Train: 148.62263 Test: 0.48053
Epoch: 6640 Train: 145.23424 Test: 0.54125
Epoch: 6720 Train: 145.05890 Test: 0.48657
Epoch: 6800 Train: 147.74094 Test: 0.61898
Epoch: 6880 Train: 143.79213 Test: 0.64102
Epoch: 6960 Train: 146.32629 Test: 0.51506
Epoch: 7040 Train: 142.90352 Test: 0.55594
Epoch: 7120 Train: 147.19102 Test: 0.54504
Epoch: 7200 Train: 138.27945 Test: 0.39117
Epoch: 7280 Train: 138.89386 Test: 0.48688
Epoch: 7360 Train: 145.75679 Test: 0.65110
Epoch: 7440 Train: 172.40602 Test: 0.96399
Epoch: 7520 Train: 147.04700 Test: 0.74371
Epoch: 7600 Train: 139.66229 Test: 0.39674
Epoch: 7680 Train: 136.18840 Test: 0.47154
Epoch: 7760 Train: 142.28223 Test: 0.44494
Epoch: 7840 Train: 132.59662 Test: 0.33932
Epoch: 7920 Train: 135.83900 Test: 0.43021
Epoch: 7999 Train: 135.48271 Test: 0.40723
Training Loss: tensor(135.4827)
Test Loss: tensor(0.4072)
Learned LE: [ 8.4208810e-01  5.6541269e-03 -1.4538367e+01]
True LE: [ 8.6603349e-01 -2.2108555e-03 -1.4551199e+01]
Relative Error: [3.0702493  3.6324317  4.13779    4.7653413  5.0973186  5.3224382
 4.9464874  4.8160534  4.8668847  5.2448835  5.5282617  5.787115
 6.3424788  6.470752   6.8416486  7.4956765  7.5783224  7.5316615
 7.4794188  7.118355   6.555572   5.8204136  4.8299603  3.9525776
 3.1741614  2.8593798  2.9797356  3.22671    3.3880796  3.3598323
 3.3045192  2.8897648  2.4676697  2.5253382  3.2051783  4.018858
 4.9516068  5.4960327  5.4358964  5.3309903  5.265931   5.1615906
 4.9208016  4.13655    3.6671333  3.2250817  3.3781507  3.5729647
 3.6805105  3.646073   3.6383495  3.938844   4.0333858  3.9766598
 3.7635353  3.3903654  3.5734274  3.6562276  3.6437466  3.637321
 3.4841623  2.8206651  2.5602942  3.1367838  3.5621963  4.0868897
 4.611187   4.8552036  4.6927843  4.454196   4.4339643  4.7186484
 5.033396   5.17409    5.4344215  5.8135185  6.1258736  6.7337847
 7.1378403  7.1374526  7.099557   6.8078785  6.1187277  5.229939
 4.5461397  3.7708714  3.0310588  2.5038884  2.4455607  2.711726
 2.8321424  2.950141   2.9625812  2.5398145  2.119786   2.0448275
 2.7047098  3.5233326  4.4713116  5.1381435  5.0700355  5.001287
 5.036383   5.003942   4.7386675  3.856443   3.3685858  2.8948486
 3.0936296  3.3621411  3.5184045  3.502865   3.5443027  3.8001525
 3.8414388  3.722873   3.4975762  3.320029   3.4278977  3.4582767
 3.4140062  3.2857013  3.1999905  2.7141044  2.1646013  2.5864446
 3.1124241  3.5190277  4.0093384  4.346566   4.547583   4.180325
 4.0568056  4.224056   4.61408    4.5210133  4.538386   4.9744616
 5.432141   5.964136   6.774399   6.701244   6.747781   6.585827
 5.641519   4.735963   4.0755954  3.6006625  2.9917614  2.323489
 2.067036   2.125692   2.3095193  2.3578954  2.4562292  2.188616
 1.756574   1.5656203  2.101322   2.9138575  3.876957   4.711043
 4.6953945  4.680994   4.746188   4.782312   4.586854   3.801269
 3.0385003  2.6120558  2.8078058  3.1816359  3.3996277  3.4079242
 3.422812   3.7061996  3.7100651  3.5185118  3.3000653  3.3642843
 3.4701958  3.4129713  3.2737777  3.0823076  2.7972403  2.4780028
 1.9187983  2.0676553  2.6982744  2.9097335  3.3281782  3.7896042
 3.9660323  4.001593   3.7787092  3.7972279  4.065381   3.9026697
 3.7249236  4.0142736  4.782875   5.3614326  6.0611587  6.3696566
 6.4816074  6.365719   5.3215756  4.3769464  3.7061083  3.2914805
 2.938651   2.4019809  1.9797126  2.0588605  2.347658   2.3915534
 2.39986    2.1174204  1.394769   1.0918807  1.3896931  2.1624255
 3.1339114  4.089112   4.277044   4.3412137  4.442433   4.499292
 4.460658   3.7512116  2.9086049  2.4196832  2.5091124  3.0053577
 3.3057542  3.3585758  3.2942648  3.573609   3.7007139  3.6881075
 3.3929691  3.4298089  3.583996   3.5064123  3.2912598  3.0037055
 2.6927407  2.2674003  1.8220371  1.5551735  2.1537476  2.4204113
 2.6482403  3.0497704  3.5215638  3.6901646  3.538588   3.3849287
 3.574858   3.5261757  3.2225888  3.4095254  4.0023694  4.927163
 5.539073   6.36423    6.311156   6.183124   5.250615   4.2281675
 3.454035   3.0279307  2.7617595  2.4700294  2.2672632  2.3450217
 2.4727938  2.7260766  2.720887   2.6483014  1.9729458  1.0882494
 0.75448537 1.2525544  2.203039   3.240949   3.7653348  3.9325728
 4.1129274  4.1633162  4.230984   3.7180684  3.0552247  2.2823806
 2.1411653  2.7892435  3.197233   3.3016105  3.15877    3.476788
 3.848822   3.9574158  3.701157   3.6601684  3.8016229  3.635849
 3.3896008  3.0520298  2.6907778  2.5206475  2.418854   2.0328386
 1.337787   1.8481746  2.1826732  2.4991128  2.914544   3.3317246
 3.4283571  3.2271824  3.0548673  3.0153012  3.0083644  2.9723706
 3.2911322  3.965574   4.9095783  5.5507407  6.054627   5.947091
 5.498072   4.4218583  3.4042466  2.8520453  2.6117337  2.4132435
 2.4660406  2.5712829  2.7930145  2.8643425  3.0163352  3.0122201
 2.7878954  1.750504   0.8494079  0.4908564  1.0493757  2.1634748
 3.0024745  3.3780072  3.6912076  3.834225   3.847242   3.6628582
 3.2244842  2.6619413  2.0070813  2.2862692  2.8080792  3.041038
 2.9825926  3.307107   3.8003962  4.0571175  4.04012    3.935592
 4.028639   3.9759648  3.7607324  3.4358115  3.090764   3.2189753
 3.41667    3.1272519  2.5289674  1.4389458  1.1787076  1.8087212
 2.3112023  2.7111592  3.1248648  3.1719573  2.9994245  2.78704
 2.6573148  2.485623   2.7896209  3.0651896  3.6437056  4.4728675
 5.161953   5.5712876  5.267279   4.63736    3.5900342  2.7217941
 2.4536138  2.2005646  2.217955   2.5969188 ]
