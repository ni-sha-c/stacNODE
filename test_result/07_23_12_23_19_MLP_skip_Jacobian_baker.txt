time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 256
n_layers: 4
reg_param: 1000.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 31.189483643 Test: 26.913028717
Epoch 0: New minimal relative error: 26.91%, model saved.
Epoch: 100 Train: 7.738353729 Test: 7.810284138
Epoch 100: New minimal relative error: 7.81%, model saved.
Epoch: 200 Train: 7.170383453 Test: 7.236675739
Epoch 200: New minimal relative error: 7.24%, model saved.
Epoch: 300 Train: 7.202620506 Test: 7.277011395
Epoch: 400 Train: 7.326471329 Test: 7.332533836
Epoch: 500 Train: 7.199462891 Test: 7.234845638
Epoch 500: New minimal relative error: 7.23%, model saved.
Epoch: 600 Train: 7.359670639 Test: 7.366620541
Epoch: 700 Train: 7.235596180 Test: 7.280999184
Epoch: 800 Train: 7.146704197 Test: 7.232739449
Epoch 800: New minimal relative error: 7.23%, model saved.
Epoch: 900 Train: 7.140531063 Test: 7.225465775
Epoch 900: New minimal relative error: 7.23%, model saved.
Epoch: 1000 Train: 7.203613758 Test: 7.260596275
Epoch: 1100 Train: 7.134810448 Test: 7.246918678
Epoch: 1200 Train: 7.156949997 Test: 7.249816895
Epoch: 1300 Train: 7.186776638 Test: 7.278908253
Epoch: 1400 Train: 7.191156387 Test: 7.296637535
Epoch: 1500 Train: 7.164735794 Test: 7.252303123
Epoch: 1600 Train: 7.158838272 Test: 7.270029068
Epoch: 1700 Train: 7.196828842 Test: 7.334834576
Epoch: 1800 Train: 7.199544907 Test: 7.288464546
Epoch: 1900 Train: 7.187967300 Test: 7.300319672
Epoch: 2000 Train: 7.131270409 Test: 7.303294182
Epoch: 2100 Train: 7.171724319 Test: 7.327986717
Epoch: 2200 Train: 7.198480606 Test: 7.341397285
Epoch: 2300 Train: 7.178137779 Test: 7.284856319
Epoch: 2400 Train: 7.160526276 Test: 7.272500992
Epoch: 2500 Train: 7.134029388 Test: 7.274571419
Epoch: 2600 Train: 7.141507149 Test: 7.305025101
Epoch: 2700 Train: 7.183679581 Test: 7.337182045
Epoch: 2800 Train: 7.180565834 Test: 7.330013275
Epoch: 2900 Train: 7.201904297 Test: 7.325845718
Epoch: 3000 Train: 7.217868805 Test: 7.344785690
Epoch: 3100 Train: 7.224873543 Test: 7.353199959
Epoch: 3200 Train: 7.275059700 Test: 7.385643959
Epoch: 3300 Train: 7.227837563 Test: 7.338293076
Epoch: 3400 Train: 7.211128235 Test: 7.330298424
Epoch: 3500 Train: 7.196984291 Test: 7.344541550
Epoch: 3600 Train: 7.199394226 Test: 7.357972145
Epoch: 3700 Train: 7.213504791 Test: 7.388242722
Epoch: 3800 Train: 7.217110634 Test: 7.388223648
Epoch: 3900 Train: 7.211845398 Test: 7.389283657
Epoch: 4000 Train: 7.213574409 Test: 7.389950752
Epoch: 4100 Train: 7.181219101 Test: 7.382176399
Epoch: 4200 Train: 7.167849541 Test: 7.395827293
Epoch: 4300 Train: 7.161845684 Test: 7.415264130
Epoch: 4400 Train: 7.183731079 Test: 7.447381020
Epoch: 4500 Train: 7.184723854 Test: 7.462285042
Epoch: 4600 Train: 7.199896812 Test: 7.467760563
Epoch: 4700 Train: 7.222874165 Test: 7.410120010
Epoch: 4800 Train: 7.219926357 Test: 7.491315842
Epoch: 4900 Train: 7.225433826 Test: 7.493906975
Epoch: 5000 Train: 7.226091385 Test: 7.489661217
Epoch: 5100 Train: 7.229894161 Test: 7.492514133
Epoch: 5200 Train: 7.242785454 Test: 7.499103546
Epoch: 5300 Train: 7.256877899 Test: 7.528234005
Epoch: 5400 Train: 7.257822037 Test: 7.487078667
Epoch: 5500 Train: 7.251488209 Test: 7.557595253
Epoch: 5600 Train: 7.248596191 Test: 7.524596214
Epoch: 5700 Train: 7.236938000 Test: 7.384941101
Epoch: 5800 Train: 7.253995895 Test: 7.402770996
Epoch: 5900 Train: 7.227921486 Test: 7.415711403
Epoch: 6000 Train: 7.180356026 Test: 7.363270283
Epoch: 6100 Train: 7.138875008 Test: 7.360215187
Epoch: 6200 Train: 7.138255119 Test: 7.433491707
Epoch: 6300 Train: 7.144179344 Test: 7.461258888
Epoch: 6400 Train: 7.166892052 Test: 7.485105991
Epoch: 6500 Train: 7.184291363 Test: 7.386284828
Epoch: 6600 Train: 7.195774078 Test: 7.419583797
Epoch: 6700 Train: 7.214610577 Test: 7.404476166
Epoch: 6800 Train: 7.212078571 Test: 7.366628170
Epoch: 6900 Train: 7.224331856 Test: 7.393795013
Epoch: 7000 Train: 7.236647606 Test: 7.436343193
Epoch: 7100 Train: 7.389079094 Test: 7.429682732
Epoch: 7200 Train: 7.300734043 Test: 7.369382858
Epoch: 7300 Train: 7.300667763 Test: 7.391096592
Epoch: 7400 Train: 7.275444031 Test: 7.416580200
Epoch: 7500 Train: 7.257007599 Test: 7.434562683
Epoch: 7600 Train: 7.231266022 Test: 7.416829109
Epoch: 7700 Train: 7.211200714 Test: 7.398511410
Epoch: 7800 Train: 7.198020935 Test: 7.339811325
Epoch: 7900 Train: 7.182763100 Test: 7.333292484
Epoch: 8000 Train: 7.166264534 Test: 7.316154480
Epoch: 8100 Train: 7.157532692 Test: 7.367828369
Epoch: 8200 Train: 7.149851799 Test: 7.333619118
Epoch: 8300 Train: 7.157890320 Test: 7.340747833
Epoch: 8400 Train: 7.159728050 Test: 7.355710983
Epoch: 8500 Train: 7.161948204 Test: 7.368311882
Epoch: 8600 Train: 7.164438248 Test: 7.367244244
Epoch: 8700 Train: 7.158462524 Test: 7.367467880
Epoch: 8800 Train: 7.163578987 Test: 7.386283398
Epoch: 8900 Train: 7.166656971 Test: 7.372115612
Epoch: 9000 Train: 7.181280136 Test: 7.373148441
Epoch: 9100 Train: 7.186310768 Test: 7.373569489
Epoch: 9200 Train: 7.198993683 Test: 7.391738892
Epoch: 9300 Train: 7.203749657 Test: 7.398773193
Epoch: 9400 Train: 7.222698212 Test: 7.427669048
Epoch: 9500 Train: 7.226329803 Test: 7.410938740
Epoch: 9600 Train: 7.230951786 Test: 7.413465023
Epoch: 9700 Train: 7.229221344 Test: 7.407979012
Epoch: 9800 Train: 7.212970734 Test: 7.396168232
Epoch: 9900 Train: 7.205190182 Test: 7.386008263
Epoch: 9999 Train: 7.196397305 Test: 7.384354591
Training Loss: tensor(7.1964)
Test Loss: tensor(7.3844)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0030)
Jacobian term Test Loss: tensor(0.0032)
Learned LE: [12.153948   1.2686797]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
