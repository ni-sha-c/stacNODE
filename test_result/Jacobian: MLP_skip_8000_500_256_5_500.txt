time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.06%, model saved.
Epoch: 0 Train: 31395.38477 Test: 3880.97437
Epoch: 80 Train: 8775.74121 Test: 1462.14246
Epoch 160: New minimal relative error: 90.07%, model saved.
Epoch: 160 Train: 8369.61523 Test: 1358.24683
Epoch: 240 Train: 7713.44678 Test: 1108.50671
Epoch: 320 Train: 7095.66553 Test: 1063.13928
Epoch 400: New minimal relative error: 66.61%, model saved.
Epoch: 400 Train: 7059.18604 Test: 1163.85522
Epoch: 480 Train: 7445.19629 Test: 1120.73218
Epoch 560: New minimal relative error: 60.01%, model saved.
Epoch: 560 Train: 5890.68262 Test: 986.46136
Epoch: 640 Train: 4055.50537 Test: 394.39456
Epoch: 720 Train: 2428.44189 Test: 167.45726
Epoch 800: New minimal relative error: 38.61%, model saved.
Epoch: 800 Train: 1607.20032 Test: 76.21794
Epoch 880: New minimal relative error: 26.20%, model saved.
Epoch: 880 Train: 974.27582 Test: 38.94936
Epoch 960: New minimal relative error: 25.67%, model saved.
Epoch: 960 Train: 841.16547 Test: 34.85353
Epoch: 1040 Train: 668.61743 Test: 20.48575
Epoch: 1120 Train: 633.03278 Test: 54.88948
Epoch 1200: New minimal relative error: 24.05%, model saved.
Epoch: 1200 Train: 546.98077 Test: 21.45466
Epoch 1280: New minimal relative error: 18.42%, model saved.
Epoch: 1280 Train: 489.15207 Test: 13.26810
Epoch: 1360 Train: 458.59415 Test: 12.63165
Epoch: 1440 Train: 443.36322 Test: 22.64263
Epoch: 1520 Train: 442.29352 Test: 11.25734
Epoch 1600: New minimal relative error: 6.00%, model saved.
Epoch: 1600 Train: 374.85318 Test: 8.86760
Epoch: 1680 Train: 333.35287 Test: 7.50070
Epoch: 1760 Train: 314.40024 Test: 6.66515
Epoch: 1840 Train: 312.62848 Test: 11.41977
Epoch: 1920 Train: 276.09119 Test: 5.29376
Epoch: 2000 Train: 267.72205 Test: 5.24560
Epoch 2080: New minimal relative error: 3.14%, model saved.
Epoch: 2080 Train: 251.63882 Test: 4.57609
Epoch: 2160 Train: 253.37491 Test: 15.33138
Epoch: 2240 Train: 217.66441 Test: 5.54073
Epoch: 2320 Train: 209.83015 Test: 10.04493
Epoch: 2400 Train: 200.15533 Test: 10.69464
Epoch: 2480 Train: 215.41124 Test: 7.12239
Epoch: 2560 Train: 181.49196 Test: 2.33401
Epoch: 2640 Train: 189.28754 Test: 6.88199
Epoch: 2720 Train: 173.77475 Test: 2.20226
Epoch: 2800 Train: 172.51675 Test: 2.36849
Epoch: 2880 Train: 181.32109 Test: 11.86673
Epoch: 2960 Train: 170.81000 Test: 5.14774
Epoch: 3040 Train: 166.32329 Test: 2.07386
Epoch: 3120 Train: 163.51961 Test: 1.88760
Epoch: 3200 Train: 156.99670 Test: 2.17192
Epoch: 3280 Train: 146.64427 Test: 1.87781
Epoch: 3360 Train: 138.21230 Test: 1.44072
Epoch: 3440 Train: 133.01494 Test: 1.39058
Epoch: 3520 Train: 130.37013 Test: 1.31893
Epoch: 3600 Train: 125.62132 Test: 1.19301
Epoch: 3680 Train: 121.43376 Test: 1.12879
Epoch: 3760 Train: 121.37779 Test: 1.38943
Epoch: 3840 Train: 119.14729 Test: 1.86890
Epoch: 3920 Train: 119.08658 Test: 1.05698
Epoch: 4000 Train: 115.71769 Test: 1.05150
Epoch: 4080 Train: 115.44556 Test: 1.81782
Epoch: 4160 Train: 112.79224 Test: 1.05623
Epoch: 4240 Train: 114.75298 Test: 1.18626
Epoch 4320: New minimal relative error: 2.88%, model saved.
Epoch: 4320 Train: 117.90237 Test: 1.19197
Epoch: 4400 Train: 122.31950 Test: 1.39057
Epoch: 4480 Train: 125.48825 Test: 4.25105
Epoch: 4560 Train: 117.41018 Test: 1.13303
Epoch: 4640 Train: 113.25137 Test: 1.75016
Epoch: 4720 Train: 110.41019 Test: 0.94905
Epoch: 4800 Train: 107.84685 Test: 0.98623
Epoch: 4880 Train: 114.87715 Test: 1.01401
Epoch: 4960 Train: 114.37666 Test: 1.05385
Epoch: 5040 Train: 116.00131 Test: 1.15220
Epoch: 5120 Train: 114.58884 Test: 1.08064
Epoch: 5200 Train: 115.52514 Test: 1.46596
Epoch: 5280 Train: 117.12523 Test: 1.17461
Epoch: 5360 Train: 113.52584 Test: 1.09693
Epoch: 5440 Train: 107.36530 Test: 1.18419
Epoch: 5520 Train: 104.62951 Test: 2.17758
Epoch: 5600 Train: 104.22862 Test: 1.15254
Epoch 5680: New minimal relative error: 2.71%, model saved.
Epoch: 5680 Train: 100.05943 Test: 0.88498
Epoch: 5760 Train: 97.93835 Test: 1.20365
Epoch: 5840 Train: 104.28794 Test: 8.58371
Epoch: 5920 Train: 92.48570 Test: 0.78463
Epoch: 6000 Train: 90.53967 Test: 0.74959
Epoch: 6080 Train: 91.18993 Test: 2.75691
Epoch: 6160 Train: 90.22191 Test: 0.74117
Epoch: 6240 Train: 99.18970 Test: 0.96793
Epoch: 6320 Train: 94.84365 Test: 0.89805
Epoch: 6400 Train: 89.18139 Test: 0.89331
Epoch: 6480 Train: 91.02595 Test: 0.90460
Epoch: 6560 Train: 88.85400 Test: 0.72402
Epoch: 6640 Train: 88.55311 Test: 0.71120
Epoch: 6720 Train: 88.91114 Test: 0.83391
Epoch: 6800 Train: 88.44842 Test: 0.72655
Epoch: 6880 Train: 88.07508 Test: 0.72165
Epoch: 6960 Train: 84.36240 Test: 0.64927
Epoch: 7040 Train: 85.10748 Test: 0.68151
Epoch: 7120 Train: 80.38397 Test: 0.87974
Epoch: 7200 Train: 78.14098 Test: 0.62371
Epoch: 7280 Train: 77.96168 Test: 0.63703
Epoch 7360: New minimal relative error: 2.36%, model saved.
Epoch: 7360 Train: 78.62920 Test: 0.65438
Epoch: 7440 Train: 77.42533 Test: 0.66632
Epoch: 7520 Train: 78.14339 Test: 1.40959
Epoch: 7600 Train: 77.84389 Test: 0.62808
Epoch: 7680 Train: 77.55220 Test: 0.67722
Epoch: 7760 Train: 80.73538 Test: 0.78000
Epoch: 7840 Train: 80.23978 Test: 0.78370
Epoch: 7920 Train: 76.71036 Test: 1.05901
Epoch: 7999 Train: 72.81261 Test: 0.59356
Training Loss: tensor(72.8126)
Test Loss: tensor(0.5936)
Learned LE: [  0.88822216  -0.0248675  -14.533181  ]
True LE: [ 8.8015002e-01 -4.0863231e-03 -1.4551449e+01]
Relative Error: [1.1738884  1.298748   1.3762468  1.4718974  1.5263002  1.9596862
 1.9698614  1.7690653  1.3833473  0.8823368  0.41573328 0.08010378
 0.11379299 0.13425297 0.154843   0.10568178 0.3432773  0.87009144
 0.734639   0.41798294 0.67906487 0.88172257 0.96516365 0.9416925
 1.0776331  1.1304427  1.0101461  0.9651871  0.9567041  0.98557884
 1.1365834  1.3884891  1.7581744  1.9200515  1.7826312  1.5562207
 1.3040932  1.3434587  1.4795398  1.5388495  1.1902528  1.276737
 0.3426824  0.6279366  1.0803136  1.3407189  1.3978364  1.6593122
 1.9826972  2.0749385  2.0839908  2.0137146  1.9133667  1.8751369
 1.8711567  1.8230057  1.8292221  1.6864506  1.5394425  1.2703117
 1.1001749  1.1311059  1.0906686  1.0228666  1.0682726  1.1652434
 1.2668605  1.6205717  1.7205186  1.6378958  1.3783182  0.9270453
 0.441118   0.10948603 0.1308433  0.15200198 0.20275886 0.17032105
 0.19550107 0.64949125 0.6850681  0.3193344  0.61207813 0.8291418
 0.86561793 0.78402233 0.7208483  0.84006983 0.847204   0.7609906
 0.71760255 0.6492044  0.74659073 1.0196959  1.4061208  1.5789784
 1.576511   1.3414041  1.0810935  1.068786   1.1425314  1.2479372
 0.8573287  0.9508052  0.57922655 0.5068248  0.92129546 1.1563898
 1.3242149  1.5437669  1.7835126  1.9417002  1.8898705  1.8513253
 1.6689426  1.5395014  1.5285708  1.5692594  1.6185066  1.5834672
 1.6404188  1.7610785  1.5441426  1.412116   1.2816216  1.1553571
 1.0235051  0.9017987  0.95984817 1.0454445  1.3319054  1.3717328
 1.2470179  0.9170885  0.5015212  0.20676678 0.166429   0.32559484
 0.2234338  0.25317237 0.13700376 0.4019677  0.73639697 0.27111635
 0.5648517  0.7793676  0.80547315 0.81832975 0.7457011  0.7320482
 0.6585299  0.71362823 0.58806586 0.45987403 0.32535124 0.53192645
 0.89149314 1.1659759  1.1320856  1.1157323  0.83042365 0.7918916
 0.8837617  0.8579822  0.7194178  0.62389904 0.9175127  0.42658183
 0.74321264 0.9986503  1.2900965  1.4904588  1.6312149  1.8215086
 1.6483357  1.5727773  1.4127972  1.2492876  1.1998657  1.3245606
 1.5118194  1.603342   1.789519   2.0284345  2.1200209  1.9208299
 1.7745208  1.5401082  1.2598375  0.9760317  0.803618   0.60266066
 0.72819173 0.9881925  1.0331379  0.871459   0.5659939  0.14947677
 0.24516508 0.362857   0.29926926 0.39419103 0.42232472 0.20951205
 0.66492295 0.21114884 0.5049332  0.7754975  0.90320355 0.94754833
 0.9029791  0.79183465 0.68404233 0.4950526  0.46214294 0.46657404
 0.5735193  0.3921869  0.39853948 0.7023636  0.6775223  0.94411683
 0.5710865  0.5149378  0.7593517  0.653954   0.7564346  0.38002968
 0.6323368  0.4720423  0.5607287  0.935948   1.3438402  1.5523001
 1.6699808  1.784219   1.7166475  1.461609   1.2124839  0.9812299
 1.0142804  1.0394385  1.3031691  1.5123272  1.7268035  2.006054
 2.3185031  2.3713768  2.196488   2.0531676  1.843919   1.4854348
 0.93899685 0.6638861  0.46556634 0.4459584  0.6916619  0.7282767
 0.552531   0.29987037 0.26316926 0.33917397 0.40955177 0.36507627
 0.5480013  0.53547645 0.33968374 0.3538238  0.3513078  0.7133451
 1.013558   1.0875472  1.0090952  0.9061184  0.76490134 0.6065046
 0.16191496 0.39346135 0.6299095  0.86045    0.8136574  0.5068827
 0.41808137 0.6277839  0.4886172  0.26112166 0.4291331  0.51932925
 0.4331814  0.3518092  0.20606187 0.65868026 0.46229675 0.7694984
 1.2671231  1.5633466  1.6588686  1.70049    1.7550594  1.5012836
 1.2740201  1.1018784  0.8994594  0.70052433 0.976036   1.2451855
 1.4742529  1.7340279  2.0451646  2.4024525  2.4378588  2.327262
 2.2837327  2.1529083  1.7102884  0.9817125  0.63629395 0.76934695
 0.5499204  0.54155254 0.49673575 0.29778564 0.19431064 0.28848118
 0.31485626 0.22337097 0.47492784 0.6248094  0.5481755  0.45470944
 0.14588396 0.54372466 1.034502   1.2529545  1.1581256  0.94417214
 0.79525673 0.78292006 0.59949976 0.3882908  0.7900685  1.109442
 1.3075297  1.3572755  0.88548994 0.92537504 0.8107826  0.545816
 0.22023919 0.3142456  0.26393855 0.24422312 0.10429735 0.25930166
 0.7736151  0.60250384 1.0509975  1.3844568  1.5377337  1.5761094
 1.6241269  1.5401585  1.2661097  1.2328914  0.96611035 0.6444446
 0.5697346  0.8351308  1.0256032  1.2393805  1.4991775  1.8258446
 2.22298    2.343917   2.3337848  2.4481933  2.3383367  1.9069397
 1.2453657  0.76283157 0.67486066 0.9979004  0.7147292  0.48219875
 0.18790662 0.23292424 0.3155805  0.22813492 0.09607369 0.5907397
 0.69132316 0.47068894 0.4591171  0.28694624 0.86331105 1.2598147
 1.3682618  1.1644341  0.77189785 0.65352863]
