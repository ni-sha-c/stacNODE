time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.71%, model saved.
Epoch: 0 Train: 4164.66455 Test: 3901.36475
Epoch: 100 Train: 103.69638 Test: 136.16953
Epoch 200: New minimal relative error: 31.07%, model saved.
Epoch: 200 Train: 12.46019 Test: 19.55838
Epoch 300: New minimal relative error: 18.29%, model saved.
Epoch: 300 Train: 5.82868 Test: 11.38821
time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.71%, model saved.
Epoch: 0 Train: 4164.66455 Test: 3901.36475
Epoch: 100 Train: 103.68994 Test: 136.17809
Epoch 200: New minimal relative error: 31.14%, model saved.
Epoch: 200 Train: 12.45091 Test: 19.54977
Epoch 300: New minimal relative error: 20.28%, model saved.
Epoch: 300 Train: 8.04767 Test: 13.00704
Epoch: 400 Train: 5.99209 Test: 10.99445
Epoch 500: New minimal relative error: 17.21%, model saved.
Epoch: 500 Train: 4.17074 Test: 8.34482
Epoch: 600 Train: 3.01035 Test: 5.51939
Epoch: 700 Train: 1.23299 Test: 4.43104
Epoch: 800 Train: 1.01119 Test: 3.58158
Epoch 900: New minimal relative error: 16.67%, model saved.
Epoch: 900 Train: 0.88325 Test: 3.48032
Epoch 1000: New minimal relative error: 16.58%, model saved.
Epoch: 1000 Train: 1.06752 Test: 3.35210
Epoch 1100: New minimal relative error: 16.00%, model saved.
Epoch: 1100 Train: 1.01566 Test: 2.95903
Epoch: 1200 Train: 0.45083 Test: 2.85586
Epoch: 1300 Train: 1.43977 Test: 3.70584
Epoch: 1400 Train: 0.68084 Test: 2.72231
Epoch: 1500 Train: 2.76037 Test: 4.61880
Epoch: 1600 Train: 0.34119 Test: 1.97038
Epoch: 1700 Train: 4.02580 Test: 6.25787
Epoch: 1800 Train: 1.42639 Test: 2.80823
Epoch 1900: New minimal relative error: 13.61%, model saved.
Epoch: 1900 Train: 0.31591 Test: 1.69867
Epoch: 2000 Train: 0.29630 Test: 1.59706
Epoch 2100: New minimal relative error: 10.20%, model saved.
Epoch: 2100 Train: 0.53297 Test: 1.97019
Epoch: 2200 Train: 0.21178 Test: 1.51892
Epoch: 2300 Train: 0.14082 Test: 1.36720
Epoch: 2400 Train: 0.13685 Test: 1.35650
Epoch: 2500 Train: 0.15745 Test: 1.36822
Epoch: 2600 Train: 0.15984 Test: 1.29410
Epoch: 2700 Train: 0.11222 Test: 1.24669
Epoch: 2800 Train: 0.11371 Test: 1.23108
Epoch: 2900 Train: 0.11333 Test: 1.20913
Epoch 3000: New minimal relative error: 9.16%, model saved.
Epoch: 3000 Train: 0.10098 Test: 1.17645
Epoch: 3100 Train: 0.12625 Test: 1.20257
Epoch: 3200 Train: 0.17082 Test: 1.20521
Epoch: 3300 Train: 1.39867 Test: 1.69882
Epoch: 3400 Train: 0.16604 Test: 1.25725
Epoch: 3500 Train: 0.07818 Test: 1.10101
Epoch: 3600 Train: 0.09994 Test: 1.09764
Epoch: 3700 Train: 0.36200 Test: 1.49705
Epoch: 3800 Train: 0.18027 Test: 1.22752
Epoch: 3900 Train: 0.18061 Test: 1.24392
Epoch: 4000 Train: 0.06624 Test: 1.05389
Epoch: 4100 Train: 0.06672 Test: 1.06272
Epoch: 4200 Train: 0.62585 Test: 1.45290
Epoch: 4300 Train: 0.58542 Test: 1.22185
Epoch: 4400 Train: 0.11568 Test: 1.07727
Epoch: 4500 Train: 0.05707 Test: 1.02796
Epoch: 4600 Train: 0.05965 Test: 1.03519
Epoch: 4700 Train: 0.11200 Test: 1.10846
Epoch: 4800 Train: 0.05439 Test: 1.02549
Epoch: 4900 Train: 0.05179 Test: 1.02054
Epoch: 5000 Train: 0.08113 Test: 1.06262
Epoch: 5100 Train: 0.04896 Test: 1.01819
Epoch: 5200 Train: 0.07955 Test: 1.02770
Epoch: 5300 Train: 0.04709 Test: 1.01320
Epoch: 5400 Train: 0.04622 Test: 1.01468
Epoch: 5500 Train: 0.45579 Test: 1.12775
Epoch: 5600 Train: 0.04355 Test: 1.01291
Epoch: 5700 Train: 0.08004 Test: 1.04170
Epoch: 5800 Train: 0.04209 Test: 1.01063
Epoch: 5900 Train: 0.04224 Test: 1.01036
Epoch: 6000 Train: 0.06378 Test: 1.00103
Epoch: 6100 Train: 0.07938 Test: 1.06897
Epoch: 6200 Train: 0.10071 Test: 1.07500
Epoch: 6300 Train: 0.05174 Test: 1.02089
Epoch: 6400 Train: 0.03726 Test: 1.01473
Epoch: 6500 Train: 0.50133 Test: 1.41143
Epoch: 6600 Train: 0.04420 Test: 1.01824
Epoch: 6700 Train: 0.03524 Test: 1.01698
Epoch: 6800 Train: 0.04485 Test: 1.01917
Epoch: 6900 Train: 0.03374 Test: 1.01709
Epoch: 7000 Train: 0.03356 Test: 1.01549
Epoch: 7100 Train: 0.04637 Test: 1.03776
Epoch: 7200 Train: 0.35169 Test: 1.48964
Epoch: 7300 Train: 0.03178 Test: 1.02292
Epoch: 7400 Train: 0.03102 Test: 1.02329
Epoch: 7500 Train: 0.03466 Test: 1.02169
Epoch: 7600 Train: 0.03024 Test: 1.02683
Epoch: 7700 Train: 0.03850 Test: 1.03094
Epoch: 7800 Train: 0.05323 Test: 1.05256
Epoch: 7900 Train: 0.03197 Test: 1.03023
Epoch: 8000 Train: 0.02851 Test: 1.03165
Epoch: 8100 Train: 0.46471 Test: 1.58616
Epoch: 8200 Train: 0.02768 Test: 1.03565
Epoch: 8300 Train: 0.02774 Test: 1.03833
Epoch: 8400 Train: 0.02827 Test: 1.04422
Epoch: 8500 Train: 0.02644 Test: 1.03986
Epoch: 8600 Train: 0.02753 Test: 1.04026
Epoch: 8700 Train: 0.02580 Test: 1.04305
Epoch: 8800 Train: 0.02628 Test: 1.04345
Epoch: 8900 Train: 0.02521 Test: 1.04587
Epoch: 9000 Train: 0.04183 Test: 1.06530
Epoch: 9100 Train: 0.02451 Test: 1.04835
Epoch: 9200 Train: 0.04206 Test: 1.05053
Epoch: 9300 Train: 0.02392 Test: 1.05064
Epoch: 9400 Train: 0.02359 Test: 1.05162
Epoch: 9500 Train: 0.02367 Test: 1.05533
Epoch: 9600 Train: 0.02315 Test: 1.05769
Epoch: 9700 Train: 0.02284 Test: 1.06089
Epoch: 9800 Train: 0.02302 Test: 1.06574
Epoch: 9900 Train: 0.02240 Test: 1.07647
Epoch: 9999 Train: 0.02209 Test: 1.08580
Training Loss: tensor(0.0221)
Test Loss: tensor(1.0858)
Learned LE: [ 0.90252745  0.05159948 -4.832102  ]
True LE: [ 8.6415792e-01  1.3476764e-03 -1.4541008e+01]
Relative Error: [8.0008545  8.349546   8.661605   8.956227   9.280094   9.58062
 9.7423525  9.621567   9.29567    8.860674   7.9506645  6.486579
 5.2289977  4.6071663  3.9475687  3.1187592  2.3833282  1.9961603
 1.8056003  1.8772357  2.0479429  1.995339   1.6061487  0.95398295
 0.9198443  1.6113175  2.1565373  2.3758883  2.330746   2.1566403
 1.9102182  1.5938537  1.3002751  1.1885854  1.3327396  1.6991684
 2.1225626  2.6932626  3.3198376  3.5080047  2.8999376  2.6345258
 2.9918916  3.582632   3.840046   3.4740555  3.022292   2.6286154
 2.2898543  2.005664   1.7882837  1.7544156  2.0062587  2.5864856
 3.3293903  4.033984   4.6202755  5.052144   5.5796266  6.311802
 6.765774   6.82967    6.986476   7.2873034  7.5678625  7.8257694
 8.154179   8.547053   8.922711   9.147337   9.0907755  8.890568
 8.440309   7.3905697  5.817051   4.6350675  3.9693475  3.1225295
 2.2254128  1.6858833  1.45919    1.368792   1.6201586  1.8121727
 1.6759869  1.1907892  0.9265356  1.6530725  2.4222627  2.8162296
 2.8312707  2.6595948  2.4305167  2.1161854  1.714266   1.3521414
 1.1369011  1.1219167  1.3653601  1.7308066  2.2285216  2.8732362
 3.179295   2.566496   2.3573966  2.7068944  3.252333   3.349165
 2.92282    2.447711   1.9596007  1.5057644  1.1353567  0.99645555
 1.0842593  1.4822026  2.1750045  2.8751948  3.494417   3.9728885
 4.4108286  5.072353   5.757595   5.9209924  5.980018   6.2144165
 6.4912214  6.7098536  6.9713793  7.3289537  7.760938   8.2044525
 8.523215   8.602852   8.558169   8.139485   7.0704093  5.445587
 4.22891    3.476339   2.4623306  1.5030092  1.0782226  0.9839801
 1.1080034  1.5566869  1.7987341  1.5931438  1.3214691  1.6471634
 2.5167794  3.101655   3.3011482  3.229351   3.05725    2.807622
 2.4048188  1.9106117  1.4846     1.1906029  1.0087545  1.0271515
 1.3033025  1.728332   2.330147   2.9022646  2.398435   2.08436
 2.3523333  2.821442   2.8910458  2.4797952  1.9718329  1.3806068
 0.8027874  0.47624683 0.3749628  0.45803845 0.9116145  1.6063918
 2.2163568  2.7737544  3.1948247  3.6865401  4.4241147  4.9550996
 4.981011   5.08915    5.351658   5.6063566  5.8328695  6.1307044
 6.5039644  6.9352493  7.4010077  7.8080883  8.045446   8.195676
 7.943148   7.016375   5.440826   4.027648   3.2267969  2.099935
 1.0216621  0.49562287 0.5883435  1.0010717  1.6387678  1.9360758
 1.7577354  1.9830346  2.4947507  3.1918333  3.6326294  3.7526093
 3.7004483  3.5770502  3.3195798  2.8610992  2.3390396  1.8182615
 1.3087187  0.97306335 0.87931484 0.9989621  1.363611   1.8976632
 2.5637798  2.4814808  1.8468654  1.9384453  2.304608   2.4759097
 2.1735263  1.6627076  1.0812742  0.72053456 0.86149424 0.7924043
 0.45524472 0.39770985 0.98265886 1.4913263  1.9805948  2.3341281
 2.8060246  3.5394058  3.9984741  3.9947658  4.120685   4.391379
 4.666029   4.9479575  5.3066163  5.711437   6.123663   6.5314097
 6.9265137  7.248477   7.581572   7.663237   7.0961013  5.8251743
 4.1671553  3.2368898  2.1769624  0.99054325 0.24023165 0.46911192
 0.96320903 1.6419314  2.1381302  2.0881922  2.6552968  3.2895207
 3.6896863  4.101096   4.1399846  4.0978637  4.038102   3.7922587
 3.3166676  2.779603   2.3043516  1.771151   1.1865106  0.8453102
 0.99493283 1.2323629  1.6602335  2.0470538  2.6175392  1.9503428
 1.5079232  1.6953194  2.0392456  2.0060756  1.4651737  0.876251
 0.9500404  1.537769   1.654459   1.1846398  0.6317674  0.562757
 0.8602995  1.3135878  1.6392753  1.9528303  2.4958217  2.9998875
 3.0237262  3.1448073  3.4015405  3.69659    4.006463   4.3944726
 4.821454   5.2301955  5.5887074  5.897505   6.1602025  6.470737
 6.879832   6.884318   6.22234    4.871629   3.4001772  2.6566122
 1.4956235  0.71321875 0.7189773  1.0803281  1.525768   2.1601543
 2.4382417  2.975984   3.8415146  4.1351376  4.1580057  4.239916
 4.1054444  4.1601152  4.1606727  3.8575087  3.3157725  2.8074014
 2.3033516  1.737534   1.2565048  0.93715787 1.0898737  1.4018629
 1.6755893  1.868885   2.3875701  1.6087161  1.0309883  1.3523604
 1.6828475  1.5312103  0.794953   0.7784916  1.6278064  1.8992891
 1.6961479  1.3084852  0.91701734 0.49873862 0.62619156 1.0320181
 1.3102635  1.6590089  2.141192   2.195913   2.2140062  2.4400256
 2.7289865  2.9863632  3.2886915  3.6226172  3.9557211  4.29045
 4.6180897  4.9015036  5.09871    5.3581038  5.782313   5.893124
 5.4028916  4.2270265  2.8224247  2.275325   1.3798795  0.97309
 1.1847892  1.5152378  1.9476827  2.471842  ]
