time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.86%, model saved.
Epoch: 0 Train: 4174.07227 Test: 3816.37842
Epoch 100: New minimal relative error: 73.74%, model saved.
Epoch: 100 Train: 137.36998 Test: 167.13156
Epoch 200: New minimal relative error: 24.17%, model saved.
Epoch: 200 Train: 14.18493 Test: 19.54108
Epoch: 300 Train: 6.44796 Test: 7.04109
Epoch 400: New minimal relative error: 14.32%, model saved.
Epoch: 400 Train: 2.50761 Test: 4.22783
Epoch 500: New minimal relative error: 14.20%, model saved.
Epoch: 500 Train: 2.55867 Test: 3.73180
Epoch: 600 Train: 2.08557 Test: 2.96055
Epoch: 700 Train: 1.36275 Test: 2.00856
Epoch: 800 Train: 10.65176 Test: 8.02312
Epoch: 900 Train: 3.57834 Test: 4.53434
Epoch: 1000 Train: 9.37625 Test: 5.22120
Epoch: 1100 Train: 0.45146 Test: 0.88174
Epoch 1200: New minimal relative error: 12.53%, model saved.
Epoch: 1200 Train: 0.59721 Test: 1.51051
Epoch: 1300 Train: 2.93876 Test: 2.12328
Epoch: 1400 Train: 6.54011 Test: 7.77691
Epoch: 1500 Train: 1.09682 Test: 1.04179
Epoch: 1600 Train: 2.63473 Test: 2.98813
Epoch: 1700 Train: 2.75103 Test: 2.59137
Epoch: 1800 Train: 1.18523 Test: 1.01859
Epoch: 1900 Train: 2.56932 Test: 2.22437
Epoch: 2000 Train: 0.91496 Test: 1.43124
Epoch: 2100 Train: 1.15882 Test: 1.17914
Epoch: 2200 Train: 0.16640 Test: 0.36535
Epoch: 2300 Train: 0.32391 Test: 0.43020
Epoch: 2400 Train: 0.79513 Test: 0.82113
Epoch: 2500 Train: 0.30516 Test: 0.43228
Epoch: 2600 Train: 1.18343 Test: 1.27755
Epoch: 2700 Train: 1.39472 Test: 2.06957
Epoch: 2800 Train: 3.93522 Test: 3.79421
Epoch: 2900 Train: 2.45222 Test: 1.90594
Epoch: 3000 Train: 2.08129 Test: 2.38875
Epoch: 3100 Train: 1.65647 Test: 2.10522
Epoch: 3200 Train: 0.70236 Test: 0.84644
Epoch: 3300 Train: 2.42353 Test: 2.07993
Epoch: 3400 Train: 2.23723 Test: 2.82649
Epoch: 3500 Train: 0.38411 Test: 0.40042
Epoch: 3600 Train: 3.06612 Test: 3.02507
Epoch: 3700 Train: 0.15921 Test: 0.26180
Epoch: 3800 Train: 0.20301 Test: 0.37217
Epoch 3900: New minimal relative error: 12.40%, model saved.
Epoch: 3900 Train: 0.91975 Test: 1.13899
Epoch: 4000 Train: 0.33696 Test: 0.24910
Epoch: 4100 Train: 0.40405 Test: 0.47240
Epoch: 4200 Train: 3.06812 Test: 2.25568
Epoch: 4300 Train: 0.24587 Test: 0.34989
Epoch: 4400 Train: 5.22645 Test: 5.45465
Epoch: 4500 Train: 4.66834 Test: 3.96494
Epoch: 4600 Train: 0.39777 Test: 0.50033
Epoch: 4700 Train: 0.30446 Test: 0.83448
Epoch: 4800 Train: 0.50379 Test: 0.46531
Epoch: 4900 Train: 1.81827 Test: 2.08736
Epoch: 5000 Train: 0.08934 Test: 0.15989
Epoch: 5100 Train: 0.04550 Test: 0.11680
Epoch: 5200 Train: 0.08806 Test: 0.20125
Epoch: 5300 Train: 0.11221 Test: 0.22452
Epoch: 5400 Train: 0.11131 Test: 0.21064
Epoch 5500: New minimal relative error: 10.21%, model saved.
Epoch: 5500 Train: 1.88739 Test: 2.47864
Epoch: 5600 Train: 0.12972 Test: 0.16496
Epoch: 5700 Train: 0.04180 Test: 0.11399
Epoch: 5800 Train: 0.04372 Test: 0.10253
Epoch: 5900 Train: 0.07736 Test: 0.13351
Epoch: 6000 Train: 0.05284 Test: 0.14309
Epoch: 6100 Train: 0.05886 Test: 0.11111
Epoch: 6200 Train: 0.04329 Test: 0.12022
Epoch: 6300 Train: 0.08953 Test: 0.13933
Epoch: 6400 Train: 0.12598 Test: 0.25596
Epoch: 6500 Train: 0.53195 Test: 0.60371
Epoch: 6600 Train: 0.09106 Test: 0.17647
Epoch: 6700 Train: 1.21120 Test: 1.32393
Epoch: 6800 Train: 0.08077 Test: 0.18796
Epoch: 6900 Train: 0.03476 Test: 0.11061
Epoch: 7000 Train: 0.22023 Test: 0.27873
Epoch: 7100 Train: 0.47749 Test: 0.77681
Epoch: 7200 Train: 0.02728 Test: 0.09347
Epoch: 7300 Train: 0.03090 Test: 0.10940
Epoch: 7400 Train: 0.16295 Test: 0.19502
Epoch: 7500 Train: 0.10894 Test: 0.26111
Epoch: 7600 Train: 0.22262 Test: 0.37481
Epoch: 7700 Train: 0.62159 Test: 0.31495
Epoch: 7800 Train: 0.34970 Test: 0.32235
Epoch: 7900 Train: 0.31300 Test: 0.46097
Epoch: 8000 Train: 0.02827 Test: 0.09057
Epoch: 8100 Train: 0.02406 Test: 0.09554
Epoch: 8200 Train: 0.03876 Test: 0.12392
Epoch: 8300 Train: 0.03609 Test: 0.09895
Epoch: 8400 Train: 0.06901 Test: 0.15377
Epoch: 8500 Train: 0.04801 Test: 0.14015
Epoch: 8600 Train: 0.06496 Test: 0.15525
Epoch: 8700 Train: 0.02165 Test: 0.09121
Epoch: 8800 Train: 0.03561 Test: 0.09858
Epoch: 8900 Train: 1.46421 Test: 0.82009
Epoch: 9000 Train: 0.02346 Test: 0.09175
Epoch: 9100 Train: 0.04126 Test: 0.13476
Epoch: 9200 Train: 0.03482 Test: 0.10649
Epoch: 9300 Train: 0.06427 Test: 0.14856
Epoch: 9400 Train: 0.04012 Test: 0.10011
Epoch: 9500 Train: 0.13357 Test: 0.15807
Epoch: 9600 Train: 0.01913 Test: 0.09516
Epoch: 9700 Train: 0.12548 Test: 0.13741
Epoch: 9800 Train: 0.64343 Test: 0.78747
Epoch: 9900 Train: 0.04123 Test: 0.13400
Epoch: 9999 Train: 0.02154 Test: 0.09900
Training Loss: tensor(0.0215)
Test Loss: tensor(0.0990)
Learned LE: [ 0.8456513  -0.02834888 -4.077211  ]
True LE: [ 8.7865961e-01 -1.1706627e-02 -1.4546408e+01]
Relative Error: [29.56472   29.93348   30.056793  30.024809  29.948282  29.342274
 28.252941  26.920193  24.7271    22.167826  20.524239  19.898336
 19.38342   17.943312  15.74412   14.065996  12.412284  11.524529
 10.736583   9.980969   9.560243   9.182282   8.668063   8.185403
  7.971572   8.017516   8.2581     7.9643497  7.684464   7.3192306
  6.9105163  6.753825   7.0007114  7.1718307  7.5813437  8.10595
  8.103067   7.650242   6.3231955  5.5518994  5.849315   6.675687
  6.930208   7.260916   6.654464   6.0295544  4.371904   4.6569533
  9.861992  15.781136  20.211134  22.808735  23.42681   23.661564
 23.469595  23.480072  24.00366   24.633902  25.420046  25.741503
 26.145748  26.831669  27.415922  27.969542  28.243021  28.22332
 28.184908  27.43679   26.380545  25.041664  23.037758  20.655867
 19.166674  18.34091   17.672739  16.42144   14.581256  13.052326
 11.40486   10.504038   9.69705    8.993171   8.399226   7.800967
  7.212468   6.828381   6.749883   6.900078   7.240358   7.2237926
  6.8130016  6.3912435  5.9048157  5.5158615  5.6510797  6.049532
  6.431801   7.018317   7.3359466  6.8718367  5.752595   5.0203733
  5.2245383  5.8496213  6.22861    6.287358   5.7745066  4.96936
  3.9766483  4.4074144  8.998248  14.677233  18.99557   21.82329
 22.409159  22.570553  22.36051   21.922379  22.315369  22.8356
 23.209824  23.397442  23.918013  24.771605  25.251272  25.85879
 26.221094  26.162937  26.052279  25.582949  24.507349  23.381134
 21.647243  19.49433   17.899727  16.848486  15.998562  15.001284
 13.595464  12.106318  10.632467   9.444361   8.594883   7.999067
  7.2652707  6.584778   5.973392   5.767159   5.792971   5.9400005
  6.273585   6.609452   6.1632757  5.6263924  5.077259   4.533746
  4.3324437  4.813272   5.343027   5.7603707  6.629602   6.0230823
  5.236357   4.515667   4.6415834  5.091653   5.1923094  5.309184
  4.9796147  3.9805346  3.5808961  4.3210745  8.2991    13.788703
 17.79357   20.583622  21.2398    21.389338  21.19251   20.646597
 20.83639   20.927214  21.179958  21.09889   21.59775   22.599026
 23.338627  23.965603  24.042831  23.986464  23.754541  23.74683
 22.880432  21.562593  20.210512  18.382257  16.79984   15.515206
 14.314935  13.475151  12.6002655 11.13717    9.86422    8.488027
  7.6372914  7.0168695  6.349758   5.681286   5.144706   4.8056183
  4.934602   5.130773   5.3842926  5.796623   5.8148336  5.14514
  4.51981    3.7907813  3.349534   3.5636046  4.2830067  4.730315
  5.7398925  5.372137   4.7484345  3.9477952  3.9072838  4.0502844
  4.070105   4.2911468  3.786805   3.3744595  3.2225657  4.2474604
  7.4540257 12.532436  16.847273  19.322279  20.139187  20.170841
 19.737442  19.30903   19.338608  18.680792  18.849903  18.903948
 19.225157  20.08672   21.204685  21.952707  22.189875  21.944433
 21.547274  21.46599   21.099903  20.119743  18.556028  17.145573
 15.867514  14.286227  13.094264  12.102559  11.425729  10.176519
  9.157771   7.7884054  6.942501   6.413266   5.889929   5.2105393
  4.747363   4.3069277  4.133789   4.3407354  4.5860586  4.8839793
  5.2761197  5.021783   4.2437563  3.5783117  2.6220038  2.434134
  3.1058085  3.9077368  4.490385   5.0067315  4.4830637  3.3801594
  3.2207568  3.2399542  3.1577413  3.2820492  3.0192213  2.8942163
  3.1712189  4.272123   6.5793357 10.844275  15.6713705 18.102428
 19.12541   19.259983  18.714424  18.277803  17.807106  16.866879
 16.5005    16.707388  16.907488  17.18326   18.287924  19.286106
 19.928946  20.048553  19.438623  19.08738   18.943827  18.610437
 17.301455  15.681894  14.74759   13.532239  12.002624  10.804625
 10.16911    9.355381   8.454801   7.258773   6.438385   6.1134753
  5.8427067  5.1710086  4.60919    4.1907     3.6829796  3.6261756
  3.8830307  4.19454    4.538278   4.8390923  4.3133793  3.5217855
  2.6543941  1.9538469  1.9420328  2.7557487  3.6517668  4.5350337
  4.235913   3.2650378  2.4624717  2.400867   2.3118079  2.418091
  2.67895    2.907331   3.0500607  4.4463797  5.7918625  7.988985
 12.580474  15.437176  17.331707  17.933231  17.458767  16.980213
 16.719135  15.641254  14.776278  14.612865  14.625582  14.857587
 15.184098  16.288248  16.911963  17.405441  17.691475  17.010363
 16.7375    16.612513  16.153952  14.578321  13.179141  12.7227545
 11.657449   9.991498   8.971376   8.473679   7.3570585  6.8307595
  6.187874   5.817347   5.606104   5.1263585  4.4865685  4.202949
  3.7884905  3.2844324  3.290215   3.6141202]
