time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 4
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 20.057853699 Test: 16.924762726
Epoch 0: New minimal relative error: 16.92%, model saved.
Epoch: 100 Train: 5.241155148 Test: 5.356534004
Epoch 100: New minimal relative error: 5.36%, model saved.
Epoch: 200 Train: 5.114521980 Test: 5.227755547
Epoch 200: New minimal relative error: 5.23%, model saved.
Epoch: 300 Train: 5.182151794 Test: 5.289488792
Epoch: 400 Train: 5.100142479 Test: 5.206113815
Epoch 400: New minimal relative error: 5.21%, model saved.
Epoch: 500 Train: 5.187828064 Test: 5.259557724
Epoch: 600 Train: 5.187733650 Test: 5.267070770
Epoch: 700 Train: 5.089832783 Test: 5.217154503
Epoch: 800 Train: 5.066759109 Test: 5.174526691
Epoch 800: New minimal relative error: 5.17%, model saved.
Epoch: 900 Train: 5.160496712 Test: 5.268512726
Epoch: 1000 Train: 5.112787724 Test: 5.259207726
Epoch: 1100 Train: 5.205983162 Test: 5.300682068
Epoch: 1200 Train: 5.162446976 Test: 5.254520416
Epoch: 1300 Train: 5.163332939 Test: 5.269286156
Epoch: 1400 Train: 5.138383865 Test: 5.254912853
Epoch: 1500 Train: 5.140966415 Test: 5.259996414
Epoch: 1600 Train: 5.143058777 Test: 5.275341988
Epoch: 1700 Train: 5.161536217 Test: 5.280323982
Epoch: 1800 Train: 5.132542133 Test: 5.253117561
Epoch: 1900 Train: 5.109149933 Test: 5.240126133
Epoch: 2000 Train: 5.118649006 Test: 5.241060257
Epoch: 2100 Train: 5.173583508 Test: 5.301364899
Epoch: 2200 Train: 5.166367531 Test: 5.283405304
Epoch: 2300 Train: 5.158370018 Test: 5.267488480
Epoch: 2400 Train: 5.170389175 Test: 5.292007923
Epoch: 2500 Train: 5.169890881 Test: 5.286482811
Epoch: 2600 Train: 5.183435440 Test: 5.309655190
Epoch: 2700 Train: 5.173259258 Test: 5.313453674
Epoch: 2800 Train: 5.162393570 Test: 5.304353237
Epoch: 2900 Train: 5.130763531 Test: 5.301213264
Epoch: 3000 Train: 5.118018150 Test: 5.314995289
Epoch: 3100 Train: 5.146033287 Test: 5.346714973
Epoch: 3200 Train: 5.173825741 Test: 5.372673035
Epoch: 3300 Train: 5.177451134 Test: 5.369914055
Epoch: 3400 Train: 5.167214394 Test: 5.359179497
Epoch: 3500 Train: 5.161514282 Test: 5.355914593
Epoch: 3600 Train: 5.161500454 Test: 5.382549286
Epoch: 3700 Train: 5.161736965 Test: 5.403256416
Epoch: 3800 Train: 5.173516273 Test: 5.398541451
Epoch: 3900 Train: 5.176084518 Test: 5.410667419
Epoch: 4000 Train: 5.157435894 Test: 5.398800850
Epoch: 4100 Train: 5.158203125 Test: 5.420742989
Epoch: 4200 Train: 5.142409325 Test: 5.387058258
Epoch: 4300 Train: 5.118694305 Test: 5.376423836
Epoch: 4400 Train: 5.144529343 Test: 5.411762238
Epoch: 4500 Train: 5.156381607 Test: 5.399396896
Epoch: 4600 Train: 5.163708687 Test: 5.429399490
Epoch: 4700 Train: 5.163585186 Test: 5.402450562
Epoch: 4800 Train: 5.169223309 Test: 5.433483601
Epoch: 4900 Train: 5.182785034 Test: 5.415683746
Epoch: 5000 Train: 5.167885780 Test: 5.404173851
Epoch: 5100 Train: 5.175254345 Test: 5.411972046
Epoch: 5200 Train: 5.178416252 Test: 5.393488884
Epoch: 5300 Train: 5.175677299 Test: 5.394882202
Epoch: 5400 Train: 5.177750587 Test: 5.400984764
Epoch: 5500 Train: 5.172192574 Test: 5.360497475
Epoch: 5600 Train: 5.192664146 Test: 5.363056183
Epoch: 5700 Train: 5.190771103 Test: 5.356286526
Epoch: 5800 Train: 5.192577362 Test: 5.352044106
Epoch: 5900 Train: 5.192897797 Test: 5.349210739
Epoch: 6000 Train: 5.189043045 Test: 5.347845554
Epoch: 6100 Train: 5.185934067 Test: 5.341875553
Epoch: 6200 Train: 5.193923950 Test: 5.346534729
Epoch: 6300 Train: 5.169078350 Test: 5.338442802
Epoch: 6400 Train: 5.150040150 Test: 5.334907532
Epoch: 6500 Train: 5.147323608 Test: 5.340156555
Epoch: 6600 Train: 5.167899609 Test: 5.360651016
Epoch: 6700 Train: 5.164910793 Test: 5.349518776
Epoch: 6800 Train: 5.171289444 Test: 5.346023083
Epoch: 6900 Train: 5.179032326 Test: 5.359613419
Epoch: 7000 Train: 5.193649292 Test: 5.369710922
Epoch: 7100 Train: 5.201685429 Test: 5.367666245
Epoch: 7200 Train: 5.196619987 Test: 5.346965790
Epoch: 7300 Train: 5.192642212 Test: 5.339896202
Epoch: 7400 Train: 5.190587997 Test: 5.347249031
Epoch: 7500 Train: 5.187016010 Test: 5.348215103
Epoch: 7600 Train: 5.187588692 Test: 5.353626728
Epoch: 7700 Train: 5.190006256 Test: 5.355730534
Epoch: 7800 Train: 5.191271782 Test: 5.354897499
Epoch: 7900 Train: 5.194043159 Test: 5.354014397
Epoch: 8000 Train: 5.200175762 Test: 5.356765747
Epoch: 8100 Train: 5.200811863 Test: 5.358940125
Epoch: 8200 Train: 5.202702999 Test: 5.354406834
Epoch: 8300 Train: 5.203946114 Test: 5.354002953
Epoch: 8400 Train: 5.203494072 Test: 5.353608131
Epoch: 8500 Train: 5.201905251 Test: 5.353921890
Epoch: 8600 Train: 5.203207016 Test: 5.355734348
Epoch: 8700 Train: 5.202528954 Test: 5.356758118
Epoch: 8800 Train: 5.204954147 Test: 5.355352879
Epoch: 8900 Train: 5.206515312 Test: 5.356828690
Epoch: 9000 Train: 5.205851078 Test: 5.356426239
Epoch: 9100 Train: 5.206151009 Test: 5.356254101
Epoch: 9200 Train: 5.207929611 Test: 5.356809616
Epoch: 9300 Train: 5.207615376 Test: 5.361294746
Epoch: 9400 Train: 5.207097054 Test: 5.360966682
Epoch: 9500 Train: 5.206367970 Test: 5.361627579
Epoch: 9600 Train: 5.206187725 Test: 5.360282898
Epoch: 9700 Train: 5.206684113 Test: 5.363451958
Epoch: 9800 Train: 5.207852364 Test: 5.365233421
Epoch: 9900 Train: 5.209444046 Test: 5.367248535
Epoch: 9999 Train: 5.210717201 Test: 5.368583202
Training Loss: tensor(5.2107)
Test Loss: tensor(5.3686)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.2897, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0041, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0051)
Jacobian term Test Loss: tensor(0.0053)
Learned LE: [0.790354   0.44177854]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
