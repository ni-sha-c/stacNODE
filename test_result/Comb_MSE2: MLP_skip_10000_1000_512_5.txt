time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.46%, model saved.
Epoch: 0 Train: 4036.16479 Test: 4069.91089
Epoch 100: New minimal relative error: 81.83%, model saved.
Epoch: 100 Train: 50.33235 Test: 46.23234
Epoch 200: New minimal relative error: 28.62%, model saved.
Epoch: 200 Train: 10.44362 Test: 12.61010
Epoch 300: New minimal relative error: 24.49%, model saved.
Epoch: 300 Train: 8.97202 Test: 10.79366
Epoch: 400 Train: 29.22950 Test: 31.20567
Epoch: 500 Train: 11.43853 Test: 9.71864
Epoch 600: New minimal relative error: 14.34%, model saved.
Epoch: 600 Train: 6.98448 Test: 8.46518
Epoch: 700 Train: 13.36604 Test: 14.81315
Epoch: 800 Train: 5.77991 Test: 5.41738
Epoch: 900 Train: 2.40371 Test: 3.25015
Epoch: 1000 Train: 3.14210 Test: 3.78840
Epoch 1100: New minimal relative error: 12.69%, model saved.
Epoch: 1100 Train: 5.70074 Test: 7.74676
Epoch: 1200 Train: 1.88273 Test: 1.81054
Epoch: 1300 Train: 5.15752 Test: 4.76360
Epoch: 1400 Train: 1.52903 Test: 1.78847
Epoch: 1500 Train: 1.16777 Test: 2.74208
Epoch: 1600 Train: 2.32345 Test: 1.66886
Epoch: 1700 Train: 2.43494 Test: 2.92752
Epoch: 1800 Train: 1.84995 Test: 2.15926
Epoch: 1900 Train: 0.92051 Test: 0.96052
Epoch: 2000 Train: 1.48169 Test: 1.09718
Epoch: 2100 Train: 4.29835 Test: 3.66247
Epoch: 2200 Train: 6.76408 Test: 6.30819
Epoch: 2300 Train: 2.15460 Test: 1.89289
Epoch 2400: New minimal relative error: 12.05%, model saved.
Epoch: 2400 Train: 0.71337 Test: 0.85160
Epoch 2500: New minimal relative error: 7.71%, model saved.
Epoch: 2500 Train: 0.47910 Test: 0.67107
Epoch: 2600 Train: 1.02463 Test: 1.06879
Epoch: 2700 Train: 2.98542 Test: 2.11343
Epoch: 2800 Train: 0.38022 Test: 0.46709
Epoch 2900: New minimal relative error: 7.16%, model saved.
Epoch: 2900 Train: 0.26085 Test: 0.27766
Epoch: 3000 Train: 0.53389 Test: 0.55799
Epoch: 3100 Train: 0.67910 Test: 0.82004
Epoch: 3200 Train: 1.86420 Test: 2.21566
Epoch: 3300 Train: 0.27077 Test: 0.29246
Epoch: 3400 Train: 0.25972 Test: 0.34818
Epoch: 3500 Train: 1.22148 Test: 1.23275
Epoch: 3600 Train: 1.14197 Test: 1.57717
Epoch: 3700 Train: 0.42678 Test: 0.36224
Epoch: 3800 Train: 0.99317 Test: 0.87051
Epoch: 3900 Train: 1.11680 Test: 1.52587
Epoch: 4000 Train: 0.49936 Test: 0.45965
Epoch: 4100 Train: 1.08891 Test: 1.11254
Epoch: 4200 Train: 0.25975 Test: 0.35394
Epoch 4300: New minimal relative error: 7.05%, model saved.
Epoch: 4300 Train: 0.34710 Test: 0.29640
Epoch: 4400 Train: 1.18926 Test: 1.34011
Epoch: 4500 Train: 0.28407 Test: 0.23152
Epoch: 4600 Train: 0.13030 Test: 0.16083
Epoch: 4700 Train: 0.24457 Test: 0.49650
Epoch: 4800 Train: 0.23528 Test: 0.37686
Epoch: 4900 Train: 3.68379 Test: 3.09361
Epoch: 5000 Train: 1.23063 Test: 1.24145
Epoch: 5100 Train: 5.48809 Test: 4.46261
Epoch: 5200 Train: 1.40543 Test: 1.35316
Epoch: 5300 Train: 0.51307 Test: 0.72143
Epoch 5400: New minimal relative error: 6.88%, model saved.
Epoch: 5400 Train: 0.09564 Test: 0.11551
Epoch: 5500 Train: 0.65076 Test: 0.68549
Epoch: 5600 Train: 0.08563 Test: 0.12886
Epoch: 5700 Train: 0.66539 Test: 0.83865
Epoch: 5800 Train: 0.08142 Test: 0.15615
Epoch: 5900 Train: 0.39200 Test: 0.32664
Epoch: 6000 Train: 0.20661 Test: 0.26261
Epoch: 6100 Train: 1.23714 Test: 1.15322
Epoch: 6200 Train: 0.39143 Test: 0.45274
Epoch: 6300 Train: 0.07858 Test: 0.10801
Epoch: 6400 Train: 0.13165 Test: 0.16445
Epoch: 6500 Train: 0.06070 Test: 0.08586
Epoch: 6600 Train: 0.05927 Test: 0.08451
Epoch: 6700 Train: 0.06008 Test: 0.11344
Epoch: 6800 Train: 0.17090 Test: 0.22845
Epoch: 6900 Train: 0.19949 Test: 0.15692
Epoch: 7000 Train: 0.08077 Test: 0.12409
Epoch: 7100 Train: 0.06302 Test: 0.09496
Epoch: 7200 Train: 0.05118 Test: 0.07608
Epoch: 7300 Train: 0.05097 Test: 0.07747
Epoch: 7400 Train: 0.04737 Test: 0.07314
Epoch: 7500 Train: 0.13076 Test: 0.17545
Epoch: 7600 Train: 0.06593 Test: 0.09256
Epoch: 7700 Train: 0.36327 Test: 0.44547
Epoch 7800: New minimal relative error: 6.86%, model saved.
Epoch: 7800 Train: 0.04931 Test: 0.07457
Epoch 7900: New minimal relative error: 5.97%, model saved.
Epoch: 7900 Train: 0.04222 Test: 0.06624
Epoch: 8000 Train: 0.04444 Test: 0.06682
Epoch: 8100 Train: 0.04941 Test: 0.06900
Epoch: 8200 Train: 0.05866 Test: 0.07661
Epoch 8300: New minimal relative error: 4.27%, model saved.
Epoch: 8300 Train: 0.04864 Test: 0.07290
Epoch: 8400 Train: 0.05997 Test: 0.09254
Epoch: 8500 Train: 0.03981 Test: 0.06274
Epoch: 8600 Train: 0.04442 Test: 0.06672
Epoch: 8700 Train: 0.03785 Test: 0.06010
Epoch: 8800 Train: 0.07354 Test: 0.07463
Epoch: 8900 Train: 0.13517 Test: 0.13141
Epoch: 9000 Train: 0.03582 Test: 0.05719
Epoch: 9100 Train: 0.03688 Test: 0.05940
Epoch: 9200 Train: 0.03719 Test: 0.05918
Epoch: 9300 Train: 0.06626 Test: 0.09140
Epoch: 9400 Train: 0.11535 Test: 0.15501
Epoch: 9500 Train: 0.03366 Test: 0.05451
Epoch: 9600 Train: 0.03496 Test: 0.05540
Epoch: 9700 Train: 0.06396 Test: 0.06454
Epoch: 9800 Train: 0.05094 Test: 0.07587
Epoch: 9900 Train: 0.05156 Test: 0.07141
Epoch: 9999 Train: 0.03373 Test: 0.05500
Training Loss: tensor(0.0337)
Test Loss: tensor(0.0550)
Learned LE: [ 0.7929274   0.02154561 -3.7031314 ]
True LE: [ 8.6253762e-01  5.4715056e-04 -1.4546426e+01]
Relative Error: [5.775759   4.797038   3.680994   3.080216   3.1044695  3.099471
 2.7905273  2.5751836  2.4858577  2.613974   2.8089008  3.1656032
 3.5659685  3.942307   4.514711   4.5736814  4.501386   4.633337
 4.681847   4.352427   4.430154   4.477774   4.558117   4.5422955
 5.3530874  6.200265   6.1879196  5.193396   3.5935977  2.1958616
 1.0601959  0.2690307  0.64990354 0.6916956  1.5681874  2.37661
 2.8106928  3.431873   4.22981    4.995189   5.6607795  6.013434
 6.0570664  5.984464   6.094366   6.192721   5.9659667  5.5287933
 5.085625   4.6934967  4.177382   3.8072293  3.6701515  3.537912
 3.604946   3.6471312  3.4285698  3.9861743  4.3299513  4.2920146
 4.1683636  4.412756   4.7053375  4.111253   3.0034714  2.355757
 2.3486652  2.4888165  2.3913822  2.205158   2.1583896  2.2397938
 2.450978   2.8561268  3.2232437  3.6139646  4.0754676  4.264139
 4.295814   4.3549423  4.4197164  4.140073   4.0662313  4.103624
 4.1650963  4.080043   4.910538   5.724918   5.9544687  5.1046453
 3.6304898  2.2323456  0.9501686  0.5726441  1.2233832  1.4028682
 1.1078209  1.6266685  1.9365823  2.501576   3.2859392  4.13367
 4.681023   5.152315   5.1248     5.0338907  4.687707   4.814859
 4.667905   4.391448   4.211583   3.9241571  3.5589154  3.0437093
 3.0132203  2.945071   2.959326   3.0983932  2.799009   3.0146651
 3.6149318  3.6753752  3.2679226  3.2427292  3.6827059  3.374996
 2.5603514  1.8189622  1.7509732  1.926708   2.0167046  1.9145803
 1.9171956  1.9255085  2.118586   2.4541888  2.8360598  3.1272364
 3.5623655  3.8122456  3.8550444  3.9872022  4.0539165  3.9226546
 3.66445    3.6644154  3.673949   3.5793033  4.10446    5.0558124
 5.6911654  5.0148177  3.8041332  2.3713346  1.0707018  0.760897
 1.5246409  1.4711806  1.6255296  1.5044264  1.6058514  1.8583374
 2.4365056  3.0298417  3.659524   4.164223   4.1413965  4.0476117
 3.7073143  3.496996   3.2370284  3.0476737  3.0691428  3.0337453
 2.9844177  2.6136804  2.3254945  2.290593   2.5169673  2.5222049
 2.3645282  2.1622918  2.58321    2.978321   2.6489353  2.3419173
 2.6245503  2.4317248  2.0766697  1.4913961  1.3544124  1.5460209
 1.7561959  1.7712497  1.8334849  1.8658476  1.8997632  2.044964
 2.3730567  2.6402476  2.9577208  3.3308618  3.3295705  3.4696891
 3.5347116  3.6548336  3.4588475  3.2863903  3.2945652  3.3280096
 3.4020848  4.17807    4.967958   4.957416   3.9482706  2.4754093
 1.2739275  0.657779   1.4105015  1.4242408  1.574254   1.7099258
 1.9166702  1.8314497  1.9530847  2.1672485  2.663027   3.1032898
 3.0885012  2.9421406  2.914047   2.7470315  2.3451455  1.9165382
 1.8786503  2.0368342  2.1458201  2.106029   1.9289947  1.7820474
 1.928446   2.0888228  1.9818349  1.6503251  1.6932366  2.1073797
 2.2822368  1.8638779  1.7616706  1.6999656  1.2850913  1.2911692
 0.9501762  1.3004692  1.5667319  1.7399954  1.7594801  1.7393347
 1.7495639  1.7987301  1.9447515  2.050693   2.3968134  2.7614741
 2.7990608  2.9226725  2.9356647  3.0544772  3.2126627  2.913507
 2.9135544  3.0209444  2.984808   3.4741812  3.9255786  4.5274134
 3.9953492  2.752862   1.3472337  0.6249088  1.1363903  1.3285972
 1.2773184  1.3584112  1.8313127  2.3643894  1.9748846  1.7026998
 1.8774221  2.3131268  2.3958771  2.1728795  2.0820663  2.120071
 1.9386656  1.6014913  1.3331629  1.1821357  1.2873665  1.3870484
 1.3957951  1.369716   1.4678121  1.4736328  1.6585248  1.3393219
 1.1327614  1.2893008  1.5664299  1.6161563  1.2648171  0.96749526
 0.71557707 0.54884744 0.6658072  1.021479   1.5098323  1.698601
 1.7110082  1.6094208  1.5511687  1.5766399  1.6285859  1.6832192
 1.8580102  2.0716894  2.2827156  2.265276   2.3557794  2.4804473
 2.630413   2.631324   2.4825642  2.6069543  2.7361164  2.8081331
 3.263249   3.5134184  3.915837   3.102141   1.8372278  0.4104345
 0.8256276  1.0953876  1.0055336  1.0478711  1.2207546  1.6850915
 2.046274   1.8470043  1.4068588  1.4117123  1.7085367  1.783658
 1.6786594  1.637974   1.6002568  1.5076431  1.3790001  1.3919756
 1.08024    0.90527713 0.81238216 0.77137464 1.022329   1.200667
 1.0963068  1.1928785  0.7684025  0.78640103 0.8448786  1.092794
 0.98200625 0.71645474 0.3896359  0.24296737 0.21157709 0.5889993
 1.1994781  1.5637066  1.7616276  1.6250482  1.4932975  1.273984
 1.2587125  1.3574446  1.4672569  1.63863    1.707839   1.7092146
 1.6422069  1.8182071  2.0829015  2.1348238  2.0868933  2.1468377
 2.2342072  2.2208683  2.4724793  2.8602061 ]
