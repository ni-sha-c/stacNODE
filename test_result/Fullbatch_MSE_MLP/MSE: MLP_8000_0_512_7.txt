time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.99%, model saved.
Epoch: 0 Train: 3788.75098 Test: 4102.70703
Epoch 80: New minimal relative error: 97.59%, model saved.
Epoch: 80 Train: 125.86611 Test: 175.73515
Epoch 160: New minimal relative error: 10.61%, model saved.
Epoch: 160 Train: 9.11052 Test: 7.65612
Epoch 240: New minimal relative error: 8.23%, model saved.
Epoch: 240 Train: 5.91394 Test: 3.33261
Epoch: 320 Train: 8.26725 Test: 6.25141
Epoch: 400 Train: 13.96038 Test: 11.47743
Epoch: 480 Train: 6.18273 Test: 9.43236
Epoch: 560 Train: 2.65766 Test: 3.32398
Epoch 640: New minimal relative error: 7.97%, model saved.
Epoch: 640 Train: 2.35561 Test: 2.22976
Epoch: 720 Train: 1.86111 Test: 0.76315
Epoch: 800 Train: 5.22751 Test: 6.49503
Epoch 880: New minimal relative error: 6.56%, model saved.
Epoch: 880 Train: 0.72779 Test: 0.86027
Epoch: 960 Train: 2.78781 Test: 2.23232
Epoch: 1040 Train: 1.42803 Test: 2.14816
Epoch: 1120 Train: 0.93125 Test: 0.99578
Epoch: 1200 Train: 2.09812 Test: 2.46311
Epoch: 1280 Train: 1.27858 Test: 1.19443
Epoch: 1360 Train: 2.89316 Test: 2.09891
Epoch: 1440 Train: 2.03374 Test: 1.41533
Epoch: 1520 Train: 0.71866 Test: 1.06996
Epoch: 1600 Train: 2.53285 Test: 2.94516
Epoch: 1680 Train: 0.31448 Test: 0.12554
Epoch: 1760 Train: 1.87459 Test: 1.95736
Epoch: 1840 Train: 3.89874 Test: 4.80666
Epoch: 1920 Train: 1.03602 Test: 1.21786
Epoch: 2000 Train: 0.54977 Test: 0.44624
Epoch: 2080 Train: 0.24208 Test: 0.38311
Epoch: 2160 Train: 2.65800 Test: 2.91762
Epoch 2240: New minimal relative error: 3.91%, model saved.
Epoch: 2240 Train: 0.10061 Test: 0.13858
Epoch: 2320 Train: 0.28880 Test: 0.24949
Epoch 2400: New minimal relative error: 3.77%, model saved.
Epoch: 2400 Train: 0.03182 Test: 0.06915
Epoch: 2480 Train: 0.36140 Test: 0.36234
Epoch: 2560 Train: 1.04673 Test: 1.58431
Epoch: 2640 Train: 0.06396 Test: 0.07184
Epoch: 2720 Train: 1.06863 Test: 0.47295
Epoch: 2800 Train: 0.38672 Test: 0.49537
Epoch: 2880 Train: 1.46699 Test: 1.84656
Epoch: 2960 Train: 1.04834 Test: 1.79589
Epoch: 3040 Train: 0.03080 Test: 0.07831
Epoch: 3120 Train: 0.28605 Test: 0.44591
Epoch: 3200 Train: 0.66314 Test: 0.74619
Epoch: 3280 Train: 0.29053 Test: 0.31043
Epoch: 3360 Train: 0.29049 Test: 0.30340
Epoch: 3440 Train: 0.39857 Test: 0.55769
Epoch: 3520 Train: 0.60724 Test: 0.68323
Epoch: 3600 Train: 0.36467 Test: 0.46573
Epoch: 3680 Train: 0.77330 Test: 0.61255
Epoch: 3760 Train: 0.51993 Test: 0.39469
Epoch: 3840 Train: 0.59021 Test: 0.43648
Epoch: 3920 Train: 1.15533 Test: 1.19372
Epoch: 4000 Train: 1.05363 Test: 1.38230
Epoch: 4080 Train: 0.63977 Test: 0.97537
Epoch: 4160 Train: 0.27569 Test: 0.36378
Epoch: 4240 Train: 0.28014 Test: 0.33169
Epoch: 4320 Train: 0.16498 Test: 0.17861
Epoch: 4400 Train: 0.32251 Test: 0.37094
Epoch: 4480 Train: 0.25328 Test: 0.25876
Epoch: 4560 Train: 0.19379 Test: 0.22172
Epoch: 4640 Train: 0.11387 Test: 0.19656
Epoch: 4720 Train: 0.22803 Test: 0.30741
Epoch: 4800 Train: 0.19314 Test: 0.21819
Epoch: 4880 Train: 0.38351 Test: 0.54766
Epoch: 4960 Train: 0.25313 Test: 0.40146
Epoch: 5040 Train: 0.69969 Test: 0.96974
Epoch: 5120 Train: 0.26212 Test: 0.39513
Epoch: 5200 Train: 0.08803 Test: 0.08659
Epoch: 5280 Train: 0.03115 Test: 0.04007
Epoch: 5360 Train: 0.34616 Test: 0.41564
Epoch: 5440 Train: 0.01755 Test: 0.04277
Epoch: 5520 Train: 0.01659 Test: 0.03812
Epoch: 5600 Train: 0.40131 Test: 0.45987
Epoch 5680: New minimal relative error: 3.37%, model saved.
Epoch: 5680 Train: 0.09361 Test: 0.08314
Epoch: 5760 Train: 0.12947 Test: 0.16884
Epoch: 5840 Train: 0.03394 Test: 0.06528
Epoch: 5920 Train: 0.05726 Test: 0.10502
Epoch: 6000 Train: 0.07843 Test: 0.10403
Epoch: 6080 Train: 0.05688 Test: 0.12518
Epoch 6160: New minimal relative error: 3.00%, model saved.
Epoch: 6160 Train: 0.04481 Test: 0.06799
Epoch: 6240 Train: 0.01131 Test: 0.05624
Epoch: 6320 Train: 0.02432 Test: 0.04676
Epoch: 6400 Train: 0.03802 Test: 0.06107
Epoch: 6480 Train: 0.26811 Test: 0.34579
Epoch: 6560 Train: 0.06093 Test: 0.10828
Epoch: 6640 Train: 0.07055 Test: 0.09140
Epoch: 6720 Train: 0.02922 Test: 0.05143
Epoch: 6800 Train: 0.04275 Test: 0.08342
Epoch: 6880 Train: 0.01659 Test: 0.05566
Epoch: 6960 Train: 0.01672 Test: 0.04416
Epoch: 7040 Train: 0.13181 Test: 0.22678
Epoch: 7120 Train: 0.00607 Test: 0.03350
Epoch: 7200 Train: 0.22777 Test: 0.49571
Epoch: 7280 Train: 0.00575 Test: 0.03406
Epoch: 7360 Train: 0.07249 Test: 0.09766
Epoch 7440: New minimal relative error: 2.18%, model saved.
Epoch: 7440 Train: 0.00846 Test: 0.04107
Epoch: 7520 Train: 0.00739 Test: 0.03437
Epoch: 7600 Train: 0.18519 Test: 0.24015
Epoch: 7680 Train: 0.18737 Test: 0.15405
Epoch: 7760 Train: 0.13422 Test: 0.16637
Epoch: 7840 Train: 0.06143 Test: 0.11864
Epoch: 7920 Train: 0.16258 Test: 0.20243
Epoch: 7999 Train: 0.00549 Test: 0.03433
Training Loss: tensor(0.0055)
Test Loss: tensor(0.0343)
Learned LE: [ 0.90389216 -0.01887123 -6.179553  ]
True LE: [ 8.7557453e-01  7.6272129e-04 -1.4557181e+01]
Relative Error: [1.3851081  1.4717644  1.57937    1.650287   1.6576633  1.5944103
 1.4262462  1.1447108  0.79219776 0.47278026 0.33853218 0.39249212
 0.40141913 0.3535713  0.5733917  1.1282291  1.8327272  2.531018
 3.081009   3.3986237  3.4889858  3.42551    3.2805138  3.0750961
 2.81498    2.5523577  2.337045   2.1501114  2.0240672  2.0208225
 2.0776908  2.135908   2.1417813  2.0630574  1.8983411  1.6945044
 1.4986873  1.3625219  1.3059012  1.237786   1.1593813  1.1189044
 1.1163756  1.0954503  0.97557557 0.7915866  0.64630073 0.6255319
 0.7398404  0.9049339  1.0415841  1.1319976  1.1891829  1.211079
 1.1881827  1.1265088  1.0471263  0.9756936  0.9316993  0.91954285
 0.9308282  0.9588547  1.0054401  1.0740175  1.1713703  1.2887818
 1.3766707  1.4100826  1.3964316  1.2875549  1.0549539  0.73772335
 0.43358722 0.25227353 0.2479958  0.21343024 0.25727227 0.66894656
 1.2805046  1.981605   2.65221    3.1677113  3.4500704  3.5042677
 3.4112618  3.252295   3.0479648  2.7994807  2.556452   2.3699677
 2.191709   2.0019524  1.9148566  1.9062372  1.893869   1.8411434
 1.7167529  1.5406277  1.335447   1.1464784  1.0147544  0.9742573
 0.9118262  0.81607753 0.7694828  0.79662406 0.8236196  0.7528244
 0.61244506 0.46209145 0.3850591  0.44640383 0.5772941  0.6933917
 0.7743625  0.82729095 0.8446683  0.8162351  0.7491842  0.66332465
 0.5852875  0.5398318  0.53680634 0.5648053  0.6089488  0.6730655
 0.76566184 0.87970227 1.0046662  1.1111972  1.1670386  1.2028682
 1.1853312  1.0465297  0.8020204  0.54508376 0.36071575 0.2874605
 0.24748324 0.36213952 0.7666868  1.3290727  1.9633094  2.5853982
 3.1027498  3.4280288  3.5177052  3.4257524  3.2544103  3.0440147
 2.7951694  2.5469139  2.3682854  2.2341855  2.0625672  1.8836513
 1.7981004  1.7304176  1.6400447  1.4872625  1.3045291  1.0947828
 0.91333836 0.76505595 0.72571087 0.68481535 0.57615805 0.49417895
 0.5144911  0.5921242  0.6184656  0.55202323 0.40858117 0.249398
 0.16969296 0.24027216 0.34642527 0.4285873  0.48123872 0.5022859
 0.4860079  0.43770102 0.36530817 0.28564626 0.22363144 0.20398197
 0.23472926 0.29014996 0.35364377 0.44144624 0.5680566  0.7149696
 0.84610915 0.92487645 0.966436   1.0267733  1.0213143  0.8957956
 0.69992644 0.53221774 0.43261877 0.3867807  0.44194558 0.7519431
 1.229166   1.7687293  2.3169343  2.846519   3.2914202  3.5255368
 3.4952338  3.310207   3.0821543  2.8198774  2.5386465  2.297383
 2.157316   2.0638955  1.9380931  1.7734113  1.655474   1.5508869
 1.4103533  1.2001843  0.988781   0.7903028  0.6356238  0.56291944
 0.5418127  0.46524337 0.34292996 0.28897285 0.3753229  0.52022815
 0.5642353  0.4784789  0.33492413 0.19838198 0.09825488 0.04310174
 0.12628491 0.17938113 0.20586517 0.22160763 0.24192189 0.24646492
 0.22229736 0.18702157 0.14804679 0.0952998  0.04623744 0.07819451
 0.14214425 0.22671959 0.35808197 0.5312278  0.68009764 0.74198806
 0.7618121  0.84163296 0.8819075  0.8151602  0.68703914 0.58751106
 0.539939   0.53113914 0.65056574 0.9863039  1.4294481  1.8815197
 2.3393657  2.8421571  3.2969766  3.5036128  3.3967726  3.1457763
 2.8831468  2.5910974  2.2916756  2.0309722  1.8708751  1.7972482
 1.7640826  1.6478807  1.5078368  1.4178256  1.2831503  1.0378895
 0.80031985 0.6148664  0.5033554  0.47275704 0.43849587 0.35120144
 0.2340124  0.19874279 0.35058194 0.52114064 0.542108   0.45103663
 0.3449364  0.28820145 0.2379112  0.14982785 0.08327533 0.06099801
 0.09842364 0.19363722 0.29231814 0.34320167 0.34610656 0.32874036
 0.3022515  0.25814304 0.1963343  0.13553822 0.1026288  0.10138503
 0.13425377 0.2661712  0.46147016 0.5811316  0.5687108  0.59706634
 0.6961475  0.7260766  0.67500603 0.622311   0.6167794  0.64093983
 0.72184604 0.9675003  1.3311923  1.6954436  2.058845   2.500534
 2.9607956  3.245724   3.2058067  2.9477012  2.6858454  2.4199047
 2.1175618  1.8419342  1.6189752  1.4832991  1.4344882  1.4551246
 1.3390489  1.2478969  1.2222972  1.0470319  0.7614881  0.5422919
 0.45302692 0.44227642 0.41549826 0.34023887 0.24206606 0.20268662
 0.32099193 0.46634418 0.47996062 0.4048536  0.32322243 0.3078311
 0.3148665  0.26637042 0.20126465 0.17774016 0.20565055 0.285465
 0.38618353 0.4478217  0.45361477 0.42851394 0.39668489 0.36126062
 0.31726313 0.26894924 0.2310752  0.21546873 0.20442523 0.15628877
 0.11968426 0.28839675 0.44901642 0.42110714 0.38346767 0.48804712
 0.55746716 0.56212175 0.56617635 0.6144597  0.68225104 0.7699291
 0.9673004  1.2499002  1.525956   1.8272862 ]
