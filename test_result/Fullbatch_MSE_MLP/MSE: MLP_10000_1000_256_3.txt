time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.40%, model saved.
Epoch: 0 Train: 4189.71143 Test: 4057.28076
Epoch 100: New minimal relative error: 83.19%, model saved.
Epoch: 100 Train: 122.69113 Test: 119.56933
Epoch 200: New minimal relative error: 66.77%, model saved.
Epoch: 200 Train: 17.28403 Test: 19.23936
Epoch 300: New minimal relative error: 31.43%, model saved.
Epoch: 300 Train: 9.19261 Test: 14.43073
Epoch: 400 Train: 4.12257 Test: 4.96839
Epoch: 500 Train: 2.58618 Test: 3.27302
Epoch: 600 Train: 8.48055 Test: 4.40295
Epoch 700: New minimal relative error: 16.38%, model saved.
Epoch: 700 Train: 2.14866 Test: 2.35466
Epoch: 800 Train: 1.54798 Test: 2.15062
Epoch: 900 Train: 0.95417 Test: 1.26536
Epoch: 1000 Train: 0.98835 Test: 1.15597
Epoch: 1100 Train: 2.21287 Test: 3.11191
Epoch: 1200 Train: 0.59704 Test: 0.78559
Epoch: 1300 Train: 0.41732 Test: 0.58178
Epoch: 1400 Train: 0.37607 Test: 0.53481
Epoch: 1500 Train: 0.34657 Test: 0.49255
Epoch: 1600 Train: 0.36831 Test: 0.51595
Epoch: 1700 Train: 2.77089 Test: 3.88268
Epoch: 1800 Train: 0.29477 Test: 0.43920
Epoch: 1900 Train: 0.22755 Test: 0.35273
Epoch: 2000 Train: 0.21616 Test: 0.33907
Epoch: 2100 Train: 0.22163 Test: 0.59571
Epoch 2200: New minimal relative error: 15.46%, model saved.
Epoch: 2200 Train: 2.20786 Test: 2.82435
Epoch: 2300 Train: 0.28172 Test: 0.42519
Epoch 2400: New minimal relative error: 11.54%, model saved.
Epoch: 2400 Train: 0.16294 Test: 0.26790
Epoch: 2500 Train: 0.16083 Test: 0.26418
Epoch: 2600 Train: 0.49076 Test: 0.73125
Epoch: 2700 Train: 0.13643 Test: 0.23128
Epoch: 2800 Train: 0.14814 Test: 0.23758
Epoch: 2900 Train: 4.82505 Test: 5.49622
Epoch: 3000 Train: 0.12113 Test: 0.20890
Epoch: 3100 Train: 1.15883 Test: 1.57317
Epoch: 3200 Train: 0.10862 Test: 0.19095
Epoch: 3300 Train: 0.10664 Test: 0.18798
Epoch: 3400 Train: 0.11358 Test: 0.19703
Epoch: 3500 Train: 0.11323 Test: 0.19359
Epoch: 3600 Train: 0.09920 Test: 0.17551
Epoch: 3700 Train: 0.09279 Test: 0.16659
Epoch: 3800 Train: 0.14285 Test: 0.16768
Epoch: 3900 Train: 0.98315 Test: 1.33792
Epoch 4000: New minimal relative error: 10.62%, model saved.
Epoch: 4000 Train: 0.08041 Test: 0.14937
Epoch: 4100 Train: 0.07904 Test: 0.14715
Epoch: 4200 Train: 0.08489 Test: 0.15366
Epoch: 4300 Train: 0.07474 Test: 0.14338
Epoch: 4400 Train: 0.10306 Test: 0.14179
Epoch: 4500 Train: 0.15047 Test: 0.23779
Epoch: 4600 Train: 0.09053 Test: 0.15770
Epoch: 4700 Train: 0.22259 Test: 0.22172
Epoch: 4800 Train: 0.73368 Test: 0.53485
Epoch: 4900 Train: 0.12589 Test: 0.22832
Epoch: 5000 Train: 0.06071 Test: 0.11794
Epoch: 5100 Train: 0.06077 Test: 0.12296
Epoch: 5200 Train: 0.07171 Test: 0.13268
Epoch: 5300 Train: 0.08530 Test: 0.15595
Epoch: 5400 Train: 0.05634 Test: 0.11145
Epoch: 5500 Train: 0.05432 Test: 0.10724
Epoch: 5600 Train: 1.77546 Test: 2.34171
Epoch: 5700 Train: 0.05239 Test: 0.10420
Epoch: 5800 Train: 0.22025 Test: 0.26150
Epoch: 5900 Train: 0.05103 Test: 0.10178
Epoch: 6000 Train: 0.04972 Test: 0.09938
Epoch: 6100 Train: 0.04805 Test: 0.09634
Epoch: 6200 Train: 0.04967 Test: 0.09857
Epoch: 6300 Train: 0.06900 Test: 0.12801
Epoch: 6400 Train: 0.04571 Test: 0.09216
Epoch: 6500 Train: 0.04495 Test: 0.09152
Epoch: 6600 Train: 0.04417 Test: 0.08934
Epoch: 6700 Train: 0.04388 Test: 0.08958
Epoch: 6800 Train: 0.50330 Test: 0.41435
Epoch: 6900 Train: 0.67742 Test: 0.44520
Epoch: 7000 Train: 0.04119 Test: 0.08476
Epoch: 7100 Train: 0.04047 Test: 0.08296
Epoch: 7200 Train: 0.04047 Test: 0.08286
Epoch: 7300 Train: 0.03914 Test: 0.08091
Epoch: 7400 Train: 0.03858 Test: 0.08009
Epoch: 7500 Train: 0.03841 Test: 0.08026
Epoch: 7600 Train: 0.03736 Test: 0.07772
Epoch: 7700 Train: 0.04209 Test: 0.07856
Epoch: 7800 Train: 0.03634 Test: 0.07594
Epoch: 7900 Train: 0.06530 Test: 0.08082
Epoch: 8000 Train: 0.03687 Test: 0.07451
Epoch 8100: New minimal relative error: 10.13%, model saved.
Epoch: 8100 Train: 0.03479 Test: 0.07314
Epoch: 8200 Train: 0.03495 Test: 0.07319
Epoch: 8300 Train: 0.03385 Test: 0.07135
Epoch: 8400 Train: 0.03994 Test: 0.07232
Epoch: 8500 Train: 0.03300 Test: 0.06984
Epoch: 8600 Train: 0.10723 Test: 0.18384
Epoch: 8700 Train: 0.03218 Test: 0.06838
Epoch: 8800 Train: 0.03172 Test: 0.06742
Epoch: 8900 Train: 0.03356 Test: 0.06944
Epoch: 9000 Train: 0.03099 Test: 0.06607
Epoch: 9100 Train: 0.03057 Test: 0.06525
Epoch: 9200 Train: 0.03082 Test: 0.06531
Epoch: 9300 Train: 0.02989 Test: 0.06404
Epoch: 9400 Train: 0.02961 Test: 0.06327
Epoch: 9500 Train: 0.02928 Test: 0.06468
Epoch: 9600 Train: 0.02886 Test: 0.06209
Epoch: 9700 Train: 0.19529 Test: 0.16409
Epoch: 9800 Train: 0.02825 Test: 0.06098
Epoch: 9900 Train: 0.04286 Test: 0.09656
Epoch: 9999 Train: 0.02761 Test: 0.05967
Training Loss: tensor(0.0276)
Test Loss: tensor(0.0597)
Learned LE: [ 0.8696869  -0.03119141 -5.380568  ]
True LE: [ 8.4428245e-01  1.1456747e-02 -1.4525654e+01]
Relative Error: [ 6.539303   7.5448523  8.693901   9.886093  11.098385  12.318778
 13.372744  14.07305   14.357617  14.240379  13.80758   13.217224
 12.602978  12.012587  11.443362  10.904888  10.435529  10.083651
  9.888406   9.872122  10.040176  10.38355   10.878634  11.486154
 12.158634  12.855387  13.546526  14.202406  14.786344  15.259086
 15.590837  15.754768  15.702234  15.357903  14.6761465 13.672524
 12.390691  10.907938   9.387413   8.083145   7.1473894  6.704472
  6.897728   7.5397673  8.276508   8.902875   9.359124   9.637041
  9.738404   9.66966    9.443359   9.077153   8.58776    7.9773793
  7.213549   6.257184   5.20837    4.3834023  4.0156364  4.0049
  4.2337646  4.8100724  5.592881   6.4943304  7.554233   8.676387
  9.8512745 11.045762  12.053662  12.71282   12.995238  12.927546
 12.592101  12.094798  11.520861  10.91817   10.307269   9.72083
  9.215066   8.846008   8.649794   8.638692   8.809053   9.150559
  9.648455  10.275824  10.986463  11.728466  12.461742  13.153614
 13.766162  14.259384  14.606137  14.786093  14.752051  14.423169
 13.752101  12.757119  11.48187   10.00804    8.522532   7.293263
  6.4705744  6.269222   6.7564173  7.566946   8.360015   9.012242
  9.493051   9.786195   9.881154   9.778658   9.493092   9.050911
  8.484646   7.822138   7.067821   6.1863403  5.1745257  4.2471375
  3.7096984  3.5617504  3.6320474  4.024104   4.691915   5.4827747
  6.442519   7.4851594  8.605897   9.747376  10.679124  11.272796
 11.556668  11.618192  11.497035  11.144608  10.579462   9.902928
  9.201984   8.545411   8.004615   7.6371164  7.467822   7.490135
  7.6807165  8.01823    8.492925   9.09999    9.816437  10.590371
 11.365926  12.100627  12.752339  13.276521  13.645651  13.849129
 13.846116  13.549331  12.904758  11.9324465 10.675575   9.216664
  7.763239   6.5916667  5.8837996  5.931393   6.6480107  7.567232
  8.427135   9.151343   9.700016  10.041691  10.157592  10.043377
  9.709247   9.183041   8.508342   7.730914   6.8924246  6.035138
  5.1556954  4.2470045  3.5372999  3.2101579  3.141545   3.3273616
  3.8645494  4.5353703  5.3795314  6.3300214  7.377818   8.449583
  9.294399   9.808868  10.107803  10.372511  10.52732   10.317704
  9.710038   8.898224   8.064968   7.3223987  6.7548633  6.4144807
  6.3067927  6.4003563  6.6465507  7.0031037  7.451564   8.001134
  8.669211   9.437213  10.242556  11.023089  11.726213  12.297902
 12.702456  12.938123  12.979791  12.735297  12.136341  11.203096
  9.977791   8.539199   7.1078525  5.970191   5.3642206  5.6244392
  6.500469   7.512112   8.46477    9.283803   9.914966  10.320798
 10.48325   10.398776  10.069948   9.505902   8.739286   7.8283696
  6.834937   5.866356   5.06576    4.33123    3.5391302  2.9857988
  2.768646   2.7552836  3.1247437  3.667181   4.3748727  5.216863
  6.1709805  7.1730123  7.9524593  8.406956   8.754885   9.229358
  9.609985   9.519602   8.869653   7.904592   6.913789   6.070179
  5.4798884  5.189206   5.1723294  5.3650284  5.6958985  6.103129
  6.549908   7.036043   7.6015763  8.292239   9.084858   9.899206
 10.661271  11.301419  11.763939  12.044104  12.143797  11.976179
 11.447549  10.572722   9.3956995  7.984267   6.5588703  5.423149
  4.8774176  5.2809176  6.2586164  7.350587   8.394331   9.308421
 10.026549  10.499291  10.707774  10.666071  10.396021   9.896559
  9.14797    8.166457   7.026852   5.8636756  4.942014   4.363703
  3.705066   2.9407535  2.5058162  2.3370507  2.4825523  2.8926125
  3.4403942  4.155605   4.9909387  5.9346547  6.6962996  7.1431255
  7.5829515  8.186408   8.650091   8.662667   8.055296   6.991028
  5.84849    4.8926897  4.269009   4.037746   4.1326027  4.43452
  4.8524184  5.3213573  5.7931128  6.2452984  6.6990113  7.2326393
  7.9207473  8.720389   9.528765  10.252641  10.805522  11.153495
 11.322591  11.258623  10.834746  10.04331    8.9365635  7.564575
  6.127519   4.9529357  4.393902   4.848334   5.8652782  7.008975
  8.144764   9.183352  10.020476  10.567545  10.801183  10.7639
 10.520231  10.109842   9.510025   8.642433   7.4816027  6.1690187
  5.0123596  4.311183   3.875865   3.1215239  2.3945508  2.0616717
  1.9634409  2.2292862  2.6030507  3.177023   3.8671465  4.7538605
  5.541637   6.0319014  6.5899186  7.222904   7.6230993  7.689628
  7.2400665  6.218867   4.9869876  3.9358475  3.25941    3.0626388
  3.2726076  3.6852074  4.1731205  4.685332   5.183541   5.6328707
  6.0207324  6.386836   6.855131   7.52404  ]
