time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.88%, model saved.
Epoch: 0 Train: 4267.02930 Test: 3944.77148
Epoch: 80 Train: 180.86079 Test: 171.54517
Epoch 160: New minimal relative error: 87.84%, model saved.
Epoch: 160 Train: 49.37830 Test: 67.24666
Epoch 240: New minimal relative error: 42.71%, model saved.
Epoch: 240 Train: 18.01331 Test: 28.46272
Epoch: 320 Train: 11.95456 Test: 27.83445
Epoch 400: New minimal relative error: 37.86%, model saved.
Epoch: 400 Train: 4.90513 Test: 19.09696
Epoch: 480 Train: 11.83447 Test: 28.08837
Epoch: 560 Train: 15.26963 Test: 31.27363
Epoch 640: New minimal relative error: 18.95%, model saved.
Epoch: 640 Train: 2.16720 Test: 16.46580
Epoch: 720 Train: 16.23918 Test: 28.64096
Epoch: 800 Train: 1.93822 Test: 16.17319
Epoch: 880 Train: 4.32754 Test: 18.85712
Epoch: 960 Train: 1.32980 Test: 15.58006
Epoch: 1040 Train: 1.11765 Test: 14.72763
Epoch 1120: New minimal relative error: 18.77%, model saved.
Epoch: 1120 Train: 0.75214 Test: 14.42782
Epoch: 1200 Train: 2.45127 Test: 17.22621
Epoch: 1280 Train: 0.49701 Test: 14.26967
Epoch: 1360 Train: 0.50545 Test: 14.35431
Epoch: 1440 Train: 1.85707 Test: 15.76425
Epoch: 1520 Train: 10.01660 Test: 22.12759
Epoch: 1600 Train: 1.00515 Test: 14.84804
Epoch: 1680 Train: 11.94616 Test: 26.77551
Epoch: 1760 Train: 0.27752 Test: 13.79780
Epoch: 1840 Train: 0.27839 Test: 13.88163
Epoch 1920: New minimal relative error: 17.26%, model saved.
Epoch: 1920 Train: 3.00941 Test: 18.14306
Epoch: 2000 Train: 0.23253 Test: 13.66322
Epoch: 2080 Train: 0.25032 Test: 13.73264
Epoch: 2160 Train: 0.36912 Test: 13.92423
Epoch: 2240 Train: 2.47359 Test: 15.63315
Epoch: 2320 Train: 7.32306 Test: 21.94436
Epoch: 2400 Train: 0.20935 Test: 13.61644
Epoch: 2480 Train: 0.35322 Test: 13.81604
Epoch: 2560 Train: 0.15627 Test: 13.66598
Epoch: 2640 Train: 0.24048 Test: 13.78866
Epoch: 2720 Train: 0.14898 Test: 13.53855
Epoch: 2800 Train: 0.15566 Test: 13.70102
Epoch: 2880 Train: 0.14886 Test: 13.67360
Epoch: 2960 Train: 0.31201 Test: 14.18182
Epoch: 3040 Train: 0.11724 Test: 13.49654
Epoch: 3120 Train: 0.11746 Test: 13.52852
Epoch: 3200 Train: 0.21432 Test: 13.55861
Epoch: 3280 Train: 0.44704 Test: 14.00192
Epoch: 3360 Train: 1.55198 Test: 15.35827
Epoch: 3440 Train: 1.21067 Test: 14.73339
Epoch: 3520 Train: 0.26910 Test: 13.57112
Epoch: 3600 Train: 0.13908 Test: 13.41721
Epoch: 3680 Train: 0.17870 Test: 13.42817
Epoch: 3760 Train: 0.14015 Test: 13.36866
Epoch: 3840 Train: 0.10750 Test: 13.35872
Epoch: 3920 Train: 0.14602 Test: 13.38125
Epoch: 4000 Train: 0.09269 Test: 13.30193
Epoch: 4080 Train: 0.08774 Test: 13.36780
Epoch: 4160 Train: 0.13799 Test: 13.34938
Epoch: 4240 Train: 0.24394 Test: 13.52444
Epoch: 4320 Train: 1.28465 Test: 14.62180
Epoch: 4400 Train: 0.23797 Test: 13.48339
Epoch: 4480 Train: 0.12677 Test: 13.31182
Epoch: 4560 Train: 0.24031 Test: 13.42532
Epoch: 4640 Train: 0.07330 Test: 13.16168
Epoch 4720: New minimal relative error: 9.98%, model saved.
Epoch: 4720 Train: 0.06478 Test: 13.14533
Epoch: 4800 Train: 0.09224 Test: 13.22200
Epoch: 4880 Train: 0.11966 Test: 13.26257
Epoch: 4960 Train: 0.06539 Test: 13.09261
Epoch: 5040 Train: 0.06052 Test: 13.11485
Epoch: 5120 Train: 0.17789 Test: 13.11737
Epoch: 5200 Train: 0.07950 Test: 13.07758
Epoch: 5280 Train: 0.05566 Test: 13.04908
Epoch: 5360 Train: 0.05952 Test: 13.06648
Epoch: 5440 Train: 1.11091 Test: 14.03331
Epoch: 5520 Train: 0.05756 Test: 13.03485
Epoch: 5600 Train: 0.05210 Test: 13.03906
Epoch: 5680 Train: 0.89875 Test: 13.77907
Epoch: 5760 Train: 0.04549 Test: 13.03492
Epoch: 5840 Train: 0.21363 Test: 13.04133
Epoch: 5920 Train: 0.91163 Test: 13.69845
Epoch: 6000 Train: 0.04942 Test: 12.97631
Epoch: 6080 Train: 0.04404 Test: 12.97911
Epoch: 6160 Train: 2.59694 Test: 15.06658
Epoch: 6240 Train: 0.04082 Test: 13.00054
Epoch: 6320 Train: 0.03885 Test: 12.95196
Epoch: 6400 Train: 0.04660 Test: 12.95483
Epoch: 6480 Train: 0.52822 Test: 13.17377
Epoch: 6560 Train: 0.03752 Test: 12.93717
Epoch: 6640 Train: 0.20757 Test: 12.95534
Epoch: 6720 Train: 0.03612 Test: 12.92873
Epoch: 6800 Train: 0.03512 Test: 12.89551
Epoch: 6880 Train: 0.04038 Test: 12.95094
Epoch: 6960 Train: 0.03427 Test: 12.91850
Epoch: 7040 Train: 0.27110 Test: 12.94252
Epoch: 7120 Train: 0.03305 Test: 12.87568
Epoch: 7200 Train: 0.04213 Test: 12.92218
Epoch: 7280 Train: 0.03227 Test: 12.88731
Epoch: 7360 Train: 0.03153 Test: 12.86035
Epoch: 7440 Train: 0.07612 Test: 12.94746
Epoch: 7520 Train: 0.03092 Test: 12.86768
Epoch: 7600 Train: 0.03024 Test: 12.84046
Epoch: 7680 Train: 0.15485 Test: 12.96430
Epoch: 7760 Train: 0.02984 Test: 12.81508
Epoch: 7840 Train: 0.03328 Test: 12.82683
Epoch: 7920 Train: 0.02869 Test: 12.80227
Epoch: 7999 Train: 0.14245 Test: 12.88235
Training Loss: tensor(0.1424)
Test Loss: tensor(12.8823)
Learned LE: [ 0.8504558  -0.14003049 -4.9719753 ]
True LE: [ 8.6718321e-01  7.1759843e-03 -1.4550652e+01]
Relative Error: [ 7.5826936  7.8824944  8.194445   8.527145   8.881553   9.263154
  9.692806  10.190166  10.734079  11.255546  11.667501  11.869905
 11.770052  11.286002  10.279899   8.643408   6.383004   3.93106
  3.7850327  7.073837  10.926982  14.002046  15.804976  16.475157
 16.467125  16.16908   15.7771845 15.375248  15.002258  14.662972
 14.328736  13.95004   13.476415  12.873311  12.133256  11.284366
 10.410941   9.687596   9.302044   9.334441   9.763692  10.403831
 10.809733  10.3966875  8.9648695  7.2224708  6.0615587  5.765111
  6.0030184  6.314287   6.490449   6.4934926  6.3503094  5.93244
  5.432892   5.1019077  4.9622097  5.0646253  5.35712    5.734453
  6.126379   6.500963   6.8482103  7.177688   7.5109534  7.864641
  8.243322   8.642629   9.069141   9.553942  10.104267  10.666242
 11.168235  11.549591  11.727604  11.610615  11.055591   9.857076
  7.918363   5.3049054  2.9022443  4.504997   8.383857  11.835969
 14.025553  14.953423  15.082682  14.866888  14.549117  14.225951
 13.935295  13.680406  13.430164  13.130249  12.7239275 12.169139
 11.450088  10.601908   9.721675   8.919936   8.325524   8.059381
  8.1669855  8.657463   9.341402   9.678628   9.009045   7.3898373
  5.8469853  5.1198263  5.1758604  5.4605536  5.663404   5.7979064
  5.7957726  5.470437   4.9926996  4.632773   4.3784347  4.3630753
  4.5805106  4.9169297  5.2973638  5.6883     6.0740514  6.4430466
  6.8018503  7.169084   7.5561633  7.9602914  8.368807   8.794912
  9.285249   9.834522  10.376628  10.857481  11.237646  11.438668
 11.360031  10.789736   9.472311   7.3186917  4.4223337  2.352256
  5.2807837  9.121863  11.901242  13.271758  13.62613   13.51901
 13.296226  13.077259  12.886365  12.721925  12.55881   12.34475
 12.016137  11.524543  10.8499    10.003386   9.066979   8.2182455
  7.5714192  7.110172   6.8721848  6.940756   7.416757   8.143903
  8.486916   7.6960983  6.0735965  4.805662   4.4423375  4.6343102
  4.8074493  5.00187    5.0966644  4.9192295  4.501515   4.1988397
  3.9055314  3.7641275  3.8811016  4.1582294  4.500703   4.863988
  5.240472   5.632687   6.024288   6.411474   6.805204   7.205505
  7.5931063  7.949811   8.312006   8.752792   9.26492    9.782149
 10.262944  10.677454  10.946016  10.964214  10.47035    9.163873
  6.9431477  3.8714447  2.0942647  5.6202135  9.145669  11.261105
 12.03903   12.106629  12.001312  11.916489  11.851667  11.786651
 11.713364  11.604202  11.39205   11.005042  10.408564   9.607711
  8.648407   7.6553054  6.8462424  6.3598046  6.0149617  5.718628
  5.650451   6.0415382  6.8290653  7.325966   6.622836   5.103431
  4.062347   3.9442391  4.1150045  4.217799   4.3553443  4.3408885
  3.9492555  3.6899574  3.462463   3.2470036  3.2355819  3.4368653
  3.742353   4.073983   4.398347   4.7350383  5.121866   5.535857
  5.9470997  6.3481903  6.7188354  7.032264   7.2883925  7.5506053
  7.9112754  8.371587   8.868499   9.362515   9.831598  10.198608
 10.36577   10.065459   8.936792   6.845984   3.7830381  1.7087092
  5.304456   8.513239  10.146887  10.586635  10.616131  10.673776
 10.772539  10.830782  10.836526  10.8164835 10.727083  10.477921
 10.024325   9.374167   8.5404     7.5470552  6.5305204  5.716408
  5.2612033  5.028359   4.673062   4.40543    4.612692   5.408208
  6.210158   5.8571396  4.467861   3.5466506  3.559818   3.6486552
  3.7531984  3.8303595  3.5554018  3.1656756  2.9723718  2.7649302
  2.6517038  2.7278438  2.9791918  3.2834902  3.5783246  3.8557296
  4.1406603  4.51008    4.9449315  5.3672     5.733145   6.0154057
  6.215338   6.3649507  6.529922   6.7955785  7.1841984  7.6483912
  8.143902   8.655249   9.127465   9.468697   9.478976   8.728558
  6.9964013  4.218397   1.2265458  4.2501087  7.3371964  8.772135
  9.106423   9.24836    9.519348   9.779069   9.898898   9.919752
  9.863275   9.67511    9.324636   8.8074465  8.152204   7.393191
  6.562139   5.7375197  4.9923835  4.3960805  4.100277   3.8322682
  3.4014623  3.3123956  3.90336    4.9654083  5.2827435  4.130565
  3.1431344  3.1766877  3.234325   3.3406868  3.389217   2.9927368
  2.6035097  2.3356464  2.1148264  2.073416   2.2014391  2.46091
  2.725187   2.9608493  3.2199051  3.4815998  3.8072507  4.227895
  4.632794   4.9256353  5.0952034  5.182665   5.2367625  5.311703
  5.4718246  5.753643   6.148661   6.6151924  7.1221747  7.657793
  8.142055   8.481793   8.315339   7.2263293  5.06061    1.9122424
  2.452547   5.631263   7.252341   7.6731114]
