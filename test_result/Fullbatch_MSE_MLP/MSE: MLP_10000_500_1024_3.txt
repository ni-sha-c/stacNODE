time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.78%, model saved.
Epoch: 0 Train: 3554.76318 Test: 4096.03955
Epoch 100: New minimal relative error: 19.77%, model saved.
Epoch: 100 Train: 22.35198 Test: 21.27107
Epoch: 200 Train: 15.41811 Test: 17.42750
Epoch: 300 Train: 14.23837 Test: 18.34417
Epoch 400: New minimal relative error: 9.26%, model saved.
Epoch: 400 Train: 2.08916 Test: 2.09891
Epoch: 500 Train: 2.76074 Test: 3.34807
Epoch: 600 Train: 1.93967 Test: 2.22056
Epoch: 700 Train: 1.57017 Test: 1.25610
Epoch 800: New minimal relative error: 8.10%, model saved.
Epoch: 800 Train: 0.78128 Test: 0.73727
Epoch: 900 Train: 0.55859 Test: 0.42413
Epoch 1000: New minimal relative error: 6.41%, model saved.
Epoch: 1000 Train: 0.48425 Test: 0.46293
Epoch: 1100 Train: 5.24986 Test: 5.98162
Epoch: 1200 Train: 0.47754 Test: 0.33757
Epoch: 1300 Train: 1.01269 Test: 1.06402
Epoch: 1400 Train: 0.58626 Test: 0.64468
Epoch: 1500 Train: 1.75157 Test: 2.36036
Epoch: 1600 Train: 1.48555 Test: 1.89814
Epoch: 1700 Train: 0.90880 Test: 0.76165
Epoch: 1800 Train: 0.23372 Test: 0.25652
Epoch: 1900 Train: 1.28816 Test: 0.96546
Epoch: 2000 Train: 0.54653 Test: 0.67401
Epoch: 2100 Train: 0.49487 Test: 0.72062
Epoch: 2200 Train: 0.13072 Test: 0.10712
Epoch: 2300 Train: 1.74350 Test: 1.81073
Epoch: 2400 Train: 0.12842 Test: 0.13645
Epoch: 2500 Train: 3.29492 Test: 3.64677
Epoch: 2600 Train: 0.40844 Test: 0.35095
Epoch 2700: New minimal relative error: 3.84%, model saved.
Epoch: 2700 Train: 0.12446 Test: 0.09543
Epoch: 2800 Train: 0.37278 Test: 0.23680
Epoch: 2900 Train: 1.35083 Test: 1.41895
Epoch 3000: New minimal relative error: 3.20%, model saved.
Epoch: 3000 Train: 0.13379 Test: 0.14685
Epoch: 3100 Train: 0.09916 Test: 0.08202
Epoch: 3200 Train: 0.11978 Test: 0.14427
Epoch: 3300 Train: 0.24078 Test: 0.23593
Epoch: 3400 Train: 0.48458 Test: 0.51782
Epoch: 3500 Train: 0.06958 Test: 0.06178
Epoch: 3600 Train: 0.07914 Test: 0.08100
Epoch: 3700 Train: 0.35396 Test: 0.33692
Epoch: 3800 Train: 0.06891 Test: 0.06620
Epoch: 3900 Train: 0.06782 Test: 0.06551
Epoch: 4000 Train: 0.32147 Test: 0.21920
Epoch: 4100 Train: 0.09560 Test: 0.10712
Epoch: 4200 Train: 0.16101 Test: 0.15225
Epoch: 4300 Train: 0.33863 Test: 0.37978
Epoch: 4400 Train: 0.15335 Test: 0.16988
Epoch: 4500 Train: 1.36658 Test: 1.33271
Epoch: 4600 Train: 0.08890 Test: 0.07440
Epoch: 4700 Train: 0.15463 Test: 0.06841
Epoch: 4800 Train: 0.17551 Test: 0.19426
Epoch: 4900 Train: 0.08989 Test: 0.10308
Epoch: 5000 Train: 0.04510 Test: 0.04937
Epoch: 5100 Train: 0.68572 Test: 0.99285
Epoch: 5200 Train: 0.07875 Test: 0.06326
Epoch: 5300 Train: 0.10006 Test: 0.10710
Epoch: 5400 Train: 0.03907 Test: 0.04468
Epoch: 5500 Train: 0.05763 Test: 0.05425
Epoch: 5600 Train: 0.65786 Test: 0.75368
Epoch: 5700 Train: 0.11939 Test: 0.13937
Epoch: 5800 Train: 0.16244 Test: 0.18998
Epoch: 5900 Train: 0.55499 Test: 0.43700
Epoch 6000: New minimal relative error: 2.76%, model saved.
Epoch: 6000 Train: 0.04830 Test: 0.04505
Epoch: 6100 Train: 0.03592 Test: 0.03904
Epoch: 6200 Train: 0.20802 Test: 0.22809
Epoch: 6300 Train: 0.03003 Test: 0.03214
Epoch: 6400 Train: 0.03795 Test: 0.04196
Epoch: 6500 Train: 0.03736 Test: 0.04504
Epoch: 6600 Train: 0.07183 Test: 0.08440
Epoch 6700: New minimal relative error: 2.38%, model saved.
Epoch: 6700 Train: 0.03748 Test: 0.03963
Epoch: 6800 Train: 0.03046 Test: 0.03360
Epoch: 6900 Train: 0.27287 Test: 0.33456
Epoch: 7000 Train: 0.08164 Test: 0.10021
Epoch: 7100 Train: 0.02452 Test: 0.02788
Epoch: 7200 Train: 0.20925 Test: 0.23138
Epoch: 7300 Train: 0.09650 Test: 0.11777
Epoch: 7400 Train: 0.03530 Test: 0.03634
Epoch: 7500 Train: 0.02411 Test: 0.02905
Epoch: 7600 Train: 0.03438 Test: 0.03951
Epoch: 7700 Train: 0.17675 Test: 0.16752
Epoch: 7800 Train: 0.07816 Test: 0.09257
Epoch: 7900 Train: 0.04731 Test: 0.05537
Epoch: 8000 Train: 0.04901 Test: 0.05094
Epoch: 8100 Train: 0.02907 Test: 0.03430
Epoch: 8200 Train: 0.02936 Test: 0.02945
Epoch: 8300 Train: 0.04902 Test: 0.03375
Epoch: 8400 Train: 0.02997 Test: 0.02879
Epoch: 8500 Train: 0.08992 Test: 0.09017
Epoch: 8600 Train: 0.01842 Test: 0.02276
Epoch: 8700 Train: 0.01817 Test: 0.02130
Epoch: 8800 Train: 0.01693 Test: 0.02094
Epoch: 8900 Train: 0.03444 Test: 0.04227
Epoch: 9000 Train: 0.05870 Test: 0.06594
Epoch: 9100 Train: 0.02038 Test: 0.02639
Epoch: 9200 Train: 0.01587 Test: 0.01841
Epoch: 9300 Train: 0.03326 Test: 0.04522
Epoch: 9400 Train: 0.07717 Test: 0.07705
Epoch: 9500 Train: 0.04898 Test: 0.06015
Epoch 9600: New minimal relative error: 2.22%, model saved.
Epoch: 9600 Train: 0.04569 Test: 0.05372
Epoch: 9700 Train: 0.01597 Test: 0.01844
Epoch: 9800 Train: 0.01502 Test: 0.01906
Epoch: 9900 Train: 0.02366 Test: 0.02884
Epoch: 9999 Train: 0.02174 Test: 0.02039
Training Loss: tensor(0.0217)
Test Loss: tensor(0.0204)
Learned LE: [ 0.85871506 -0.01695238 -5.9564953 ]
True LE: [ 8.5089195e-01 -8.3990354e-04 -1.4521540e+01]
Relative Error: [1.2945547  1.2457347  1.240959   1.2703055  1.3220837  1.3870634
 1.4611512  1.5447879  1.6407499  1.7507797  1.8731037  2.0036058
 2.1391468  2.2756302  2.4007165  2.4924893  2.528415   2.4946144
 2.389193   2.2256396  2.0299933  1.8253618  1.6216404  1.4153812
 1.2086222  1.023658   0.8873387  0.81881934 0.82362473 0.8853172
 0.9717352  1.0552685  1.1220452  1.1724352  1.2182863  1.271125
 1.3327636  1.3940833  1.441667   1.4692575  1.4810096  1.4841043
 1.4823793  1.4756308  1.4597337  1.4617834  1.6082376  1.9749886
 2.3566015  2.5463152  2.5343368  2.4092023  2.2649622  2.1582038
 2.084596   2.0218232  1.9644692  1.9138926  1.8558482  1.7554935
 1.5946727  1.4120964  1.2697154  1.192143   1.1724396  1.1952832
 1.2427717  1.3004924  1.3622125  1.4301335  1.511929   1.6144695
 1.7388396  1.8804924  2.0324595  2.1862543  2.3257277  2.4256604
 2.4618912  2.42204    2.307903   2.1370478  1.9362373  1.7266897
 1.5199629  1.3194559  1.1296177  0.9633586  0.8482769  0.8122774
 0.8492733  0.91937685 0.9855655  1.0294005  1.0486311  1.0579727
 1.0828207  1.1398679  1.2256933  1.3206357  1.3989302  1.4426485
 1.4519993  1.4379519  1.4117962  1.3846419  1.3601519  1.3676902
 1.5553188  1.9459596  2.2514935  2.3307335  2.2615952  2.145836
 2.049981   1.9921952  1.9475847  1.8996481  1.8547384  1.8259503
 1.8093435  1.7631929  1.6329281  1.4322096  1.2506666  1.1469694
 1.117472   1.1383901  1.1833853  1.2326274  1.278471   1.3263485
 1.3898067  1.482037   1.607695   1.7604344  1.9268701  2.092146
 2.2377512  2.3386545  2.370334   2.3223696  2.2022243  2.0331173
 1.842356   1.6485542  1.4647055  1.2952814  1.1300989  0.97386855
 0.87338734 0.86142486 0.9086152  0.96262    0.99254644 0.9883616
 0.9573999  0.92661333 0.9278003  0.981957   1.0898404  1.2248148
 1.344604   1.4144006  1.4282209  1.402      1.3527321  1.3026383
 1.271893   1.2994109  1.5212498  1.8656371  2.0520036  2.0519607
 1.9795587  1.9081845  1.869159   1.8518604  1.8278077  1.7900288
 1.74829    1.7197185  1.7165065  1.713622   1.6339419  1.4399182
 1.2266757  1.1006585  1.0674431  1.0908192  1.1351722  1.1756831
 1.2046816  1.2316761  1.2763784  1.358129   1.4842238  1.6443998
 1.8166842  1.9805806  2.1201797  2.2146928  2.239289   2.1836107
 2.0617814  1.904089   1.739247   1.583218   1.4483831  1.3263018
 1.1809489  1.0309472  0.95057523 0.95598286 0.9934374  1.0133941
 0.99690694 0.94735295 0.88780487 0.8454781  0.8334239  0.85824513
 0.9426482  1.0954275  1.2627255  1.370831   1.3975763  1.3678138
 1.30404    1.2320923  1.1977919  1.2611692  1.4770033  1.7147303
 1.8089248  1.7882572  1.741971   1.7155037  1.719294   1.7281376
 1.7194232  1.6924154  1.6528908  1.6130291  1.5955412  1.6054556
 1.5777586  1.4208343  1.1918917  1.0472227  1.0152674  1.0445793
 1.0895741  1.1222943  1.1366614  1.146312   1.176433   1.250158
 1.3737099  1.530664   1.6928965  1.8397613  1.962856   2.0482202
 2.0683675  2.0089083  1.8903071  1.7516736  1.6235629  1.519474
 1.4462212  1.3720536  1.248077   1.1267828  1.0812191  1.0896341
 1.09161    1.0556079  0.99356866 0.9332963  0.8780229  0.8337997
 0.8157693  0.8263598  0.8658712  0.9646273  1.1378163  1.2967345
 1.3519804  1.3290563  1.2655709  1.1800568  1.1442467  1.2471414
 1.4010903  1.5309116  1.5967615  1.5863925  1.5607524  1.5618231
 1.5891032  1.611088   1.6135308  1.598025   1.5618883  1.5098625
 1.468732   1.4611257  1.4591134  1.3599846  1.1430212  0.9846959
 0.95732224 0.99425733 1.0400276  1.0666133  1.0711817  1.0714105
 1.0952715  1.1647787  1.278623   1.4145881  1.5485187  1.667638
 1.7709563  1.8502527  1.8730911  1.8167813  1.7065829  1.5916129
 1.5040632  1.452163   1.4313526  1.3946662  1.3145778  1.2618088
 1.2523192  1.229078   1.1578276  1.0614738  1.0114807  0.9952338
 0.9625816  0.9089525  0.86219573 0.84119475 0.8579705  0.9174602
 1.0229988  1.1774065  1.286148   1.2863195  1.2385138  1.1573658
 1.1217358  1.2442446  1.3109688  1.3767952  1.4529403  1.4527423
 1.433978   1.4443371  1.4767236  1.4998672  1.5062603  1.4994475
 1.4668267  1.4064465  1.348596   1.3162637  1.3004113  1.247688
 1.0795395  0.9186478  0.89722836 0.9420709  0.9872885  1.0080782
 1.0076584  1.0076789  1.0355479  1.1035596  1.1967447  1.2934252
 1.3871028  1.4775308  1.5643384  1.6413288  1.6738203  1.6281756
 1.5313349  1.443011   1.3961883  1.3880125  1.3969783  1.3853947
 1.3725891  1.4073501  1.4144614  1.3206152 ]
