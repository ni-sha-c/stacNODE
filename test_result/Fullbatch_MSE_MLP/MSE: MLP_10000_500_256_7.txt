time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.01%, model saved.
Epoch: 0 Train: 3710.51318 Test: 4175.53857
Epoch 100: New minimal relative error: 57.99%, model saved.
Epoch: 100 Train: 160.15620 Test: 125.00650
Epoch 200: New minimal relative error: 25.40%, model saved.
Epoch: 200 Train: 26.51132 Test: 26.96274
Epoch: 300 Train: 41.76909 Test: 17.96287
Epoch: 400 Train: 5.19113 Test: 5.68673
Epoch: 500 Train: 12.20808 Test: 14.86588
Epoch 600: New minimal relative error: 18.16%, model saved.
Epoch: 600 Train: 1.35259 Test: 1.56762
Epoch: 700 Train: 3.06868 Test: 2.63867
Epoch: 800 Train: 6.10679 Test: 6.39021
Epoch: 900 Train: 2.71224 Test: 4.07824
Epoch: 1000 Train: 1.08190 Test: 1.24806
Epoch: 1100 Train: 0.71623 Test: 1.67770
Epoch: 1200 Train: 2.25636 Test: 2.61104
Epoch: 1300 Train: 4.70073 Test: 6.69976
Epoch 1400: New minimal relative error: 14.56%, model saved.
Epoch: 1400 Train: 2.57677 Test: 1.35929
Epoch: 1500 Train: 1.27146 Test: 2.07135
Epoch: 1600 Train: 0.61027 Test: 1.67067
Epoch: 1700 Train: 0.79957 Test: 0.47226
Epoch 1800: New minimal relative error: 13.14%, model saved.
Epoch: 1800 Train: 5.07886 Test: 1.22050
Epoch: 1900 Train: 0.22943 Test: 0.61689
Epoch: 2000 Train: 0.25760 Test: 0.30678
Epoch: 2100 Train: 0.16526 Test: 0.28977
Epoch: 2200 Train: 0.19055 Test: 0.30590
Epoch: 2300 Train: 3.73107 Test: 4.29210
Epoch: 2400 Train: 0.18751 Test: 0.35887
Epoch: 2500 Train: 0.17686 Test: 0.25524
Epoch: 2600 Train: 0.11768 Test: 0.22743
Epoch: 2700 Train: 0.10619 Test: 0.20227
Epoch: 2800 Train: 0.10125 Test: 0.19223
Epoch: 2900 Train: 1.06576 Test: 0.94412
Epoch: 3000 Train: 0.17287 Test: 0.29770
Epoch 3100: New minimal relative error: 12.08%, model saved.
Epoch: 3100 Train: 0.49670 Test: 0.58135
Epoch: 3200 Train: 0.13610 Test: 0.20821
Epoch: 3300 Train: 0.12488 Test: 0.22246
Epoch: 3400 Train: 0.24465 Test: 0.36892
Epoch: 3500 Train: 0.07649 Test: 0.15291
Epoch: 3600 Train: 0.94618 Test: 1.46420
Epoch: 3700 Train: 1.46199 Test: 2.16752
Epoch: 3800 Train: 1.33409 Test: 2.19317
Epoch: 3900 Train: 7.44819 Test: 6.76355
Epoch: 4000 Train: 4.20430 Test: 2.87276
Epoch: 4100 Train: 1.37853 Test: 0.69553
Epoch 4200: New minimal relative error: 11.91%, model saved.
Epoch: 4200 Train: 0.21952 Test: 0.16515
Epoch: 4300 Train: 0.22741 Test: 0.32412
Epoch: 4400 Train: 0.36225 Test: 0.39852
Epoch: 4500 Train: 0.69570 Test: 1.03916
Epoch: 4600 Train: 3.40613 Test: 4.02230
Epoch: 4700 Train: 0.16224 Test: 0.18662
Epoch 4800: New minimal relative error: 10.74%, model saved.
Epoch: 4800 Train: 0.13547 Test: 0.17851
Epoch 4900: New minimal relative error: 6.20%, model saved.
Epoch: 4900 Train: 0.23561 Test: 0.34014
Epoch: 5000 Train: 0.25028 Test: 0.23937
Epoch: 5100 Train: 0.08850 Test: 0.14175
Epoch: 5200 Train: 0.22936 Test: 0.35383
Epoch: 5300 Train: 0.46362 Test: 0.57539
Epoch: 5400 Train: 1.77290 Test: 1.72792
Epoch: 5500 Train: 0.06344 Test: 0.09895
Epoch: 5600 Train: 0.16479 Test: 0.19754
Epoch: 5700 Train: 0.15130 Test: 0.20371
Epoch: 5800 Train: 0.14242 Test: 0.13947
Epoch: 5900 Train: 0.92834 Test: 0.62594
Epoch: 6000 Train: 3.96824 Test: 5.05202
Epoch: 6100 Train: 0.13211 Test: 0.19763
Epoch: 6200 Train: 0.14107 Test: 0.19874
Epoch: 6300 Train: 0.05597 Test: 0.17821
Epoch: 6400 Train: 2.23213 Test: 1.93006
Epoch: 6500 Train: 0.05018 Test: 0.09975
Epoch: 6600 Train: 0.04844 Test: 0.08391
Epoch: 6700 Train: 0.09014 Test: 0.17337
Epoch: 6800 Train: 0.18761 Test: 0.29449
Epoch: 6900 Train: 0.59214 Test: 0.75932
Epoch: 7000 Train: 0.04248 Test: 0.08844
Epoch: 7100 Train: 0.02627 Test: 0.06482
Epoch: 7200 Train: 0.09165 Test: 0.10881
Epoch: 7300 Train: 0.02502 Test: 0.06220
Epoch: 7400 Train: 0.02481 Test: 0.06161
Epoch: 7500 Train: 0.35535 Test: 0.49173
Epoch: 7600 Train: 0.02269 Test: 0.05723
Epoch: 7700 Train: 0.02385 Test: 0.06085
Epoch: 7800 Train: 0.02135 Test: 0.05372
Epoch: 7900 Train: 0.02347 Test: 0.05634
Epoch: 8000 Train: 0.02390 Test: 0.05678
Epoch: 8100 Train: 0.02156 Test: 0.05292
Epoch: 8200 Train: 0.02111 Test: 0.05370
Epoch: 8300 Train: 0.01958 Test: 0.04968
Epoch: 8400 Train: 0.02152 Test: 0.05244
Epoch: 8500 Train: 0.09446 Test: 0.07896
Epoch: 8600 Train: 0.95354 Test: 1.15462
Epoch: 8700 Train: 0.01855 Test: 0.04735
Epoch: 8800 Train: 0.01999 Test: 0.05523
Epoch: 8900 Train: 0.12049 Test: 0.04834
Epoch: 9000 Train: 0.02065 Test: 0.04971
Epoch: 9100 Train: 0.01785 Test: 0.04590
Epoch: 9200 Train: 0.01690 Test: 0.04308
Epoch: 9300 Train: 0.01787 Test: 0.04496
Epoch: 9400 Train: 0.04693 Test: 0.09120
Epoch: 9500 Train: 0.01616 Test: 0.04166
Epoch: 9600 Train: 0.01675 Test: 0.04397
Epoch: 9700 Train: 0.01568 Test: 0.04002
Epoch: 9800 Train: 0.01838 Test: 0.04545
Epoch: 9900 Train: 0.01787 Test: 0.04129
Epoch: 9999 Train: 0.01499 Test: 0.03844
Training Loss: tensor(0.0150)
Test Loss: tensor(0.0384)
Learned LE: [ 0.9055783   0.00812967 -4.284772  ]
True LE: [ 8.6540431e-01 -2.4910469e-03 -1.4538153e+01]
Relative Error: [3.9486792  3.4757447  2.9841592  2.5159392  2.1072187  1.7935679
 1.6062375  1.5599436  1.6395284  1.7821732  1.8907676  1.9683132
 2.1557965  2.5352569  3.0777473  3.7128553  4.370936   5.029218
 5.707018   6.399988   7.044258   7.545266   7.9282417  8.306028
 8.683374   8.993646   9.166668   9.134173   8.905937   8.601568
 8.283662   8.06863    7.9924355  7.9926767  8.014221   7.9946117
 7.9393716  7.8466296  7.6519766  7.2872972  6.7811413  6.2397695
 5.749451   5.339816   5.0129304  4.76897    4.6089067  4.5325046
 4.542608   4.6581383  4.9134707  5.3217845  5.7609158  6.0823064
 6.2660985  6.293897   6.1220927  5.706239   5.0707207  4.5090427
 4.2362823  3.962746   3.5420067  3.036917   2.5004869  1.9787165
 1.5169724  1.1616623  0.9489084  0.88328964 0.93843174 1.0618862
 1.1667677  1.2518073  1.4526131  1.8300166  2.3535652  2.9637012
 3.5927773  4.2159758  4.8653884  5.5338254  6.1052823  6.473264
 6.7496233  7.0862675  7.458715   7.7798834  7.9756656  7.974562
 7.800173   7.5893455  7.3747582  7.240218   7.2447777  7.3022738
 7.3118196  7.251725   7.158339   7.051249   6.889288   6.5901465
 6.1377344  5.6176004  5.133631   4.735864   4.431119   4.2168913
 4.090119   4.043513   4.0649858  4.1439424  4.292012   4.5533752
 4.9517074  5.348363   5.659364   5.8496623  5.8390236  5.547683
 4.9504614  4.283049   3.9225497  3.6443603  3.2156713  2.6893294
 2.1183069  1.5584817  1.0811485  0.7615791  0.6169582  0.57136506
 0.5650198  0.6044033  0.6831836  0.79804146 1.0132936  1.3424759
 1.787197   2.3256059  2.9013035  3.4764361  4.0852885  4.7334347
 5.253554   5.498328   5.668584   5.9571023  6.314449   6.6385546
 6.8533235  6.8830266  6.757336   6.627976   6.5066457  6.4442964
 6.52325    6.6165843  6.6028686  6.511452   6.3847265  6.2546334
 6.112777   5.8836794  5.5076423  5.024105   4.5433364  4.148609
 3.8613033  3.6758177  3.5835662  3.5704124  3.6136634  3.6890588
 3.7783628  3.894486   4.1191363  4.5075197  4.9239683  5.288754
 5.4867635  5.385782   4.9163723  4.1923056  3.6902099  3.3985138
 2.974178   2.441082   1.8567032  1.3002242  0.90676045 0.7709037
 0.77301145 0.75940615 0.6964927  0.6116697  0.58091193 0.68201786
 0.8788157  1.1162884  1.4343632  1.8447005  2.31681    2.8186815
 3.3670697  3.9904172  4.501993   4.6556363  4.712599   4.9334636
 5.258985   5.573616   5.7983737  5.857422   5.775104   5.7148647
 5.6761603  5.670041   5.8019204  5.920398   5.8876405  5.7721643
 5.62166    5.470161   5.334861   5.1661606  4.8856254  4.4708138
 4.0007086  3.5952342  3.311117   3.145925   3.0820732  3.0982363
 3.1645365  3.2515302  3.3364851  3.3955655  3.4364195  3.6188893
 4.0432944  4.5458727  4.973036   5.127484   4.882134   4.2374396
 3.5692606  3.2273207  2.8231378  2.2941892  1.7190613  1.2101362
 0.95363456 0.95365316 1.0066315  1.0098135  0.95424336 0.856709
 0.7578326  0.774599   0.9054356  1.0232112  1.198494   1.5056195
 1.8723716  2.2604768  2.7162726  3.2876709  3.8359582  3.973775
 3.9128053  4.037041   4.3071504  4.596306   4.8157697  4.8981504
 4.853159   4.8518443  4.8851304  4.9115276  5.0587864  5.202543
 5.1633763  5.0252953  4.85899    4.6982865  4.5645056  4.435913
 4.2488894  3.9488447  3.5329263  3.1140585  2.8069704  2.6383626
 2.5857463  2.6134655  2.6885703  2.7891147  2.9016998  2.9958553
 3.0180202  2.9579523  3.113267   3.6194305  4.227622   4.67568
 4.736055   4.3351407  3.6054664  3.120318   2.7644544  2.2490363
 1.68774    1.2240331  1.0304834  1.054368   1.1295049  1.1767682
 1.1677068  1.1071136  1.0126461  0.955364   1.0263802  1.0667005
 1.040309   1.1497725  1.4728826  1.8213043  2.154549   2.6154454
 3.1911738  3.44039    3.2961488  3.293389   3.4821813  3.7282481
 3.9246588  4.0144796  3.9972327  4.0394998  4.1386776  4.1804953
 4.297777   4.4551625  4.436529   4.279848   4.0949607  3.9378426
 3.8133688  3.7085705  3.5836835  3.411284   3.1358366  2.7546637
 2.408413   2.1933107  2.116483   2.1303506  2.1820982  2.266843
 2.4025376  2.5748196  2.7049015  2.6974075  2.5240426  2.6525416
 3.2512724  3.942823   4.371227   4.332918   3.8039782  3.1082447
 2.7536976  2.3094702  1.7487962  1.2800885  1.0476267  1.0210265
 1.1184388  1.2332255  1.2876854  1.2693816  1.20572    1.122245
 1.117829   1.1821412  1.1031514  0.94705254 0.98560303 1.3267531
 1.6980402  2.0175412  2.4982166  2.9456236  2.8613443  2.710986
 2.7940848  2.9926422  3.1606257  3.2345273 ]
