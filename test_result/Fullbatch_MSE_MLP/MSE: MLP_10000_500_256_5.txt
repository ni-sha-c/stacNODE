time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.94%, model saved.
Epoch: 0 Train: 3835.04712 Test: 4134.98535
Epoch: 100 Train: 89.60399 Test: 82.44383
Epoch 200: New minimal relative error: 66.37%, model saved.
Epoch: 200 Train: 12.70185 Test: 13.32245
Epoch 300: New minimal relative error: 32.79%, model saved.
Epoch: 300 Train: 5.32137 Test: 5.29857
Epoch: 400 Train: 3.78293 Test: 6.14659
Epoch: 500 Train: 5.16402 Test: 6.88110
Epoch: 600 Train: 2.27514 Test: 1.84717
Epoch 700: New minimal relative error: 17.75%, model saved.
Epoch: 700 Train: 4.28582 Test: 4.33562
Epoch: 800 Train: 4.11287 Test: 4.44538
Epoch: 900 Train: 5.86921 Test: 4.15077
Epoch: 1000 Train: 5.26945 Test: 4.48363
Epoch 1100: New minimal relative error: 9.08%, model saved.
Epoch: 1100 Train: 1.51623 Test: 1.62740
Epoch: 1200 Train: 5.09866 Test: 5.34153
Epoch: 1300 Train: 1.66054 Test: 1.79055
Epoch: 1400 Train: 1.65092 Test: 1.54549
Epoch: 1500 Train: 0.69087 Test: 0.71730
Epoch: 1600 Train: 12.12806 Test: 9.13852
Epoch: 1700 Train: 0.78670 Test: 0.86004
Epoch: 1800 Train: 0.25640 Test: 0.24930
Epoch: 1900 Train: 4.47049 Test: 3.27100
Epoch: 2000 Train: 2.94616 Test: 4.43853
Epoch: 2100 Train: 1.48000 Test: 1.62423
Epoch: 2200 Train: 0.24652 Test: 0.20047
Epoch: 2300 Train: 0.97487 Test: 0.77161
Epoch: 2400 Train: 4.51830 Test: 2.86481
Epoch: 2500 Train: 1.61389 Test: 1.56842
Epoch: 2600 Train: 0.92592 Test: 1.15905
Epoch: 2700 Train: 0.69692 Test: 0.46277
Epoch: 2800 Train: 0.98085 Test: 1.32298
Epoch: 2900 Train: 2.85562 Test: 3.34983
Epoch: 3000 Train: 0.15687 Test: 0.16531
Epoch: 3100 Train: 0.12591 Test: 0.12170
Epoch: 3200 Train: 0.20895 Test: 0.17715
Epoch: 3300 Train: 0.20401 Test: 0.18346
Epoch: 3400 Train: 1.23129 Test: 0.96910
Epoch: 3500 Train: 0.17476 Test: 0.46615
Epoch: 3600 Train: 0.34546 Test: 0.52712
Epoch: 3700 Train: 0.24829 Test: 0.24246
Epoch: 3800 Train: 0.09306 Test: 0.08785
Epoch: 3900 Train: 0.46861 Test: 0.57900
Epoch: 4000 Train: 0.58981 Test: 0.25712
Epoch: 4100 Train: 0.72299 Test: 0.15365
Epoch: 4200 Train: 0.26856 Test: 0.33879
Epoch: 4300 Train: 0.15319 Test: 0.18137
Epoch: 4400 Train: 4.42457 Test: 5.02170
Epoch: 4500 Train: 0.07567 Test: 0.07410
Epoch: 4600 Train: 0.06885 Test: 0.06690
Epoch: 4700 Train: 0.17270 Test: 0.20155
Epoch: 4800 Train: 0.26138 Test: 0.32763
Epoch: 4900 Train: 0.21206 Test: 0.26764
Epoch: 5000 Train: 0.08626 Test: 0.08990
Epoch: 5100 Train: 0.08676 Test: 0.07645
Epoch: 5200 Train: 0.15151 Test: 0.13737
Epoch: 5300 Train: 0.08826 Test: 0.06918
Epoch: 5400 Train: 0.85052 Test: 1.04352
Epoch: 5500 Train: 0.62831 Test: 0.62314
Epoch: 5600 Train: 0.09288 Test: 0.08563
Epoch: 5700 Train: 0.95031 Test: 1.09327
Epoch: 5800 Train: 0.39060 Test: 0.40086
Epoch: 5900 Train: 0.26827 Test: 0.28712
Epoch: 6000 Train: 0.47725 Test: 0.36570
Epoch: 6100 Train: 0.18354 Test: 0.10456
Epoch: 6200 Train: 0.10992 Test: 0.12125
Epoch: 6300 Train: 0.05944 Test: 0.09553
Epoch: 6400 Train: 0.09250 Test: 0.06234
Epoch: 6500 Train: 0.07304 Test: 0.07125
Epoch: 6600 Train: 0.04342 Test: 0.04363
Epoch: 6700 Train: 0.04675 Test: 0.04623
Epoch: 6800 Train: 0.07530 Test: 0.10113
Epoch: 6900 Train: 0.04538 Test: 0.08112
Epoch: 7000 Train: 0.13498 Test: 0.09888
Epoch: 7100 Train: 0.30428 Test: 0.21238
Epoch: 7200 Train: 0.07172 Test: 0.05143
Epoch: 7300 Train: 1.08068 Test: 0.79611
Epoch: 7400 Train: 0.15931 Test: 0.20116
Epoch: 7500 Train: 0.03589 Test: 0.03725
Epoch: 7600 Train: 0.03508 Test: 0.03618
Epoch: 7700 Train: 0.03540 Test: 0.03881
Epoch: 7800 Train: 0.03614 Test: 0.03706
Epoch: 7900 Train: 0.04512 Test: 0.05929
Epoch: 8000 Train: 0.03192 Test: 0.03326
Epoch: 8100 Train: 0.03167 Test: 0.03316
Epoch: 8200 Train: 0.03182 Test: 0.03343
Epoch: 8300 Train: 0.03227 Test: 0.03564
Epoch: 8400 Train: 0.23886 Test: 0.19104
Epoch: 8500 Train: 0.02948 Test: 0.03106
Epoch: 8600 Train: 0.04413 Test: 0.04982
Epoch: 8700 Train: 0.02879 Test: 0.03044
Epoch: 8800 Train: 0.02916 Test: 0.03094
Epoch: 8900 Train: 0.05347 Test: 0.03083
Epoch: 9000 Train: 0.06795 Test: 0.08946
Epoch: 9100 Train: 0.02656 Test: 0.02837
Epoch: 9200 Train: 0.02712 Test: 0.02915
Epoch: 9300 Train: 0.02573 Test: 0.02761
Epoch: 9400 Train: 0.02575 Test: 0.02739
Epoch: 9500 Train: 0.04952 Test: 0.03313
Epoch: 9600 Train: 0.37276 Test: 0.28741
Epoch: 9700 Train: 0.02435 Test: 0.02624
Epoch: 9800 Train: 0.05263 Test: 0.07498
Epoch: 9900 Train: 0.02364 Test: 0.02557
Epoch: 9999 Train: 0.05585 Test: 0.03585
Training Loss: tensor(0.0558)
Test Loss: tensor(0.0358)
Learned LE: [ 0.6223035  -0.07199829 -3.3451815 ]
True LE: [ 8.4066641e-01  3.0157564e-03 -1.4520190e+01]
Relative Error: [11.80629   13.397614  14.954702  16.39134   17.693449  18.88871
 19.86113   20.274971  19.975826  19.16299   18.022448  16.67395
 15.267112  13.924193  12.708202  11.650987  10.771815  10.076368
  9.544449   9.124556   8.756407   8.400512   8.046562   7.7031546
  7.3860536  7.1096907  6.881551   6.6999145  6.559095   6.471066
  6.5134273  6.8624983  7.6662736  8.84115   10.191029  11.621044
 13.046315  14.333269  15.355854  15.99921   16.179844  15.941916
 15.495342  14.924753  13.819589  12.323974  10.916038   9.657991
  8.5321245  7.5526023  6.727831   6.050872   5.5048256  5.075685
  4.7700663  4.6264443  4.701818   5.0394373  5.6469197  6.5097036
  7.61345    8.94075   10.447551  12.044657  13.610193  15.039903
 16.309656  17.47053   18.465324  18.942274  18.644955  17.7712
 16.576447  15.220517  13.860615  12.603524  11.496958  10.565075
  9.820081   9.257543   8.843405   8.512611   8.200143   7.872419
  7.5278416  7.1824737  6.85876    6.5777326  6.3497477  6.1679425
  6.011048   5.864074   5.776561   5.9343076  6.5645957  7.6342115
  8.916131  10.3095875 11.73233   13.023302  14.025524  14.604055
 14.697346  14.4219475 14.022949  13.443949  12.226649  10.777899
  9.467657   8.289698   7.2544947  6.3818665  5.6716385  5.105959
  4.656005   4.2930727  4.009483   3.8358257  3.8356218  4.072175
  4.574035   5.3387704  6.360164   7.6290636  9.107208  10.703805
 12.283098  13.718525  14.967458  16.091364  17.099611  17.664236
 17.406912  16.490892  15.256975  13.910288  12.608096  11.439903
 10.442289   9.634517   9.020365   8.584239   8.278827   8.026834
  7.760965   7.4548354  7.1143856  6.7608943  6.423374   6.131953
  5.90573    5.738598   5.5957737  5.4328704  5.254931   5.2161694
  5.6063423  6.5036902  7.6769524  9.009889  10.425729  11.729484
 12.725617  13.253871  13.278726  12.995497  12.661936  12.055143
 10.771046   9.403305   8.180033   7.093897   6.1747413  5.434187
  4.860387   4.4254956  4.0890074  3.807699   3.557168   3.3532844
  3.2554228  3.3446298  3.6833334  4.2950196  5.186092   6.3561907
  7.7757053  9.357197  10.953125  12.408797  13.654019  14.744782
 15.750896  16.420866  16.25885   15.331101  14.072804  12.746768
 11.505876  10.423714   9.5308     8.842699   8.351667   8.030876
  7.822376   7.63875    7.4126577  7.124894   6.7871833  6.4241962
  6.0686646  5.758512   5.5267973  5.3791084  5.2759333  5.1461363
  4.943946   4.75213    4.853468   5.4852533  6.4906654  7.725691
  9.120841  10.445062  11.453568  11.956263  11.935375  11.666335
 11.404882  10.7691345  9.469657   8.190496   7.056508   6.0867653
  5.3096776  4.72156    4.2999406  4.0080233  3.7946663  3.6051083
  3.4020286  3.185194   2.9968607  2.9160993  3.0362828  3.4266107
  4.1230974  5.1381936  6.4523206  7.9887137  9.595929  11.087594
 12.351571  13.422704  14.409834  15.183176  15.187895  14.299719
 13.031056  11.730496  10.54791    9.5440035  8.747467   8.16941
  7.7862577  7.5622864  7.4359403  7.312351   7.1229234  6.8529935
  6.518495   6.148162   5.777438   5.4456873  5.195661   5.055885
  5.001624   4.9457116  4.795178   4.5464783  4.3825617  4.64666
  5.393542   6.4706974  7.812777   9.160248  10.20258   10.715108
 10.675916  10.432024  10.242661   9.602479   8.327412   7.1353436
  6.1066384  5.2783504  4.66062    4.2340827  3.9678688  3.8164382
  3.7201643  3.6191542  3.471632   3.2668307  3.0272393  2.8079956
  2.6996152  2.8091674  3.230457   4.013833   5.1546087  6.5918484
  8.185253   9.723217  11.034196  12.109958  13.070507  13.917251
 14.157708  13.397501  12.136362  10.859452   9.72692    8.789152
  8.075094   7.588304   7.287558   7.13601    7.081175   7.020418
  6.875095   6.6273503  6.2940454  5.911049   5.5237746  5.1747103
  4.901836   4.7464104  4.7224045  4.756481   4.7193475  4.530177
  4.2304654  4.094254   4.4550943  5.280424   6.503334   7.863308
  8.961131   9.528437   9.50561    9.286772   9.165012   8.569906
  7.34198    6.238076   5.3343353  4.6627603  4.2075276  3.9366179
  3.8125463  3.7830143  3.784536   3.7585516  3.6655002  3.491913
  3.246814   2.958844   2.6875322  2.527243   2.6075976  3.0593438
  3.9311914  5.183552   6.7022314  8.275245   9.6639    10.780139
 11.7264185 12.598282  13.097806  12.604286  11.392374  10.131206
  9.036215   8.148796   7.4953556  7.070899   6.8199143  6.7180924
  6.737933   6.7641883  6.6904426  6.4829445  6.1528196  5.741494
  5.311175   4.9259534  4.629407   4.446125 ]
