time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.55%, model saved.
Epoch: 0 Train: 3906.76831 Test: 4142.60107
Epoch 80: New minimal relative error: 50.22%, model saved.
Epoch: 80 Train: 88.57241 Test: 77.65849
Epoch 160: New minimal relative error: 16.04%, model saved.
Epoch: 160 Train: 16.40152 Test: 12.10155
Epoch 240: New minimal relative error: 12.00%, model saved.
Epoch: 240 Train: 7.98301 Test: 5.04671
Epoch: 320 Train: 9.15620 Test: 7.80284
Epoch: 400 Train: 9.23696 Test: 8.26670
Epoch: 480 Train: 16.50258 Test: 13.03582
Epoch: 560 Train: 2.38174 Test: 1.82863
Epoch: 640 Train: 2.22968 Test: 1.70282
Epoch: 720 Train: 1.49944 Test: 1.33184
Epoch: 800 Train: 4.50581 Test: 4.41165
Epoch: 880 Train: 1.06107 Test: 1.09078
Epoch: 960 Train: 1.65760 Test: 1.22584
Epoch: 1040 Train: 1.28053 Test: 1.01169
Epoch 1120: New minimal relative error: 7.36%, model saved.
Epoch: 1120 Train: 0.70288 Test: 0.37299
Epoch: 1200 Train: 0.90354 Test: 0.63758
Epoch: 1280 Train: 3.92751 Test: 4.21093
Epoch 1360: New minimal relative error: 6.65%, model saved.
Epoch: 1360 Train: 0.79002 Test: 0.69034
Epoch: 1440 Train: 0.46881 Test: 0.37945
Epoch: 1520 Train: 2.06463 Test: 1.94392
Epoch: 1600 Train: 1.41077 Test: 1.65018
Epoch: 1680 Train: 1.20558 Test: 1.29485
Epoch: 1760 Train: 1.76982 Test: 1.07909
Epoch: 1840 Train: 0.71942 Test: 0.68451
Epoch: 1920 Train: 0.43380 Test: 0.27101
Epoch: 2000 Train: 0.64644 Test: 0.46612
Epoch: 2080 Train: 1.05540 Test: 1.12028
Epoch: 2160 Train: 2.75503 Test: 2.86365
Epoch: 2240 Train: 2.03083 Test: 1.73119
Epoch: 2320 Train: 0.25682 Test: 0.19379
Epoch 2400: New minimal relative error: 5.58%, model saved.
Epoch: 2400 Train: 0.24616 Test: 0.17347
Epoch 2480: New minimal relative error: 5.52%, model saved.
Epoch: 2480 Train: 0.19487 Test: 0.12693
Epoch: 2560 Train: 0.52157 Test: 0.44810
Epoch: 2640 Train: 0.44631 Test: 0.35616
Epoch: 2720 Train: 0.90486 Test: 1.20181
Epoch: 2800 Train: 0.16198 Test: 0.10684
Epoch: 2880 Train: 0.66834 Test: 0.86617
Epoch: 2960 Train: 0.14439 Test: 0.09709
Epoch: 3040 Train: 0.16578 Test: 0.12949
Epoch: 3120 Train: 0.15494 Test: 0.11389
Epoch 3200: New minimal relative error: 3.38%, model saved.
Epoch: 3200 Train: 0.15993 Test: 0.12024
Epoch: 3280 Train: 0.29224 Test: 0.18647
Epoch: 3360 Train: 0.54212 Test: 0.67339
Epoch: 3440 Train: 0.38467 Test: 0.41745
Epoch: 3520 Train: 0.20048 Test: 0.19458
Epoch: 3600 Train: 0.10910 Test: 0.08151
Epoch: 3680 Train: 0.12410 Test: 0.09373
Epoch: 3760 Train: 0.17177 Test: 0.18078
Epoch: 3840 Train: 0.10136 Test: 0.07785
Epoch: 3920 Train: 0.11390 Test: 0.14762
Epoch: 4000 Train: 0.22646 Test: 0.28183
Epoch: 4080 Train: 0.20328 Test: 0.21860
Epoch: 4160 Train: 0.08834 Test: 0.07096
Epoch: 4240 Train: 0.15538 Test: 0.12703
Epoch: 4320 Train: 0.84492 Test: 0.84378
Epoch: 4400 Train: 1.04637 Test: 0.50417
Epoch: 4480 Train: 0.07731 Test: 0.06409
Epoch: 4560 Train: 0.07723 Test: 0.06640
Epoch: 4640 Train: 0.07548 Test: 0.06550
Epoch: 4720 Train: 0.07164 Test: 0.06088
Epoch: 4800 Train: 0.08701 Test: 0.08443
Epoch: 4880 Train: 0.07095 Test: 0.06153
Epoch 4960: New minimal relative error: 2.53%, model saved.
Epoch: 4960 Train: 0.06828 Test: 0.06056
Epoch: 5040 Train: 0.06933 Test: 0.06060
Epoch: 5120 Train: 0.06329 Test: 0.05612
Epoch: 5200 Train: 0.11298 Test: 0.09887
Epoch: 5280 Train: 0.06067 Test: 0.05447
Epoch: 5360 Train: 0.06341 Test: 0.06412
Epoch: 5440 Train: 0.05841 Test: 0.05336
Epoch: 5520 Train: 0.05879 Test: 0.05538
Epoch 5600: New minimal relative error: 1.84%, model saved.
Epoch: 5600 Train: 0.05833 Test: 0.05449
Epoch: 5680 Train: 0.05487 Test: 0.05111
Epoch: 5760 Train: 1.47970 Test: 1.59904
Epoch: 5840 Train: 0.05351 Test: 0.05051
Epoch: 5920 Train: 0.05242 Test: 0.04992
Epoch: 6000 Train: 0.05197 Test: 0.04908
Epoch: 6080 Train: 0.04999 Test: 0.04810
Epoch: 6160 Train: 0.05246 Test: 0.04873
Epoch: 6240 Train: 0.04825 Test: 0.04693
Epoch: 6320 Train: 0.16246 Test: 0.18213
Epoch: 6400 Train: 0.04673 Test: 0.04607
Epoch: 6480 Train: 0.21612 Test: 0.28239
Epoch: 6560 Train: 0.04539 Test: 0.04516
Epoch: 6640 Train: 0.04426 Test: 0.04439
Epoch: 6720 Train: 0.06444 Test: 0.05735
Epoch: 6800 Train: 0.04297 Test: 0.04357
Epoch: 6880 Train: 0.06243 Test: 0.09992
Epoch: 6960 Train: 0.28927 Test: 0.20923
Epoch: 7040 Train: 0.04102 Test: 0.04219
Epoch: 7120 Train: 0.19225 Test: 0.09987
Epoch: 7200 Train: 0.03988 Test: 0.04144
Epoch: 7280 Train: 0.03915 Test: 0.04095
Epoch: 7360 Train: 0.03895 Test: 0.04146
Epoch: 7440 Train: 0.03809 Test: 0.04016
Epoch: 7520 Train: 0.07474 Test: 0.06897
Epoch: 7600 Train: 0.03704 Test: 0.03954
Epoch: 7680 Train: 0.13091 Test: 0.05574
Epoch: 7760 Train: 0.03608 Test: 0.03883
Epoch: 7840 Train: 0.03553 Test: 0.03850
Epoch: 7920 Train: 0.03501 Test: 0.03818
Epoch: 7999 Train: 0.03783 Test: 0.04184
Training Loss: tensor(0.0378)
Test Loss: tensor(0.0418)
Learned LE: [ 8.8151133e-01 -3.2477858e-03 -5.6708674e+00]
True LE: [ 8.5514241e-01  8.6117024e-04 -1.4529507e+01]
Relative Error: [1.383131   1.2995847  1.1949826  1.0887074  1.0102459  1.0149268
 1.0477595  1.1197668  1.2505776  1.295973   1.2740682  1.5693632
 2.2234945  2.8870301  3.4702778  3.9780762  4.2576213  4.1735325
 3.7383757  3.0862565  2.6638005  2.7969077  3.1652846  3.539
 3.8028758  3.9394689  3.983568   3.8739965  3.8038082  3.9506395
 4.179125   4.382198   4.540738   4.7339535  5.058685   5.531373
 5.9076333  5.9295835  5.8389835  5.4531884  4.702235   4.004314
 3.571579   3.3127856  3.0714285  2.9696796  3.192082   3.6696022
 4.160307   4.1014323  3.4611497  2.5693188  1.7983924  1.2423407
 0.82424784 0.61668694 0.68702793 0.8821468  0.91003317 0.87086785
 1.0145752  1.142015   1.1205475  1.0078082  0.89467514 0.8008174
 0.7332094  0.74896884 0.7786128  0.8151192  1.0477712  1.2804548
 1.2638916  1.1541708  1.5184218  2.1885085  2.8096962  3.3719945
 3.790911   3.903202   3.6375232  3.0195513  2.4158678  2.3108528
 2.6669788  3.2493248  3.7258313  3.9724097  4.0136123  3.7033093
 3.3921268  3.4608989  3.7052565  3.913627   4.015591   4.0550866
 4.1459975  4.357516   4.7612247  5.1723948  5.2140107  5.2240505
 4.8293905  3.9645705  3.3241432  2.9841368  2.7310033  2.5217097
 2.6483848  3.1633236  3.6674948  3.6395338  3.1125906  2.2672799
 1.482788   0.98165435 0.6667756  0.4521485  0.3935862  0.55331635
 0.8446051  1.0319471  1.0149574  1.0850354  1.046472   0.89372975
 0.74997914 0.66281927 0.6094348  0.58768374 0.5832709  0.51154834
 0.66047555 1.0631794  1.3218051  1.2135433  1.0198973  1.4203857
 2.0643737  2.648056   3.1469758  3.4355078  3.4165242  2.9899452
 2.303697   1.9283265  2.0987926  2.75116    3.452694   3.8856342
 4.002943   3.6237082  3.1372507  3.0390813  3.2429054  3.4528353
 3.5531533  3.5643868  3.5377603  3.5144281  3.616804   3.9565074
 4.4266343  4.495947   4.5194583  4.1896753  3.2506511  2.6822367
 2.4335792  2.206615   2.1447275  2.5516148  3.0160003  3.1240602
 2.8068097  2.0844638  1.3172765  0.8118836  0.60138255 0.47952121
 0.36842713 0.39218488 0.5859925  0.86868304 1.2007453  1.226648
 1.1559957  0.97295314 0.7661834  0.63229245 0.5833283  0.537775
 0.4486192  0.36737657 0.2555139  0.5519175  1.055649   1.3481126
 1.1916238  0.8698607  1.2122681  1.8195735  2.3638577  2.766276
 2.9432926  2.828846   2.310496   1.7499517  1.60405    2.0949285
 2.860739   3.5376196  3.942781   3.7279322  3.193222   2.8785386
 2.924068   3.1179082  3.1873565  3.121169   3.0626078  3.0291617
 2.9340744  2.9342623  3.1651456  3.6074915  3.7373402  3.6852016
 3.5492024  2.5672896  2.0507631  1.8975211  1.7637736  1.9398901
 2.3519938  2.4626412  2.4555533  1.9401487  1.2570915  0.72528076
 0.5585216  0.531916   0.47138795 0.41573453 0.5516045  0.7168983
 0.91528463 1.2698675  1.3677876  1.1482297  0.90691435 0.7137198
 0.6091244  0.55339026 0.41775414 0.24857102 0.22149251 0.12735881
 0.4263469  0.9929241  1.3735181  1.254413   0.7753235  0.91303736
 1.4824444  1.9800136  2.284099   2.3779647  2.1839101  1.7203794
 1.3498294  1.4447492  2.1014783  2.8483634  3.5500054  3.8775127
 3.4602444  3.034623   2.836232   2.9645016  3.1333208  3.0651407
 2.8242142  2.6223097  2.5374231  2.487078   2.352759   2.3809104
 2.6591706  2.9001663  2.8237746  2.8500142  2.0066748  1.4352038
 1.3547001  1.360417   1.7832212  1.7556587  1.9264231  1.7255934
 1.227778   0.69670796 0.45909706 0.51716256 0.5126506  0.48115146
 0.5636363  0.71394986 0.7434054  0.80372643 1.0530267  1.2700678
 0.95876706 0.7106082  0.6058304  0.60588974 0.58189905 0.40560132
 0.10354716 0.16640098 0.21026346 0.27423006 0.8054035  1.2992084
 1.3386222  0.83794296 0.64176965 1.1551354  1.6091433  1.8315692
 1.8505375  1.6170392  1.2764269  1.1066593  1.321032   1.9857255
 2.7671723  3.5045896  3.683481   3.3343804  3.027031   2.9556942
 3.15566    3.319654   3.2031446  2.8815594  2.5164907  2.2148912
 2.068915   1.930693   1.672802   1.6992354  1.9689491  2.0014534
 1.9291097  1.6976221  0.87617385 0.78845173 0.93796605 1.3204926
 1.2116922  1.3643416  1.0875165  0.70398146 0.37446576 0.36031073
 0.44862664 0.43681556 0.5279344  0.63225687 0.6342673  0.4794368
 0.40012604 0.56849235 0.8288534  0.7239608  0.31762257 0.27178362
 0.44652033 0.5818155  0.54233927 0.28901303 0.1688778  0.25049663
 0.21454117 0.43400437 0.93029004 1.2261698  0.9558547  0.50390506
 0.88986886 1.3255246  1.5143416  1.4774708  1.2588446  1.0531962
 1.0607367  1.2639899  1.7650439  2.5390344 ]
