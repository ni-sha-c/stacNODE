time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.92%, model saved.
Epoch: 0 Train: 3858.02856 Test: 3948.46411
Epoch 80: New minimal relative error: 40.83%, model saved.
Epoch: 80 Train: 116.92795 Test: 126.71465
Epoch 160: New minimal relative error: 31.67%, model saved.
Epoch: 160 Train: 16.34044 Test: 18.73800
Epoch: 240 Train: 8.99303 Test: 14.98337
Epoch 320: New minimal relative error: 30.83%, model saved.
Epoch: 320 Train: 4.90001 Test: 5.73858
Epoch: 400 Train: 10.27242 Test: 14.41085
Epoch 480: New minimal relative error: 25.39%, model saved.
Epoch: 480 Train: 2.76798 Test: 3.64041
Epoch: 560 Train: 2.35189 Test: 2.49481
Epoch 640: New minimal relative error: 16.73%, model saved.
Epoch: 640 Train: 6.50399 Test: 5.65089
Epoch: 720 Train: 6.31372 Test: 7.88061
Epoch: 800 Train: 12.57634 Test: 10.77703
Epoch: 880 Train: 7.72296 Test: 8.87404
Epoch: 960 Train: 1.86461 Test: 1.47214
Epoch 1040: New minimal relative error: 15.13%, model saved.
Epoch: 1040 Train: 1.28577 Test: 1.68831
Epoch: 1120 Train: 1.40498 Test: 1.67754
Epoch: 1200 Train: 1.48928 Test: 1.81123
Epoch: 1280 Train: 1.16414 Test: 1.29119
Epoch 1360: New minimal relative error: 12.65%, model saved.
Epoch: 1360 Train: 0.76343 Test: 0.87423
Epoch: 1440 Train: 0.45409 Test: 0.53967
Epoch: 1520 Train: 1.58943 Test: 1.68166
Epoch: 1600 Train: 1.15603 Test: 1.68346
Epoch: 1680 Train: 5.06008 Test: 4.00260
Epoch: 1760 Train: 2.66474 Test: 2.61023
Epoch: 1840 Train: 1.33463 Test: 1.00225
Epoch: 1920 Train: 0.96572 Test: 1.01797
Epoch: 2000 Train: 0.37680 Test: 0.41027
Epoch: 2080 Train: 0.25278 Test: 0.31499
Epoch: 2160 Train: 1.49872 Test: 1.84395
Epoch: 2240 Train: 0.42377 Test: 0.71443
Epoch: 2320 Train: 2.51429 Test: 2.92070
Epoch: 2400 Train: 0.83494 Test: 0.86188
Epoch: 2480 Train: 0.58774 Test: 0.55692
Epoch: 2560 Train: 1.06946 Test: 1.19307
Epoch: 2640 Train: 0.63774 Test: 0.61009
Epoch: 2720 Train: 5.15182 Test: 6.84960
Epoch: 2800 Train: 0.11786 Test: 0.17876
Epoch: 2880 Train: 0.13668 Test: 0.21932
Epoch 2960: New minimal relative error: 9.66%, model saved.
Epoch: 2960 Train: 0.13387 Test: 0.14511
Epoch: 3040 Train: 0.11061 Test: 0.17369
Epoch: 3120 Train: 4.74921 Test: 5.54670
Epoch: 3200 Train: 0.08859 Test: 0.13386
Epoch: 3280 Train: 0.24901 Test: 0.24261
Epoch: 3360 Train: 1.50162 Test: 1.28141
Epoch: 3440 Train: 0.19937 Test: 0.29250
Epoch: 3520 Train: 0.15925 Test: 0.22131
Epoch: 3600 Train: 0.13081 Test: 0.13943
Epoch: 3680 Train: 0.11564 Test: 0.18206
Epoch: 3760 Train: 0.12378 Test: 0.14415
Epoch 3840: New minimal relative error: 6.90%, model saved.
Epoch: 3840 Train: 0.15880 Test: 0.17796
Epoch: 3920 Train: 0.63994 Test: 0.67487
Epoch: 4000 Train: 0.09893 Test: 0.15335
Epoch: 4080 Train: 0.06650 Test: 0.10736
Epoch: 4160 Train: 0.10289 Test: 0.12531
Epoch: 4240 Train: 0.07713 Test: 0.12011
Epoch: 4320 Train: 0.06459 Test: 0.10462
Epoch: 4400 Train: 0.14495 Test: 0.15473
Epoch: 4480 Train: 0.40399 Test: 0.31179
Epoch: 4560 Train: 0.82616 Test: 0.46327
Epoch: 4640 Train: 0.11407 Test: 0.14619
Epoch: 4720 Train: 0.07385 Test: 0.09510
Epoch: 4800 Train: 0.06438 Test: 0.08484
Epoch: 4880 Train: 0.06057 Test: 0.09503
Epoch: 4960 Train: 0.14616 Test: 0.12660
Epoch: 5040 Train: 0.40806 Test: 0.45801
Epoch: 5120 Train: 0.42268 Test: 0.19907
Epoch: 5200 Train: 0.04352 Test: 0.07226
Epoch: 5280 Train: 1.64847 Test: 0.97500
Epoch: 5360 Train: 0.04277 Test: 0.06740
Epoch: 5440 Train: 0.03987 Test: 0.06606
Epoch: 5520 Train: 0.17349 Test: 0.17417
Epoch: 5600 Train: 2.16831 Test: 2.35318
Epoch: 5680 Train: 0.03889 Test: 0.06178
Epoch: 5760 Train: 0.03918 Test: 0.06940
Epoch: 5840 Train: 0.03694 Test: 0.06055
Epoch: 5920 Train: 0.03514 Test: 0.05875
Epoch: 6000 Train: 0.03688 Test: 0.07707
Epoch: 6080 Train: 0.09000 Test: 0.18904
Epoch: 6160 Train: 0.03376 Test: 0.05541
Epoch: 6240 Train: 0.25299 Test: 0.11257
Epoch: 6320 Train: 0.03248 Test: 0.05429
Epoch: 6400 Train: 0.48002 Test: 0.42863
Epoch: 6480 Train: 0.03125 Test: 0.05225
Epoch: 6560 Train: 0.03175 Test: 0.05351
Epoch: 6640 Train: 0.02996 Test: 0.05144
Epoch: 6720 Train: 0.23625 Test: 0.18906
Epoch: 6800 Train: 0.02928 Test: 0.04963
Epoch: 6880 Train: 0.03246 Test: 0.06057
Epoch: 6960 Train: 0.02994 Test: 0.05205
Epoch: 7040 Train: 0.02859 Test: 0.04896
Epoch: 7120 Train: 0.04126 Test: 0.05663
Epoch: 7200 Train: 0.02708 Test: 0.04806
Epoch 7280: New minimal relative error: 6.51%, model saved.
Epoch: 7280 Train: 0.02617 Test: 0.04562
Epoch: 7360 Train: 0.03792 Test: 0.08063
Epoch: 7440 Train: 0.02559 Test: 0.04464
Epoch: 7520 Train: 0.02529 Test: 0.05193
Epoch: 7600 Train: 0.47104 Test: 0.26031
Epoch: 7680 Train: 0.02457 Test: 0.04209
Epoch: 7760 Train: 0.02395 Test: 0.04128
Epoch: 7840 Train: 0.02560 Test: 0.04161
Epoch 7920: New minimal relative error: 4.33%, model saved.
Epoch: 7920 Train: 0.02337 Test: 0.04111
Epoch: 7999 Train: 0.31539 Test: 0.21598
Training Loss: tensor(0.3154)
Test Loss: tensor(0.2160)
Learned LE: [ 0.89433163 -0.00855793 -6.0892735 ]
True LE: [ 8.6382562e-01  8.6537572e-03 -1.4544301e+01]
Relative Error: [3.1535707  2.9308844  2.9572632  3.31907    3.876337   4.461436
 5.0608063  5.727808   6.452696   7.1812553  7.8487554  8.375292
 8.68267    8.720572   8.473691   7.956788   7.2167215  6.338636
 5.433604   4.6068482  3.9403229  3.4897432  3.2714798  3.2565448
 3.3952672  3.6440923  3.9701238  4.3410845  4.7181845  5.072111
 5.412026   5.7191653  5.899451   6.073248   6.3856153  6.66994
 6.6841855  6.256777   5.482223   4.6419134  3.9292965  3.3506227
 2.8106363  2.267926   1.7854816  1.5194024  1.6076144  1.9590048
 2.393067   2.811569   3.1811457  3.495131   3.753717   3.9588761
 4.1129966  4.2163434  4.2630477  4.2440987  4.1593833  4.0235653
 3.8487315  3.6215096  3.3292146  3.0451026  2.9617722  3.222137
 3.7181833  4.2549515  4.803951   5.4307013  6.1310673  6.851253
 7.5225277  8.056445   8.368098   8.404947   8.147883   7.6061563
 6.8271527  5.9072075  4.970783   4.1255813  3.4499598  3.0008588
 2.7943258  2.7953742  2.952226   3.2246807  3.5776753  3.9678197
 4.3444285  4.664507   4.9318657  5.1818438  5.341797   5.4734173
 5.77796    6.0981646  6.1624603  5.7825303  5.047462   4.2178435
 3.5056443  2.9680703  2.501852   2.030183   1.6185728  1.449702
 1.6388152  2.0472097  2.5000432  2.9146004  3.2686858  3.5629158
 3.801322   3.988786   4.1324406  4.238131   4.30304    4.314498
 4.2605896  4.14598    3.986687   3.7784803  3.497298   3.1774607
 2.9899378  3.1312022  3.5585415  4.0557384  4.5587277  5.139748
 5.804616   6.5020537  7.1644125  7.6993985  8.020056   8.0714245
 7.8264737  7.2829275  6.4842687  5.5362     4.5752296  3.7083936
 3.0092413  2.5409439  2.3257182  2.3235207  2.4811018  2.7671928
 3.1502876  3.5764263  3.9771323  4.289983   4.496242   4.6585927
 4.786142   4.8648453  5.1387563  5.497542   5.6283474  5.3290925
 4.6827297  3.8959868  3.1686306  2.6420825  2.2417805  1.8410997
 1.4931906  1.4007425  1.6572015  2.0981276  2.5551674  2.9555438
 3.2877712  3.5587854  3.774329   3.9411592  4.069971   4.170605
 4.2446027  4.2809553  4.261492   4.177659   4.0420313  3.863495
 3.6186357  3.3012316  3.0320313  3.03785    3.3779461  3.8448627
 4.314737   4.85104    5.4719253  6.130651   6.7674026  7.2952657
 7.630262   7.7158146  7.5120053  6.9978895  6.205511   5.245352
 4.2663617  3.3740785  2.6322892  2.114453   1.8614839  1.83348
 1.9714193  2.2530727  2.6574647  3.127538   3.5815864  3.931268
 4.120882   4.1894937  4.2466493  4.267445   4.464755   4.853922
 5.068833   4.8730416  4.3508906  3.6633093  2.933989   2.3768034
 2.0162952  1.6831957  1.3867574  1.3390504  1.6303971  2.087441
 2.5388286  2.9175556  3.2236052  3.4687858  3.6586306  3.8018491
 3.912855   4.0034237  4.076744   4.125482   4.132935   4.08227
 3.9748538  3.8282366  3.635654   3.3650537  3.0642326  2.9368906
 3.155696   3.5910559  4.04928    4.55324    5.1323223  5.741181
 6.333799   6.8416486  7.192483   7.330685   7.200175   6.754025
 6.003053   5.053761   4.0689025  3.1561933  2.3608627  1.7594769
 1.4257663  1.3370085  1.4230931  1.6659168  2.067755   2.5737102
 3.097344   3.537871   3.78826    3.8220632  3.764746   3.7196617
 3.777156   4.150286   4.464454   4.3971243  4.0040174  3.4709866
 2.8089705  2.1961954  1.8170711  1.5384396  1.2821872  1.2388319
 1.5320296  1.9961377  2.4384282  2.7919374  3.0696144  3.2852583
 3.4430737  3.555825   3.6431584  3.7191703  3.7867208  3.8387544
 3.8613493  3.8372993  3.757729   3.6365285  3.4905791  3.2952688
 3.0312943  2.8166854  2.88082    3.2532532  3.7190642  4.2157164
 4.7738     5.3400946  5.876164   6.3464203  6.703779   6.9031024
 6.8752885  6.5419235  5.882274   4.9833336  4.018555   3.1100075
 2.279685   1.5859909  1.1315095  0.93484724 0.9284735  1.0677612
 1.3865201  1.8819872  2.4599452  3.015581   3.420759   3.5467756
 3.4243572  3.2681093  3.161872   3.3870764  3.7814188  3.8815365
 3.6136734  3.2353182  2.7468626  2.1400414  1.6628394  1.3895715
 1.1727654  1.0967195  1.3487929  1.8133495  2.2524915  2.5847683
 2.8349752  3.0174115  3.1347349  3.2057018  3.2572773  3.3078935
 3.3628166  3.4155977  3.4509697  3.449685   3.39618    3.2935395
 3.1687274  3.034372   2.8549533  2.6398025  2.5610328  2.8032894
 3.2635622  3.777919   4.349881   4.9157457  5.4103866  5.8304677
 6.17188    6.418991   6.5073805  6.330113   5.827606   5.045601
 4.1445026  3.2774832  2.4603612  1.7103636  1.1389052  0.79519236
 0.64629287 0.6465375  0.7844213  1.1155418 ]
