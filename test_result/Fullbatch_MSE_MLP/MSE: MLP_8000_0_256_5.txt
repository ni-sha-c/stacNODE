time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.93%, model saved.
Epoch: 0 Train: 3763.72217 Test: 4022.70459
Epoch 80: New minimal relative error: 36.40%, model saved.
Epoch: 80 Train: 127.76445 Test: 157.11412
Epoch: 160 Train: 23.59807 Test: 28.29107
Epoch: 240 Train: 13.13088 Test: 47.09019
Epoch 320: New minimal relative error: 26.42%, model saved.
Epoch: 320 Train: 7.70662 Test: 10.21277
Epoch: 400 Train: 5.47159 Test: 11.27526
Epoch: 480 Train: 17.32552 Test: 29.76674
Epoch: 560 Train: 10.37850 Test: 21.72929
Epoch: 640 Train: 40.81518 Test: 69.28532
Epoch: 720 Train: 1.28370 Test: 4.31701
Epoch 800: New minimal relative error: 21.41%, model saved.
Epoch: 800 Train: 1.32972 Test: 3.92558
Epoch: 880 Train: 1.01851 Test: 3.66404
Epoch 960: New minimal relative error: 10.12%, model saved.
Epoch: 960 Train: 1.00016 Test: 3.49882
Epoch: 1040 Train: 4.70177 Test: 8.34936
Epoch 1120: New minimal relative error: 9.15%, model saved.
Epoch: 1120 Train: 3.10884 Test: 3.47638
Epoch: 1200 Train: 2.69282 Test: 6.36256
Epoch: 1280 Train: 1.53204 Test: 3.34823
Epoch 1360: New minimal relative error: 6.42%, model saved.
Epoch: 1360 Train: 1.15890 Test: 2.91225
Epoch: 1440 Train: 0.93194 Test: 2.78223
Epoch: 1520 Train: 0.60149 Test: 2.65541
Epoch: 1600 Train: 0.44891 Test: 2.31780
Epoch: 1680 Train: 0.32467 Test: 2.12821
Epoch: 1760 Train: 0.37504 Test: 2.05411
Epoch: 1840 Train: 0.38395 Test: 2.22247
Epoch: 1920 Train: 0.36889 Test: 2.28893
Epoch: 2000 Train: 0.28789 Test: 2.08338
Epoch: 2080 Train: 0.54472 Test: 2.21908
Epoch: 2160 Train: 0.34899 Test: 1.91401
Epoch: 2240 Train: 0.66207 Test: 1.87364
Epoch: 2320 Train: 8.60768 Test: 9.54020
Epoch: 2400 Train: 0.17778 Test: 1.67989
Epoch 2480: New minimal relative error: 5.04%, model saved.
Epoch: 2480 Train: 0.21501 Test: 1.70383
Epoch: 2560 Train: 0.98577 Test: 2.51309
Epoch: 2640 Train: 5.33904 Test: 6.92975
Epoch 2720: New minimal relative error: 3.19%, model saved.
Epoch: 2720 Train: 0.16347 Test: 1.48958
Epoch: 2800 Train: 0.15206 Test: 1.50086
Epoch: 2880 Train: 0.15534 Test: 1.62308
Epoch: 2960 Train: 3.22253 Test: 2.97010
Epoch: 3040 Train: 0.12351 Test: 1.45346
Epoch: 3120 Train: 1.64593 Test: 2.62295
Epoch: 3200 Train: 0.11411 Test: 1.39588
Epoch: 3280 Train: 1.02341 Test: 2.64810
Epoch: 3360 Train: 0.10624 Test: 1.34980
Epoch: 3440 Train: 0.15979 Test: 1.37724
Epoch: 3520 Train: 0.22459 Test: 1.40356
Epoch: 3600 Train: 0.09361 Test: 1.30365
Epoch: 3680 Train: 0.28386 Test: 1.49476
Epoch: 3760 Train: 0.08925 Test: 1.27199
Epoch: 3840 Train: 0.65908 Test: 1.30685
Epoch 3920: New minimal relative error: 2.77%, model saved.
Epoch: 3920 Train: 0.08452 Test: 1.23319
Epoch: 4000 Train: 0.14168 Test: 1.30142
Epoch: 4080 Train: 0.70388 Test: 1.95311
Epoch: 4160 Train: 0.08099 Test: 1.20118
Epoch: 4240 Train: 4.30020 Test: 4.33934
Epoch: 4320 Train: 0.07411 Test: 1.18659
Epoch: 4400 Train: 1.90695 Test: 1.91988
Epoch: 4480 Train: 0.07019 Test: 1.17223
Epoch: 4560 Train: 0.22838 Test: 1.29501
Epoch: 4640 Train: 0.06583 Test: 1.15270
Epoch: 4720 Train: 2.90389 Test: 4.28702
Epoch: 4800 Train: 0.06328 Test: 1.11313
Epoch: 4880 Train: 0.05898 Test: 1.12468
Epoch: 4960 Train: 0.08019 Test: 1.07861
Epoch: 5040 Train: 0.06409 Test: 1.11529
Epoch: 5120 Train: 0.05601 Test: 1.11152
Epoch: 5200 Train: 0.09892 Test: 1.15076
Epoch: 5280 Train: 0.05262 Test: 1.07823
Epoch: 5360 Train: 0.36806 Test: 1.77199
Epoch: 5440 Train: 0.05280 Test: 1.04264
Epoch: 5520 Train: 0.04966 Test: 1.05854
Epoch: 5600 Train: 0.05204 Test: 1.06413
Epoch: 5680 Train: 0.10942 Test: 1.13400
Epoch: 5760 Train: 0.04876 Test: 1.04165
Epoch: 5840 Train: 0.04524 Test: 1.05357
Epoch: 5920 Train: 1.17460 Test: 1.93721
Epoch: 6000 Train: 0.04488 Test: 1.01372
Epoch 6080: New minimal relative error: 2.00%, model saved.
Epoch: 6080 Train: 0.04251 Test: 1.03290
Epoch: 6160 Train: 0.58430 Test: 1.18103
Epoch: 6240 Train: 0.04111 Test: 1.03893
Epoch: 6320 Train: 0.05285 Test: 0.99831
Epoch: 6400 Train: 0.03932 Test: 1.01410
Epoch: 6480 Train: 0.20223 Test: 1.01304
Epoch: 6560 Train: 0.03878 Test: 0.98004
Epoch: 6640 Train: 0.03712 Test: 0.99166
Epoch: 6720 Train: 1.56367 Test: 2.66908
Epoch: 6800 Train: 0.03734 Test: 0.96808
Epoch: 6880 Train: 0.03529 Test: 0.98273
Epoch: 6960 Train: 0.06745 Test: 0.97844
Epoch: 7040 Train: 0.13799 Test: 1.09716
Epoch: 7120 Train: 0.03433 Test: 0.97299
Epoch 7200: New minimal relative error: 1.99%, model saved.
Epoch: 7200 Train: 0.03264 Test: 0.98919
Epoch: 7280 Train: 0.14452 Test: 1.02924
Epoch: 7360 Train: 0.03208 Test: 0.95612
Epoch: 7440 Train: 0.03110 Test: 0.96981
Epoch: 7520 Train: 0.03185 Test: 0.98136
Epoch: 7600 Train: 0.03271 Test: 0.92713
Epoch: 7680 Train: 0.02988 Test: 0.94714
Epoch: 7760 Train: 0.02916 Test: 0.95615
Epoch: 7840 Train: 0.03126 Test: 0.92631
Epoch 7920: New minimal relative error: 1.61%, model saved.
Epoch: 7920 Train: 0.02827 Test: 0.94777
Epoch: 7999 Train: 0.03870 Test: 0.98378
Training Loss: tensor(0.0387)
Test Loss: tensor(0.9838)
Learned LE: [ 8.8547415e-01 -6.4894691e-04 -5.5621276e+00]
True LE: [ 8.6382562e-01  8.6537572e-03 -1.4544301e+01]
Relative Error: [2.246891   2.0064163  1.8623186  2.1092703  3.0015528  4.3266883
 5.757183   7.051404   8.049617   8.671067   8.904538   8.768433
 8.263294   7.4408507  6.4923787  5.477781   4.3424177  3.2230158
 2.444886   2.3681567  2.860022   3.4599795  3.8982074  4.0764666
 3.9916553  3.687327   3.220058   2.6659722  2.1461108  1.8402122
 1.9175897  2.3456354  2.9368174  3.530299   3.9537137  4.0088315
 4.0288863  4.1862     4.3443565  4.4359536  4.460975   4.3764687
 4.168421   3.9096541  3.663638   3.4598773  3.3552651  3.373007
 3.450679   3.5217628  3.5819516  3.6598723  3.7643385  3.8634677
 3.8983896  3.8264859  3.6613996  3.456822   3.250606   3.0394905
 2.8005638  2.5226057  2.2169833  1.8981798  1.589184   1.539336
 2.2102191  3.4319806  4.809044   6.082602   7.083914   7.7274337
 8.004894   7.9330683  7.499573   6.764234   5.9202886  4.960299
 3.842792   2.7791562  2.1950748  2.448555   3.183496   3.9321504
 4.4771085  4.732531   4.6928244  4.403524   3.9182014  3.2932906
 2.628071   2.0798872  1.8487345  2.0464964  2.5459821  3.1328087
 3.6417444  3.756613   3.6686406  3.7490926  3.8504581  3.9113715
 3.9391682  3.8694158  3.6899917  3.4989436  3.337564   3.2089505
 3.181709   3.2725308  3.395142   3.4810479  3.5404344  3.6132298
 3.7184522  3.835819   3.9047267  3.8590682  3.6918156  3.466377
 3.2484725  3.0481584  2.832907   2.5709813  2.2581496  1.9094311
 1.5285566  1.231244   1.5665786  2.6032076  3.876314   5.1022215
 6.1002564  6.774741   7.1134167  7.125963   6.7836614  6.1497774
 5.414655   4.51905    3.4466665  2.4701033  2.1028416  2.613461
 3.500216   4.3513865  4.9897704  5.331149   5.3542323  5.099934
 4.6291165  3.9811265  3.222717   2.493037   1.9833038  1.8816067
 2.198028   2.7370899  3.286763   3.5575795  3.3921885  3.371699
 3.399216   3.4127607  3.437219   3.3927479  3.2706666  3.1798284
 3.1099308  3.0416698  3.0751913  3.223664   3.3740196  3.4544773
 3.4871268  3.5213628  3.588377   3.6889884  3.776813   3.7695098
 3.6233013  3.3912015  3.1670644  2.988706   2.8217707  2.611498
 2.3278816  1.9787803  1.5839616  1.1822162  1.1464214  1.884067
 2.9864206  4.124649   5.1022882  5.8089223  6.2211843  6.333966
 6.0989394  5.5728807  4.952354   4.1488776  3.160001   2.2987196
 2.1233478  2.7905765  3.7617598  4.681866   5.4004765  5.8371677
 5.9486     5.7513895  5.316361   4.6918287  3.900422   3.042168
 2.3005934  1.8888984  1.9339006  2.343434   2.8875067  3.3095365
 3.2189946  3.0735388  3.0107374  2.9500642  2.9511783  2.939154
 2.9022365  2.9301658  2.946197   2.9215498  3.0032647  3.1997285
 3.3674736  3.4281812  3.4146156  3.3866205  3.385213   3.4295359
 3.4997969  3.521351   3.4172785  3.2040927  2.9856958  2.8365097
 2.7352278  2.6093671  2.3980904  2.0823977  1.6891315  1.2676395
 0.9890096  1.3282583  2.1831737  3.1819735  4.1071796  4.8324676
 5.319773   5.543204   5.430285   5.0125012  4.511277   3.8393095
 2.9760897  2.2432992  2.1936114  2.9272158  3.9386225  4.9048285
 5.6875086  6.2160435  6.43954    6.336227   5.9570813  5.3832684
 4.630506   3.718379   2.7965918  2.0885165  1.8098278  1.9827332
 2.4531896  2.929374   3.1027505  2.8791647  2.7180629  2.551617
 2.4856076  2.494454   2.5566454  2.7067044  2.8066509  2.8190417
 2.9385502  3.17633    3.35828    3.3921793  3.321011   3.220625
 3.1392808  3.100747   3.1061862  3.1140852  3.0476873  2.881112
 2.694023   2.583263   2.5545213  2.532067   2.4319305  2.2013578
 1.8387423  1.4063581  1.0163625  0.9716974  1.5102835  2.3222713
 3.1522048  3.8629503  4.4074407  4.742109   4.7666607  4.460157
 4.0685325  3.563116   2.863033   2.2523706  2.247958   2.9760861
 3.996943   4.9982247  5.8371196  6.446126   6.785597   6.8168364
 6.5372543  6.027941   5.357634   4.5003943  3.4904704  2.5302591
 1.8803632  1.7338133  1.9974825  2.463688   2.8147762  2.806557
 2.5537245  2.272446   2.0737257  2.0451481  2.1914544  2.454748
 2.6518102  2.7097652  2.8551588  3.128836   3.3316386  3.3445973
 3.2145135  3.0417345  2.8848164  2.76038    2.6687474  2.6028383
 2.527548   2.407656   2.2825928  2.2341092  2.2836633  2.3669977
 2.3933942  2.2914078  2.0214438  1.6090493  1.1577026  0.82373303
 0.97519493 1.5831771  2.2920246  2.9446065  3.5026252  3.9280343
 4.100559   3.9260302  3.6123357  3.275695   2.7642183  2.2633796
 2.2335482  2.8890777  3.8809333  4.899893   5.7948184  6.486817
 6.9414206  7.1363635  7.029645   6.6273355 ]
