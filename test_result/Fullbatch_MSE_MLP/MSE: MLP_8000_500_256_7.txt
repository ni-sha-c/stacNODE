time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.98%, model saved.
Epoch: 0 Train: 3720.13428 Test: 4195.91357
Epoch: 80 Train: 463.17249 Test: 419.62601
Epoch 160: New minimal relative error: 52.83%, model saved.
Epoch: 160 Train: 50.85189 Test: 61.62049
Epoch: 240 Train: 76.83691 Test: 87.36295
Epoch 320: New minimal relative error: 25.76%, model saved.
Epoch: 320 Train: 11.65756 Test: 13.74535
Epoch 400: New minimal relative error: 21.38%, model saved.
Epoch: 400 Train: 7.32281 Test: 6.56510
Epoch 480: New minimal relative error: 20.04%, model saved.
Epoch: 480 Train: 4.20240 Test: 4.84365
Epoch: 560 Train: 2.94074 Test: 3.07171
Epoch: 640 Train: 2.48992 Test: 2.55434
Epoch 720: New minimal relative error: 18.69%, model saved.
Epoch: 720 Train: 1.80987 Test: 2.19763
Epoch 800: New minimal relative error: 17.87%, model saved.
Epoch: 800 Train: 1.41640 Test: 1.67127
Epoch: 880 Train: 1.93566 Test: 2.67894
Epoch: 960 Train: 1.30775 Test: 1.60017
Epoch: 1040 Train: 4.29389 Test: 6.66589
Epoch: 1120 Train: 6.93074 Test: 11.44416
Epoch: 1200 Train: 0.83735 Test: 1.98179
Epoch: 1280 Train: 2.67595 Test: 1.51749
Epoch: 1360 Train: 7.79028 Test: 12.31125
Epoch: 1440 Train: 15.98639 Test: 6.40000
Epoch: 1520 Train: 0.35641 Test: 0.46636
Epoch: 1600 Train: 1.08513 Test: 0.65852
Epoch: 1680 Train: 1.59376 Test: 0.92593
Epoch 1760: New minimal relative error: 13.56%, model saved.
Epoch: 1760 Train: 0.26094 Test: 0.34805
Epoch: 1840 Train: 0.26283 Test: 0.35137
Epoch: 1920 Train: 0.59872 Test: 0.74068
Epoch: 2000 Train: 0.22984 Test: 0.31493
Epoch: 2080 Train: 0.23688 Test: 0.31700
Epoch: 2160 Train: 0.38227 Test: 0.32320
Epoch: 2240 Train: 0.23578 Test: 0.28025
Epoch: 2320 Train: 2.42580 Test: 2.01389
Epoch: 2400 Train: 0.21534 Test: 0.31078
Epoch: 2480 Train: 2.82451 Test: 3.10578
Epoch: 2560 Train: 0.36615 Test: 0.98836
Epoch: 2640 Train: 0.83425 Test: 1.27891
Epoch: 2720 Train: 0.14741 Test: 0.21133
Epoch: 2800 Train: 0.13437 Test: 0.18986
Epoch: 2880 Train: 0.17407 Test: 0.36995
Epoch 2960: New minimal relative error: 10.82%, model saved.
Epoch: 2960 Train: 0.27195 Test: 0.42728
Epoch: 3040 Train: 0.10481 Test: 0.15550
Epoch: 3120 Train: 0.42295 Test: 0.39535
Epoch: 3200 Train: 0.08691 Test: 0.13294
Epoch: 3280 Train: 0.11361 Test: 0.16424
Epoch: 3360 Train: 0.61078 Test: 0.41367
Epoch: 3440 Train: 1.19702 Test: 2.05361
Epoch: 3520 Train: 0.07165 Test: 0.11276
Epoch: 3600 Train: 0.08962 Test: 0.16349
Epoch: 3680 Train: 0.43315 Test: 0.71825
Epoch 3760: New minimal relative error: 9.67%, model saved.
Epoch: 3760 Train: 0.06673 Test: 0.10528
Epoch: 3840 Train: 0.09261 Test: 0.13423
Epoch: 3920 Train: 1.16917 Test: 1.91707
Epoch: 4000 Train: 0.27432 Test: 0.42019
Epoch: 4080 Train: 0.06083 Test: 0.09877
Epoch: 4160 Train: 0.08474 Test: 0.10932
Epoch: 4240 Train: 0.65346 Test: 0.57588
Epoch: 4320 Train: 0.28668 Test: 0.47173
Epoch: 4400 Train: 0.04972 Test: 0.08477
Epoch: 4480 Train: 0.12557 Test: 0.10103
Epoch: 4560 Train: 0.04694 Test: 0.08017
Epoch: 4640 Train: 0.05431 Test: 0.08169
Epoch: 4720 Train: 0.07172 Test: 0.11872
Epoch: 4800 Train: 0.04423 Test: 0.07757
Epoch: 4880 Train: 0.04124 Test: 0.07181
Epoch: 4960 Train: 0.05564 Test: 0.09530
Epoch: 5040 Train: 0.03915 Test: 0.06963
Epoch: 5120 Train: 0.08635 Test: 0.12435
Epoch: 5200 Train: 0.20818 Test: 0.34933
Epoch: 5280 Train: 0.04489 Test: 0.07566
Epoch: 5360 Train: 0.03612 Test: 0.06515
Epoch: 5440 Train: 0.06621 Test: 0.11262
Epoch: 5520 Train: 0.04235 Test: 0.07191
Epoch: 5600 Train: 0.03332 Test: 0.06089
Epoch: 5680 Train: 0.15217 Test: 0.07289
Epoch: 5760 Train: 1.43817 Test: 2.01492
Epoch 5840: New minimal relative error: 9.32%, model saved.
Epoch: 5840 Train: 0.03273 Test: 0.06019
Epoch: 5920 Train: 0.03038 Test: 0.05624
Epoch: 6000 Train: 0.03607 Test: 0.07305
Epoch: 6080 Train: 0.02964 Test: 0.05534
Epoch: 6160 Train: 0.03007 Test: 0.05522
Epoch: 6240 Train: 0.03817 Test: 0.06371
Epoch: 6320 Train: 0.02796 Test: 0.05306
Epoch: 6400 Train: 0.04490 Test: 0.06600
Epoch: 6480 Train: 0.02670 Test: 0.05054
Epoch: 6560 Train: 0.09969 Test: 0.28302
Epoch: 6640 Train: 0.02635 Test: 0.05031
Epoch: 6720 Train: 0.02566 Test: 0.04854
Epoch: 6800 Train: 0.02809 Test: 0.05133
Epoch: 6880 Train: 0.06426 Test: 0.09647
Epoch: 6960 Train: 0.02394 Test: 0.04625
Epoch: 7040 Train: 0.03158 Test: 0.06725
Epoch: 7120 Train: 0.02321 Test: 0.04519
Epoch: 7200 Train: 0.28139 Test: 0.43672
Epoch: 7280 Train: 0.02409 Test: 0.04532
Epoch: 7360 Train: 0.08996 Test: 0.08817
Epoch: 7440 Train: 0.02174 Test: 0.04284
Epoch: 7520 Train: 0.71319 Test: 0.98886
Epoch: 7600 Train: 0.02184 Test: 0.04291
Epoch: 7680 Train: 0.03184 Test: 0.05556
Epoch: 7760 Train: 0.02085 Test: 0.04063
Epoch: 7840 Train: 0.06978 Test: 0.05669
Epoch 7920: New minimal relative error: 8.32%, model saved.
Epoch: 7920 Train: 0.01990 Test: 0.03976
Epoch: 7999 Train: 0.01968 Test: 0.03922
Training Loss: tensor(0.0197)
Test Loss: tensor(0.0392)
Learned LE: [ 0.8950728  -0.03546998 -4.8664355 ]
True LE: [ 8.6655664e-01  1.0710658e-02 -1.4553177e+01]
Relative Error: [1.9407842  1.8401924  1.8086005  1.76086    1.593563   1.4197559
 1.3596188  1.4538718  1.763447   2.174807   2.4111311  2.4035883
 2.2957647  2.2012827  2.1163878  1.9936986  1.8315398  1.7069353
 1.664368   1.6731448  1.6952853  1.68482    1.6534954  1.6155083
 1.5500547  1.4547714  1.3406429  1.1556007  0.9705007  0.9195002
 1.0012786  1.0923493  0.9285203  0.9563441  1.1661768  1.2104841
 1.0575724  0.9568224  1.0221487  1.0824081  1.0984     1.1735797
 1.3453788  1.4710237  1.3783498  1.1645566  0.9847759  0.87496525
 0.839278   0.88785005 1.0495038  1.32987    1.6839495  2.011106
 2.2139976  2.3056376  2.3545449  2.3672316  2.2936468  2.094426
 1.8241953  1.6293973  1.5447134  1.463235   1.3839363  1.385414
 1.347768   1.2396704  1.1987323  1.2782688  1.5422989  1.9679096
 2.2844477  2.3401926  2.2557487  2.1812396  2.1409836  2.081866
 1.9544998  1.794377   1.6862959  1.6540257  1.6807047  1.7195568
 1.7241336  1.7124625  1.6711444  1.5746957  1.4386629  1.3017405
 1.1187952  0.9718581  0.9317183  0.98415285 0.96987736 0.76206106
 0.9109002  1.0088855  0.9178158  0.79110336 0.8542001  0.961792
 0.9765392  0.9762991  1.0643702  1.2270602  1.2025163  0.9535117
 0.696299   0.5315767  0.44927448 0.43052268 0.52585495 0.7415247
 1.0475974  1.3755771  1.6216471  1.7597877  1.856434   1.9289564
 1.9292209  1.8205523  1.5988003  1.3394539  1.1995283  1.1823031
 1.1347297  1.0647414  1.0919265  1.0919122  1.0568639  1.0895107
 1.237035   1.5881472  1.9910365  2.1770687  2.140417   2.0569117
 2.0211642  2.0246842  1.9968994  1.8831117  1.736133   1.631811
 1.6034448  1.6528256  1.7195705  1.7353222  1.716684   1.6579777
 1.5443355  1.3903506  1.2496749  1.0773497  0.9335106  0.8937926
 0.932368   0.8484754  0.68825495 0.8322769  0.81006205 0.67291415
 0.6545554  0.80622387 0.882411   0.87127846 0.84422976 0.94320524
 1.0636843  0.8838965  0.56596655 0.31471124 0.20800912 0.10417744
 0.13304865 0.28705224 0.5098687  0.7962792  1.0565332  1.209282
 1.3010788  1.3981712  1.4662528  1.4514065  1.3493232  1.1621333
 0.95544386 0.8906494  0.94493127 0.92766935 0.8581549  0.9094096
 0.95667636 0.9459732  0.97154903 1.1142198  1.449347   1.7926763
 1.9121121  1.865193   1.7963058  1.7760038  1.8248098  1.8493602
 1.7658241  1.6442225  1.5587817  1.5276198  1.5654967  1.6188625
 1.5992364  1.5523496  1.5050924  1.4347584  1.3253632  1.2155324
 1.0757616  0.8895251  0.8353043  0.88024485 0.7794002  0.6095535
 0.7006563  0.64250505 0.5197372  0.55169    0.7079486  0.7744044
 0.7486799  0.6831003  0.8115954  0.86028415 0.6198063  0.2661421
 0.17824364 0.20912902 0.19704409 0.20818707 0.20467834 0.3389257
 0.5778838  0.7682159  0.8415547  0.8827596  0.94827646 0.9893674
 0.98020023 0.9269979  0.80866504 0.6642717  0.6585435  0.76658636
 0.78264165 0.7143726  0.774015   0.87039584 0.84995776 0.8163562
 0.8839675  1.1231757  1.4202123  1.5374596  1.5263209  1.4701885
 1.4357371  1.5146294  1.6025033  1.5654458  1.4876745  1.4444754
 1.4014174  1.3641787  1.3526914  1.2910602  1.2170095  1.1855683
 1.1731733  1.1595429  1.1348215  1.0832574  0.878212   0.7605952
 0.8158159  0.79341966 0.52683324 0.5091613  0.5397461  0.48218957
 0.41213956 0.5442874  0.6324092  0.6164025  0.5155782  0.6456137
 0.6815605  0.42573857 0.1531419  0.34882858 0.3795863  0.36616668
 0.32154426 0.23040158 0.20685358 0.36353156 0.5112534  0.54531825
 0.5522     0.5727062  0.56151175 0.546355   0.5498712  0.52179897
 0.4402245  0.43565688 0.5674391  0.6394285  0.5841139  0.6104983
 0.7528577  0.77506196 0.6693458  0.6195216  0.69790244 0.9445773
 1.1036386  1.14885    1.1248299  1.0506363  1.0876646  1.2076619
 1.2353572  1.207927   1.2296021  1.2426856  1.1498175  1.0527519
 0.99206454 0.90884    0.8533094  0.8192099  0.8220878  0.8886604
 0.9723246  0.91236866 0.6870726  0.6553533  0.78935045 0.59858036
 0.33710444 0.3684483  0.55488443 0.33475175 0.34207904 0.44613066
 0.49514407 0.3929711  0.45044306 0.5178307  0.3175538  0.25226587
 0.48508552 0.452714   0.38513988 0.32114807 0.25003418 0.15950814
 0.17552352 0.2863992  0.30274153 0.32529393 0.36825785 0.3437362
 0.29710543 0.27040684 0.3001019  0.32417148 0.2804282  0.32159632
 0.44471216 0.4561265  0.4094612  0.50624907 0.6480939  0.61504626
 0.45362884 0.34104505 0.40951386 0.65139353 0.7829827  0.839013
 0.7713659  0.6883039  0.70730615 0.7674212  0.7947727  0.8729743
 1.0222865  1.0505452  0.9002815  0.79302144]
