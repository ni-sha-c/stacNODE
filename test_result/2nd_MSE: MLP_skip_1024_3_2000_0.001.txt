time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.01%, model saved.
Epoch: 0 Train: 3934.61084 Test: 3765.70215
Epoch 100: New minimal relative error: 40.79%, model saved.
Epoch: 100 Train: 36.26002 Test: 45.07962
Epoch 200: New minimal relative error: 20.33%, model saved.
Epoch: 200 Train: 8.59162 Test: 11.84142
Epoch 300: New minimal relative error: 18.73%, model saved.
Epoch: 300 Train: 4.87579 Test: 7.29550
Epoch: 400 Train: 16.64648 Test: 21.53718
Epoch 500: New minimal relative error: 16.69%, model saved.
Epoch: 500 Train: 3.78882 Test: 5.31314
Epoch: 600 Train: 18.56200 Test: 15.27112
Epoch: 700 Train: 2.97431 Test: 5.62853
Epoch: 800 Train: 4.00681 Test: 5.58082
Epoch: 900 Train: 10.12518 Test: 14.02810
Epoch 1000: New minimal relative error: 14.46%, model saved.
Epoch: 1000 Train: 4.65078 Test: 4.41972
Epoch 1100: New minimal relative error: 14.18%, model saved.
Epoch: 1100 Train: 3.78329 Test: 4.72706
Epoch 1200: New minimal relative error: 13.28%, model saved.
Epoch: 1200 Train: 1.55279 Test: 2.50677
Epoch: 1300 Train: 3.45829 Test: 4.96441
Epoch: 1400 Train: 5.26123 Test: 6.78560
Epoch 1500: New minimal relative error: 12.83%, model saved.
Epoch: 1500 Train: 1.27101 Test: 2.12329
Epoch: 1600 Train: 0.81948 Test: 1.57227
Epoch: 1700 Train: 4.27133 Test: 3.87585
Epoch 1800: New minimal relative error: 12.76%, model saved.
Epoch: 1800 Train: 0.72406 Test: 1.43357
Epoch 1900: New minimal relative error: 11.09%, model saved.
Epoch: 1900 Train: 0.57965 Test: 1.31437
Epoch: 2000 Train: 2.79340 Test: 4.15912
Epoch 2100: New minimal relative error: 8.35%, model saved.
Epoch: 2100 Train: 0.42000 Test: 0.91453
Epoch 2200: New minimal relative error: 7.75%, model saved.
Epoch: 2200 Train: 0.40294 Test: 0.91432
Epoch: 2300 Train: 6.23972 Test: 5.03928
Epoch: 2400 Train: 1.60510 Test: 2.36748
Epoch: 2500 Train: 0.45567 Test: 0.85593
Epoch: 2600 Train: 3.11546 Test: 4.17523
Epoch: 2700 Train: 1.08193 Test: 2.00077
Epoch: 2800 Train: 0.60450 Test: 0.86013
Epoch: 2900 Train: 2.02441 Test: 2.41870
Epoch: 3000 Train: 0.36585 Test: 0.79841
Epoch: 3100 Train: 3.32045 Test: 3.50062
Epoch: 3200 Train: 0.71614 Test: 1.01804
Epoch: 3300 Train: 0.19463 Test: 0.51635
Epoch: 3400 Train: 0.38933 Test: 0.73155
Epoch: 3500 Train: 0.47196 Test: 0.76441
Epoch: 3600 Train: 0.92078 Test: 1.41277
Epoch: 3700 Train: 0.34718 Test: 0.57882
Epoch: 3800 Train: 0.63493 Test: 0.97601
Epoch: 3900 Train: 0.15851 Test: 0.43444
Epoch: 4000 Train: 3.04090 Test: 3.49555
Epoch: 4100 Train: 0.65368 Test: 1.05768
Epoch: 4200 Train: 0.57345 Test: 0.63224
Epoch: 4300 Train: 1.78803 Test: 2.29627
Epoch: 4400 Train: 1.44822 Test: 2.04355
Epoch: 4500 Train: 1.28426 Test: 1.53756
Epoch: 4600 Train: 0.30825 Test: 0.58636
Epoch: 4700 Train: 0.32531 Test: 0.58035
Epoch 4800: New minimal relative error: 7.69%, model saved.
Epoch: 4800 Train: 0.69076 Test: 0.99115
Epoch: 4900 Train: 0.13272 Test: 0.32798
Epoch 5000: New minimal relative error: 6.40%, model saved.
Epoch: 5000 Train: 0.16802 Test: 0.43119
Epoch: 5100 Train: 0.21019 Test: 0.43075
Epoch: 5200 Train: 0.11496 Test: 0.32463
Epoch: 5300 Train: 0.10876 Test: 0.32465
Epoch: 5400 Train: 0.26479 Test: 0.53933
Epoch: 5500 Train: 0.14732 Test: 0.29525
Epoch: 5600 Train: 0.10721 Test: 0.32694
Epoch: 5700 Train: 0.12763 Test: 0.33432
Epoch: 5800 Train: 0.10145 Test: 0.28078
Epoch: 5900 Train: 0.08245 Test: 0.28789
Epoch: 6000 Train: 0.28319 Test: 0.56425
Epoch: 6100 Train: 0.16412 Test: 0.35000
Epoch 6200: New minimal relative error: 5.50%, model saved.
Epoch: 6200 Train: 0.10072 Test: 0.27371
Epoch: 6300 Train: 0.11222 Test: 0.28483
Epoch: 6400 Train: 0.07054 Test: 0.23954
Epoch: 6500 Train: 0.07892 Test: 0.23896
Epoch: 6600 Train: 0.13787 Test: 0.27959
Epoch: 6700 Train: 0.10699 Test: 0.26807
Epoch: 6800 Train: 0.12242 Test: 0.30913
Epoch: 6900 Train: 0.06253 Test: 0.23405
Epoch: 7000 Train: 0.21649 Test: 0.47232
Epoch: 7100 Train: 0.08062 Test: 0.24108
Epoch: 7200 Train: 0.07043 Test: 0.22753
Epoch: 7300 Train: 0.05845 Test: 0.21598
Epoch: 7400 Train: 0.69043 Test: 0.78759
Epoch: 7500 Train: 0.11244 Test: 0.25782
Epoch: 7600 Train: 0.11986 Test: 0.24583
Epoch: 7700 Train: 0.22561 Test: 0.39782
Epoch: 7800 Train: 0.11706 Test: 0.21600
Epoch: 7900 Train: 0.07844 Test: 0.25658
Epoch: 8000 Train: 0.05685 Test: 0.20439
Epoch: 8100 Train: 0.07063 Test: 0.22409
Epoch: 8200 Train: 0.05494 Test: 0.20814
Epoch: 8300 Train: 0.28412 Test: 0.39908
Epoch: 8400 Train: 0.07496 Test: 0.21787
Epoch: 8500 Train: 0.05029 Test: 0.19346
Epoch: 8600 Train: 0.08837 Test: 0.19532
Epoch: 8700 Train: 0.07109 Test: 0.19472
Epoch: 8800 Train: 0.06038 Test: 0.20805
Epoch: 8900 Train: 0.30516 Test: 0.47707
Epoch: 9000 Train: 0.18309 Test: 0.27349
Epoch: 9100 Train: 0.42537 Test: 0.61804
Epoch: 9200 Train: 0.04533 Test: 0.17890
Epoch: 9300 Train: 0.04184 Test: 0.17021
Epoch: 9400 Train: 0.04167 Test: 0.16386
Epoch 9500: New minimal relative error: 5.15%, model saved.
Epoch: 9500 Train: 0.04823 Test: 0.18265
Epoch: 9600 Train: 0.04205 Test: 0.17001
Epoch: 9700 Train: 0.03913 Test: 0.16235
Epoch: 9800 Train: 0.03961 Test: 0.15869
Epoch: 9900 Train: 0.04255 Test: 0.16098
Epoch: 9999 Train: 0.06668 Test: 0.16805
Training Loss: tensor(0.0667)
Test Loss: tensor(0.1681)
Learned LE: [ 0.81376976  0.06905083 -4.0726447 ]
True LE: [ 8.5081857e-01  9.3254074e-03 -1.4537817e+01]
Relative Error: [0.2997215  0.16616978 0.13407291 0.25129887 0.3744649  0.34399512
 0.3880155  0.66728306 0.79948586 0.82498884 0.8410252  0.57318956
 0.44704768 0.3004912  0.29074165 0.36076078 0.40220985 0.41327035
 0.44413987 0.5168745  0.4967893  0.33969864 0.51341045 0.57134306
 0.6658517  0.65181094 0.40206745 0.3174219  0.21776628 0.28109917
 0.28706282 0.550025   0.6330122  0.5678644  0.50893754 0.56673914
 0.45435402 0.3602665  0.14301057 0.27785686 0.32694784 0.37089384
 0.37155634 0.71698606 0.62684405 0.54571915 0.4167445  0.299155
 0.38145667 0.44696963 0.3681128  0.5163651  0.6651732  0.8441278
 0.589847   0.44692823 0.57003146 0.74033636 0.7265491  0.6759982
 0.606036   0.5792021  0.4047174  0.3384993  0.28807378 0.26873404
 0.346663   0.3743819  0.2786904  0.37207833 0.6059194  0.6348834
 0.7356176  0.591136   0.33532247 0.25159124 0.16048051 0.15663363
 0.25394267 0.3065162  0.32659402 0.3967103  0.42640734 0.28899872
 0.379244   0.65789366 1.0187287  0.83106506 0.5283557  0.2666562
 0.05733305 0.22240894 0.22054584 0.46015018 0.5959125  0.5513343
 0.5566677  0.58038473 0.625812   0.5074454  0.20799938 0.20162939
 0.26166052 0.3391602  0.3984616  0.5666719  0.6976535  0.54410475
 0.4357113  0.2495178  0.20737508 0.4938354  0.33835816 0.34369257
 0.6395652  0.6424155  0.48759976 0.38042223 0.40049455 0.62677956
 0.6580277  0.52809465 0.52876866 0.5584898  0.46988195 0.45869228
 0.4448741  0.37085882 0.384135   0.38065344 0.3045668  0.2710535
 0.44981512 0.47630686 0.5600356  0.54264563 0.45701963 0.20316528
 0.10902851 0.08309032 0.19447027 0.2057597  0.25252312 0.32578045
 0.3112131  0.23611665 0.3332073  0.59077716 0.9589697  1.0612919
 0.607728   0.3690518  0.05086976 0.07077107 0.16893205 0.2403684
 0.30930606 0.27801305 0.3206161  0.3656597  0.60329515 0.6896857
 0.5532717  0.2005138  0.22167945 0.2911691  0.39245403 0.4498172
 0.6220005  0.6365822  0.43884474 0.277055   0.1854262  0.49328026
 0.41701305 0.29225153 0.49126223 0.56385136 0.38853204 0.22044514
 0.27065438 0.39696935 0.47433433 0.39022842 0.43438995 0.35389614
 0.35555464 0.4303524  0.40166685 0.38258535 0.4395124  0.4241503
 0.3170488  0.2409057  0.40723866 0.4333599  0.45589837 0.4413833
 0.38948566 0.30315822 0.14694723 0.07833579 0.09682227 0.1842241
 0.22264591 0.24233311 0.31502157 0.2548727  0.20100716 0.48650584
 0.7535065  0.9949727  0.76850194 0.4558914  0.24326423 0.11663172
 0.08619831 0.13166313 0.11093272 0.11452944 0.12658787 0.18909308
 0.22528094 0.5292487  0.64147687 0.4426788  0.2593304  0.14485283
 0.3554862  0.49703774 0.35941872 0.5095473  0.5220975  0.30730745
 0.27826548 0.27589595 0.4481391  0.34219265 0.45730844 0.47792453
 0.483234   0.18354428 0.20178412 0.3132829  0.30715305 0.1561833
 0.25245965 0.31173226 0.3216553  0.38409102 0.34980762 0.34059942
 0.384896   0.47793183 0.43615735 0.3408641  0.28443277 0.43378854
 0.47185266 0.49867794 0.4137271  0.24323957 0.13682602 0.09768344
 0.08866727 0.09685601 0.19142215 0.37370688 0.31726792 0.43465453
 0.2937197  0.24789427 0.5636103  0.7950024  0.7052806  0.72472787
 0.45225635 0.40196157 0.4520551  0.5223798  0.48300698 0.31671453
 0.2566393  0.17385462 0.2504286  0.1838685  0.28981462 0.4704136
 0.4156832  0.24048835 0.11979532 0.39111617 0.5449784  0.2588275
 0.50739425 0.48138675 0.3084243  0.3688836  0.47821483 0.37830457
 0.4714547  0.5297072  0.49158823 0.41217968 0.36686227 0.4345005
 0.34543976 0.28310376 0.23181479 0.16137806 0.17350361 0.39279428
 0.25885507 0.37883055 0.3989581  0.47143018 0.4222082  0.347336
 0.34536502 0.36757714 0.38192225 0.46945778 0.4651105  0.22763619
 0.14345953 0.0859993  0.24132642 0.15958008 0.10924813 0.2922829
 0.5284694  0.423495   0.5378637  0.53952646 0.40290454 0.54626095
 0.5115635  0.6582654  0.883491   0.7824735  0.6128827  0.7382167
 0.49774045 0.52716315 0.4437909  0.46085018 0.33181387 0.16942656
 0.30016276 0.23629737 0.29137748 0.47003195 0.3973792  0.08154589
 0.27417195 0.67973197 0.39190772 0.5567083  0.5235241  0.37813023
 0.4336922  0.5403887  0.3488296  0.54945344 0.53098726 0.6520913
 0.6307509  0.73381674 0.77194136 0.66485035 0.4974088  0.40197536
 0.20571998 0.19987871 0.25336438 0.12452917 0.33025554 0.5007453
 0.6692818  0.5273372  0.2258399  0.23935933 0.42156526 0.40441546
 0.4256963  0.4402529  0.24398884 0.18871707 0.12074287 0.26095128
 0.2937483  0.16633712 0.36700052 0.5232554  0.49867356 0.6465788
 0.7695868  0.6578214  0.5916987  0.40054005]
