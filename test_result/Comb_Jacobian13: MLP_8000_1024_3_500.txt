time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.34%, model saved.
Epoch: 0 Train: 31937.48828 Test: 3948.47339
Epoch 80: New minimal relative error: 74.35%, model saved.
Epoch: 80 Train: 2783.14209 Test: 537.84991
Epoch 160: New minimal relative error: 30.05%, model saved.
Epoch: 160 Train: 305.26233 Test: 23.82757
Epoch 240: New minimal relative error: 19.05%, model saved.
Epoch: 240 Train: 390.62363 Test: 27.04792
Epoch 320: New minimal relative error: 3.95%, model saved.
Epoch: 320 Train: 31.09939 Test: 1.54695
Epoch: 400 Train: 20.24443 Test: 0.81473
Epoch: 480 Train: 44.77442 Test: 33.88851
Epoch: 560 Train: 32.56607 Test: 2.93164
Epoch: 640 Train: 18.22928 Test: 4.01570
Epoch: 720 Train: 71.55135 Test: 68.95441
Epoch: 800 Train: 55.37985 Test: 39.16150
Epoch 880: New minimal relative error: 2.03%, model saved.
Epoch: 880 Train: 10.63324 Test: 2.86813
Epoch: 960 Train: 14.35979 Test: 7.59711
Epoch: 1040 Train: 23.66074 Test: 16.83811
Epoch: 1120 Train: 23.00827 Test: 2.41910
Epoch: 1200 Train: 24.71490 Test: 6.98658
Epoch: 1280 Train: 15.48189 Test: 11.37304
Epoch: 1360 Train: 5.59240 Test: 1.42529
Epoch: 1440 Train: 3.33092 Test: 0.27645
Epoch: 1520 Train: 4.14094 Test: 1.44145
Epoch: 1600 Train: 65.07883 Test: 35.28648
Epoch: 1680 Train: 6.73299 Test: 4.13301
Epoch: 1760 Train: 11.99882 Test: 5.48748
Epoch: 1840 Train: 39.47656 Test: 30.70193
Epoch: 1920 Train: 16.55993 Test: 13.19410
Epoch: 2000 Train: 3.87611 Test: 0.87666
Epoch: 2080 Train: 2.74318 Test: 0.94847
Epoch: 2160 Train: 4.00171 Test: 1.28302
Epoch: 2240 Train: 2.29515 Test: 0.95898
Epoch: 2320 Train: 24.99028 Test: 17.32491
Epoch: 2400 Train: 4.08060 Test: 2.52701
Epoch: 2480 Train: 1.86939 Test: 0.42052
Epoch: 2560 Train: 3.14006 Test: 1.04051
Epoch: 2640 Train: 4.15977 Test: 1.63987
Epoch: 2720 Train: 6.07861 Test: 1.61248
Epoch: 2800 Train: 1.04063 Test: 0.11906
Epoch: 2880 Train: 1.97937 Test: 0.82568
Epoch: 2960 Train: 3.69288 Test: 2.62978
Epoch: 3040 Train: 1.40042 Test: 0.51333
Epoch: 3120 Train: 4.20980 Test: 2.58411
Epoch: 3200 Train: 4.31349 Test: 1.38977
Epoch: 3280 Train: 1.13333 Test: 0.23198
Epoch: 3360 Train: 1.19419 Test: 0.23436
Epoch: 3440 Train: 2.78282 Test: 1.40196
Epoch: 3520 Train: 2.15967 Test: 1.12562
Epoch: 3600 Train: 1.09322 Test: 0.30448
Epoch: 3680 Train: 0.71000 Test: 0.10255
Epoch: 3760 Train: 1.33741 Test: 0.37642
Epoch: 3840 Train: 1.23361 Test: 0.55886
Epoch: 3920 Train: 2.42842 Test: 0.98906
Epoch: 4000 Train: 0.53929 Test: 0.02857
Epoch: 4080 Train: 0.69052 Test: 0.10470
Epoch: 4160 Train: 4.15897 Test: 2.48033
Epoch: 4240 Train: 0.42174 Test: 0.01921
Epoch: 4320 Train: 0.63746 Test: 0.03550
Epoch: 4400 Train: 0.95719 Test: 0.34985
Epoch 4480: New minimal relative error: 1.71%, model saved.
Epoch: 4480 Train: 1.20262 Test: 0.48846
Epoch: 4560 Train: 3.10633 Test: 1.64301
Epoch: 4640 Train: 0.43336 Test: 0.08034
Epoch: 4720 Train: 0.34257 Test: 0.01718
Epoch: 4800 Train: 0.37725 Test: 0.02177
Epoch 4880: New minimal relative error: 1.14%, model saved.
Epoch: 4880 Train: 0.53468 Test: 0.03007
Epoch: 4960 Train: 0.34671 Test: 0.04700
Epoch: 5040 Train: 0.33423 Test: 0.03247
Epoch: 5120 Train: 0.31173 Test: 0.03331
Epoch: 5200 Train: 3.63001 Test: 1.32797
Epoch: 5280 Train: 13.08795 Test: 8.66036
Epoch 5360: New minimal relative error: 1.09%, model saved.
Epoch: 5360 Train: 0.27471 Test: 0.01857
Epoch: 5440 Train: 0.42050 Test: 0.10025
Epoch: 5520 Train: 0.27395 Test: 0.02214
Epoch: 5600 Train: 0.39332 Test: 0.09159
Epoch: 5680 Train: 4.51088 Test: 2.94867
Epoch: 5760 Train: 1.63415 Test: 1.21308
Epoch: 5840 Train: 0.30251 Test: 0.03563
Epoch: 5920 Train: 0.69871 Test: 0.37576
Epoch: 6000 Train: 7.93974 Test: 5.63255
Epoch: 6080 Train: 0.29334 Test: 0.06649
Epoch: 6160 Train: 0.56212 Test: 0.22860
Epoch: 6240 Train: 0.22325 Test: 0.02929
Epoch: 6320 Train: 0.21813 Test: 0.01751
Epoch: 6400 Train: 4.38045 Test: 2.19024
Epoch: 6480 Train: 0.18381 Test: 0.01075
Epoch 6560: New minimal relative error: 0.90%, model saved.
Epoch: 6560 Train: 0.18669 Test: 0.01919
Epoch: 6640 Train: 0.24591 Test: 0.03378
Epoch 6720: New minimal relative error: 0.63%, model saved.
Epoch: 6720 Train: 0.16745 Test: 0.00964
Epoch: 6800 Train: 0.28069 Test: 0.06055
Epoch: 6880 Train: 0.17186 Test: 0.01944
Epoch: 6960 Train: 0.17719 Test: 0.01564
Epoch: 7040 Train: 0.86528 Test: 0.43860
Epoch: 7120 Train: 1.94899 Test: 1.08510
Epoch: 7200 Train: 0.99269 Test: 0.72064
Epoch: 7280 Train: 0.17069 Test: 0.01927
Epoch 7360: New minimal relative error: 0.52%, model saved.
Epoch: 7360 Train: 0.15574 Test: 0.01699
Epoch: 7440 Train: 0.14299 Test: 0.01220
Epoch: 7520 Train: 0.13433 Test: 0.00915
Epoch: 7600 Train: 0.38501 Test: 0.10048
Epoch: 7680 Train: 0.17245 Test: 0.03348
Epoch: 7760 Train: 0.18385 Test: 0.05216
Epoch: 7840 Train: 0.59287 Test: 0.32649
Epoch: 7920 Train: 4.99523 Test: 3.57599
Epoch 7999: New minimal relative error: 0.21%, model saved.
Epoch: 7999 Train: 0.11988 Test: 0.00908
Training Loss: tensor(0.1199)
Test Loss: tensor(0.0091)
Learned LE: [ 8.8195336e-01 -5.3878492e-03 -1.4552116e+01]
True LE: [ 8.7924862e-01 -3.1753713e-03 -1.4549825e+01]
Relative Error: [1.9043278 2.0263014 2.2329872 2.5101657 2.8092456 3.0807235 3.3019702
 3.4733813 3.6072154 3.7172463 3.8153508 3.910936  4.0127444 4.128347
 4.2659464 4.433769  4.64054   4.8922315 5.19027   5.528245  5.889885
 6.248922  6.569952  6.8099184 6.9237113 6.880253  6.6853733 6.3911595
 6.0624595 5.733751  5.4059544 5.077462  4.758309  4.465283  4.216807
 4.028676  3.9142153 3.8775525 3.9117541 4.000004  4.1204944 4.2526135
 4.3777633 4.480762  4.5490146 4.5727534 4.545488  4.4647627 4.331237
 4.148666  3.922785  3.65954   3.3648596 3.04594   2.7129467 2.3830664
 2.0817697 1.8423193 1.6931577 1.6367327 1.6451283 1.6857507 1.759933
 1.9008461 2.1308873 2.4200299 2.710566  2.9582565 3.1510823 3.2971673
 3.4114263 3.5066836 3.5939894 3.6813672 3.7760444 3.8846831 4.0151296
 4.176078  4.3770547 4.6256423 4.923861  5.263704  5.625242  5.978188
 6.2836013 6.496975  6.572806  6.4790473 6.22864   5.8859024 5.5257263
 5.179853  4.843713  4.5133977 4.2001467 3.922062  3.6982715 3.5475392
 3.482504  3.5024009 3.5923495 3.7287588 3.8886883 4.051467  4.199733
 4.3193502 4.398396  4.427459  4.4003887 4.3149805 4.173273  3.9806015
 3.7441976 3.471731  3.1700585 2.847288  2.51431   2.1885161 1.8952852
 1.6672105 1.5312331 1.4863822 1.5016685 1.5466505 1.6288558 1.7878901
 2.0356736 2.3269165 2.5991077 2.817957  2.9821234 3.1053402 3.2027164
 3.2860365 3.364674  3.4453962 3.5339692 3.636055  3.7582815 3.9101095
 4.10288   4.3467393 4.6447787 4.9871964 5.350279  5.6973877 5.986397
 6.171619  6.2077603 6.065367  5.761921  5.3756423 4.990248  4.6333017
 4.292609  3.9638863 3.6592338 3.3987765 3.2050457 3.0992873 3.0902135
 3.168267  3.3096592 3.4877818 3.679184  3.8657587 4.031866  4.1646814
 4.2522845 4.285718  4.258738  4.1691947 4.0202036 3.8182259 3.5725698
 3.29221   2.985558  2.6610422 2.3308842 2.0122745 1.7299492 1.5139059
 1.3888468 1.3522278 1.3718987 1.4187747 1.507978  1.682128  1.9401472
 2.222864  2.4694195 2.6573431 2.7945185 2.8975816 2.9806304 3.0543716
 3.1265764 3.2027967 3.2871284 3.382692  3.4950593 3.634467  3.8150835
 4.050856  4.347495  4.6943555 5.0613356 5.405186  5.6780825 5.8344235
 5.831908  5.6430693 5.290943  4.86679   4.4622154 4.0987444 3.7571988
 3.4321022 3.1380138 2.898776  2.7422667 2.6892028 2.7396743 2.8724275
 3.0566702 3.2665842 3.4811263 3.6846478 3.8636982 4.0063457 4.1013875
 4.1392612 4.1134596 4.021562  3.8668346 3.6578012 3.4049592 3.1190987
 2.8099244 2.4865997 2.1619575 1.8535886 1.5842142 1.3806679 1.2637026
 1.2318101 1.2522799 1.2991326 1.3932616 1.5773389 1.8369963 2.1020985
 2.318374  2.4757636 2.588653  2.6744018 2.7456143 2.8120146 2.8805523
 2.9554415 3.0377178 3.1272218 3.227454  3.349758  3.5124016 3.7349203
 4.027684  4.3796687 4.75481   5.0997343 5.3594365 5.488286  5.4490232
 5.218281  4.8228564 4.3660703 3.9478161 3.5818257 3.241418  2.921219
 2.639891  2.427606  2.3162432 2.3215835 2.4298067 2.6075792 2.8238614
 3.055042  3.284134  3.498076  3.6853638 3.8351185 3.9367807 3.9799623
 3.9572372 3.865436  3.708363  3.495099  3.2382817 2.9503677 2.6416397
 2.3222544 2.0061173 1.7109188 1.4568255 1.2653838 1.1540993 1.1220517
 1.1406946 1.1849085 1.2799608 1.4678639 1.7206076 1.961573  2.1458359
 2.2744923 2.365937  2.436548  2.4983487 2.5604312 2.629708  2.7077472
 2.7909899 2.8750038 2.960589  3.0601227 3.1967711 3.398091  3.6813128
 4.03764   4.425206  4.7780805 5.0305786 5.13622   5.0647917 4.7984266
 4.3666596 3.8821275 3.4538672 3.087848  2.749709  2.435241  2.1694107
 1.990617  1.9321434 1.9971861 2.1541548 2.364933  2.6008952 2.8436103
 3.0791776 3.2972817 3.488481  3.6431425 3.7508905 3.8005207 3.7833755
 3.695402  3.5397    3.3262112 3.0693917 2.7830305 2.4785209 2.1665368
 1.8615514 1.581708  1.345     1.1657629 1.0570213 1.0208575 1.0343028
 1.0730608 1.1646609 1.3494451 1.5880982 1.8007869 1.9532282 2.0557609
 2.128511  2.1862655 2.2411342 2.3037405 2.3791773 2.4651022 2.552842
 2.6325817 2.7018106 2.77291   2.8748343 3.044507  3.3086321 3.6644208
 4.0666103 4.435935  4.6915445 4.78206   4.685502  4.392239  3.9319913
 3.4238014]
