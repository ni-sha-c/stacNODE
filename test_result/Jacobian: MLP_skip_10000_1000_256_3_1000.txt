time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.03%, model saved.
Epoch: 0 Train: 60549.35156 Test: 4211.57910
Epoch 100: New minimal relative error: 99.17%, model saved.
Epoch: 100 Train: 16723.42773 Test: 1684.34412
Epoch 200: New minimal relative error: 75.79%, model saved.
Epoch: 200 Train: 15997.87891 Test: 1565.52429
Epoch: 300 Train: 16139.96289 Test: 1390.70447
Epoch: 400 Train: 14545.88086 Test: 1282.66150
Epoch: 500 Train: 15354.73633 Test: 1378.91541
Epoch 600: New minimal relative error: 72.61%, model saved.
Epoch: 600 Train: 14620.71094 Test: 1352.96631
Epoch 700: New minimal relative error: 72.15%, model saved.
Epoch: 700 Train: 13791.07031 Test: 1369.03809
Epoch: 800 Train: 10581.57031 Test: 788.09882
Epoch: 900 Train: 8585.83105 Test: 455.56873
Epoch: 1000 Train: 4897.03418 Test: 180.16638
Epoch 1100: New minimal relative error: 32.58%, model saved.
Epoch: 1100 Train: 2067.37793 Test: 42.70035
Epoch: 1200 Train: 1161.83667 Test: 16.79924
Epoch: 1300 Train: 850.06171 Test: 8.04635
Epoch: 1400 Train: 675.34601 Test: 5.56512
Epoch 1500: New minimal relative error: 18.87%, model saved.
Epoch: 1500 Train: 605.10480 Test: 4.54178
Epoch 1600: New minimal relative error: 10.73%, model saved.
Epoch: 1600 Train: 527.07635 Test: 3.79285
Epoch: 1700 Train: 497.57779 Test: 4.60928
Epoch: 1800 Train: 577.45288 Test: 6.57911
Epoch: 1900 Train: 621.73456 Test: 8.25217
Epoch: 2000 Train: 644.31580 Test: 7.25824
Epoch 2100: New minimal relative error: 7.96%, model saved.
Epoch: 2100 Train: 462.44461 Test: 3.43168
Epoch: 2200 Train: 421.90039 Test: 2.72815
Epoch: 2300 Train: 400.91193 Test: 2.40191
Epoch: 2400 Train: 366.37546 Test: 1.86053
Epoch: 2500 Train: 346.26498 Test: 2.82688
Epoch: 2600 Train: 315.47009 Test: 2.54202
Epoch: 2700 Train: 275.00586 Test: 1.09385
Epoch 2800: New minimal relative error: 5.23%, model saved.
Epoch: 2800 Train: 255.55975 Test: 1.05799
Epoch: 2900 Train: 238.91504 Test: 0.96212
Epoch: 3000 Train: 230.64983 Test: 0.75669
Epoch: 3100 Train: 228.23271 Test: 1.05035
Epoch: 3200 Train: 229.93233 Test: 1.36669
Epoch 3300: New minimal relative error: 4.71%, model saved.
Epoch: 3300 Train: 217.22002 Test: 0.91712
Epoch: 3400 Train: 228.19585 Test: 0.77676
Epoch: 3500 Train: 212.13289 Test: 0.90151
Epoch: 3600 Train: 234.04044 Test: 1.13653
Epoch: 3700 Train: 233.98146 Test: 1.09059
Epoch: 3800 Train: 227.72008 Test: 0.97219
Epoch: 3900 Train: 239.93498 Test: 1.41866
Epoch: 4000 Train: 215.41296 Test: 0.91158
Epoch: 4100 Train: 218.90691 Test: 0.96079
Epoch: 4200 Train: 215.33606 Test: 1.26072
Epoch: 4300 Train: 218.31523 Test: 1.04557
Epoch: 4400 Train: 205.17049 Test: 0.94137
Epoch: 4500 Train: 211.77731 Test: 0.80146
Epoch: 4600 Train: 212.28783 Test: 1.07944
Epoch: 4700 Train: 211.06070 Test: 1.06503
Epoch: 4800 Train: 224.43466 Test: 1.17623
Epoch: 4900 Train: 224.31793 Test: 1.18247
Epoch: 5000 Train: 240.44881 Test: 1.88312
Epoch: 5100 Train: 212.43114 Test: 1.07089
Epoch: 5200 Train: 204.29410 Test: 1.08882
Epoch: 5300 Train: 202.77344 Test: 1.61152
Epoch: 5400 Train: 193.29361 Test: 1.03861
Epoch: 5500 Train: 189.36946 Test: 0.88018
Epoch: 5600 Train: 184.33249 Test: 1.10354
Epoch: 5700 Train: 189.69147 Test: 1.08866
Epoch: 5800 Train: 175.38245 Test: 0.99915
Epoch: 5900 Train: 170.95233 Test: 0.82574
Epoch: 6000 Train: 166.94130 Test: 0.61888
Epoch: 6100 Train: 162.63361 Test: 0.72615
Epoch: 6200 Train: 179.36102 Test: 1.27895
Epoch 6300: New minimal relative error: 3.40%, model saved.
Epoch: 6300 Train: 149.65942 Test: 0.52093
Epoch: 6400 Train: 148.53214 Test: 0.48917
Epoch: 6500 Train: 143.66507 Test: 0.49385
Epoch: 6600 Train: 136.76601 Test: 0.39897
Epoch: 6700 Train: 140.17711 Test: 0.52147
Epoch: 6800 Train: 128.88541 Test: 0.39329
Epoch: 6900 Train: 130.94711 Test: 0.42683
Epoch: 7000 Train: 133.62355 Test: 0.45716
Epoch: 7100 Train: 118.92715 Test: 0.28732
Epoch: 7200 Train: 121.87698 Test: 0.32782
Epoch: 7300 Train: 118.79107 Test: 0.33230
Epoch: 7400 Train: 114.28263 Test: 0.25982
Epoch: 7500 Train: 120.03435 Test: 0.37624
Epoch: 7600 Train: 116.53175 Test: 0.33062
Epoch: 7700 Train: 113.36628 Test: 0.30519
Epoch: 7800 Train: 115.59872 Test: 0.33948
Epoch: 7900 Train: 117.85715 Test: 0.31463
Epoch: 8000 Train: 117.27341 Test: 0.33104
Epoch: 8100 Train: 120.25797 Test: 0.61832
Epoch: 8200 Train: 104.26360 Test: 0.24522
Epoch: 8300 Train: 100.67599 Test: 0.21029
Epoch: 8400 Train: 100.30296 Test: 0.30615
Epoch 8500: New minimal relative error: 3.32%, model saved.
Epoch: 8500 Train: 99.43679 Test: 0.23023
Epoch: 8600 Train: 98.56696 Test: 0.22255
Epoch 8700: New minimal relative error: 2.21%, model saved.
Epoch: 8700 Train: 95.76769 Test: 0.26772
Epoch: 8800 Train: 103.87521 Test: 0.33861
Epoch: 8900 Train: 95.83350 Test: 0.19937
Epoch: 9000 Train: 91.31448 Test: 0.16976
Epoch: 9100 Train: 89.99764 Test: 0.13503
Epoch: 9200 Train: 92.47542 Test: 0.15868
Epoch: 9300 Train: 93.41352 Test: 0.23602
Epoch: 9400 Train: 94.02921 Test: 0.30111
Epoch: 9500 Train: 104.70895 Test: 0.47862
Epoch: 9600 Train: 98.48557 Test: 0.28326
Epoch: 9700 Train: 93.97899 Test: 0.22005
Epoch: 9800 Train: 92.74687 Test: 0.21114
Epoch: 9900 Train: 98.22384 Test: 0.23854
Epoch: 9999 Train: 98.43994 Test: 0.25348
Training Loss: tensor(98.4399)
Test Loss: tensor(0.2535)
Learned LE: [  0.8068641    0.05707036 -14.54097   ]
True LE: [ 8.6603558e-01 -2.2113859e-03 -1.4551199e+01]
Relative Error: [3.0991352  3.3104017  3.4463308  3.5961635  3.96493    4.277094
 4.445049   4.689058   4.4955316  4.142617   3.5600753  2.8443782
 2.0716076  1.9348543  1.9883517  2.1910803  2.6229303  3.22319
 3.5603075  3.6592205  3.670456   3.6688366  3.558861   3.5661404
 4.156226   4.6387944  4.968561   5.140558   5.2211084  5.2682114
 5.559073   5.861411   5.7719254  5.7780747  5.7557974  5.3730545
 4.896773   4.300136   4.1900473  4.4049497  4.7386203  5.0793657
 5.3747725  5.509589   5.3306656  4.907183   4.4453254  3.9982479
 3.7301466  3.7945838  3.9548535  4.0614085  4.0017223  3.8826556
 3.6573474  3.4819472  3.3045936  3.0972435  3.0177405  2.8779998
 2.762505   2.6540816  2.6456926  2.7622259  2.8571246  3.0072696
 3.4771469  3.8954322  4.124654   4.4687357  4.437786   4.204935
 3.78284    3.142488   2.3517823  1.8480659  1.7950863  2.1037292
 2.4606683  3.102042   3.387959   3.3942685  3.2795732  3.1683142
 3.0119565  3.061322   3.7600152  4.348795   4.6525846  4.815324
 4.8287992  4.7930355  4.9115615  5.1809115  5.3059382  5.322999
 5.6157823  5.2256618  4.7334485  4.1644835  3.7620847  3.9379194
 4.3220377  4.783106   5.187806   5.3429203  5.1748166  4.7901216
 4.2886853  3.827074   3.5715885  3.7018113  3.9152124  4.0480385
 3.7158206  3.435595   3.3516173  3.177096   2.971279   2.8162758
 2.6041884  2.5247977  2.4648347  2.458715   2.2925308  2.1738093
 2.3459117  2.5120647  2.964233   3.4647245  3.7805648  4.216814
 4.4575596  4.2902765  3.926591   3.3482497  2.6383233  1.9095262
 1.7848817  1.9274039  2.3179996  2.865767   3.0932798  2.9915829
 2.836106   2.7006302  2.547657   2.6388888  3.4876752  4.047476
 4.4111743  4.4967694  4.458697   4.372986   4.3424754  4.466975
 4.685837   4.936834   5.1351275  5.1064987  4.5734935  4.0529823
 3.5961242  3.4521685  3.8696675  4.422279   4.9339576  5.0773816
 5.024623   4.735701   4.284199   3.7807856  3.476525   3.6323164
 3.9215796  3.728785   3.3682318  3.0780249  3.0228772  2.948976
 2.7711425  2.4877987  2.3253856  2.1180968  1.9576743  2.003819
 2.2134023  2.0261114  1.8748913  2.0494838  2.3335443  2.8548093
 3.2514534  3.7532904  4.2277346  4.2948017  4.0622797  3.5777054
 2.9572134  2.219771   1.6733315  1.7491934  2.0460954  2.6410036
 2.7869947  2.4015892  2.251048   2.1827796  2.0987358  2.321232
 3.1554933  3.8247342  4.1858087  4.2981052  4.151166   3.9489162
 3.837694   3.8079474  3.959439   4.3622584  4.6568036  4.8142834
 4.422747   3.9521246  3.4932568  3.191791   3.3454254  3.9594908
 4.515154   4.7697883  4.8674335  4.714177   4.3711677  3.867495
 3.4376085  3.4819212  3.6236875  3.375277   3.1119454  2.856993
 2.8122165  2.7386234  2.595615   2.379314   2.0655634  1.8639714
 1.6168872  1.5662173  1.8923521  2.0914097  2.1306953  2.154534
 2.2285573  2.5317495  2.821659   3.1093996  3.685085   4.094833
 4.039278   3.712741   3.3017833  2.7095618  1.7611334  1.6028547
 1.8438051  2.3433387  2.6112502  1.935533   1.7266119  1.7017636
 1.6323751  1.7637331  2.8374815  3.5060196  3.9735713  4.1386695
 4.0246906  3.6776085  3.3437908  3.2081168  3.2271743  3.6052053
 4.056782   4.322991   4.231533   3.819472   3.3749197  3.0892973
 2.9550743  3.3968172  3.9996927  4.376969   4.600874   4.6152234
 4.362216   3.9478242  3.5072215  3.3646214  3.2673557  3.0891078
 2.91124    2.7222188  2.5730464  2.5263085  2.411251   2.2549307
 2.0407395  1.7550491  1.5627381  1.4382286  1.6723005  1.8971202
 1.9747438  2.329977   2.3779817  2.3465817  2.5734978  2.8222911
 3.293895   3.695121   3.8019023  3.6650095  3.4399548  3.077926
 2.2471821  1.5646573  1.6908951  2.0740876  2.5660064  1.7524776
 1.2843385  1.2558575  1.270253   1.1377934  2.1883178  3.0982983
 3.6502805  3.922999   3.90866    3.5991392  3.1133978  2.7007425
 2.4829676  2.80394    3.1459875  3.6702137  3.750064   3.6709204
 3.2404516  2.9386292  2.8746452  2.8572152  3.289634   3.7887793
 4.189612   4.4213696  4.3439994  4.0360446  3.6200635  3.2437067
 2.8940609  2.7221127  2.5097022  2.3991232  2.2476406  2.2596343
 2.1711268  2.0111215  1.8439815  1.7511351  1.654798   1.5869462
 1.6575875  1.7537464  1.8556674  1.9354814  2.3204806  2.6017706
 2.378958   2.410695   2.70054    3.2644014  3.663291   3.550699
 3.2295334  3.061087   2.6140695  1.9312866  1.5965707  1.9462824
 2.4004695  1.8644046  1.163462   0.8176593  0.91862607 0.91843224
 1.3792838  2.4417565  3.1537766  3.6046321 ]
