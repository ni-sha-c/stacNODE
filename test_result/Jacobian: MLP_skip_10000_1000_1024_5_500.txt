time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.22%, model saved.
Epoch: 0 Train: 32253.15820 Test: 4247.17529
Epoch: 100 Train: 7148.98535 Test: 1512.55054
Epoch 200: New minimal relative error: 77.31%, model saved.
Epoch: 200 Train: 7326.48877 Test: 941.74634
Epoch 300: New minimal relative error: 60.70%, model saved.
Epoch: 300 Train: 6281.80908 Test: 1160.64673
Epoch: 400 Train: 4681.15918 Test: 556.85541
Epoch: 500 Train: 3933.76587 Test: 433.69363
Epoch: 600 Train: 1867.47754 Test: 145.55562
Epoch 700: New minimal relative error: 19.12%, model saved.
Epoch: 700 Train: 512.83148 Test: 30.12083
Epoch 800: New minimal relative error: 18.63%, model saved.
Epoch: 800 Train: 222.10394 Test: 3.84233
Epoch: 900 Train: 175.63124 Test: 8.22467
Epoch 1000: New minimal relative error: 15.38%, model saved.
Epoch: 1000 Train: 199.16264 Test: 30.57755
Epoch: 1100 Train: 112.53268 Test: 2.15125
Epoch 1200: New minimal relative error: 5.04%, model saved.
Epoch: 1200 Train: 103.30696 Test: 6.29250
Epoch: 1300 Train: 121.63893 Test: 10.29140
Epoch: 1400 Train: 153.86343 Test: 20.52523
Epoch: 1500 Train: 69.80737 Test: 0.62489
Epoch: 1600 Train: 64.50869 Test: 0.94493
Epoch 1700: New minimal relative error: 2.41%, model saved.
Epoch: 1700 Train: 56.53973 Test: 0.71311
Epoch: 1800 Train: 54.06081 Test: 1.26882
Epoch: 1900 Train: 61.32147 Test: 2.51578
Epoch: 2000 Train: 71.34554 Test: 8.51092
Epoch: 2100 Train: 44.67069 Test: 1.45791
Epoch: 2200 Train: 42.57533 Test: 1.10991
Epoch: 2300 Train: 45.63839 Test: 1.91072
Epoch: 2400 Train: 50.60894 Test: 5.91929
Epoch: 2500 Train: 48.03795 Test: 3.25524
Epoch: 2600 Train: 49.56446 Test: 5.29856
Epoch: 2700 Train: 40.06205 Test: 0.72355
Epoch: 2800 Train: 33.31254 Test: 1.39512
Epoch: 2900 Train: 48.34129 Test: 3.97236
Epoch: 3000 Train: 34.82927 Test: 0.75943
Epoch: 3100 Train: 43.49295 Test: 1.50529
Epoch: 3200 Train: 37.42672 Test: 3.24864
Epoch: 3300 Train: 29.34347 Test: 1.51241
Epoch: 3400 Train: 59.99912 Test: 9.20996
Epoch: 3500 Train: 38.72726 Test: 4.81846
Epoch: 3600 Train: 33.54350 Test: 1.09237
Epoch: 3700 Train: 28.70036 Test: 2.65338
Epoch: 3800 Train: 28.07845 Test: 1.71332
Epoch: 3900 Train: 36.20121 Test: 4.52230
Epoch: 4000 Train: 27.79159 Test: 1.75138
Epoch: 4100 Train: 32.38391 Test: 4.13240
Epoch: 4200 Train: 55.76936 Test: 7.69367
Epoch: 4300 Train: 26.36711 Test: 1.23565
Epoch: 4400 Train: 24.95823 Test: 0.79851
Epoch: 4500 Train: 27.99612 Test: 3.21125
Epoch: 4600 Train: 22.77394 Test: 0.42462
Epoch: 4700 Train: 29.22882 Test: 2.06176
Epoch: 4800 Train: 19.44877 Test: 0.16590
Epoch: 4900 Train: 19.72688 Test: 0.41385
Epoch: 5000 Train: 19.38388 Test: 0.51320
Epoch: 5100 Train: 19.60926 Test: 0.32320
Epoch: 5200 Train: 17.38537 Test: 0.11844
Epoch: 5300 Train: 18.91813 Test: 0.13245
Epoch: 5400 Train: 19.27433 Test: 0.48579
Epoch: 5500 Train: 42.96922 Test: 7.35268
Epoch: 5600 Train: 21.02889 Test: 1.23297
Epoch: 5700 Train: 17.99648 Test: 0.21057
Epoch: 5800 Train: 17.98735 Test: 0.11287
Epoch: 5900 Train: 16.52388 Test: 0.21475
Epoch: 6000 Train: 16.51937 Test: 0.50290
Epoch: 6100 Train: 15.54684 Test: 0.44125
Epoch: 6200 Train: 16.97704 Test: 0.26805
Epoch: 6300 Train: 16.46768 Test: 0.22517
Epoch: 6400 Train: 15.04563 Test: 0.18608
Epoch: 6500 Train: 15.89390 Test: 0.11347
Epoch: 6600 Train: 17.28490 Test: 1.46797
Epoch: 6700 Train: 16.98124 Test: 0.15790
Epoch: 6800 Train: 15.05037 Test: 0.49132
Epoch: 6900 Train: 13.05498 Test: 0.26193
Epoch 7000: New minimal relative error: 2.35%, model saved.
Epoch: 7000 Train: 12.69078 Test: 0.13994
Epoch: 7100 Train: 16.82407 Test: 0.28413
Epoch: 7200 Train: 13.95598 Test: 0.11710
Epoch: 7300 Train: 15.12835 Test: 0.31449
Epoch: 7400 Train: 15.92117 Test: 0.06196
Epoch 7500: New minimal relative error: 1.42%, model saved.
Epoch: 7500 Train: 13.95243 Test: 0.07212
Epoch: 7600 Train: 13.30748 Test: 0.13514
Epoch: 7700 Train: 13.69960 Test: 0.47849
Epoch: 7800 Train: 12.24542 Test: 0.04274
Epoch: 7900 Train: 14.06158 Test: 0.06493
Epoch: 8000 Train: 12.37417 Test: 0.03957
Epoch: 8100 Train: 18.99553 Test: 2.96257
Epoch: 8200 Train: 12.40222 Test: 0.04703
Epoch: 8300 Train: 16.05367 Test: 0.47500
Epoch: 8400 Train: 12.82007 Test: 0.11157
Epoch: 8500 Train: 12.78191 Test: 0.14147
Epoch: 8600 Train: 15.76557 Test: 1.29229
Epoch: 8700 Train: 14.14013 Test: 0.29739
Epoch: 8800 Train: 12.04156 Test: 0.03916
Epoch: 8900 Train: 12.34398 Test: 0.06674
Epoch: 9000 Train: 12.65853 Test: 0.70686
Epoch: 9100 Train: 11.54727 Test: 0.24239
Epoch: 9200 Train: 12.36672 Test: 0.71335
Epoch: 9300 Train: 11.77755 Test: 0.05129
Epoch: 9400 Train: 11.41298 Test: 0.16857
Epoch: 9500 Train: 11.52460 Test: 0.19820
Epoch: 9600 Train: 10.92026 Test: 0.04796
Epoch: 9700 Train: 12.34660 Test: 0.12428
Epoch: 9800 Train: 11.06762 Test: 0.03907
Epoch: 9900 Train: 11.06582 Test: 0.11535
Epoch: 9999 Train: 11.31067 Test: 0.19366
Training Loss: tensor(11.3107)
Test Loss: tensor(0.1937)
Learned LE: [  0.8146256    0.05954164 -14.541178  ]
True LE: [ 8.61350417e-01  5.21556521e-03 -1.45379305e+01]
Relative Error: [0.6120156  0.76981664 0.7935445  0.64462847 0.6180765  0.6647224
 0.7236889  0.80903405 0.85425127 1.0537515  1.1057239  1.1438854
 1.2147719  1.1682738  1.2388451  1.2693837  1.0830436  0.82386285
 0.68145585 0.60057354 0.6820724  0.82476485 0.91172916 0.8558326
 0.9194295  0.8928292  0.8785394  0.91949946 0.8847336  0.77781355
 0.7849054  0.83302045 0.6931012  0.68376094 0.95756334 1.301258
 1.4201025  1.4591718  1.2228655  0.902179   0.6715038  0.51949716
 0.7040917  1.013214   1.1621476  1.235409   1.2871066  1.1886592
 0.96851915 0.8000935  0.68483    0.6239323  0.51788807 0.41541728
 0.43056905 0.5684204  0.5250152  0.5193821  0.37296847 0.32115436
 0.3389892  0.52110714 0.7059434  0.74616647 0.711757   0.61851645
 0.6392742  0.70837337 0.72245944 0.75211555 0.84274507 1.0515224
 1.056143   1.1390048  1.2586769  1.2934113  1.3829758  1.3205243
 1.0889498  0.7912364  0.68426895 0.68055147 0.76646775 0.91620547
 0.96904075 0.9049417  0.9044162  0.9094     0.8813693  0.9178048
 0.79394925 0.67581433 0.6841545  0.71382064 0.64243615 0.746067
 1.1184777  1.4053528  1.5638325  1.5604689  1.263052   0.9099169
 0.6794759  0.44461972 0.53877574 0.8363085  1.0557389  1.0298188
 1.0183175  0.991364   0.7809087  0.60787016 0.4955219  0.45143935
 0.37949964 0.28692758 0.2630759  0.4519587  0.45145544 0.46033305
 0.49899602 0.47420204 0.48403376 0.62695366 0.69811195 0.6543693
 0.6866445  0.6756171  0.7069337  0.7551679  0.7557184  0.7170768
 0.8481989  1.012579   1.0517828  1.2398344  1.3602414  1.467511
 1.5103858  1.3678875  1.112966   0.8355363  0.75867206 0.80752385
 0.87938374 1.0419656  1.0402116  0.93559986 0.9073817  0.8595609
 0.8645566  0.8133261  0.71733934 0.61918724 0.586997   0.68377095
 0.74198943 0.8860095  1.2633389  1.5217425  1.6945448  1.633199
 1.3489466  1.0729017  0.71459806 0.42805752 0.4309194  0.6986454
 0.9227539  0.8560815  0.80391836 0.83756036 0.68033665 0.50494844
 0.3786819  0.31254432 0.2819831  0.20183139 0.20096816 0.35028672
 0.45303175 0.5349589  0.66312045 0.68698055 0.60784143 0.6418545
 0.6172141  0.6043636  0.6552978  0.72663426 0.762848   0.7792196
 0.74792534 0.70724624 0.8704078  1.0086024  1.1088427  1.289569
 1.4609295  1.5986102  1.5544604  1.3910985  1.173178   0.9602739
 0.8980875  0.9447197  1.022755   1.1488181  1.1077595  1.0170215
 0.92160565 0.87764674 0.8269879  0.7390482  0.6532132  0.5406987
 0.63303524 0.778243   0.86334276 1.0106721  1.3631798  1.6092699
 1.7690448  1.680987   1.450421   1.2310735  0.83248734 0.44894758
 0.3715159  0.52036786 0.8124291  0.7525166  0.6864078  0.7112948
 0.60363144 0.42570835 0.2664379  0.2034674  0.20114851 0.15020262
 0.19182849 0.27831683 0.48502633 0.7116244  0.8272206  0.84007734
 0.7012817  0.6114652  0.6090632  0.6268877  0.6966649  0.77933997
 0.84835637 0.89912295 0.84311    0.8288975  0.98647803 1.0581584
 1.1645391  1.3275886  1.5302118  1.6271806  1.5888389  1.4366128
 1.2281024  1.0755962  1.0394106  1.0716295  1.1142617  1.1774017
 1.1646124  1.1191165  1.0407984  0.95353556 0.79902375 0.6965649
 0.6337404  0.5043108  0.7056388  0.8912073  0.93495715 1.0907671
 1.405149   1.6351587  1.7901258  1.6989483  1.5549778  1.3521699
 0.95951074 0.3661701  0.2784924  0.406253   0.66705346 0.6946344
 0.6088079  0.5878972  0.5280951  0.34077907 0.16139898 0.10558704
 0.13219616 0.11223428 0.20123605 0.29089898 0.45971698 0.77646184
 0.9261484  0.86526966 0.6974293  0.655133   0.7297703  0.723975
 0.83829826 0.97834074 1.0469475  1.0661123  0.97956693 1.0444235
 1.1313928  1.2675694  1.2893938  1.3562882  1.5162921  1.6084208
 1.6014241  1.4696355  1.2867826  1.1856635  1.1680073  1.229976
 1.1756107  1.1915069  1.1831672  1.1410395  1.1981144  1.1125025
 0.8401975  0.70184565 0.6383111  0.56586826 0.7500756  0.96734315
 0.98526704 1.1074026  1.3570076  1.5678482  1.6960584  1.6737664
 1.6094029  1.457926   1.0182335  0.47039124 0.27026677 0.35017407
 0.5341341  0.69058985 0.53412884 0.4732748  0.42639682 0.2796166
 0.09277629 0.02775608 0.05226878 0.08169415 0.24556202 0.33286867
 0.49857745 0.7422722  0.88793886 0.84296167 0.7331051  0.78371483
 0.8450303  0.92439425 1.0512736  1.1644847  1.2299057  1.2203279
 1.1796502  1.1962026  1.2714965  1.4424348  1.4761473  1.5045437
 1.4872683  1.5438546  1.5700532  1.491965   1.3635204  1.2760398
 1.270615   1.3050077  1.2359942  1.1927907  1.1895521  1.1446099
 1.1990671  1.2280968  0.9314883  0.6970213 ]
