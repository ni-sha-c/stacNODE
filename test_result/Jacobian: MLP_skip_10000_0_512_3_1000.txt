time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.19%, model saved.
Epoch: 0 Train: 59555.96484 Test: 4149.19531
Epoch: 100 Train: 15230.90332 Test: 1526.71411
Epoch: 200 Train: 15063.89551 Test: 1361.60046
Epoch 300: New minimal relative error: 98.41%, model saved.
Epoch: 300 Train: 13416.19922 Test: 1186.60095
Epoch 400: New minimal relative error: 62.09%, model saved.
Epoch: 400 Train: 13315.30762 Test: 1372.79541
Epoch: 500 Train: 11175.05762 Test: 859.66876
Epoch: 600 Train: 9530.87793 Test: 737.17401
Epoch: 700 Train: 8094.08154 Test: 515.31232
Epoch: 800 Train: 6315.36768 Test: 301.71851
Epoch: 900 Train: 4059.33765 Test: 141.96521
Epoch 1000: New minimal relative error: 14.39%, model saved.
Epoch: 1000 Train: 1648.46497 Test: 36.82619
Epoch: 1100 Train: 976.87860 Test: 34.61768
Epoch: 1200 Train: 752.70679 Test: 11.43924
Epoch: 1300 Train: 604.33600 Test: 9.41893
Epoch 1400: New minimal relative error: 7.15%, model saved.
Epoch: 1400 Train: 462.96741 Test: 4.07721
Epoch: 1500 Train: 453.80688 Test: 5.50521
Epoch: 1600 Train: 397.39871 Test: 3.35114
Epoch: 1700 Train: 414.87949 Test: 3.82380
Epoch: 1800 Train: 389.25388 Test: 3.73050
Epoch 1900: New minimal relative error: 4.04%, model saved.
Epoch: 1900 Train: 341.46805 Test: 2.75555
Epoch: 2000 Train: 327.12378 Test: 2.77416
Epoch: 2100 Train: 312.23477 Test: 2.75556
Epoch: 2200 Train: 282.54489 Test: 2.38124
Epoch: 2300 Train: 263.01651 Test: 1.88107
Epoch: 2400 Train: 252.11368 Test: 2.45137
Epoch: 2500 Train: 258.05188 Test: 1.69529
Epoch: 2600 Train: 240.73871 Test: 1.61267
Epoch: 2700 Train: 235.00810 Test: 6.63384
Epoch: 2800 Train: 273.37805 Test: 7.40427
Epoch: 2900 Train: 292.81992 Test: 10.69004
Epoch: 3000 Train: 250.69841 Test: 2.72259
Epoch: 3100 Train: 238.53845 Test: 2.88562
Epoch: 3200 Train: 209.00287 Test: 1.47280
Epoch: 3300 Train: 222.94098 Test: 1.46810
Epoch: 3400 Train: 249.76096 Test: 3.00331
Epoch: 3500 Train: 210.83427 Test: 1.26698
Epoch: 3600 Train: 196.90245 Test: 1.31580
Epoch: 3700 Train: 304.87961 Test: 4.21795
Epoch: 3800 Train: 225.55649 Test: 2.76697
Epoch: 3900 Train: 206.49294 Test: 2.10223
Epoch: 4000 Train: 172.07269 Test: 1.28846
Epoch: 4100 Train: 167.62666 Test: 1.16976
Epoch: 4200 Train: 156.64177 Test: 1.26532
Epoch: 4300 Train: 176.15654 Test: 1.27622
Epoch: 4400 Train: 156.50142 Test: 1.02325
Epoch: 4500 Train: 145.48888 Test: 0.63804
Epoch: 4600 Train: 157.65048 Test: 3.32269
Epoch: 4700 Train: 148.86792 Test: 5.90250
Epoch: 4800 Train: 184.86270 Test: 1.68220
Epoch: 4900 Train: 150.18561 Test: 1.30030
Epoch: 5000 Train: 155.67757 Test: 1.36030
Epoch: 5100 Train: 155.95302 Test: 1.09575
Epoch: 5200 Train: 150.46902 Test: 1.84086
Epoch: 5300 Train: 134.72867 Test: 0.67619
Epoch: 5400 Train: 128.09731 Test: 1.09559
Epoch: 5500 Train: 128.07973 Test: 0.39227
Epoch: 5600 Train: 140.18570 Test: 1.71627
Epoch: 5700 Train: 145.22989 Test: 2.73067
Epoch: 5800 Train: 152.58743 Test: 0.87191
Epoch: 5900 Train: 170.88651 Test: 1.15761
Epoch: 6000 Train: 136.78883 Test: 0.65573
Epoch: 6100 Train: 133.52576 Test: 0.47697
Epoch 6200: New minimal relative error: 4.01%, model saved.
Epoch: 6200 Train: 132.59706 Test: 0.50166
Epoch: 6300 Train: 126.36000 Test: 0.71115
Epoch: 6400 Train: 134.05533 Test: 0.73608
Epoch: 6500 Train: 131.09125 Test: 0.79805
Epoch: 6600 Train: 128.00586 Test: 0.83711
Epoch: 6700 Train: 126.21368 Test: 1.85792
Epoch: 6800 Train: 113.36128 Test: 0.40448
Epoch: 6900 Train: 111.32092 Test: 0.32359
Epoch: 7000 Train: 106.71140 Test: 0.46443
Epoch: 7100 Train: 112.94160 Test: 0.91757
Epoch: 7200 Train: 109.31487 Test: 0.69225
Epoch: 7300 Train: 108.46629 Test: 0.36252
Epoch: 7400 Train: 104.48271 Test: 0.34710
Epoch: 7500 Train: 102.55658 Test: 0.41545
Epoch: 7600 Train: 121.23468 Test: 0.91294
Epoch: 7700 Train: 163.55617 Test: 1.19382
Epoch: 7800 Train: 139.72940 Test: 1.13705
Epoch: 7900 Train: 99.70193 Test: 0.32085
Epoch 8000: New minimal relative error: 3.44%, model saved.
Epoch: 8000 Train: 113.38455 Test: 0.82332
Epoch: 8100 Train: 121.80173 Test: 0.69414
Epoch: 8200 Train: 132.30951 Test: 0.78799
Epoch 8300: New minimal relative error: 3.00%, model saved.
Epoch: 8300 Train: 116.02171 Test: 0.70576
Epoch: 8400 Train: 115.22956 Test: 0.64311
Epoch: 8500 Train: 101.79409 Test: 0.41578
Epoch: 8600 Train: 106.65692 Test: 0.41818
Epoch: 8700 Train: 107.27293 Test: 0.48682
Epoch: 8800 Train: 104.28201 Test: 0.47758
Epoch: 8900 Train: 98.73711 Test: 0.45089
Epoch: 9000 Train: 115.34824 Test: 0.76763
Epoch: 9100 Train: 113.59831 Test: 0.86507
Epoch: 9200 Train: 95.34973 Test: 0.37827
Epoch: 9300 Train: 89.67128 Test: 0.31280
Epoch: 9400 Train: 101.25237 Test: 0.71548
Epoch: 9500 Train: 104.38197 Test: 0.63915
Epoch: 9600 Train: 83.48139 Test: 0.37344
Epoch: 9700 Train: 79.94292 Test: 0.28139
Epoch: 9800 Train: 75.55562 Test: 0.27627
Epoch: 9900 Train: 75.31672 Test: 0.28047
Epoch: 9999 Train: 86.00006 Test: 0.43976
Training Loss: tensor(86.0001)
Test Loss: tensor(0.4398)
Learned LE: [ 9.0334779e-01 -6.9031985e-03 -1.4572464e+01]
True LE: [ 8.8197899e-01  2.1636169e-03 -1.4556835e+01]
Relative Error: [ 5.2672243  5.3803797  5.6843824  5.833267   5.8813586  5.8944063
  5.7825584  5.6478434  5.4012012  5.232126   5.2240515  5.3188877
  5.742653   6.1446314  6.114751   5.746761   5.4933333  5.2328253
  5.05125    4.976847   4.9896817  4.9687104  4.8492885  4.628028
  4.3711734  4.1676335  4.2687554  4.76078    5.6354876  6.850698
  8.386867  10.335159  12.134923  13.03847   14.155822  15.438175
 15.98022   15.989818  15.496773  14.780103  14.023636  13.317927
 12.653777  12.221333  11.953859  11.57705   11.295028  11.189775
 11.243526  11.488041  11.999432  11.55324   11.166742  10.770603
 10.114872   9.173152   8.451284   7.474241   6.5641246  5.813351
  5.2083178  4.854967   4.8839374  5.1097665  5.3424735  5.5158405
  5.54854    5.618437   5.5497146  5.4289193  5.1976876  5.0727654
  5.0581207  5.1425943  5.4169617  5.847548   5.996297   5.761286
  5.3734684  5.0308886  4.7951155  4.727925   4.7280526  4.710961
  4.575829   4.348237   4.068391   3.8099527  3.9003685  4.27604
  5.0396757  6.2009377  7.707123   9.605021  11.160814  12.1036415
 13.242502  14.516844  14.607666  14.456203  13.925284  13.267329
 12.704139  12.188972  11.518888  10.972168  10.671876  10.227934
  9.954832   9.873037   9.97719   10.298156  10.789992  10.751275
 10.205761   9.725791   9.403911   8.399788   7.5510345  6.7707276
  5.834784   5.066789   4.4303274  4.313052   4.5324187  4.935582
  5.1326914  5.1647468  5.1799135  5.235414   5.25511    5.168114
  5.0298276  4.8849535  4.849599   4.911771   5.030071   5.4101615
  5.64666    5.564163   5.4030104  4.978747   4.6304736  4.5282927
  4.458861   4.4027395  4.251278   4.0175595  3.773866   3.558541
  3.6201596  3.8549087  4.4441376  5.5250397  6.9781513  8.804422
 10.233028  11.10719   12.156475  13.020669  13.082597  12.894675
 12.405625  11.746742  11.198308  10.729638  10.472915   9.977184
  9.526776   9.057442   8.76518    8.706533   8.858092   9.229882
  9.705106   9.9841175  9.368737   8.877484   8.488944   7.7081337
  6.7933536  6.1442065  5.2334504  4.3846693  4.0031333  3.9521322
  4.2443438  4.609121   4.918373   5.0155916  4.814954   4.7882676
  4.8715477  4.8260336  4.7987485  4.6375985  4.592845   4.630732
  4.7641287  5.1005673  5.279464   5.344489   5.2790318  5.0807137
  4.603156   4.314533   4.176924   4.0877047  3.9242916  3.74725
  3.5231757  3.2475424  3.2501128  3.4067523  3.8721495  4.7560587
  6.1323185  7.8839316  9.389992  10.100373  10.600405  11.529416
 11.7493105 11.575395  11.087765  10.485024   9.858095   9.403458
  9.115024   8.98221    8.611054   8.081868   7.7209616  7.6711783
  7.8610697  8.26929    8.695308   9.067836   8.740895   8.147921
  7.63814    7.229802   6.21268    5.4580045  4.7462573  3.7968655
  3.5001528  3.6038113  3.94874    4.323177   4.607302   4.824844
  4.61525    4.404564   4.3395543  4.3928275  4.4337397  4.3615355
  4.3001275  4.3630223  4.5522084  4.8413453  5.081304   5.150782
  5.217782   5.097711   4.7687163  4.218845   3.9494655  3.799392
  3.5980046  3.4534466  3.260135   2.9623897  2.865375   2.9414918
  3.2251694  3.949289   5.152049   6.8021135  8.5527315  9.021846
  9.217582   9.949781  10.476908  10.383178  10.022247   9.456454
  8.809629   8.282276   7.968598   7.8240113  7.688643   7.3463306
  6.850949   6.7494946  6.9633484  7.416448   7.7810636  8.0730295
  8.251863   7.559882   6.960935   6.5238976  5.7835727  4.9133215
  4.2137375  3.4259088  2.9554524  3.063451   3.5033114  3.9454885
  4.321871   4.490764   4.4402347  4.1798987  3.9520688  3.8940077
  3.961841   3.9724193  3.9612308  4.0289984  4.033469   4.0813956
  4.3244457  4.599228   4.8702846  5.1806912  4.917962   4.5572357
  3.8769085  3.5030503  3.2960613  3.2133167  3.0246117  2.7253513
  2.5267549  2.433621   2.5725763  3.129971   4.117869   5.5937223
  7.4099936  8.003936   8.073466   8.555238   9.195751   9.2715025
  9.169635   8.5600395  8.077156   7.448967   7.027907   6.8383636
  6.737035   6.4858093  6.234664   5.9767675  6.116423   6.612522
  6.984628   7.2214236  7.4710617  7.1776986  6.4931684  5.900512
  5.4787436  4.588393   3.7462246  3.2567573  2.5889192  2.5206895
  2.9541998  3.485036   3.8568761  4.0486436  4.210025   3.9148753
  3.6543999  3.4530115  3.4312124  3.5022874  3.418921   3.2934492
  3.2569296  3.264706   3.3462076  3.4479043  3.585383   3.8314495
  4.370144   4.6740055  4.387073   3.6470149  3.1395276  2.9298024
  2.7967024  2.601518   2.2670488  2.1246564]
