time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.71%, model saved.
Epoch: 0 Train: 9084.31836 Test: 4104.83057
Epoch 100: New minimal relative error: 63.74%, model saved.
Epoch: 100 Train: 2202.01514 Test: 1132.73767
Epoch: 200 Train: 1373.30615 Test: 518.30811
Epoch 300: New minimal relative error: 31.56%, model saved.
Epoch: 300 Train: 668.10962 Test: 238.16960
Epoch: 400 Train: 287.41135 Test: 57.76979
Epoch: 500 Train: 355.26422 Test: 158.07387
Epoch 600: New minimal relative error: 20.40%, model saved.
Epoch: 600 Train: 152.75284 Test: 64.52541
Epoch 700: New minimal relative error: 8.14%, model saved.
Epoch: 700 Train: 67.80228 Test: 7.92579
Epoch: 800 Train: 92.94097 Test: 26.37512
Epoch: 900 Train: 59.15327 Test: 17.51113
Epoch: 1000 Train: 46.66624 Test: 19.62457
Epoch: 1100 Train: 54.41076 Test: 5.88829
Epoch: 1200 Train: 37.94044 Test: 11.85758
Epoch 1300: New minimal relative error: 7.96%, model saved.
Epoch: 1300 Train: 25.02341 Test: 1.59942
Epoch: 1400 Train: 46.55273 Test: 21.81541
Epoch: 1500 Train: 21.37986 Test: 4.27480
Epoch 1600: New minimal relative error: 6.33%, model saved.
Epoch: 1600 Train: 20.30717 Test: 6.62154
Epoch: 1700 Train: 24.63989 Test: 5.57071
Epoch 1800: New minimal relative error: 4.77%, model saved.
Epoch: 1800 Train: 16.52692 Test: 4.71105
Epoch 1900: New minimal relative error: 3.94%, model saved.
Epoch: 1900 Train: 15.48775 Test: 1.14513
Epoch: 2000 Train: 13.84639 Test: 1.22591
Epoch: 2100 Train: 13.56318 Test: 2.40258
Epoch: 2200 Train: 13.04310 Test: 1.05858
Epoch: 2300 Train: 14.21659 Test: 0.59722
Epoch: 2400 Train: 13.74230 Test: 2.21219
Epoch 2500: New minimal relative error: 3.24%, model saved.
Epoch: 2500 Train: 10.65503 Test: 0.16249
Epoch: 2600 Train: 24.83158 Test: 14.35072
Epoch: 2700 Train: 17.23292 Test: 8.81104
Epoch: 2800 Train: 13.40910 Test: 2.61653
Epoch: 2900 Train: 10.69243 Test: 1.19533
Epoch 3000: New minimal relative error: 3.07%, model saved.
Epoch: 3000 Train: 11.74249 Test: 0.72582
Epoch: 3100 Train: 10.18188 Test: 0.52290
Epoch 3200: New minimal relative error: 1.68%, model saved.
Epoch: 3200 Train: 11.67269 Test: 1.00378
Epoch: 3300 Train: 10.88439 Test: 0.25906
Epoch 3400: New minimal relative error: 1.01%, model saved.
Epoch: 3400 Train: 8.41838 Test: 0.11708
Epoch: 3500 Train: 22.83160 Test: 2.58236
Epoch: 3600 Train: 9.73313 Test: 0.28433
Epoch: 3700 Train: 13.10636 Test: 4.43361
Epoch: 3800 Train: 7.86524 Test: 0.46875
Epoch: 3900 Train: 9.19685 Test: 0.31407
Epoch: 4000 Train: 10.46935 Test: 2.09120
Epoch: 4100 Train: 15.11174 Test: 6.72666
Epoch: 4200 Train: 7.87470 Test: 0.68405
Epoch: 4300 Train: 12.53256 Test: 3.22767
Epoch: 4400 Train: 6.96528 Test: 0.10069
Epoch: 4500 Train: 7.25112 Test: 1.64571
Epoch: 4600 Train: 15.38974 Test: 2.05359
Epoch: 4700 Train: 7.83689 Test: 2.13337
Epoch: 4800 Train: 5.77651 Test: 0.44070
Epoch: 4900 Train: 5.13977 Test: 0.16447
Epoch: 5000 Train: 4.99145 Test: 0.06208
Epoch: 5100 Train: 6.93993 Test: 1.52397
Epoch: 5200 Train: 16.01278 Test: 2.66134
Epoch: 5300 Train: 6.09033 Test: 0.39007
Epoch: 5400 Train: 4.53612 Test: 0.14119
Epoch: 5500 Train: 4.47532 Test: 0.06410
Epoch: 5600 Train: 4.18894 Test: 0.14796
Epoch: 5700 Train: 5.99820 Test: 0.64719
Epoch: 5800 Train: 5.01318 Test: 0.47480
Epoch: 5900 Train: 3.78514 Test: 0.03280
Epoch: 6000 Train: 3.71435 Test: 0.05138
Epoch: 6100 Train: 4.96701 Test: 0.76868
Epoch: 6200 Train: 5.39063 Test: 1.25706
Epoch: 6300 Train: 3.63788 Test: 0.30452
Epoch: 6400 Train: 3.58731 Test: 0.03290
Epoch: 6500 Train: 3.27436 Test: 0.03090
Epoch: 6600 Train: 3.40132 Test: 0.11799
Epoch: 6700 Train: 4.12154 Test: 0.50885
Epoch 6800: New minimal relative error: 0.66%, model saved.
Epoch: 6800 Train: 3.11647 Test: 0.02823
Epoch: 6900 Train: 3.46243 Test: 0.42293
Epoch: 7000 Train: 3.23694 Test: 0.13532
Epoch: 7100 Train: 4.11912 Test: 0.30960
Epoch: 7200 Train: 2.95725 Test: 0.03935
Epoch: 7300 Train: 2.95228 Test: 0.04693
Epoch: 7400 Train: 4.51944 Test: 0.13445
Epoch: 7500 Train: 3.44428 Test: 0.68959
Epoch: 7600 Train: 2.81586 Test: 0.02301
Epoch: 7700 Train: 3.09505 Test: 0.42355
Epoch: 7800 Train: 3.60191 Test: 0.61094
Epoch: 7900 Train: 2.82396 Test: 0.03911
Epoch: 8000 Train: 4.20492 Test: 0.72411
Epoch: 8100 Train: 2.92765 Test: 0.15828
Epoch: 8200 Train: 3.33241 Test: 0.58612
Epoch: 8300 Train: 3.50818 Test: 0.92452
Epoch: 8400 Train: 3.98376 Test: 0.86895
Epoch: 8500 Train: 2.73293 Test: 0.10162
Epoch: 8600 Train: 2.52476 Test: 0.01924
Epoch: 8700 Train: 2.60014 Test: 0.08594
Epoch: 8800 Train: 4.19053 Test: 0.54622
Epoch: 8900 Train: 2.73619 Test: 0.35560
Epoch: 9000 Train: 2.79363 Test: 0.19415
Epoch 9100: New minimal relative error: 0.57%, model saved.
Epoch: 9100 Train: 2.56952 Test: 0.02174
Epoch: 9200 Train: 2.38946 Test: 0.03423
Epoch: 9300 Train: 2.63445 Test: 0.14350
Epoch: 9400 Train: 2.82214 Test: 0.51152
Epoch: 9500 Train: 2.47011 Test: 0.21125
Epoch: 9600 Train: 2.14486 Test: 0.03532
Epoch: 9700 Train: 2.23400 Test: 0.07967
Epoch: 9800 Train: 2.13500 Test: 0.01987
Epoch: 9900 Train: 2.32256 Test: 0.19826
Epoch: 9999 Train: 2.34993 Test: 0.02669
Training Loss: tensor(2.3499)
Test Loss: tensor(0.0267)
Learned LE: [  0.8527184    0.01793764 -14.557779  ]
True LE: [ 8.8119853e-01  4.5595951e-03 -1.4555761e+01]
Relative Error: [3.3615935  3.3137348  3.1899862  3.0563226  2.813567   2.8795025
 3.077458   3.349342   3.3215187  3.2658725  3.080843   2.7671072
 2.768368   2.8372016  2.9894662  3.1522121  3.3489811  3.6164424
 3.9417136  4.194919   4.2972856  4.2787013  4.1951427  4.0228972
 3.894604   3.7540185  3.6101568  3.6013298  3.5677314  3.797139
 4.1103144  4.359331   4.9387703  5.6019034  6.052388   5.6985197
 4.787124   3.6816268  2.7640352  2.1849024  1.7201881  1.4666144
 1.6662191  2.2917285  2.8344388  2.8616881  2.6066244  2.4538262
 2.2957165  2.2308218  2.1165686  2.100136   2.1054022  2.1372974
 2.2891178  2.3093886  2.356317   2.5560272  2.9079862  3.152297
 3.41387    3.4560428  3.3008006  3.2529573  3.0649343  2.7055824
 2.4279773  2.438476   2.6034014  2.9294357  2.9316156  2.8486624
 2.6252444  2.3936346  2.3861678  2.4341342  2.5739162  2.7150707
 2.919324   3.1279857  3.3982775  3.6263113  3.7056093  3.7938018
 3.8995543  3.6934364  3.5139496  3.3015563  3.1021123  3.1935654
 3.2074175  3.3351612  3.7353735  4.0184865  4.5179186  5.279435
 5.758145   5.3522716  4.339575   3.1824079  2.3080685  1.7327044
 1.2413136  0.9292533  1.0721722  1.7124323  2.184179   2.4245317
 2.3409731  2.212324   1.9327102  1.9054443  1.8548181  1.889713
 1.9344893  2.0806415  2.2189655  2.2941918  2.3096359  2.4874218
 2.8502347  3.1317186  3.344155   3.2694464  3.2322526  3.257686
 3.0478797  2.448188   2.0938017  2.0516725  2.2868326  2.5400293
 2.6323204  2.4293203  2.1954577  2.071216   2.0550196  2.1167386
 2.245288   2.356163   2.540519   2.727733   2.9238067  3.0368118
 3.1601484  3.3160832  3.4084537  3.422973   3.1845913  2.89087
 2.6785092  2.700475   2.9249418  2.888585   3.297268   3.6581845
 4.1611114  4.9382353  5.382738   4.9295216  3.945652   2.797508
 1.9375238  1.3609425  0.8434913  0.4981362  0.6052432  1.210346
 1.6932322  1.9013654  2.1116595  2.022584   1.7063359  1.6702121
 1.6246917  1.756998   1.7825328  1.9440385  2.039476   2.1815073
 2.213777   2.392577   2.765352   3.1114635  3.1638227  3.0068147
 2.9689934  3.12679    2.8435783  2.261364   1.7938966  1.6455932
 1.8969145  2.1655283  2.3202164  2.1370144  1.8738174  1.7887937
 1.7846438  1.8681597  1.9511085  2.0512981  2.2115405  2.3749313
 2.5089147  2.525903   2.6775794  2.8577104  2.9350226  3.1313593
 2.911311   2.555832   2.27799    2.273694   2.5037096  2.5744393
 2.8898304  3.2924886  3.867807   4.5492697  4.8393993  4.4139466
 3.7040954  2.5918317  1.6865546  1.0849184  0.5696708  0.1836332
 0.26795074 0.7826767  1.2654064  1.4830673  1.668076   1.8197832
 1.5485343  1.4918491  1.4727851  1.5685688  1.6556393  1.6214645
 1.7611835  2.0395558  2.1338549  2.29563    2.598857   3.1064732
 2.9412198  2.839813   2.7662392  2.9108775  2.5463917  2.1308236
 1.6717236  1.4398422  1.5669987  1.8191539  2.015176   1.9068129
 1.6777552  1.5954508  1.5584133  1.5976952  1.6418889  1.7545918
 1.8987818  2.0392182  2.081244   1.9981815  2.093832   2.284692
 2.4416656  2.5964332  2.671192   2.2673478  1.9042068  1.8363224
 2.0411859  2.3361123  2.4618852  2.8191628  3.327493   4.03517
 4.252667   3.9681275  3.4298592  2.5487623  1.5850585  0.94983596
 0.43164846 0.2600467  0.23842323 0.437776   0.88838756 1.1522512
 1.3010389  1.5398811  1.4912939  1.3720033  1.3834728  1.4250998
 1.4341557  1.3157263  1.4486464  1.7303567  2.0365822  2.1684253
 2.4782772  2.8948061  2.7511668  2.683966   2.648794   2.6834352
 2.3707821  1.9703948  1.7012262  1.4368708  1.3855236  1.5654018
 1.7191715  1.7454244  1.5872055  1.5233797  1.3895566  1.3497891
 1.3643252  1.4614091  1.6011676  1.7264704  1.7280273  1.5942361
 1.5836991  1.6961873  1.8730491  2.0502155  2.3626668  2.1887112
 1.7261354  1.4900825  1.5607411  1.7690016  2.0514798  2.358304
 2.7928917  3.4323533  3.7444136  3.4574285  3.0231242  2.535674
 1.7044398  0.9265911  0.41073444 0.3361329  0.3592609  0.2508663
 0.5835235  0.9282523  0.91502106 1.0565847  1.4590751  1.3036543
 1.3215678  1.2963957  1.2126437  0.9598366  0.975727   1.2616597
 1.7559936  1.9470009  2.230769   2.549439   2.6171486  2.4767017
 2.4308937  2.4609332  2.218989   1.83569    1.5589389  1.3929307
 1.4364398  1.385092   1.5664177  1.6455188  1.6101321  1.4042093
 1.2813773  1.1639086  1.0967357  1.2060618  1.356623   1.4568268
 1.4609687  1.2846062  1.2340821  1.162276   1.3080748  1.6336629
 1.8369293  2.1343691  1.7211894  1.4188694 ]
