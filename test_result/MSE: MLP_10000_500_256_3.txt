time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.54%, model saved.
Epoch: 0 Train: 3427.24463 Test: 3928.30200
Epoch 100: New minimal relative error: 93.54%, model saved.
Epoch: 100 Train: 135.69218 Test: 137.98109
Epoch 200: New minimal relative error: 22.60%, model saved.
Epoch: 200 Train: 16.12649 Test: 13.60023
Epoch 300: New minimal relative error: 21.67%, model saved.
Epoch: 300 Train: 6.35095 Test: 5.77505
Epoch: 400 Train: 3.96561 Test: 3.22564
Epoch 500: New minimal relative error: 13.49%, model saved.
Epoch: 500 Train: 3.34390 Test: 3.14181
Epoch: 600 Train: 2.05722 Test: 2.09542
Epoch: 700 Train: 2.04997 Test: 2.00297
Epoch: 800 Train: 6.16661 Test: 5.72203
Epoch: 900 Train: 0.74792 Test: 0.61137
Epoch 1000: New minimal relative error: 11.36%, model saved.
Epoch: 1000 Train: 0.65979 Test: 0.47903
Epoch: 1100 Train: 0.71035 Test: 0.51134
Epoch: 1200 Train: 4.76789 Test: 4.11033
Epoch 1300: New minimal relative error: 7.57%, model saved.
Epoch: 1300 Train: 0.76302 Test: 0.69410
Epoch: 1400 Train: 1.44539 Test: 1.00327
Epoch: 1500 Train: 0.90532 Test: 1.62444
Epoch: 1600 Train: 0.35913 Test: 0.32626
Epoch: 1700 Train: 0.26210 Test: 0.20020
Epoch: 1800 Train: 0.24707 Test: 0.18654
Epoch: 1900 Train: 0.25228 Test: 0.18863
Epoch: 2000 Train: 0.62376 Test: 0.54484
Epoch: 2100 Train: 0.19762 Test: 0.15555
Epoch: 2200 Train: 0.19500 Test: 0.15901
Epoch: 2300 Train: 1.21987 Test: 1.09500
Epoch: 2400 Train: 0.17048 Test: 0.13814
Epoch: 2500 Train: 0.16429 Test: 0.13464
Epoch: 2600 Train: 0.15928 Test: 0.13506
Epoch: 2700 Train: 5.11696 Test: 2.59553
Epoch 2800: New minimal relative error: 4.58%, model saved.
Epoch: 2800 Train: 0.14225 Test: 0.11843
Epoch: 2900 Train: 0.13587 Test: 0.11709
Epoch: 3000 Train: 0.13897 Test: 0.12408
Epoch: 3100 Train: 0.25320 Test: 0.19582
Epoch: 3200 Train: 3.21908 Test: 4.03613
Epoch: 3300 Train: 0.11687 Test: 0.10081
Epoch: 3400 Train: 0.54557 Test: 0.68640
Epoch: 3500 Train: 0.11846 Test: 0.10895
Epoch: 3600 Train: 0.10520 Test: 0.09254
Epoch: 3700 Train: 0.10450 Test: 0.09356
Epoch: 3800 Train: 0.17366 Test: 0.25255
Epoch: 3900 Train: 0.25114 Test: 0.30918
Epoch: 4000 Train: 0.14978 Test: 0.11991
Epoch: 4100 Train: 0.09205 Test: 0.08876
Epoch: 4200 Train: 0.14997 Test: 0.16902
Epoch 4300: New minimal relative error: 3.70%, model saved.
Epoch: 4300 Train: 0.13939 Test: 0.15414
Epoch: 4400 Train: 0.08410 Test: 0.07764
Epoch: 4500 Train: 0.45134 Test: 0.27639
Epoch: 4600 Train: 0.08941 Test: 0.13488
Epoch: 4700 Train: 1.23370 Test: 1.53733
Epoch: 4800 Train: 0.07588 Test: 0.07144
Epoch: 4900 Train: 0.07543 Test: 0.07157
Epoch: 5000 Train: 0.07482 Test: 0.07091
Epoch: 5100 Train: 0.07175 Test: 0.07313
Epoch: 5200 Train: 0.06899 Test: 0.06653
Epoch: 5300 Train: 0.06784 Test: 0.06529
Epoch: 5400 Train: 0.06700 Test: 0.06598
Epoch: 5500 Train: 0.32947 Test: 0.23741
Epoch: 5600 Train: 0.06328 Test: 0.06175
Epoch: 5700 Train: 0.06328 Test: 0.06217
Epoch: 5800 Train: 0.07635 Test: 0.08267
Epoch: 5900 Train: 0.06686 Test: 0.06099
Epoch: 6000 Train: 1.20998 Test: 1.05809
Epoch: 6100 Train: 0.05738 Test: 0.05703
Epoch: 6200 Train: 0.12085 Test: 0.12141
Epoch: 6300 Train: 0.05516 Test: 0.05518
Epoch: 6400 Train: 0.05554 Test: 0.05485
Epoch: 6500 Train: 0.07564 Test: 0.08640
Epoch: 6600 Train: 0.05239 Test: 0.05286
Epoch: 6700 Train: 0.05190 Test: 0.05243
Epoch: 6800 Train: 0.06474 Test: 0.06947
Epoch: 6900 Train: 0.05004 Test: 0.05068
Epoch: 7000 Train: 0.24009 Test: 0.33063
Epoch: 7100 Train: 0.04820 Test: 0.04932
Epoch: 7200 Train: 0.05638 Test: 0.06276
Epoch: 7300 Train: 0.04675 Test: 0.04803
Epoch: 7400 Train: 0.08920 Test: 0.05766
Epoch: 7500 Train: 0.13406 Test: 0.11260
Epoch: 7600 Train: 0.04452 Test: 0.04616
Epoch: 7700 Train: 0.04420 Test: 0.04594
Epoch: 7800 Train: 0.04348 Test: 0.04526
Epoch: 7900 Train: 0.04256 Test: 0.04442
Epoch: 8000 Train: 0.06386 Test: 0.06841
Epoch: 8100 Train: 0.04122 Test: 0.04331
Epoch: 8200 Train: 0.04644 Test: 0.04711
Epoch: 8300 Train: 0.04007 Test: 0.04230
Epoch: 8400 Train: 0.06722 Test: 0.05993
Epoch 8500: New minimal relative error: 3.32%, model saved.
Epoch: 8500 Train: 0.03896 Test: 0.04131
Epoch: 8600 Train: 0.04500 Test: 0.04876
Epoch: 8700 Train: 0.03795 Test: 0.04039
Epoch: 8800 Train: 0.05545 Test: 0.06734
Epoch: 8900 Train: 0.03709 Test: 0.03963
Epoch: 9000 Train: 0.03648 Test: 0.03906
Epoch: 9100 Train: 0.03619 Test: 0.03879
Epoch: 9200 Train: 0.03634 Test: 0.03858
Epoch: 9300 Train: 0.03510 Test: 0.03781
Epoch: 9400 Train: 0.03504 Test: 0.03742
Epoch: 9500 Train: 0.03424 Test: 0.03700
Epoch: 9600 Train: 0.03728 Test: 0.04509
Epoch: 9700 Train: 0.03346 Test: 0.03625
Epoch: 9800 Train: 0.03399 Test: 0.03651
Epoch: 9900 Train: 0.10048 Test: 0.06106
Epoch: 9999 Train: 0.03228 Test: 0.03513
Training Loss: tensor(0.0323)
Test Loss: tensor(0.0351)
Learned LE: [ 0.9016366  -0.00745271 -5.9143953 ]
True LE: [ 8.6616755e-01 -4.6568661e-04 -1.4542551e+01]
Relative Error: [0.57662994 0.8597236  0.70367086 0.38046545 0.631669   0.57894385
 0.35890624 0.4997766  0.45449996 0.32371745 0.45904523 0.42865956
 0.34123668 0.53230846 0.59391093 0.3666198  0.2959107  0.52528137
 0.9135221  0.87114304 0.6859166  0.77128124 1.0650698  1.1975839
 1.1683246  0.97032154 0.5900065  0.36546183 0.4149015  0.37249997
 0.345844   0.51868594 0.8538604  0.57137555 0.6004555  0.6563515
 0.3596965  0.3835916  0.44918904 0.19865744 0.30127117 0.4962762
 0.32329693 0.27720526 0.2176034  0.45278352 0.26652846 0.17498726
 0.29010186 0.5034069  0.6788511  0.50065076 0.2183521  0.23951043
 0.21656084 0.26480138 0.2319461  0.21647173 0.35669428 0.44699842
 0.36029768 0.25568584 0.37571707 0.7365456  0.96429324 0.71225137
 0.3977626  0.5942269  0.5679883  0.28276917 0.45054922 0.5228689
 0.45003915 0.6106135  0.57244456 0.35267296 0.53605247 0.70179564
 0.4825187  0.2589507  0.36049664 0.89313334 1.0986831  1.0556453
 0.91384155 0.98718226 0.9983305  0.9057721  0.75461704 0.5586962
 0.6932393  0.8882319  0.6799025  0.4357094  0.36861855 0.59314996
 0.49070022 0.4754539  0.60395074 0.45806175 0.4076365  0.41275364
 0.1680498  0.19528536 0.4046941  0.24712689 0.25815085 0.19067582
 0.5903751  0.53520894 0.2056438  0.04118791 0.18539429 0.29823983
 0.47017425 0.34851873 0.23958492 0.21214543 0.27328914 0.21113455
 0.265288   0.36765343 0.49457467 0.50861377 0.3332094  0.3644429
 0.6385015  0.8091355  0.53234756 0.25425428 0.48932657 0.8130758
 0.49952453 0.4348054  0.38396588 0.32859203 0.4301019  0.747102
 0.6049969  0.29382    0.5627711  0.5784264  0.46587446 0.36440974
 0.6570342  1.0927688  1.2205664  0.90941226 0.49639344 0.44111982
 0.38169163 0.2664993  0.07607712 0.09598619 0.35986668 0.73769253
 0.9058713  0.620602   0.33718023 0.5110908  0.17193447 0.45043892
 0.44186538 0.243424   0.43678004 0.20533097 0.19300713 0.24537167
 0.29515672 0.19441552 0.19251104 0.36802188 0.78503424 0.44506142
 0.2108371  0.36408502 0.4610397  0.25374746 0.4496326  0.36341873
 0.14172857 0.1902982  0.18635212 0.2243069  0.30683574 0.34239164
 0.38577676 0.38011736 0.34234834 0.34988204 0.50055546 0.3583489
 0.15559301 0.25382334 0.6248274  0.7547349  0.52969444 0.58885545
 0.49912855 0.42668638 0.08032735 0.5411441  0.59595335 0.27024835
 0.2578246  0.3826413  0.48373666 0.6335983  0.8032063  0.90395695
 0.88075083 0.4130627  0.08722147 0.16173756 0.18280424 0.09958419
 0.16429697 0.1081012  0.13955489 0.25015005 0.37994912 0.7788523
 0.4848433  0.22267547 0.21265589 0.4230293  0.3161697  0.16038953
 0.4137275  0.32263106 0.35418585 0.27926335 0.34442282 0.2708242
 0.13785355 0.34665805 0.69272345 0.41373363 0.33067548 0.61016536
 0.711393   0.4246425  0.44604832 0.4877536  0.19206892 0.08146346
 0.12345341 0.17691582 0.26415214 0.2667906  0.29116407 0.26596877
 0.40578625 0.35889858 0.32863304 0.1599328  0.2622363  0.3769312
 0.5412843  0.7061162  0.46683127 0.52384675 0.5979975  0.6471014
 0.58220017 0.35674912 0.44807062 0.3270178  0.39473158 0.4608498
 0.38300788 0.6382205  0.8167402  0.58930886 0.4119628  0.2800875
 0.12609938 0.11113982 0.33569968 0.4043869  0.2505946  0.2763409
 0.3586193  0.2845047  0.22158879 0.36154234 0.36607867 0.49229646
 0.4027082  0.15568785 0.4065278  0.07825374 0.15984835 0.3730023
 0.45297927 0.35463694 0.43658423 0.4156688  0.04585515 0.21312304
 0.39390153 0.3891554  0.35851783 0.5547245  0.6617933  0.45179102
 0.222293   0.4080066  0.5049164  0.24278869 0.27879733 0.23468198
 0.24459155 0.5158878  0.5360695  0.31717715 0.1725426  0.23173538
 0.27968612 0.21978508 0.3449205  0.3955341  0.46288967 0.62602234
 0.6664999  0.61803544 0.51888555 0.47750333 0.55771387 0.63539284
 0.49195668 0.46992093 0.24555123 0.39183214 0.59001    0.4286285
 0.5335302  0.60381705 0.3928745  0.53855956 0.43526962 0.44625932
 0.32323062 0.32868806 0.3011799  0.17530788 0.32785252 0.42698613
 0.47154245 0.44768253 0.32495418 0.29945397 0.21284164 0.5327264
 0.2632047  0.284084   0.06683709 0.18023598 0.13539249 0.20236902
 0.27180925 0.28478727 0.2790167  0.12538077 0.15942349 0.19509998
 0.41659904 0.43787855 0.3681953  0.35752213 0.35154197 0.22902223
 0.28359395 0.5714902  0.28451476 0.31759846 0.2506826  0.21195027
 0.54846925 0.50418663 0.4714909  0.37156904 0.24227078 0.1604746
 0.29306325 0.5062577  0.56841207 0.50346696 0.44646055 0.566839
 0.504265   0.63528943 0.5484082  0.33898154 0.2712736  0.37724864
 0.34610644 0.2906045  0.19384047 0.3334744 ]
