time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.73%, model saved.
Epoch: 0 Train: 32769.50781 Test: 4130.89355
Epoch: 100 Train: 8195.61426 Test: 1277.97961
Epoch: 200 Train: 6875.00879 Test: 1199.59448
Epoch: 300 Train: 6382.09131 Test: 910.95850
Epoch: 400 Train: 4566.56445 Test: 565.02417
Epoch: 500 Train: 2853.32104 Test: 212.58188
Epoch 600: New minimal relative error: 51.45%, model saved.
Epoch: 600 Train: 2084.23047 Test: 190.64703
Epoch: 700 Train: 1051.14832 Test: 163.70256
Epoch: 800 Train: 426.10199 Test: 33.99560
Epoch 900: New minimal relative error: 35.58%, model saved.
Epoch: 900 Train: 291.12598 Test: 5.56456
Epoch: 1000 Train: 498.93399 Test: 82.36272
Epoch 1100: New minimal relative error: 15.74%, model saved.
Epoch: 1100 Train: 194.45831 Test: 2.71344
Epoch 1200: New minimal relative error: 9.34%, model saved.
Epoch: 1200 Train: 175.14317 Test: 2.38347
Epoch: 1300 Train: 287.99725 Test: 17.80169
Epoch 1400: New minimal relative error: 6.46%, model saved.
Epoch: 1400 Train: 135.31665 Test: 1.46550
Epoch: 1500 Train: 132.69293 Test: 1.41222
Epoch 1600: New minimal relative error: 6.09%, model saved.
Epoch: 1600 Train: 133.66388 Test: 2.22892
Epoch: 1700 Train: 132.90244 Test: 1.45867
Epoch: 1800 Train: 125.33502 Test: 21.57759
Epoch: 1900 Train: 178.83241 Test: 6.01664
Epoch: 2000 Train: 98.45197 Test: 0.88127
Epoch: 2100 Train: 95.48854 Test: 4.19728
Epoch: 2200 Train: 86.64471 Test: 0.96030
Epoch: 2300 Train: 155.65582 Test: 3.72032
Epoch 2400: New minimal relative error: 5.64%, model saved.
Epoch: 2400 Train: 74.31915 Test: 0.53797
Epoch 2500: New minimal relative error: 4.88%, model saved.
Epoch: 2500 Train: 75.89043 Test: 0.73083
Epoch: 2600 Train: 83.27299 Test: 11.90857
Epoch: 2700 Train: 91.83174 Test: 1.49043
Epoch: 2800 Train: 64.06909 Test: 0.43557
Epoch: 2900 Train: 68.31718 Test: 0.67996
Epoch: 3000 Train: 61.19540 Test: 0.78941
Epoch: 3100 Train: 56.91234 Test: 0.84615
Epoch: 3200 Train: 58.02628 Test: 0.58442
Epoch: 3300 Train: 58.37705 Test: 3.51414
Epoch: 3400 Train: 54.69712 Test: 0.69271
Epoch: 3500 Train: 53.69287 Test: 0.35415
Epoch: 3600 Train: 49.15205 Test: 0.37302
Epoch: 3700 Train: 48.92342 Test: 0.30030
Epoch: 3800 Train: 46.46343 Test: 0.43147
Epoch 3900: New minimal relative error: 2.80%, model saved.
Epoch: 3900 Train: 46.20341 Test: 0.27535
Epoch: 4000 Train: 63.45911 Test: 1.27103
Epoch 4100: New minimal relative error: 2.09%, model saved.
Epoch: 4100 Train: 41.04580 Test: 0.17347
Epoch: 4200 Train: 49.97842 Test: 0.82055
Epoch: 4300 Train: 37.11312 Test: 0.14026
Epoch: 4400 Train: 37.63816 Test: 0.20709
Epoch: 4500 Train: 40.26998 Test: 0.52259
Epoch: 4600 Train: 35.06150 Test: 0.19714
Epoch: 4700 Train: 39.69009 Test: 2.20763
Epoch: 4800 Train: 37.63044 Test: 1.16421
Epoch: 4900 Train: 43.60498 Test: 0.82254
Epoch: 5000 Train: 33.10400 Test: 0.19571
Epoch: 5100 Train: 57.44178 Test: 0.52355
Epoch: 5200 Train: 30.04883 Test: 0.12300
Epoch: 5300 Train: 28.10980 Test: 0.09811
Epoch: 5400 Train: 27.18923 Test: 0.08905
Epoch: 5500 Train: 26.45507 Test: 0.17893
Epoch: 5600 Train: 25.87483 Test: 0.15705
Epoch: 5700 Train: 30.15228 Test: 6.25621
Epoch: 5800 Train: 24.23992 Test: 0.08014
Epoch: 5900 Train: 24.36275 Test: 0.77139
Epoch: 6000 Train: 23.00741 Test: 0.06951
Epoch: 6100 Train: 22.40898 Test: 0.08537
Epoch: 6200 Train: 23.20283 Test: 0.07211
Epoch: 6300 Train: 21.07617 Test: 0.12497
Epoch: 6400 Train: 25.94757 Test: 4.67366
Epoch: 6500 Train: 20.19420 Test: 0.05385
Epoch: 6600 Train: 20.12152 Test: 0.05175
Epoch: 6700 Train: 19.96912 Test: 0.09945
Epoch: 6800 Train: 20.63188 Test: 0.12554
Epoch: 6900 Train: 18.53490 Test: 0.04271
Epoch: 7000 Train: 18.84766 Test: 0.05289
Epoch: 7100 Train: 19.20738 Test: 0.07461
Epoch: 7200 Train: 18.89083 Test: 0.09118
Epoch: 7300 Train: 18.65451 Test: 0.05078
Epoch: 7400 Train: 18.28923 Test: 0.16127
Epoch: 7500 Train: 17.74820 Test: 0.04061
Epoch: 7600 Train: 17.81132 Test: 0.04372
Epoch: 7700 Train: 17.83863 Test: 0.05517
Epoch: 7800 Train: 17.38068 Test: 0.05586
Epoch: 7900 Train: 17.49032 Test: 0.05251
Epoch: 8000 Train: 17.59488 Test: 0.13281
Epoch: 8100 Train: 18.07689 Test: 1.24752
Epoch: 8200 Train: 16.74370 Test: 0.04205
Epoch: 8300 Train: 16.53852 Test: 0.04647
Epoch: 8400 Train: 16.75386 Test: 0.05936
Epoch: 8500 Train: 17.11717 Test: 0.11499
Epoch: 8600 Train: 16.36454 Test: 0.04364
Epoch: 8700 Train: 15.56093 Test: 0.08340
Epoch: 8800 Train: 15.35441 Test: 0.25479
Epoch: 8900 Train: 15.79766 Test: 0.44422
Epoch: 9000 Train: 15.56273 Test: 0.05804
Epoch: 9100 Train: 17.05822 Test: 1.21367
Epoch: 9200 Train: 15.31969 Test: 0.04300
Epoch: 9300 Train: 16.13319 Test: 0.05153
Epoch: 9400 Train: 15.65231 Test: 0.08999
Epoch: 9500 Train: 17.69840 Test: 1.58931
Epoch: 9600 Train: 15.91286 Test: 0.83368
Epoch: 9700 Train: 14.75514 Test: 0.03845
Epoch: 9800 Train: 14.78556 Test: 0.04923
Epoch: 9900 Train: 14.17283 Test: 0.04069
Epoch: 9999 Train: 13.95230 Test: 0.03594
Training Loss: tensor(13.9523)
Test Loss: tensor(0.0359)
Learned LE: [  0.865123     0.01872967 -14.551799  ]
True LE: [ 8.7465066e-01  4.0821671e-03 -1.4560904e+01]
Relative Error: [5.3204     5.2335877  5.068965   4.634104   4.228004   4.269107
 3.9732437  3.6154964  3.1916294  2.8647919  2.7801156  2.7257862
 2.6856306  2.722278   2.5444894  2.5445745  2.4851182  2.365192
 2.5053105  2.8773131  3.004858   3.081706   2.9529076  3.1360586
 3.2259982  3.6874917  3.8384702  3.9763837  4.354992   4.535524
 4.706409   4.8400393  5.0056853  4.9243274  5.308023   5.8460975
 6.258735   6.713985   6.684844   6.4781322  6.1573052  5.618536
 5.165915   4.58507    4.2274528  4.237524   4.17456    4.1087575
 4.1759186  4.0855894  3.8529818  3.70726    3.5135913  3.5202065
 3.8271837  3.8469489  3.7229679  3.6413972  3.981914   4.114488
 4.3002715  4.5712433  4.9576225  4.8679953  4.8279185  4.4353204
 3.9920342  3.909291   3.6926966  3.3032508  2.9446516  2.7020729
 2.5711417  2.371866   2.3697321  2.3830228  2.3104396  2.1401923
 2.237106   2.11365    2.1094198  2.549351   2.8299222  2.983108
 2.915811   3.1597261  3.4105353  3.6511328  3.8584013  3.7892148
 3.964993   4.318973   4.5086937  4.5756493  4.7145085  4.711448
 4.8562965  5.4786997  5.951128   6.4652524  6.543316   6.3319364
 5.9325047  5.437829   4.7938824  4.101318   3.5517006  3.6404178
 3.656576   3.4847035  3.5483067  3.484545   3.3114593  3.1383605
 2.89107    3.0177066  3.3783092  3.3130255  3.1684973  3.143555
 3.4094982  3.5315025  3.5295439  3.7378106  4.048742   4.4020734
 4.4748745  4.3334475  3.8566883  3.5445542  3.3438363  3.0226967
 2.6115012  2.4146569  2.401357   2.0192952  2.0911906  2.1042552
 1.9994471  1.9517547  1.9428637  1.9288242  1.7082566  2.1733706
 2.5934684  2.8457103  2.8435166  3.1638849  3.5152256  3.5339289
 3.7972445  3.7946954  3.6184742  3.941658   4.2362733  4.3056855
 4.359947   4.4391766  4.590326   4.9939203  5.606436   6.17775
 6.4017057  6.301874   5.824994   5.3443     4.598541   3.7861018
 3.075164   3.1289194  3.1733632  3.0462542  3.0689018  3.1220663
 3.0093024  2.8433864  2.6027493  2.8034914  3.0413718  2.9442434
 2.6578925  2.552744   2.7538097  2.9202287  2.8973038  2.993237
 3.1684558  3.5790749  3.8589914  4.0387077  3.8144827  3.3152275
 3.0308583  2.7084951  2.2956383  2.0281303  2.0689595  1.8256654
 1.7100934  1.7769959  1.8259767  1.7273443  1.7780015  1.7556713
 1.5813586  1.6458027  2.1799405  2.6060169  2.7298005  3.046169
 3.5261056  3.6467428  3.5807583  3.7846475  3.684013   3.4404273
 3.7512996  3.9530656  3.93757    4.1572347  4.2308383  4.635897
 5.1083117  5.734876   6.197189   6.238727   5.9097247  5.3349414
 4.6219707  3.715696   2.9223988  2.6430917  2.7622209  2.7202225
 2.7574458  2.8060148  2.7150018  2.5883532  2.421687   2.6438124
 2.802838   2.6564274  2.381608   2.233391   2.1795454  2.2120948
 2.3503947  2.2801423  2.3556368  2.6438744  3.0768983  3.3866751
 3.5657692  3.4191222  2.9071352  2.4441955  2.006383   1.6849068
 1.6450152  1.4590595  1.4348629  1.2983018  1.5098673  1.7316196
 1.7892443  1.7548337  1.4814565  1.2323294  1.6375996  2.0630987
 2.202275   2.4815376  3.3058047  3.6908581  3.5462537  3.537015
 3.6506977  3.4149184  3.1747327  3.3760118  3.7314086  3.8098867
 3.9919603  4.19482    4.623014   4.9654284  5.602348   5.787606
 5.6950946  5.2336364  4.619588   3.841295   2.9949791  2.4453044
 2.3596492  2.464054   2.371821   2.4513657  2.377124   2.3213887
 2.2566032  2.4217484  2.6286077  2.4671779  2.14021    2.010903
 2.039702   1.6670082  1.546131   1.7760061  1.8172542  1.8318828
 2.0978804  2.6145892  2.9959686  3.1864889  3.033946   2.7344997
 2.188125   1.6791508  1.3154205  1.0876607  0.95641595 0.99580485
 1.0563542  1.6312696  1.985293   1.8066224  1.6330206  1.3134111
 1.0959255  1.4990226  1.8185016  1.932135   2.3823268  3.0521722
 3.5656464  3.4415078  3.411518   3.3378377  3.0677192  2.9126356
 3.4519806  3.896004   3.9962714  4.1436663  4.321401   4.423754
 4.703238   5.214007   5.33962    4.997587   4.5287294  3.9176068
 3.251871   2.6512525  2.080628   2.1779966  2.153756   1.9720204
 1.9954367  1.9982569  2.011659   1.9921373  2.220031   2.1477358
 1.9052137  1.7298387  1.7262645  1.5023692  1.2895137  1.0636762
 1.1898013  1.435501   1.4957923  1.8237504  2.3244452  2.781351
 2.8491063  2.728082   2.5279295  2.1867764  1.6757814  1.2685691
 0.9448848  0.5036089  0.6867809  1.1530328  1.7515752  1.9586024
 1.8611041  1.42842    0.99616534 0.8351569  1.0768129  1.3059711
 1.4930568  2.014984   2.6200755  3.0072412 ]
