time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.69%, model saved.
Epoch: 0 Train: 172530.45312 Test: 4132.57568
Epoch: 80 Train: 51579.05859 Test: 2271.18970
Epoch: 160 Train: 46737.86328 Test: 1728.38928
Epoch 240: New minimal relative error: 78.85%, model saved.
Epoch: 240 Train: 42530.55859 Test: 1382.05994
Epoch 320: New minimal relative error: 73.45%, model saved.
Epoch: 320 Train: 43978.32031 Test: 1479.79016
Epoch: 400 Train: 49881.71484 Test: 1658.73279
Epoch 480: New minimal relative error: 71.11%, model saved.
Epoch: 480 Train: 40438.30859 Test: 1477.93909
Epoch: 560 Train: 46500.77344 Test: 1980.72229
Epoch: 640 Train: 45778.18359 Test: 1486.30750
Epoch: 720 Train: 44277.13672 Test: 1623.15295
Epoch: 800 Train: 43492.62891 Test: 2383.22754
Epoch 880: New minimal relative error: 64.14%, model saved.
Epoch: 880 Train: 45666.69922 Test: 1508.18359
Epoch: 960 Train: 41983.56250 Test: 1430.18994
Epoch 1040: New minimal relative error: 62.37%, model saved.
Epoch: 1040 Train: 44384.53906 Test: 1488.60876
Epoch: 1120 Train: 44771.73047 Test: 1896.91443
Epoch: 1200 Train: 44026.49609 Test: 1535.10632
Epoch: 1280 Train: 45979.14844 Test: 1574.05103
Epoch 1360: New minimal relative error: 57.51%, model saved.
Epoch: 1360 Train: 42814.27344 Test: 1496.44141
Epoch: 1440 Train: 46826.37109 Test: 1561.17529
Epoch: 1520 Train: 43070.02344 Test: 1492.58777
Epoch: 1600 Train: 45554.28516 Test: 1574.43909
Epoch: 1680 Train: 44897.16016 Test: 1567.53552
Epoch: 1760 Train: 44663.82031 Test: 1526.52991
Epoch: 1840 Train: 46598.43750 Test: 1589.79980
Epoch: 1920 Train: 44307.83203 Test: 1555.49646
Epoch: 2000 Train: 45119.40625 Test: 1551.28296
Epoch: 2080 Train: 43431.83203 Test: 1573.77185
Epoch: 2160 Train: 42504.50781 Test: 1513.39307
Epoch 2240: New minimal relative error: 56.71%, model saved.
Epoch: 2240 Train: 44419.53906 Test: 1554.67432
Epoch: 2320 Train: 42260.77344 Test: 1489.75269
Epoch: 2400 Train: 41706.33984 Test: 1500.47009
Epoch: 2480 Train: 41860.48438 Test: 1488.61951
Epoch: 2560 Train: 37251.50000 Test: 1309.30273
Epoch: 2640 Train: 42222.29688 Test: 1472.97412
Epoch 2720: New minimal relative error: 50.25%, model saved.
Epoch: 2720 Train: 37690.31250 Test: 1344.24280
Epoch: 2800 Train: 41331.30078 Test: 1410.82202
Epoch: 2880 Train: 37942.61328 Test: 1302.61780
Epoch: 2960 Train: 38374.80859 Test: 1228.15015
Epoch: 3040 Train: 38215.17188 Test: 1300.98877
Epoch: 3120 Train: 34571.19531 Test: 1173.00195
Epoch: 3200 Train: 36273.08984 Test: 1186.62292
Epoch: 3280 Train: 35678.14844 Test: 1170.02771
Epoch: 3360 Train: 35073.86719 Test: 1090.18665
Epoch: 3440 Train: 34700.01172 Test: 1005.53156
Epoch: 3520 Train: 30807.53125 Test: 954.43347
Epoch: 3600 Train: 30939.61133 Test: 896.19061
Epoch: 3680 Train: 31768.31055 Test: 894.92401
Epoch: 3760 Train: 29735.13086 Test: 811.60315
Epoch: 3840 Train: 29633.83398 Test: 816.33563
Epoch: 3920 Train: 29399.85547 Test: 758.65179
Epoch: 4000 Train: 28731.27148 Test: 783.62311
Epoch: 4080 Train: 28324.49414 Test: 692.51605
Epoch: 4160 Train: 27643.06836 Test: 739.33301
Epoch: 4240 Train: 26056.59766 Test: 610.18188
Epoch: 4320 Train: 24639.25000 Test: 577.47937
Epoch: 4400 Train: 24324.17773 Test: 489.96732
Epoch: 4480 Train: 23897.41797 Test: 489.92764
Epoch: 4560 Train: 24468.08594 Test: 510.09033
Epoch: 4640 Train: 23111.58008 Test: 471.48590
Epoch: 4720 Train: 20989.66406 Test: 401.61365
Epoch: 4800 Train: 19706.67969 Test: 343.45755
Epoch: 4880 Train: 18376.68359 Test: 278.11993
Epoch: 4960 Train: 16811.42578 Test: 221.50977
Epoch: 5040 Train: 16320.87500 Test: 222.51320
Epoch: 5120 Train: 17219.12891 Test: 238.74007
Epoch: 5200 Train: 16068.02539 Test: 215.53876
Epoch: 5280 Train: 10893.85449 Test: 131.19589
Epoch: 5360 Train: 6970.87305 Test: 87.61207
Epoch: 5440 Train: 4957.76807 Test: 64.69054
Epoch 5520: New minimal relative error: 14.57%, model saved.
Epoch: 5520 Train: 3676.37476 Test: 32.21504
Epoch: 5600 Train: 3161.56177 Test: 29.57495
Epoch 5680: New minimal relative error: 10.32%, model saved.
Epoch: 5680 Train: 2544.39648 Test: 14.11056
Epoch 5760: New minimal relative error: 10.22%, model saved.
Epoch: 5760 Train: 2138.16455 Test: 10.70356
Epoch: 5840 Train: 1865.64050 Test: 8.05201
Epoch: 5920 Train: 1598.63477 Test: 5.65696
Epoch: 6000 Train: 1389.95898 Test: 4.07223
Epoch 6080: New minimal relative error: 10.09%, model saved.
Epoch: 6080 Train: 1220.64343 Test: 3.66285
Epoch: 6160 Train: 1117.28662 Test: 3.24022
Epoch 6240: New minimal relative error: 6.47%, model saved.
Epoch: 6240 Train: 1054.68787 Test: 2.86652
Epoch: 6320 Train: 1025.83630 Test: 4.14924
Epoch: 6400 Train: 1083.15759 Test: 4.28283
Epoch: 6480 Train: 1019.13538 Test: 4.60132
Epoch: 6560 Train: 1069.09399 Test: 4.05351
Epoch: 6640 Train: 981.42944 Test: 8.31085
Epoch: 6720 Train: 983.08594 Test: 3.24094
Epoch: 6800 Train: 887.09534 Test: 3.07974
Epoch: 6880 Train: 825.29205 Test: 2.96793
Epoch: 6960 Train: 870.34821 Test: 2.65572
Epoch: 7040 Train: 821.89673 Test: 2.68383
Epoch: 7120 Train: 922.22156 Test: 3.35602
Epoch: 7200 Train: 1019.12225 Test: 8.90425
Epoch: 7280 Train: 1050.66626 Test: 6.06153
Epoch: 7360 Train: 972.11511 Test: 3.80789
Epoch: 7440 Train: 1105.79065 Test: 5.04522
Epoch: 7520 Train: 876.76276 Test: 2.59280
Epoch: 7600 Train: 741.51764 Test: 1.56956
Epoch: 7680 Train: 673.58301 Test: 1.22015
Epoch: 7760 Train: 703.66180 Test: 1.40823
Epoch: 7840 Train: 703.52643 Test: 1.51704
Epoch: 7920 Train: 692.99719 Test: 1.43700
Epoch: 7999 Train: 676.66156 Test: 1.29157
Training Loss: tensor(676.6616)
Test Loss: tensor(1.2916)
Learned LE: [  0.9766725   -0.13100345 -14.499232  ]
True LE: [ 8.4091556e-01  3.2651401e-03 -1.4515334e+01]
Relative Error: [12.151909  12.488598  13.0099    13.690932  14.082978  14.315338
 14.152804  13.847648  13.502592  13.3531885 13.103718  12.685114
 12.474727  12.4025955 12.400447  12.582597  12.935416  13.433499
 13.927939  14.411263  14.307547  14.340875  14.711584  14.909058
 15.097146  14.861783  14.53587   13.803804  12.919949  11.93045
 11.158703  10.862343  10.180315   8.949232   8.142969   7.738649
  7.5602837  7.7840633  7.546465   6.9496713  6.2798986  5.616206
  5.198977   4.2516685  3.4925728  3.0811176  2.8904433  2.911322
  2.998722   3.4498122  3.9274545  4.5537133  5.2596207  6.072117
  7.0865035  8.183797   9.304299   9.888668  10.079305  10.277557
 10.498751  10.811672  11.137121  11.361542  11.76408   12.449358
 12.7361355 13.118902  13.155012  12.844661  12.405789  12.262346
 12.097965  11.683946  11.438124  11.389553  11.424778  11.638694
 12.026178  12.479709  12.756224  13.081604  12.708836  12.60198
 12.935242  13.429443  13.714302  13.863296  13.588812  12.904642
 11.943178  10.959362  10.178782  10.032628   9.297046   7.9850726
  7.1278243  6.8157663  6.8062363  7.0416927  7.0750327  6.6022224
  5.8605123  5.154006   4.779311   3.6967204  2.869048   2.430591
  2.2703483  2.309038   2.397083   2.7414927  3.2949264  3.9012406
  4.6141376  5.348183   6.177536   7.1170316  8.148527   9.158927
  9.40073    9.573529   9.765273  10.016286  10.262858  10.378711
 10.582128  11.14529   11.396794  11.769491  12.260337  11.854252
 11.393237  11.198938  11.186953  10.757308  10.319874  10.074755
 10.066317  10.315068  10.779081  11.25782   11.492764  11.685007
 11.252084  10.9853115 11.166617  11.672439  12.102301  12.631456
 12.60559   12.184938  11.094914  10.020861   9.353421   9.165895
  8.534106   7.157339   6.211654   5.896319   6.11556    6.263644
  6.4634995  6.2701573  5.472065   4.85396    4.515979   3.4095373
  2.6158252  2.1052608  1.8777553  1.8528031  1.9189326  2.1595805
  2.6107156  3.317811   3.8806546  4.5246196  5.166919   5.9826636
  6.893171   8.005097   8.45587    8.639954   8.963339   9.191467
  9.448636   9.541396   9.622047   9.97197   10.108737  10.350551
 10.978783  10.940288  10.48933   10.269347  10.344319   9.774911
  9.097401   8.767019   8.683843   8.798565   9.062636   9.3882
  9.883353  10.262051   9.613319   9.269406   9.232178   9.669767
 10.308421  10.803715  11.315272  11.349037  10.333621   9.23207
  8.704664   8.433818   7.922852   6.567324   5.491906   5.1128073
  5.188457   5.519517   5.7140255  5.7709193  5.291255   4.7315407
  4.343506   3.2218382  2.4291782  1.9709687  1.675428   1.6297562
  1.6416168  1.7208003  2.1200333  2.483712   3.12046    3.758624
  4.38319    5.043958   5.821461   6.84833    7.7543745  7.7871428
  7.950566   8.241924   8.636492   8.685129   8.758238   8.909268
  9.0344     9.19216    9.625532  10.070565   9.977257   9.58146
  9.297886   8.738745   8.042353   7.684828   7.6061172  7.5779176
  7.522208   7.588483   7.8339405  8.112476   7.950168   7.776565
  7.6240463  7.8350296  8.365711   8.978931   9.711538   9.940152
  9.655477   8.590405   8.103177   7.862787   7.4036202  6.2090306
  5.0027294  4.4980664  4.3778987  4.638092   4.9357886  5.190223
  5.143574   4.6847625  4.3208575  3.1445212  2.3109236  1.8600278
  1.5736859  1.5314432  1.4350978  1.4800801  1.5923665  1.9162737
  2.3028185  3.0339577  3.7387893  4.3205786  4.9172955  5.729211
  6.6429086  7.176198   7.1779757  7.277793   7.515796   7.857417
  8.054099   8.131209   8.17612    8.228067   8.419538   8.840999
  9.419868   9.147273   8.427116   7.779339   7.2276635  6.751076
  6.598804   6.491645   6.290076   6.181533   6.036912   6.238141
  5.8787165  5.925251   6.158701   6.140165   6.5305514  7.1601434
  7.6940327  8.236867   8.492409   8.034211   7.579047   7.364252
  7.1791964  5.8626933  4.794515   4.0930552  3.7558932  3.7588384
  4.0696874  4.470211   4.648884   4.644247   4.2718353  3.206902
  2.303079   1.7849886  1.5485682  1.5324502  1.3967392  1.2355431
  1.3258885  1.4557035  1.7778906  2.3397408  3.0343344  3.7194614
  4.1874204  4.7842026  5.580591   6.2552443  6.503244   6.4135923
  6.4269977  6.7782984  7.241209   7.552095   7.4876347  7.3082952
  7.4491835  7.734023   8.139611   8.684215   7.8480325  7.1737976
  6.552091   6.111379   5.6891446  5.5401363  5.1332283  4.7921085
  4.5901337  4.793554   4.386152   3.8548748  4.097933   4.7353873
  4.8512974  5.3500166  5.910048   6.346295 ]
