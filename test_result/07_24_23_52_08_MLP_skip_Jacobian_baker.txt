time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 5000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 5
reg_param: 100.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 148.079757690 Test: 18.567504883
Epoch 0: New minimal relative error: 18.57%, model saved.
Epoch: 50 Train: 10.645799637 Test: 5.502382755
Epoch 50: New minimal relative error: 5.50%, model saved.
Epoch: 100 Train: 10.348283768 Test: 5.573299885
Epoch: 150 Train: 11.122842789 Test: 5.324150085
Epoch 150: New minimal relative error: 5.32%, model saved.
Epoch: 200 Train: 11.563383102 Test: 4.546304226
Epoch 200: New minimal relative error: 4.55%, model saved.
Epoch: 250 Train: 10.241703987 Test: 4.821096420
Epoch: 300 Train: 9.635461807 Test: 4.827116966
Epoch: 350 Train: 7.913848877 Test: 5.171783924
Epoch: 400 Train: 7.415411949 Test: 5.093410015
Epoch: 450 Train: 6.582431793 Test: 5.241038799
Epoch: 500 Train: 6.724229336 Test: 5.131601334
Epoch: 550 Train: 6.942131996 Test: 5.032154083
Epoch: 600 Train: 6.775845528 Test: 4.965535641
Epoch: 650 Train: 7.322712421 Test: 4.782725334
Epoch: 700 Train: 7.069522858 Test: 5.128462315
Epoch: 750 Train: 6.927055359 Test: 5.137418747
Epoch: 800 Train: 6.742328167 Test: 5.037707329
Epoch: 850 Train: 7.057454109 Test: 4.895881653
Epoch: 900 Train: 6.828804016 Test: 5.162832737
Epoch: 950 Train: 6.636816025 Test: 5.067826748
Epoch: 1000 Train: 6.391774654 Test: 5.102340698
Epoch: 1050 Train: 6.308526993 Test: 5.130887032
Epoch: 1100 Train: 6.417127132 Test: 5.191363811
Epoch: 1150 Train: 6.480495453 Test: 5.163990021
Epoch: 1200 Train: 6.543609142 Test: 5.197278976
Epoch: 1250 Train: 6.585453987 Test: 5.223185539
Epoch: 1300 Train: 6.443897247 Test: 5.240521431
Epoch: 1350 Train: 7.259799004 Test: 5.023415565
Epoch: 1400 Train: 6.651143074 Test: 5.164107323
Epoch: 1450 Train: 6.528436661 Test: 5.217343330
Epoch: 1500 Train: 6.668775558 Test: 5.234604359
Epoch: 1550 Train: 6.678503990 Test: 5.169603348
Epoch: 1600 Train: 6.633020401 Test: 5.211481571
Epoch: 1650 Train: 6.835957527 Test: 5.178006649
Epoch: 1700 Train: 6.767258644 Test: 5.229564667
Epoch: 1750 Train: 6.691391468 Test: 5.246081829
Epoch: 1800 Train: 6.678397179 Test: 5.219442368
Epoch: 1850 Train: 6.559022427 Test: 5.221364498
Epoch: 1900 Train: 6.624644279 Test: 5.213014603
Epoch: 1950 Train: 6.660140991 Test: 5.253848553
Epoch: 2000 Train: 6.591743946 Test: 5.288205624
Epoch: 2050 Train: 6.427418232 Test: 5.255104542
Epoch: 2100 Train: 6.512970924 Test: 5.192517281
Epoch: 2150 Train: 6.444377422 Test: 5.223304272
Epoch: 2200 Train: 6.389933109 Test: 5.236591816
Epoch: 2250 Train: 6.235397339 Test: 5.190983772
Epoch: 2300 Train: 6.271519661 Test: 5.196135044
Epoch: 2350 Train: 6.389081478 Test: 5.190928936
Epoch: 2400 Train: 6.475501060 Test: 5.224001408
Epoch: 2450 Train: 6.428968906 Test: 5.276317120
Epoch: 2500 Train: 6.445224285 Test: 5.268665791
Epoch: 2550 Train: 6.565524578 Test: 5.344763756
Epoch: 2600 Train: 6.556169987 Test: 5.295935631
Epoch: 2650 Train: 6.431342125 Test: 5.277275085
Epoch: 2700 Train: 6.454606056 Test: 5.298111916
Epoch: 2750 Train: 6.432594299 Test: 5.398995399
Epoch: 2800 Train: 6.368524551 Test: 5.293682098
Epoch: 2850 Train: 6.498054981 Test: 5.297465801
Epoch: 2900 Train: 6.472231865 Test: 5.293060780
Epoch: 2950 Train: 6.365256310 Test: 5.267753124
Epoch: 3000 Train: 6.281538963 Test: 5.276181698
Epoch: 3050 Train: 6.460427284 Test: 5.163321495
Epoch: 3100 Train: 6.534460068 Test: 5.351895332
Epoch: 3150 Train: 6.455116272 Test: 5.281358242
Epoch: 3200 Train: 6.879498959 Test: 5.194329739
Epoch: 3250 Train: 6.513446808 Test: 5.318105221
Epoch: 3300 Train: 6.416045189 Test: 5.285620213
Epoch: 3350 Train: 6.418468952 Test: 5.275930405
Epoch: 3400 Train: 6.339785576 Test: 5.296776772
Epoch: 3450 Train: 6.384375095 Test: 5.307270050
Epoch: 3500 Train: 6.400512695 Test: 5.290489197
Epoch: 3550 Train: 6.396234512 Test: 5.273342609
Epoch: 3600 Train: 6.546683311 Test: 5.266594887
Epoch: 3650 Train: 6.639913559 Test: 5.288381100
Epoch: 3700 Train: 6.661156654 Test: 5.258230209
Epoch: 3750 Train: 6.633269787 Test: 5.272171497
Epoch: 3800 Train: 6.555738926 Test: 5.253543377
Epoch: 3850 Train: 6.459063530 Test: 5.278384686
Epoch: 3900 Train: 6.618354797 Test: 5.238081455
Epoch: 3950 Train: 6.529206753 Test: 5.196158886
Epoch: 4000 Train: 6.526081085 Test: 5.173995018
Epoch: 4050 Train: 6.581846714 Test: 5.212954998
Epoch: 4100 Train: 6.578752518 Test: 5.215730190
Epoch: 4150 Train: 6.645306110 Test: 5.217471123
Epoch: 4200 Train: 6.656950951 Test: 5.224867821
Epoch: 4250 Train: 6.642120838 Test: 5.210029125
Epoch: 4300 Train: 6.625008583 Test: 5.218211174
Epoch: 4350 Train: 6.617034912 Test: 5.218109131
Epoch: 4400 Train: 6.563039303 Test: 5.218714237
Epoch: 4450 Train: 6.494490147 Test: 5.215733528
Epoch: 4500 Train: 6.461343288 Test: 5.217971325
Epoch: 4550 Train: 6.409968376 Test: 5.215655327
Epoch: 4600 Train: 6.383601189 Test: 5.215518475
Epoch: 4650 Train: 6.359826088 Test: 5.215485573
Epoch: 4700 Train: 6.350029945 Test: 5.222002029
Epoch: 4750 Train: 6.346533775 Test: 5.218538761
Epoch: 4800 Train: 6.351416111 Test: 5.222489357
Epoch: 4850 Train: 6.343506336 Test: 5.223466873
Epoch: 4900 Train: 6.332067013 Test: 5.231282234
Epoch: 4950 Train: 6.315307617 Test: 5.235596180
Epoch: 4999 Train: 6.325137138 Test: 5.241432190
Training Loss: tensor(6.3251)
Test Loss: tensor(5.2414)
True Mean x: tensor(3.4447, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(4.0541e+27, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.5065, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(inf, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0111)
Jacobian term Test Loss: tensor(0.0002)
Learned LE: [1.9086611  0.29521573]
True LE: tensor([ 0.6932, -0.7446], dtype=torch.float64)
