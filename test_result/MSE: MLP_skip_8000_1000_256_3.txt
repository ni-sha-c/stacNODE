time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.97%, model saved.
Epoch: 0 Train: 4136.50684 Test: 4260.80859
Epoch: 80 Train: 410.58029 Test: 393.69684
Epoch 160: New minimal relative error: 79.23%, model saved.
Epoch: 160 Train: 58.09031 Test: 48.17081
Epoch 240: New minimal relative error: 20.48%, model saved.
Epoch: 240 Train: 18.17134 Test: 15.27544
Epoch 320: New minimal relative error: 18.27%, model saved.
Epoch: 320 Train: 10.69194 Test: 8.41581
Epoch: 400 Train: 8.09972 Test: 6.57660
Epoch: 480 Train: 6.33382 Test: 4.92089
Epoch: 560 Train: 7.02401 Test: 5.39657
Epoch: 640 Train: 4.58480 Test: 3.53485
Epoch: 720 Train: 3.97809 Test: 3.04923
Epoch 800: New minimal relative error: 16.12%, model saved.
Epoch: 800 Train: 3.56019 Test: 2.71108
Epoch: 880 Train: 3.13831 Test: 2.38825
Epoch: 960 Train: 2.82984 Test: 2.16345
Epoch 1040: New minimal relative error: 15.98%, model saved.
Epoch: 1040 Train: 2.51308 Test: 1.89956
Epoch: 1120 Train: 14.27895 Test: 12.78277
Epoch 1200: New minimal relative error: 15.96%, model saved.
Epoch: 1200 Train: 2.02020 Test: 1.51393
Epoch: 1280 Train: 4.43559 Test: 2.97777
Epoch: 1360 Train: 3.09076 Test: 2.81562
Epoch 1440: New minimal relative error: 12.70%, model saved.
Epoch: 1440 Train: 1.53908 Test: 1.15054
Epoch: 1520 Train: 1.42841 Test: 1.06204
Epoch: 1600 Train: 1.43985 Test: 1.09716
Epoch: 1680 Train: 1.52031 Test: 1.25467
Epoch: 1760 Train: 1.33679 Test: 0.99648
Epoch: 1840 Train: 1.38791 Test: 1.10901
Epoch: 1920 Train: 1.21090 Test: 0.87001
Epoch 2000: New minimal relative error: 12.61%, model saved.
Epoch: 2000 Train: 1.27410 Test: 1.04726
Epoch 2080: New minimal relative error: 11.50%, model saved.
Epoch: 2080 Train: 0.88336 Test: 0.64400
Epoch 2160: New minimal relative error: 11.13%, model saved.
Epoch: 2160 Train: 0.83215 Test: 0.60969
Epoch: 2240 Train: 1.02824 Test: 0.70904
Epoch: 2320 Train: 1.62815 Test: 1.37002
Epoch: 2400 Train: 0.88320 Test: 0.73222
Epoch: 2480 Train: 0.78022 Test: 0.64955
Epoch: 2560 Train: 0.83304 Test: 0.66085
Epoch: 2640 Train: 0.84686 Test: 0.68568
Epoch: 2720 Train: 0.73878 Test: 0.59012
Epoch 2800: New minimal relative error: 9.87%, model saved.
Epoch: 2800 Train: 0.59777 Test: 0.43587
Epoch: 2880 Train: 0.71569 Test: 0.52788
Epoch 2960: New minimal relative error: 8.99%, model saved.
Epoch: 2960 Train: 0.68774 Test: 0.55638
Epoch: 3040 Train: 0.48687 Test: 0.35335
Epoch: 3120 Train: 0.46912 Test: 0.34110
Epoch: 3200 Train: 0.45962 Test: 0.34197
Epoch: 3280 Train: 0.54977 Test: 0.42429
Epoch: 3360 Train: 0.44965 Test: 0.34197
Epoch: 3440 Train: 0.54976 Test: 0.42620
Epoch: 3520 Train: 0.41890 Test: 0.34827
Epoch: 3600 Train: 0.40055 Test: 0.29861
Epoch: 3680 Train: 0.39015 Test: 0.29052
Epoch: 3760 Train: 0.36644 Test: 0.26868
Epoch: 3840 Train: 0.36386 Test: 0.26475
Epoch: 3920 Train: 0.55913 Test: 0.51015
Epoch: 4000 Train: 0.34165 Test: 0.25267
Epoch: 4080 Train: 0.33707 Test: 0.24844
Epoch: 4160 Train: 0.33655 Test: 0.25774
Epoch: 4240 Train: 0.31580 Test: 0.23432
Epoch: 4320 Train: 0.30777 Test: 0.22846
Epoch: 4400 Train: 0.30427 Test: 0.22714
Epoch: 4480 Train: 0.29399 Test: 0.21862
Epoch: 4560 Train: 0.31589 Test: 0.24261
Epoch: 4640 Train: 0.28211 Test: 0.21050
Epoch: 4720 Train: 0.34117 Test: 0.25810
Epoch: 4800 Train: 0.27143 Test: 0.20336
Epoch: 4880 Train: 0.26725 Test: 0.20146
Epoch 4960: New minimal relative error: 8.84%, model saved.
Epoch: 4960 Train: 0.26378 Test: 0.19903
Epoch: 5040 Train: 0.25688 Test: 0.19361
Epoch: 5120 Train: 0.27067 Test: 0.19687
Epoch: 5200 Train: 0.24829 Test: 0.18785
Epoch 5280: New minimal relative error: 6.71%, model saved.
Epoch: 5280 Train: 0.31918 Test: 0.29053
Epoch: 5360 Train: 0.24185 Test: 0.18409
Epoch 5440: New minimal relative error: 6.54%, model saved.
Epoch: 5440 Train: 0.24121 Test: 0.18444
Epoch: 5520 Train: 0.26127 Test: 0.21587
Epoch: 5600 Train: 0.22976 Test: 0.17588
Epoch: 5680 Train: 0.22606 Test: 0.17336
Epoch 5760: New minimal relative error: 6.18%, model saved.
Epoch: 5760 Train: 0.23773 Test: 0.18849
Epoch: 5840 Train: 0.21980 Test: 0.16929
Epoch: 5920 Train: 0.29263 Test: 0.26796
Epoch: 6000 Train: 0.26979 Test: 0.22983
Epoch: 6080 Train: 0.22429 Test: 0.17925
Epoch: 6160 Train: 0.20896 Test: 0.16264
Epoch: 6240 Train: 0.20762 Test: 0.16100
Epoch: 6320 Train: 0.20311 Test: 0.15867
Epoch: 6400 Train: 0.20666 Test: 0.16194
Epoch: 6480 Train: 0.19798 Test: 0.15511
Epoch: 6560 Train: 0.31701 Test: 0.27024
Epoch: 6640 Train: 0.19365 Test: 0.15262
Epoch: 6720 Train: 0.19155 Test: 0.15154
Epoch: 6800 Train: 0.18903 Test: 0.14954
Epoch: 6880 Train: 0.19042 Test: 0.15586
Epoch: 6960 Train: 0.18503 Test: 0.14704
Epoch: 7040 Train: 0.18309 Test: 0.14605
Epoch: 7120 Train: 0.19690 Test: 0.16408
Epoch: 7200 Train: 0.17933 Test: 0.14358
Epoch: 7280 Train: 0.18478 Test: 0.14940
Epoch: 7360 Train: 0.17586 Test: 0.14149
Epoch: 7440 Train: 0.17416 Test: 0.14045
Epoch: 7520 Train: 0.17893 Test: 0.14744
Epoch: 7600 Train: 0.17092 Test: 0.13844
Epoch: 7680 Train: 0.26468 Test: 0.25016
Epoch: 7760 Train: 0.16791 Test: 0.13663
Epoch: 7840 Train: 0.16644 Test: 0.13562
Epoch: 7920 Train: 0.16507 Test: 0.13458
Epoch: 7999 Train: 0.16352 Test: 0.13366
Training Loss: tensor(0.1635)
Test Loss: tensor(0.1337)
Learned LE: [ 0.8131964  -0.00628369 -4.005159  ]
True LE: [ 8.5439867e-01  1.1106186e-02 -1.4546585e+01]
Relative Error: [2.9112563  3.446975   4.1700716  4.8258886  4.721591   4.3895135
 4.2679048  4.369631   4.714112   5.29267    6.0815377  6.7311378
 6.284768   6.054022   5.8616524  5.8886685  5.8361917  5.5661435
 5.351238   4.1825833  2.015603   2.270679   4.0875845  5.7184434
 5.4766693  4.855195   3.8067598  3.0493238  2.313301   1.3167342
 0.88981307 1.0807438  1.2703898  1.2627013  1.0971488  0.9382208
 1.0955025  1.6449836  1.8959323  2.2053216  2.877705   3.4909687
 4.3507338  3.9603286  2.595595   2.097052   2.4782934  3.161378
 4.06781    4.409332   5.1752496  5.1664643  4.616916   3.5188053
 2.684928   2.1820307  1.8682238  1.6991407  1.5631629  1.52163
 1.8049961  2.1709633  2.4894156  3.0540507  3.8326342  4.632783
 4.161948   3.8445425  3.7214112  3.8090992  4.12114    4.6720824
 5.4408712  6.4025364  5.955886   5.76367    5.5867925  5.6942115
 5.7637987  5.6490498  5.5346465  4.5671763  2.487374   1.7186841
 3.7324104  5.3691573  6.0122414  5.2452326  4.060853   3.165722
 2.4112654  1.2070254  0.96189976 1.3579357  1.583977   1.5942674
 1.3709041  1.0510277  0.9743061  1.3793846  1.6104784  1.8185949
 2.2827635  3.054344   3.746048   4.254279   2.9218848  1.6812315
 1.7052541  2.1419325  2.898931   3.8087838  5.0436835  5.5723114
 5.2124286  4.020943   3.0215821  2.3518798  1.9021676  1.6505525
 1.4251437  1.2753582  1.4755276  1.7595537  2.0715578  2.685223
 3.5285652  4.2388406  3.7294111  3.3541057  3.2139373  3.2846472
 3.5602722  4.039258   4.749637   5.664651   5.6306176  5.34387
 5.257991   5.340572   5.5402284  5.581873   5.562216   4.984165
 3.1290412  1.4771004  3.07235    4.9737067  5.6944976  5.715834
 4.4467287  3.2503934  2.1840305  0.87247676 1.0459347  1.4431534
 1.7088356  1.7406904  1.5601006  1.2668943  1.0389194  1.0921035
 1.3167816  1.5271258  1.8945247  2.4824789  3.0983407  4.269141
 3.3923655  2.1159828  1.2440779  1.4383657  1.9007506  2.5804174
 4.1692333  5.4673853  5.546925   4.701599   3.5070617  2.6219554
 2.014588   1.6795906  1.4152358  1.1194338  1.1043656  1.3086491
 1.6203938  2.285515   3.1840985  3.7344482  3.2439053  2.8965025
 2.7016938  2.7187102  2.9903257  3.4572017  4.080214   4.8826036
 5.3641357  4.9860096  4.8619027  4.8635406  5.1267414  5.3056226
 5.4627714  5.3673453  3.7926354  1.8950754  2.07776    4.302738
 5.324991   5.617306   4.885644   3.1117878  1.776366   0.82204866
 1.088571   1.6778791  1.9213852  1.9347773  1.7992584  1.5248412
 1.2850639  1.1572874  1.0658175  1.0508782  1.3106855  1.7597308
 2.3425014  3.2913885  3.8875124  2.7292805  1.5988506  1.1052691
 1.3203924  1.680631   2.661391   4.436557   5.297978   5.543519
 4.209099   3.072975   2.2460883  1.8148853  1.5222048  1.134957
 0.786772   0.8798194  1.146312   1.8237809  2.5897973  3.22682
 2.7516832  2.3350358  2.123941   2.1128347  2.3064501  2.762275
 3.4026105  4.185508   5.0709205  4.774765   4.453806   4.4163136
 4.522394   4.7794795  5.0923886  5.382149   4.363992   2.676947
 1.1033242  2.5414772  4.783265   5.003491   4.8358736  3.370358
 1.9314969  0.9615029  0.99555355 1.6186244  1.8420447  1.7872578
 1.5699635  1.465443   1.4615387  1.5545487  1.3437562  1.1224238
 0.9239455  0.9705575  1.3125396  2.3899233  3.689152   3.3335419
 2.2653577  1.2941191  1.1176634  1.3368833  1.4556904  2.585472
 4.6293235  4.922137   5.075418   3.8213232  2.6777084  2.0623946
 1.7352703  1.3670295  0.92501146 0.68519294 0.66741776 1.1520158
 1.9124689  2.7109942  2.2842379  1.8861516  1.6526548  1.6071677
 1.7623239  2.1203523  2.6704485  3.4143083  4.2814035  4.8945894
 4.4204106  4.0435653  3.9947305  4.0060186  4.315925   4.7865586
 4.7539105  3.6198616  1.8438637  0.9062926  2.7847583  4.3651733
 3.810934   3.28448    2.4508183  1.2486494  0.53385377 1.2149392
 1.7228149  1.8762561  1.637611   1.2401396  1.206071   1.3848476
 1.6322223  1.6467638  1.2751005  0.88463736 0.6578462  1.2095982
 2.3829937  3.747487   3.1278205  1.9525766  1.1654724  1.0942289
 1.0657411  1.117352   2.2412534  3.936078   4.310915   4.173029
 3.2839081  2.500084   2.036019   1.8040946  1.4308109  0.84717256
 0.37839505 0.4204595  1.0389228  1.8017836  1.8041406  1.5631261
 1.3819015  1.3232245  1.3887357  1.64547    2.0983129  2.7038286
 3.4776192  4.3678713  4.711457   4.324308   3.8698733  3.4386199
 3.1838152  3.6820922  4.4015126  4.019813   3.0993166  1.3671633
 0.85145664 2.553686   3.2669265  2.6780958 ]
