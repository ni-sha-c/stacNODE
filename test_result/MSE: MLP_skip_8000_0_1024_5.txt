time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.67%, model saved.
Epoch: 0 Train: 3715.25073 Test: 3937.33179
Epoch 80: New minimal relative error: 61.50%, model saved.
Epoch: 80 Train: 162.77414 Test: 268.35059
Epoch: 160 Train: 98.26787 Test: 134.11972
Epoch 240: New minimal relative error: 28.74%, model saved.
Epoch: 240 Train: 6.85802 Test: 8.35570
Epoch 320: New minimal relative error: 22.09%, model saved.
Epoch: 320 Train: 25.86436 Test: 23.50095
Epoch: 400 Train: 21.73746 Test: 21.79510
Epoch: 480 Train: 4.08776 Test: 3.40446
Epoch 560: New minimal relative error: 20.82%, model saved.
Epoch: 560 Train: 9.05769 Test: 8.75487
Epoch 640: New minimal relative error: 17.94%, model saved.
Epoch: 640 Train: 10.59025 Test: 5.98689
Epoch: 720 Train: 3.28552 Test: 3.82837
Epoch: 800 Train: 3.88147 Test: 4.81895
Epoch 880: New minimal relative error: 14.62%, model saved.
Epoch: 880 Train: 5.46402 Test: 4.21001
Epoch: 960 Train: 1.96901 Test: 3.13190
Epoch: 1040 Train: 1.23833 Test: 1.21043
Epoch: 1120 Train: 4.55274 Test: 5.71058
Epoch: 1200 Train: 0.90900 Test: 1.31600
Epoch 1280: New minimal relative error: 10.84%, model saved.
Epoch: 1280 Train: 1.19683 Test: 0.94751
Epoch 1360: New minimal relative error: 8.52%, model saved.
Epoch: 1360 Train: 4.65537 Test: 6.20077
Epoch: 1440 Train: 2.37331 Test: 1.85073
Epoch: 1520 Train: 0.65753 Test: 0.63713
Epoch 1600: New minimal relative error: 4.96%, model saved.
Epoch: 1600 Train: 0.92169 Test: 1.36291
Epoch: 1680 Train: 1.60860 Test: 1.58209
Epoch: 1760 Train: 2.39875 Test: 3.02088
Epoch: 1840 Train: 0.78243 Test: 0.66219
Epoch: 1920 Train: 2.19613 Test: 2.72345
Epoch: 2000 Train: 0.74507 Test: 1.32017
Epoch 2080: New minimal relative error: 3.30%, model saved.
Epoch: 2080 Train: 0.22013 Test: 0.26532
Epoch: 2160 Train: 1.47520 Test: 1.40952
Epoch: 2240 Train: 2.29840 Test: 3.67591
Epoch: 2320 Train: 1.54231 Test: 1.80422
Epoch: 2400 Train: 0.25506 Test: 0.31230
Epoch: 2480 Train: 1.73204 Test: 2.13217
Epoch: 2560 Train: 4.18269 Test: 3.44522
Epoch: 2640 Train: 0.27693 Test: 0.26180
Epoch: 2720 Train: 7.10743 Test: 2.82979
Epoch: 2800 Train: 0.10910 Test: 0.14094
Epoch: 2880 Train: 0.35561 Test: 0.34473
Epoch: 2960 Train: 4.31124 Test: 5.27073
Epoch: 3040 Train: 1.70892 Test: 2.19544
Epoch: 3120 Train: 0.62236 Test: 0.71490
Epoch: 3200 Train: 1.14921 Test: 1.27665
Epoch: 3280 Train: 7.32909 Test: 6.64864
Epoch: 3360 Train: 0.23335 Test: 0.19333
Epoch: 3440 Train: 1.13696 Test: 1.04283
Epoch: 3520 Train: 0.17233 Test: 0.21853
Epoch: 3600 Train: 0.16207 Test: 0.22515
Epoch: 3680 Train: 0.29660 Test: 0.35521
Epoch: 3760 Train: 0.12024 Test: 0.18799
Epoch: 3840 Train: 0.38739 Test: 0.48296
Epoch: 3920 Train: 0.29182 Test: 0.29986
Epoch: 4000 Train: 0.41424 Test: 0.54207
Epoch: 4080 Train: 0.36600 Test: 0.51582
Epoch: 4160 Train: 0.59359 Test: 0.63144
Epoch: 4240 Train: 0.38056 Test: 0.49347
Epoch: 4320 Train: 0.18938 Test: 0.41209
Epoch: 4400 Train: 0.22819 Test: 0.35623
Epoch: 4480 Train: 0.08854 Test: 0.13277
Epoch: 4560 Train: 0.10781 Test: 0.12827
Epoch: 4640 Train: 1.85146 Test: 1.83833
Epoch: 4720 Train: 0.84716 Test: 1.11294
Epoch: 4800 Train: 0.15160 Test: 0.40749
Epoch: 4880 Train: 0.07476 Test: 0.10866
Epoch: 4960 Train: 0.26996 Test: 0.25781
Epoch: 5040 Train: 0.17415 Test: 0.23210
Epoch 5120: New minimal relative error: 3.21%, model saved.
Epoch: 5120 Train: 0.12172 Test: 0.20798
Epoch 5200: New minimal relative error: 2.36%, model saved.
Epoch: 5200 Train: 0.12824 Test: 0.12638
Epoch: 5280 Train: 0.07038 Test: 0.12871
Epoch: 5360 Train: 0.04555 Test: 0.08656
Epoch: 5440 Train: 0.30738 Test: 0.17092
Epoch: 5520 Train: 0.10720 Test: 0.18230
Epoch: 5600 Train: 0.08100 Test: 0.18011
Epoch: 5680 Train: 0.21590 Test: 0.27371
Epoch: 5760 Train: 0.30849 Test: 0.35854
Epoch: 5840 Train: 0.10750 Test: 0.18049
Epoch: 5920 Train: 0.05300 Test: 0.10369
Epoch: 6000 Train: 0.25671 Test: 0.28758
Epoch: 6080 Train: 0.09476 Test: 0.22433
Epoch: 6160 Train: 0.04273 Test: 0.08530
Epoch: 6240 Train: 0.04926 Test: 0.09806
Epoch: 6320 Train: 0.08606 Test: 0.08912
Epoch: 6400 Train: 0.20432 Test: 0.22805
Epoch 6480: New minimal relative error: 2.22%, model saved.
Epoch: 6480 Train: 0.08884 Test: 0.11686
Epoch: 6560 Train: 0.19232 Test: 0.21644
Epoch: 6640 Train: 0.61488 Test: 0.65698
Epoch: 6720 Train: 0.12152 Test: 0.18315
Epoch: 6800 Train: 0.11504 Test: 0.16585
Epoch: 6880 Train: 0.04477 Test: 0.09316
Epoch: 6960 Train: 0.39989 Test: 0.56538
Epoch: 7040 Train: 0.02781 Test: 0.06552
Epoch: 7120 Train: 0.04821 Test: 0.08575
Epoch: 7200 Train: 0.07625 Test: 0.11582
Epoch: 7280 Train: 0.06404 Test: 0.10511
Epoch 7360: New minimal relative error: 1.45%, model saved.
Epoch: 7360 Train: 0.27162 Test: 0.29121
Epoch: 7440 Train: 0.22853 Test: 0.22575
Epoch: 7520 Train: 0.03465 Test: 0.07330
Epoch: 7600 Train: 0.05752 Test: 0.08510
Epoch: 7680 Train: 0.06181 Test: 0.08375
Epoch: 7760 Train: 0.12334 Test: 0.11904
Epoch: 7840 Train: 0.06239 Test: 0.09853
Epoch: 7920 Train: 0.62717 Test: 0.91224
Epoch: 7999 Train: 0.09756 Test: 0.14620
Training Loss: tensor(0.0976)
Test Loss: tensor(0.1462)
Learned LE: [ 0.8075608  -0.01021887 -3.695176  ]
True LE: [ 8.5874432e-01  3.6538092e-03 -1.4537642e+01]
Relative Error: [0.77247876 0.91479284 0.76501375 0.5636187  0.49390602 0.5483169
 0.48123622 0.5522495  0.62573445 0.59672254 0.45054495 0.32197508
 0.1724418  0.17868547 0.30185384 0.2864813  0.18712513 0.20399512
 0.22104937 0.32601753 0.5444375  0.68990517 0.35812697 0.33918136
 0.2693172  0.3656188  0.42355025 0.5547744  0.66494316 0.70385534
 0.7616462  1.2570753  1.5277294  1.3016453  1.4460236  1.4191823
 0.9233773  0.470326   0.2959036  0.3381538  0.46981856 0.5933715
 0.7070513  0.65156573 0.785365   0.89187336 0.6388355  0.58390474
 0.6978778  0.6945473  0.7895143  0.87449497 0.7141508  0.49553597
 0.7717085  0.8311775  0.99090886 1.0202389  1.0618826  0.72793645
 0.46984452 0.5356499  0.71440303 0.80133307 0.77007437 0.61747783
 0.48887333 0.54597783 0.59619045 0.5013254  0.513839   0.6366131
 0.62198585 0.4598254  0.31921074 0.29696473 0.18777269 0.33581665
 0.23523818 0.17216496 0.11768486 0.18219492 0.44929293 0.60219765
 0.46020433 0.27607125 0.32278076 0.31252077 0.38525265 0.45714825
 0.55535275 0.68482924 0.7176876  0.7822321  1.709249   1.5015674
 1.3496716  1.3897882  0.99125814 0.5426821  0.39161003 0.4990157
 0.4642969  0.434231   0.41423193 0.5769459  0.5120012  0.61259437
 0.74215287 0.53902906 0.4938081  0.63396466 0.5414322  0.79545593
 0.80472213 0.5790847  0.4535705  0.7703486  0.8583481  0.77835596
 0.81789464 0.8194789  0.4935864  0.40398976 0.52641183 0.7640784
 0.69607043 0.71033764 0.5659461  0.4870961  0.5735537  0.49289066
 0.4656239  0.5009374  0.503192   0.5937417  0.51564604 0.37205842
 0.29105935 0.25650382 0.27893007 0.19696203 0.13597775 0.1003611
 0.24507509 0.54791456 0.56474626 0.34681723 0.17859644 0.24832937
 0.2853113  0.3374443  0.58024913 0.55635583 0.6594144  0.59110045
 1.1305941  1.9991474  1.4969516  1.2791533  0.9015474  0.58263046
 0.46222997 0.5007671  0.48507267 0.4318085  0.4832148  0.33240992
 0.61206883 0.7427157  0.64806086 0.5792244  0.4219302  0.5976432
 0.65873826 0.43092397 0.72671294 0.7581704  0.5657871  0.5641025
 0.8992291  0.84147084 0.63682616 0.6006559  0.7556442  0.6114827
 0.5228835  0.5527296  0.68435115 0.47061172 0.6053455  0.5214904
 0.5136278  0.5001395  0.456142   0.55387557 0.5002793  0.44856253
 0.46524382 0.5931902  0.4158212  0.32323733 0.2997947  0.30830187
 0.28531352 0.19115126 0.16300644 0.27663726 0.49303806 0.5875097
 0.44198796 0.14811379 0.18284598 0.2100485  0.28449634 0.37316182
 0.6240377  0.56779355 0.56506556 1.3341262  1.8241475  1.4380807
 1.042422   0.69024104 0.45724058 0.45697182 0.4002686  0.39440826
 0.3262763  0.61647797 0.42162687 0.5288536  0.79698634 0.9062648
 0.7467039  0.35930312 0.50710154 0.7629673  0.5738669  0.59868586
 0.80080825 0.5595806  0.6252715  0.7855408  0.8388498  0.8134289
 0.50661767 0.93613917 0.8683728  0.7823671  0.6943095  0.6281015
 0.53789073 0.46615767 0.56907415 0.5362921  0.39932448 0.42949948
 0.5226701  0.48096663 0.44955373 0.48253238 0.4933931  0.46464297
 0.30845144 0.3307326  0.37365907 0.39866218 0.29598942 0.2829463
 0.16384022 0.50936055 0.4673721  0.49237126 0.20970616 0.14380369
 0.2381495  0.15414812 0.38017923 0.5733073  0.58776104 0.4830987
 1.3404062  1.6127703  1.4751672  0.8686915  0.6360457  0.50636923
 0.40729302 0.2563602  0.08001456 0.11983085 0.3203874  0.29851675
 0.2608447  0.62754947 0.9245197  0.89568216 0.78239346 0.51701695
 0.66167086 0.7974974  0.6334159  0.77239734 0.69067764 0.5071842
 0.64655286 0.7052885  0.76425576 0.7162217  0.9900088  0.85288215
 0.69087636 0.62279665 0.5612148  0.55254406 0.7409294  0.7141466
 0.66835636 0.40479767 0.4057283  0.47513232 0.44105807 0.52983457
 0.66372466 0.5732055  0.34256148 0.41221902 0.42596766 0.3374678
 0.45611164 0.3683625  0.3390429  0.12416494 0.41192174 0.53101665
 0.40582898 0.20966905 0.11933454 0.2612395  0.22067353 0.2910675
 0.305415   0.59819615 0.4735672  0.9618276  1.325638   1.3339254
 1.076638   0.8007723  0.532457   0.47488418 0.20162414 0.26596674
 0.41225055 0.34003592 0.26324114 0.22373174 0.13939664 0.25047538
 0.6058103  0.5873503  0.42417157 0.55194426 0.71440613 0.8214686
 0.7709952  0.7300014  0.6197061  0.37383884 0.5540242  0.6675728
 0.8081463  0.7321979  0.6036976  0.6479649  0.6318831  0.6369798
 0.43877953 0.48718503 0.5993878  0.65565723 0.6571502  0.3950389
 0.38638332 0.41076708 0.50613016 0.58706856 0.5924363  0.4103947
 0.25160563 0.39430824 0.4280182  0.36464307 0.56583863 0.5902076
 0.11761356 0.27081785 0.5654912  0.624699  ]
