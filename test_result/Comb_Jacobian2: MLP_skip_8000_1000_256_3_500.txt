time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.10%, model saved.
Epoch: 0 Train: 33030.03516 Test: 4221.52686
Epoch 80: New minimal relative error: 89.70%, model saved.
Epoch: 80 Train: 9293.18457 Test: 1565.90906
Epoch: 160 Train: 8983.09473 Test: 1474.90625
Epoch: 240 Train: 8817.99414 Test: 1320.09302
Epoch: 320 Train: 8400.61328 Test: 1276.99072
Epoch: 400 Train: 7925.79785 Test: 1154.78149
Epoch: 480 Train: 8317.17188 Test: 1154.85815
Epoch: 560 Train: 6075.63379 Test: 731.48792
Epoch: 640 Train: 4469.23438 Test: 379.06985
Epoch: 720 Train: 2194.19556 Test: 119.46830
Epoch: 800 Train: 1165.59937 Test: 95.21926
Epoch 880: New minimal relative error: 33.95%, model saved.
Epoch: 880 Train: 608.34314 Test: 14.58575
Epoch: 960 Train: 463.49905 Test: 29.75167
Epoch 1040: New minimal relative error: 23.18%, model saved.
Epoch: 1040 Train: 345.51602 Test: 5.43936
Epoch: 1120 Train: 283.02551 Test: 4.59222
Epoch: 1200 Train: 260.36987 Test: 3.65723
Epoch: 1280 Train: 226.62103 Test: 2.70943
Epoch: 1360 Train: 196.56752 Test: 2.63945
Epoch: 1440 Train: 189.17522 Test: 9.59561
Epoch: 1520 Train: 172.06363 Test: 11.49452
Epoch: 1600 Train: 141.57890 Test: 1.11093
Epoch 1680: New minimal relative error: 18.05%, model saved.
Epoch: 1680 Train: 138.20062 Test: 10.52008
Epoch: 1760 Train: 118.69151 Test: 0.86718
Epoch 1840: New minimal relative error: 13.30%, model saved.
Epoch: 1840 Train: 111.68207 Test: 0.76737
Epoch: 1920 Train: 107.02000 Test: 0.74146
Epoch: 2000 Train: 102.87453 Test: 0.91159
Epoch 2080: New minimal relative error: 5.07%, model saved.
Epoch: 2080 Train: 101.27223 Test: 3.04183
Epoch: 2160 Train: 97.40513 Test: 0.81011
Epoch: 2240 Train: 98.00449 Test: 0.64242
Epoch: 2320 Train: 97.74532 Test: 1.33917
Epoch: 2400 Train: 90.26230 Test: 0.58403
Epoch: 2480 Train: 86.56072 Test: 2.42137
Epoch: 2560 Train: 86.32574 Test: 6.00413
Epoch: 2640 Train: 76.54710 Test: 0.43848
Epoch: 2720 Train: 78.39241 Test: 5.96280
Epoch: 2800 Train: 72.91369 Test: 0.40479
Epoch: 2880 Train: 71.46868 Test: 0.40296
Epoch: 2960 Train: 69.12283 Test: 0.64017
Epoch: 3040 Train: 67.27282 Test: 0.35623
Epoch: 3120 Train: 67.98788 Test: 0.35067
Epoch: 3200 Train: 68.23606 Test: 0.36774
Epoch: 3280 Train: 68.13214 Test: 0.51990
Epoch: 3360 Train: 65.52957 Test: 0.31399
Epoch: 3440 Train: 66.25882 Test: 0.43312
Epoch: 3520 Train: 65.86361 Test: 0.29739
Epoch: 3600 Train: 65.56420 Test: 0.32875
Epoch: 3680 Train: 66.57573 Test: 0.35761
Epoch: 3760 Train: 67.91759 Test: 0.40782
Epoch: 3840 Train: 68.09688 Test: 0.39459
Epoch: 3920 Train: 64.97861 Test: 0.33628
Epoch: 4000 Train: 61.58611 Test: 0.32242
Epoch: 4080 Train: 59.44854 Test: 0.28255
Epoch: 4160 Train: 58.72595 Test: 0.39384
Epoch: 4240 Train: 57.40269 Test: 0.26293
Epoch 4320: New minimal relative error: 5.02%, model saved.
Epoch: 4320 Train: 56.87027 Test: 0.30238
Epoch: 4400 Train: 57.89675 Test: 0.24782
Epoch: 4480 Train: 57.52608 Test: 0.26791
Epoch: 4560 Train: 56.32668 Test: 0.23524
Epoch: 4640 Train: 56.90989 Test: 0.23329
Epoch: 4720 Train: 55.25886 Test: 0.25425
Epoch: 4800 Train: 54.38531 Test: 0.86048
Epoch: 4880 Train: 54.61057 Test: 2.00463
Epoch: 4960 Train: 51.43870 Test: 0.17893
Epoch: 5040 Train: 49.79757 Test: 0.17151
Epoch: 5120 Train: 49.81024 Test: 0.19137
Epoch: 5200 Train: 50.36475 Test: 0.17969
Epoch: 5280 Train: 50.01995 Test: 0.21415
Epoch 5360: New minimal relative error: 3.53%, model saved.
Epoch: 5360 Train: 49.14687 Test: 0.17170
Epoch 5440: New minimal relative error: 2.64%, model saved.
Epoch: 5440 Train: 49.85772 Test: 0.17472
Epoch: 5520 Train: 49.92318 Test: 0.17747
Epoch: 5600 Train: 51.09035 Test: 0.18314
Epoch: 5680 Train: 52.22912 Test: 0.43314
Epoch: 5760 Train: 52.53546 Test: 0.21547
Epoch: 5840 Train: 50.43337 Test: 0.22647
Epoch: 5920 Train: 48.93445 Test: 0.48596
Epoch: 6000 Train: 46.33891 Test: 0.18300
Epoch: 6080 Train: 46.14639 Test: 0.17875
Epoch: 6160 Train: 45.77879 Test: 0.17244
Epoch: 6240 Train: 45.12959 Test: 0.16475
Epoch: 6320 Train: 44.79136 Test: 0.46487
Epoch: 6400 Train: 44.01801 Test: 0.15702
Epoch: 6480 Train: 42.33736 Test: 0.15414
Epoch: 6560 Train: 41.18584 Test: 0.13275
Epoch: 6640 Train: 41.43246 Test: 0.34546
Epoch: 6720 Train: 40.61982 Test: 0.13435
Epoch: 6800 Train: 41.73021 Test: 0.22188
Epoch: 6880 Train: 44.23951 Test: 0.20647
Epoch: 6960 Train: 47.65740 Test: 0.16244
Epoch: 7040 Train: 48.07412 Test: 0.26775
Epoch: 7120 Train: 45.61523 Test: 0.14117
Epoch: 7200 Train: 43.26926 Test: 0.13724
Epoch: 7280 Train: 42.65755 Test: 0.15842
Epoch: 7360 Train: 40.97656 Test: 0.11587
Epoch: 7440 Train: 41.06259 Test: 0.13394
Epoch: 7520 Train: 39.62634 Test: 0.18124
Epoch: 7600 Train: 38.81164 Test: 0.12660
Epoch: 7680 Train: 38.42471 Test: 0.36627
Epoch: 7760 Train: 37.95777 Test: 0.13269
Epoch: 7840 Train: 39.21003 Test: 0.12818
Epoch: 7920 Train: 41.45515 Test: 0.15213
Epoch: 7999 Train: 42.63867 Test: 0.19418
Training Loss: tensor(42.6387)
Test Loss: tensor(0.1942)
Learned LE: [ 8.1512916e-01  4.8479573e-03 -1.4463351e+01]
True LE: [ 8.4026527e-01  5.7189856e-03 -1.4515159e+01]
Relative Error: [2.0304356  1.9863367  1.9828327  1.949721   1.7078274  1.5488886
 1.4422704  1.2331991  1.3985583  1.6766316  1.9345281  1.868032
 1.5693684  1.3591307  1.2256073  1.1299992  1.2179475  1.1643974
 1.412021   2.3340933  3.032974   3.51615    3.7613642  3.8039055
 3.677898   3.444352   3.1035695  3.0541172  3.0897715  3.0282238
 2.7973228  2.5675359  2.3447936  2.2558696  2.4570825  2.6244788
 2.4197435  2.1534364  2.7169976  3.277291   3.6739721  3.7698128
 3.5895479  3.1630068  2.487634   1.6038965  1.3856916  1.5985177
 1.7526871  1.9796666  2.0365875  1.8558608  1.508417   1.0431305
 0.84004617 0.97267944 1.2034227  1.4092921  1.59588    1.7854978
 1.8470592  1.8065903  1.7970786  1.8280596  1.8549455  1.785041
 1.887193   1.8094559  1.6941434  1.5479907  1.4764669  1.6127024
 1.8392752  1.7633905  1.4957044  1.2738613  1.0945624  0.9933897
 1.0441996  1.0823762  1.1556245  2.1290562  2.8579216  3.3208714
 3.5618956  3.6604345  3.5191846  3.2208488  2.848562   2.5234525
 2.4853258  2.442739   2.2486672  2.0734901  1.9226207  1.8321955
 1.9253498  2.1181552  1.9588375  1.7610725  1.9725496  2.681821
 3.2121253  3.406394   3.3298268  2.9682653  2.2823594  1.4627391
 1.1685867  1.3086038  1.5420351  1.7570238  1.7793967  1.646223
 1.4087019  1.0135427  0.72037995 0.7264881  0.89303267 1.1314576
 1.4191645  1.6368942  1.6687423  1.6485565  1.6218516  1.6038663
 1.6629801  1.5845991  1.6172165  1.9393924  2.1501336  1.9896978
 1.8237813  1.6525265  1.5949318  1.5147824  1.3529466  1.1998967
 0.93498445 0.8077125  0.82884365 0.94383174 1.0497539  1.8077776
 2.5897384  3.1252055  3.382171   3.4125552  3.3137906  3.0422404
 2.603746   2.1323779  1.9024429  1.8431236  1.6756524  1.5167848
 1.5183699  1.5450209  1.6098973  1.5389272  1.5024731  1.3817457
 1.2708055  1.973966   2.6773577  3.0371687  3.08445    2.7580762
 2.1508112  1.3158884  1.0110252  1.0923418  1.3699178  1.5867809
 1.5914221  1.422596   1.1458672  0.7764102  0.6236518  0.47994405
 0.5998407  0.75263745 0.7364494  0.8290316  0.9875125  1.2348424
 1.3680737  1.3273889  1.4577141  1.6595328  1.520281   1.5992496
 1.9582874  2.2134783  2.1533709  1.9042618  1.5032842  1.3085432
 1.1441256  1.0305337  0.81445974 0.526107   0.53123116 0.7318569
 0.87562305 1.3919067  2.1990469  2.8309348  3.1627965  3.2689934
 3.107112   2.8118756  2.435241   1.8734869  1.3506342  1.2686765
 1.2072498  1.0598104  1.1198258  1.450845   1.5862612  1.2691536
 1.144372   1.0268029  0.93744713 1.0400767  1.9659581  2.5530512
 2.7819066  2.6050124  2.1145315  1.3481547  0.7713448  0.88229936
 1.2281532  1.4653077  1.4831817  1.3380121  1.0638455  0.6249656
 0.40112185 0.44084632 0.4880208  0.43206224 0.38667625 0.3677341
 0.41108266 0.49922457 0.689113   0.8945543  1.1536909  1.250366
 1.2262385  1.2835823  1.6499497  1.990614   2.2524455  2.3175435
 2.034695   1.4833118  1.0775306  0.8022969  0.73662126 0.5192072
 0.37466666 0.5144325  0.75446033 1.0854222  1.6559495  2.4555578
 2.9111695  3.035099   3.0166905  2.7206054  2.2349038  1.7261537
 1.1009862  0.7022583  0.84426606 1.0150551  1.1409636  1.3588254
 1.7682214  1.5499728  1.2249714  1.0134627  0.79809797 0.7131403
 0.9988884  1.8555366  2.3318563  2.403743   2.1282609  1.5155537
 0.7332873  0.5349674  0.9155928  1.3652712  1.4361255  1.4045379
 1.1619745  0.6945486  0.25572684 0.5156648  0.55523527 0.43983102
 0.55072194 0.45534623 0.3403543  0.25945568 0.25308937 0.32216573
 0.37408403 0.60000896 0.93761915 0.7152546  0.94169134 1.4735451
 2.076208   2.3417873  2.4670546  2.3293028  1.5401859  0.9201373
 0.6492296  0.5569689  0.41637304 0.45811138 0.48889267 0.7361855
 1.236943   1.7766247  2.4903166  2.8647523  2.9141011  2.6479046
 2.3211443  1.7223954  1.035872   0.35610336 0.6590268  0.93735594
 1.1161355  1.332337   1.5646967  1.4816399  1.0353124  0.71101475
 0.58827186 0.6392055  0.64894104 0.91079736 1.6483357  2.0432663
 2.066029   1.720989   1.0102762  0.44236115 0.33314148 0.9991234
 1.3851895  1.5070834  1.3703909  0.9563464  0.24058768 0.45005473
 0.7885473  0.36941844 0.43263716 0.5582805  0.52547765 0.39409563
 0.3138289  0.21467796 0.17293994 0.12542404 0.24649738 0.451068
 0.3614325  0.74432063 1.2879591  1.758126   2.029073   2.6723993
 2.5921195  1.8979224  1.02444    0.70589066 0.53908896 0.3819465
 0.37113902 0.30672696 0.5484614  1.103444   1.5912095  2.2303667
 2.6441348  2.7315035  2.4464617  1.9418491 ]
