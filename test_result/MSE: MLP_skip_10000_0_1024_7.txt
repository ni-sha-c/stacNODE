time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.58%, model saved.
Epoch: 0 Train: 3854.82910 Test: 4154.93408
Epoch 100: New minimal relative error: 91.53%, model saved.
Epoch: 100 Train: 87.22671 Test: 94.13040
Epoch 200: New minimal relative error: 47.87%, model saved.
Epoch: 200 Train: 56.43809 Test: 104.59039
Epoch: 300 Train: 9.89083 Test: 14.15453
Epoch 400: New minimal relative error: 18.12%, model saved.
Epoch: 400 Train: 2.43275 Test: 2.84666
Epoch: 500 Train: 2.15251 Test: 1.42639
Epoch 600: New minimal relative error: 13.90%, model saved.
Epoch: 600 Train: 5.46392 Test: 4.29495
Epoch 700: New minimal relative error: 12.60%, model saved.
Epoch: 700 Train: 4.04639 Test: 2.94821
Epoch 800: New minimal relative error: 9.22%, model saved.
Epoch: 800 Train: 6.00192 Test: 8.08303
Epoch: 900 Train: 3.21784 Test: 4.58829
Epoch: 1000 Train: 7.89188 Test: 7.78017
Epoch: 1100 Train: 5.62945 Test: 9.13903
Epoch 1200: New minimal relative error: 8.17%, model saved.
Epoch: 1200 Train: 3.02224 Test: 1.48606
Epoch: 1300 Train: 1.97714 Test: 2.75933
Epoch: 1400 Train: 0.75219 Test: 0.78094
Epoch: 1500 Train: 0.77003 Test: 0.94525
Epoch: 1600 Train: 9.91020 Test: 8.88560
Epoch: 1700 Train: 3.08107 Test: 3.76395
Epoch: 1800 Train: 0.99561 Test: 1.02559
Epoch 1900: New minimal relative error: 4.99%, model saved.
Epoch: 1900 Train: 0.65798 Test: 0.82317
Epoch: 2000 Train: 3.14735 Test: 2.93162
Epoch 2100: New minimal relative error: 4.57%, model saved.
Epoch: 2100 Train: 1.35762 Test: 1.12820
Epoch: 2200 Train: 1.41754 Test: 2.16310
Epoch: 2300 Train: 3.13028 Test: 3.62045
Epoch: 2400 Train: 1.35150 Test: 1.52553
Epoch: 2500 Train: 0.98433 Test: 1.33228
Epoch: 2600 Train: 0.23104 Test: 0.56020
Epoch: 2700 Train: 1.80573 Test: 2.00520
Epoch: 2800 Train: 0.09549 Test: 0.29601
Epoch: 2900 Train: 0.43354 Test: 0.73449
Epoch: 3000 Train: 0.68252 Test: 0.78552
Epoch: 3100 Train: 0.80399 Test: 1.05518
Epoch: 3200 Train: 0.40265 Test: 1.04465
Epoch 3300: New minimal relative error: 4.24%, model saved.
Epoch: 3300 Train: 0.20277 Test: 0.41729
Epoch: 3400 Train: 2.44168 Test: 2.80609
Epoch: 3500 Train: 2.86553 Test: 2.34770
Epoch: 3600 Train: 0.82955 Test: 1.29135
Epoch: 3700 Train: 0.13055 Test: 0.32569
Epoch: 3800 Train: 2.62792 Test: 2.38092
Epoch 3900: New minimal relative error: 3.66%, model saved.
Epoch: 3900 Train: 0.53338 Test: 0.86764
Epoch: 4000 Train: 0.22620 Test: 0.42447
Epoch: 4100 Train: 0.13107 Test: 0.26037
Epoch: 4200 Train: 1.66327 Test: 1.64706
Epoch: 4300 Train: 0.20707 Test: 0.78069
Epoch: 4400 Train: 0.04146 Test: 0.25910
Epoch: 4500 Train: 0.13163 Test: 0.39679
Epoch: 4600 Train: 0.47266 Test: 0.83765
Epoch: 4700 Train: 0.18268 Test: 0.46019
Epoch: 4800 Train: 0.14767 Test: 0.46172
Epoch: 4900 Train: 0.73002 Test: 0.81332
Epoch: 5000 Train: 0.14336 Test: 0.30129
Epoch 5100: New minimal relative error: 3.08%, model saved.
Epoch: 5100 Train: 0.15266 Test: 0.38725
Epoch: 5200 Train: 0.74361 Test: 0.58933
Epoch: 5300 Train: 0.12558 Test: 0.31935
Epoch: 5400 Train: 0.12446 Test: 0.28557
Epoch: 5500 Train: 0.61525 Test: 0.54874
Epoch 5600: New minimal relative error: 2.84%, model saved.
Epoch: 5600 Train: 0.07000 Test: 0.28770
Epoch: 5700 Train: 0.05148 Test: 0.25406
Epoch: 5800 Train: 0.42836 Test: 0.88637
Epoch: 5900 Train: 0.38889 Test: 0.75746
Epoch: 6000 Train: 1.70236 Test: 1.80320
Epoch: 6100 Train: 0.08317 Test: 0.31695
Epoch: 6200 Train: 0.83372 Test: 1.39276
Epoch: 6300 Train: 0.02987 Test: 0.23923
Epoch 6400: New minimal relative error: 2.06%, model saved.
Epoch: 6400 Train: 0.09770 Test: 0.25017
Epoch: 6500 Train: 0.07348 Test: 0.25814
Epoch: 6600 Train: 1.10169 Test: 1.42740
Epoch: 6700 Train: 2.01732 Test: 2.54485
Epoch: 6800 Train: 0.25300 Test: 0.44889
Epoch: 6900 Train: 0.06832 Test: 0.27271
Epoch: 7000 Train: 0.03409 Test: 0.26702
Epoch 7100: New minimal relative error: 1.48%, model saved.
Epoch: 7100 Train: 0.02121 Test: 0.24074
Epoch: 7200 Train: 0.02537 Test: 0.22660
Epoch: 7300 Train: 0.02196 Test: 0.23730
Epoch: 7400 Train: 0.04950 Test: 0.24923
Epoch: 7500 Train: 0.02310 Test: 0.24285
Epoch: 7600 Train: 0.89914 Test: 1.39627
Epoch: 7700 Train: 0.08535 Test: 0.32022
Epoch: 7800 Train: 0.10726 Test: 0.31051
Epoch: 7900 Train: 0.28974 Test: 0.62237
Epoch: 8000 Train: 0.06409 Test: 0.34129
Epoch: 8100 Train: 0.02069 Test: 0.23709
Epoch: 8200 Train: 0.01719 Test: 0.23656
Epoch: 8300 Train: 0.02888 Test: 0.22934
Epoch: 8400 Train: 0.05166 Test: 0.27729
Epoch: 8500 Train: 0.02061 Test: 0.24447
Epoch: 8600 Train: 0.07875 Test: 0.29938
Epoch: 8700 Train: 0.01800 Test: 0.22045
Epoch: 8800 Train: 0.01753 Test: 0.23962
Epoch: 8900 Train: 0.01645 Test: 0.22672
Epoch: 9000 Train: 0.03654 Test: 0.26143
Epoch: 9100 Train: 0.02082 Test: 0.23225
Epoch: 9200 Train: 0.16062 Test: 0.33719
Epoch: 9300 Train: 0.01437 Test: 0.23317
Epoch: 9400 Train: 0.01856 Test: 0.23474
Epoch: 9500 Train: 0.05501 Test: 0.24600
Epoch: 9600 Train: 1.13053 Test: 1.31437
Epoch: 9700 Train: 0.14505 Test: 0.33177
Epoch: 9800 Train: 0.01364 Test: 0.22728
Epoch: 9900 Train: 0.01693 Test: 0.23796
Epoch: 9999 Train: 0.01542 Test: 0.22733
Training Loss: tensor(0.0154)
Test Loss: tensor(0.2273)
Learned LE: [ 8.3535844e-01  1.3645291e-03 -3.9483833e+00]
True LE: [ 8.616654e-01  3.753119e-03 -1.453984e+01]
Relative Error: [0.17573677 0.29904878 0.18252197 0.26056203 0.62486774 0.7825954
 0.84684193 0.30477914 0.17672762 0.18853995 0.39426053 0.45168027
 0.21320957 0.20922816 0.29547313 0.1622318  0.13066621 0.14503098
 0.24925467 0.31181747 0.33634833 0.13222285 0.11843292 0.08644917
 0.12001163 0.19132052 0.3295216  0.44828746 0.39905807 0.21990973
 0.21335901 0.32743132 0.27662617 0.34905258 0.1261185  0.29649198
 0.156122   0.1819812  0.3997038  0.4542105  0.28399378 0.21009411
 0.35249588 0.3305182  0.17532489 0.2135749  0.10032821 0.15849215
 0.05831253 0.04542734 0.12776907 0.22342229 0.20752057 0.20317324
 0.27571464 0.20211662 0.20410761 0.20604481 0.1908421  0.09947167
 0.15824802 0.14563179 0.1663366  0.22346519 0.22149074 0.21870533
 0.89005566 0.6146236  0.6590055  0.2384481  0.28111738 0.38215363
 0.38713524 0.5210892  0.25514647 0.28217542 0.24884616 0.21011257
 0.18197112 0.18224043 0.31627953 0.3686482  0.4293874  0.22793168
 0.2565289  0.28333765 0.09651496 0.06340715 0.4505635  0.2372324
 0.40735608 0.2920246  0.15743142 0.28757438 0.38705823 0.43841344
 0.29385734 0.11658008 0.20662877 0.29009905 0.02660364 0.22622584
 0.4056402  0.2192652  0.24905011 0.1990458  0.14454113 0.18752386
 0.05207836 0.12510589 0.08638705 0.12936708 0.09827362 0.11373618
 0.17719752 0.18904117 0.20273149 0.17073247 0.08865537 0.15397541
 0.24052751 0.30025154 0.17756586 0.10660528 0.24548878 0.13122162
 0.20765145 0.24749905 0.7558089  0.4048014  0.48567536 0.19439216
 0.14196876 0.42234206 0.35538092 0.8256705  0.42221978 0.15471008
 0.17805432 0.2630593  0.15951496 0.31032717 0.32813832 0.31977248
 0.22344843 0.11662621 0.27808475 0.31490052 0.13993762 0.20771715
 0.34164762 0.24601156 0.32399312 0.39532432 0.27563864 0.23013458
 0.259083   0.42272663 0.31296244 0.19183259 0.14273654 0.2581931
 0.19302806 0.161176   0.39071953 0.21193254 0.19061394 0.15028115
 0.23508328 0.09728562 0.08760875 0.09582862 0.12323251 0.10299806
 0.10360239 0.12863368 0.24343556 0.26091588 0.16082022 0.23940636
 0.30615315 0.14463857 0.10826549 0.19426925 0.08279664 0.21020645
 0.04268476 0.29089302 0.29866642 0.21615091 0.14677587 0.4554771
 0.35322702 0.2141125  0.2592781  0.16198069 0.21141838 0.33162293
 0.70508856 0.2368826  0.18494073 0.21022123 0.08556174 0.4716855
 0.17748812 0.32433394 0.35527083 0.22182204 0.09188405 0.30890134
 0.0998431  0.13713169 0.06929472 0.5620488  0.13063492 0.36538967
 0.3643946  0.377148   0.2553601  0.2625567  0.27110818 0.22834398
 0.1333389  0.12281737 0.16468598 0.13353753 0.16951536 0.27679253
 0.26545206 0.24333501 0.19266865 0.22796583 0.11563848 0.11590386
 0.09331291 0.1555972  0.04619404 0.14338398 0.19485101 0.30249867
 0.44768974 0.23199205 0.24688512 0.418487   0.21849804 0.12240861
 0.22609122 0.25025484 0.23469491 0.04853664 0.18022372 0.3601312
 0.26073772 0.30865252 0.16641518 0.33327246 0.350065   0.2531761
 0.09459462 0.17529756 0.5544272  0.76642346 0.2627335  0.08056133
 0.16142939 0.18096116 0.35296598 0.11333136 0.42374492 0.27702552
 0.16402528 0.13415529 0.16388236 0.10213134 0.27052948 0.2536062
 0.32356146 0.13813794 0.23762165 0.33305857 0.44497094 0.18873997
 0.16216078 0.16632615 0.12952495 0.10909659 0.14875636 0.11572594
 0.28370804 0.12162965 0.25563568 0.18243405 0.26899734 0.16374812
 0.25062284 0.04548188 0.0643548  0.02142693 0.08119594 0.10429528
 0.14683793 0.21228376 0.21734479 0.3234618  0.2248045  0.13527776
 0.17544267 0.41611755 0.13408618 0.22348554 0.30413657 0.26919344
 0.16471326 0.05422228 0.3188717  0.3451929  0.3938223  0.28166857
 0.15439916 0.4184034  0.14054112 0.14804253 0.48929206 0.20775962
 0.6891877  0.27393642 0.11114435 0.35741156 0.17550892 0.30227855
 0.16695914 0.4351384  0.49375123 0.4725996  0.32428506 0.25134027
 0.22267592 0.28740638 0.40797523 0.34890607 0.12014405 0.1443959
 0.21183346 0.31741646 0.13808937 0.05410956 0.08179966 0.03566933
 0.10082029 0.06901797 0.28550506 0.31835333 0.10072628 0.10140228
 0.26356813 0.09337374 0.13351302 0.273324   0.16318578 0.09215488
 0.02981904 0.1052717  0.08627309 0.11410691 0.06254247 0.08495078
 0.23764572 0.21241494 0.06613488 0.4709222  0.2655286  0.34342328
 0.0974809  0.21854988 0.33445907 0.13863961 0.13654551 0.3797177
 0.5107507  0.4136915  0.31243068 0.2558553  0.25364357 0.10851128
 0.22368832 0.88631064 0.2535659  0.47896978 0.17164496 0.09341597
 0.21254875 0.05351115 0.2310769  0.27044627 0.16195466 0.31067872
 0.3791107  0.35252297 0.2546446  0.39594886]
