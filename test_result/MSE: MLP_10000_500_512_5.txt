time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.92%, model saved.
Epoch: 0 Train: 3569.78394 Test: 4162.63184
Epoch 100: New minimal relative error: 66.25%, model saved.
Epoch: 100 Train: 98.82711 Test: 41.85307
Epoch 200: New minimal relative error: 43.59%, model saved.
Epoch: 200 Train: 10.32842 Test: 6.51677
Epoch 300: New minimal relative error: 30.27%, model saved.
Epoch: 300 Train: 4.24404 Test: 2.82234
Epoch: 400 Train: 14.29138 Test: 11.85853
Epoch 500: New minimal relative error: 20.61%, model saved.
Epoch: 500 Train: 1.39350 Test: 2.35765
Epoch: 600 Train: 5.24768 Test: 7.41777
Epoch: 700 Train: 3.47589 Test: 4.64273
Epoch: 800 Train: 7.11910 Test: 6.96871
Epoch 900: New minimal relative error: 17.02%, model saved.
Epoch: 900 Train: 0.38559 Test: 0.75378
Epoch: 1000 Train: 0.76088 Test: 0.61248
Epoch: 1100 Train: 0.69195 Test: 0.52104
Epoch: 1200 Train: 2.88287 Test: 2.66491
Epoch: 1300 Train: 2.79860 Test: 3.65251
Epoch: 1400 Train: 0.75601 Test: 0.83983
Epoch: 1500 Train: 0.21693 Test: 0.26598
Epoch: 1600 Train: 0.20606 Test: 0.32531
Epoch: 1700 Train: 0.23062 Test: 0.31383
Epoch: 1800 Train: 0.56749 Test: 0.72777
Epoch: 1900 Train: 3.31228 Test: 4.16310
Epoch: 2000 Train: 3.68658 Test: 4.43169
Epoch: 2100 Train: 0.15673 Test: 0.25969
Epoch: 2200 Train: 4.03827 Test: 4.84938
Epoch: 2300 Train: 2.63433 Test: 3.25571
Epoch: 2400 Train: 1.64646 Test: 1.95960
Epoch: 2500 Train: 0.30053 Test: 0.47953
Epoch: 2600 Train: 0.31725 Test: 0.46037
Epoch: 2700 Train: 1.29809 Test: 1.54837
Epoch: 2800 Train: 0.12526 Test: 0.17054
Epoch 2900: New minimal relative error: 15.46%, model saved.
Epoch: 2900 Train: 0.08008 Test: 0.12044
Epoch: 3000 Train: 0.12176 Test: 0.17972
Epoch: 3100 Train: 1.10570 Test: 1.27111
Epoch: 3200 Train: 0.09873 Test: 0.17329
Epoch: 3300 Train: 0.06528 Test: 0.11568
Epoch: 3400 Train: 0.84576 Test: 1.01094
Epoch: 3500 Train: 0.25422 Test: 0.40357
Epoch: 3600 Train: 0.23074 Test: 0.27153
Epoch: 3700 Train: 0.09808 Test: 0.17863
Epoch: 3800 Train: 0.42043 Test: 0.42257
Epoch: 3900 Train: 0.55043 Test: 0.83840
Epoch: 4000 Train: 1.65072 Test: 1.05044
Epoch: 4100 Train: 0.48612 Test: 0.39160
Epoch: 4200 Train: 0.06667 Test: 0.12381
Epoch: 4300 Train: 0.86914 Test: 0.58986
Epoch: 4400 Train: 0.05857 Test: 0.09464
Epoch: 4500 Train: 0.58547 Test: 0.69219
Epoch: 4600 Train: 0.40338 Test: 0.44301
Epoch: 4700 Train: 0.78439 Test: 1.13413
Epoch: 4800 Train: 1.07191 Test: 1.22755
Epoch: 4900 Train: 0.27315 Test: 0.32750
Epoch: 5000 Train: 0.17893 Test: 0.17972
Epoch: 5100 Train: 0.47530 Test: 0.58444
Epoch: 5200 Train: 0.99668 Test: 1.41519
Epoch: 5300 Train: 0.02456 Test: 0.05608
Epoch: 5400 Train: 0.28800 Test: 0.45005
Epoch: 5500 Train: 0.66085 Test: 0.84987
Epoch: 5600 Train: 0.20963 Test: 0.26301
Epoch: 5700 Train: 0.38368 Test: 0.39321
Epoch: 5800 Train: 0.02515 Test: 0.06080
Epoch: 5900 Train: 0.02572 Test: 0.05480
Epoch: 6000 Train: 0.09822 Test: 0.15821
Epoch: 6100 Train: 0.17978 Test: 0.28741
Epoch: 6200 Train: 0.06100 Test: 0.09542
Epoch: 6300 Train: 0.05765 Test: 0.10306
Epoch: 6400 Train: 1.40410 Test: 1.40056
Epoch: 6500 Train: 0.24259 Test: 0.37140
Epoch: 6600 Train: 0.34279 Test: 0.51287
Epoch: 6700 Train: 0.80243 Test: 0.84997
Epoch: 6800 Train: 0.10158 Test: 0.13342
Epoch: 6900 Train: 0.10212 Test: 0.17176
Epoch: 7000 Train: 1.21276 Test: 1.38473
Epoch: 7100 Train: 1.34928 Test: 1.06856
Epoch: 7200 Train: 0.15089 Test: 0.13747
Epoch: 7300 Train: 0.03912 Test: 0.06753
Epoch: 7400 Train: 0.01490 Test: 0.04911
Epoch: 7500 Train: 0.01330 Test: 0.04464
Epoch: 7600 Train: 0.35390 Test: 0.43156
Epoch: 7700 Train: 0.02138 Test: 0.05130
Epoch: 7800 Train: 0.16172 Test: 0.16696
Epoch: 7900 Train: 0.15204 Test: 0.23298
Epoch: 8000 Train: 0.14476 Test: 0.23518
Epoch: 8100 Train: 0.41156 Test: 0.37117
Epoch 8200: New minimal relative error: 13.04%, model saved.
Epoch: 8200 Train: 0.30404 Test: 0.44739
Epoch: 8300 Train: 0.01222 Test: 0.04321
Epoch: 8400 Train: 0.44761 Test: 0.56981
Epoch: 8500 Train: 0.01086 Test: 0.04502
Epoch: 8600 Train: 0.01834 Test: 0.04996
Epoch: 8700 Train: 0.01292 Test: 0.04784
Epoch: 8800 Train: 0.01083 Test: 0.04289
Epoch: 8900 Train: 0.03974 Test: 0.08546
Epoch: 9000 Train: 0.57724 Test: 0.60227
Epoch: 9100 Train: 0.01279 Test: 0.04688
Epoch: 9200 Train: 0.00985 Test: 0.04123
Epoch: 9300 Train: 0.00930 Test: 0.04120
Epoch: 9400 Train: 0.00982 Test: 0.04125
Epoch: 9500 Train: 0.03228 Test: 0.05585
Epoch: 9600 Train: 0.01957 Test: 0.04930
Epoch: 9700 Train: 0.01355 Test: 0.04493
Epoch: 9800 Train: 0.13375 Test: 0.18209
Epoch: 9900 Train: 0.47404 Test: 0.47446
Epoch: 9999 Train: 0.01478 Test: 0.04889
Training Loss: tensor(0.0148)
Test Loss: tensor(0.0489)
Learned LE: [ 0.8602851   0.01271146 -5.884394  ]
True LE: [ 8.8402945e-01 -8.8643972e-03 -1.4549658e+01]
Relative Error: [2.2271862  2.36256    2.4142587  2.3889694  2.3706584  2.4196358
 2.5166702  2.6043806  2.6482847  2.6474676  2.6214113  2.6109922
 2.6569824  2.7405117  2.7997727  2.8386667  2.8826528  2.8638606
 2.7926884  2.7603843  2.7186055  2.6005697  2.499534   2.505529
 2.4994864  2.4078746  2.3857949  2.53366    2.71862    2.8839686
 2.7858913  2.5783405  2.4126449  2.4241323  2.4790561  2.4093082
 2.2734282  2.1974933  2.1259687  2.038997   1.989492   1.9221146
 1.830355   1.7572274  1.7414248  1.7777122  1.7973989  1.728579
 1.6229249  1.5819808  1.6013297  1.6274353  1.6329743  1.6239597
 1.5370194  1.4719893  1.4671968  1.4958417  1.5436524  1.6060908
 1.6874069  1.7948933  1.9452803  2.1212454  2.2613902  2.3063087
 2.28019    2.269549   2.3341262  2.4548788  2.5547006  2.587555
 2.570568   2.546356   2.558664   2.6465104  2.780533   2.8687081
 2.9115412  2.9464056  2.896899   2.8017862  2.7687879  2.7333446
 2.6258893  2.533878   2.5429678  2.4966388  2.362969   2.3613322
 2.5429568  2.7175553  2.7907612  2.7001996  2.4794157  2.330477
 2.4075978  2.4969914  2.3729146  2.1639097  2.0782964  1.9735314
 1.8709359  1.8342915  1.763048   1.6622841  1.5909576  1.5837905
 1.6252226  1.6495214  1.6005231  1.5210553  1.4928813  1.5147347
 1.531158   1.5037185  1.45132    1.3767751  1.3452225  1.3693651
 1.4125954  1.4644216  1.521516   1.5848123  1.6720748  1.815439
 1.9989667  2.1491592  2.20046    2.1691885  2.1441708  2.2280765
 2.3951213  2.5052733  2.4981515  2.440258   2.4127069  2.4439783
 2.5566561  2.7370646  2.8765302  2.93735    2.9703338  2.9064565
 2.7991774  2.7569041  2.7130675  2.63438    2.5591326  2.5229478
 2.455502   2.322691   2.3065124  2.4768288  2.6171527  2.6084938
 2.5602455  2.3604228  2.1974566  2.3091352  2.4510162  2.3506706
 2.0823991  1.8894106  1.7780592  1.7159559  1.7196264  1.6547
 1.5382049  1.4450235  1.4101406  1.41517    1.4134201  1.3919098
 1.384092   1.395044   1.4110302  1.4185361  1.3773618  1.299994
 1.2501106  1.2422459  1.288592   1.3406743  1.3932986  1.4441899
 1.4884684  1.5451667  1.6632487  1.8430233  2.0106287  2.0889308
 2.0567229  1.9974017  2.078897   2.2772806  2.3965755  2.3602276
 2.2659893  2.238447   2.293736   2.4049754  2.5801287  2.7590303
 2.8442461  2.8914452  2.858457   2.7483807  2.6859016  2.6397817
 2.5722558  2.5225365  2.4307783  2.3590264  2.2723293  2.1912005
 2.2907505  2.4202464  2.3532114  2.3288732  2.2357512  2.0291939
 2.1053314  2.3249984  2.3403304  1.9964834  1.682201   1.6254181
 1.5831527  1.600459   1.5727136  1.4502312  1.3200665  1.2428417
 1.2172856  1.1950716  1.1574882  1.151804   1.216116   1.2785622
 1.2919595  1.2642422  1.1944069  1.1419187  1.1438442  1.202922
 1.2648907  1.3169513  1.3635873  1.3972293  1.4201025  1.4853364
 1.6328083  1.8127958  1.942838   1.9540012  1.8624742  1.8698908
 2.0423872  2.2019303  2.2103574  2.1093369  2.0575821  2.124757
 2.241515   2.365479   2.5173311  2.637992   2.6744967  2.6936643
 2.6400099  2.5602157  2.4860713  2.4461753  2.3969011  2.2986765
 2.1945348  2.1575036  2.082134   2.0369778  2.1638713  2.165501
 2.027813   2.0637217  1.917135   1.835277   2.0998116  2.2832923
 1.9557443  1.5973293  1.4861234  1.5027428  1.466773   1.4838239
 1.3937476  1.2380918  1.1224484  1.0712767  1.0557245  1.035037
 1.0062796  1.0078442  1.0711489  1.1446133  1.1423689  1.1069734
 1.0550171  1.0494794  1.0926774  1.1667305  1.2239764  1.2661234
 1.2991223  1.3090492  1.3105332  1.3712856  1.5164553  1.6842216
 1.8029803  1.7794017  1.6871998  1.7199165  1.8800564  2.0027912
 1.9757228  1.8913684  1.9070473  2.0363922  2.16799    2.2581189
 2.342991   2.412708   2.4258206  2.42574    2.3983924  2.3491247
 2.2550676  2.2666833  2.2160263  2.0526063  2.0183218  1.9975858
 1.9406481  1.9143097  2.0381303  1.982018   1.8313417  1.8876235
 1.750596   1.7447604  2.0786307  2.0078955  1.6510288  1.4547175
 1.4073702  1.4764748  1.4078039  1.3300608  1.2018448  1.0860516
 1.0212605  0.995511   0.99073136 0.9766756  0.9515852  0.9537911
 0.98222953 1.0136514  0.9989365  0.9662343  0.94584024 0.9611531
 1.0318794  1.1001934  1.1519029  1.1819949  1.2022463  1.195122
 1.1644192  1.1775236  1.2791581  1.4306743  1.5771971  1.6069913
 1.5252516  1.4978815  1.6233228  1.7498878  1.7428247  1.6860956
 1.7200596  1.8670136  2.0085478  2.0752409  2.1064727  2.1346953
 2.1528733  2.15601    2.1396208  2.1623404 ]
