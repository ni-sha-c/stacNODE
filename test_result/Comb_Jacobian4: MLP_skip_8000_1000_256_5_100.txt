time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.70%, model saved.
Epoch: 0 Train: 10070.51758 Test: 4007.69214
Epoch: 80 Train: 2873.54224 Test: 1101.98010
Epoch: 160 Train: 2825.02734 Test: 1089.15393
Epoch: 240 Train: 2611.57446 Test: 923.16980
Epoch 320: New minimal relative error: 77.46%, model saved.
Epoch: 320 Train: 1127.07275 Test: 246.75143
Epoch: 400 Train: 589.07605 Test: 170.93175
Epoch 480: New minimal relative error: 33.18%, model saved.
Epoch: 480 Train: 245.65723 Test: 57.20273
Epoch: 560 Train: 144.66748 Test: 25.65999
Epoch: 640 Train: 303.95157 Test: 134.85431
Epoch 720: New minimal relative error: 20.29%, model saved.
Epoch: 720 Train: 74.64474 Test: 12.55992
Epoch 800: New minimal relative error: 18.04%, model saved.
Epoch: 800 Train: 58.36592 Test: 4.58413
Epoch 880: New minimal relative error: 10.60%, model saved.
Epoch: 880 Train: 48.83826 Test: 2.81965
Epoch: 960 Train: 48.64707 Test: 5.83868
Epoch: 1040 Train: 40.98169 Test: 3.00861
Epoch 1120: New minimal relative error: 9.30%, model saved.
Epoch: 1120 Train: 35.45551 Test: 2.14354
Epoch: 1200 Train: 33.24872 Test: 1.70167
Epoch: 1280 Train: 31.54343 Test: 2.56516
Epoch: 1360 Train: 29.42155 Test: 2.44227
Epoch: 1440 Train: 26.08970 Test: 1.23166
Epoch: 1520 Train: 28.99368 Test: 2.20636
Epoch: 1600 Train: 37.96120 Test: 17.75235
Epoch 1680: New minimal relative error: 8.77%, model saved.
Epoch: 1680 Train: 21.58291 Test: 0.80059
Epoch 1760: New minimal relative error: 6.28%, model saved.
Epoch: 1760 Train: 22.03016 Test: 0.89715
Epoch: 1840 Train: 23.15100 Test: 2.25603
Epoch: 1920 Train: 22.74911 Test: 1.09110
Epoch: 2000 Train: 21.41092 Test: 2.94171
Epoch: 2080 Train: 19.90550 Test: 2.06522
Epoch: 2160 Train: 23.12500 Test: 1.38604
Epoch: 2240 Train: 17.62657 Test: 0.63907
Epoch: 2320 Train: 17.27608 Test: 0.68130
Epoch: 2400 Train: 16.82446 Test: 0.60493
Epoch: 2480 Train: 16.93302 Test: 0.61787
Epoch: 2560 Train: 18.43173 Test: 1.45277
Epoch: 2640 Train: 15.53694 Test: 0.55274
Epoch: 2720 Train: 15.46185 Test: 0.61304
Epoch: 2800 Train: 16.18976 Test: 1.38978
Epoch: 2880 Train: 14.45402 Test: 0.44641
Epoch: 2960 Train: 14.49924 Test: 0.48950
Epoch: 3040 Train: 18.20242 Test: 1.01035
Epoch: 3120 Train: 13.45145 Test: 0.42097
Epoch: 3200 Train: 14.91794 Test: 2.17007
Epoch: 3280 Train: 12.97932 Test: 0.40918
Epoch: 3360 Train: 12.76441 Test: 0.53387
Epoch: 3440 Train: 13.13217 Test: 0.89329
Epoch: 3520 Train: 12.81640 Test: 0.99532
Epoch 3600: New minimal relative error: 4.05%, model saved.
Epoch: 3600 Train: 12.44475 Test: 0.57987
Epoch 3680: New minimal relative error: 3.14%, model saved.
Epoch: 3680 Train: 12.93221 Test: 1.06803
Epoch: 3760 Train: 15.38063 Test: 3.75338
Epoch: 3840 Train: 11.70462 Test: 0.67280
Epoch: 3920 Train: 11.43490 Test: 0.35925
Epoch: 4000 Train: 11.81805 Test: 0.45075
Epoch: 4080 Train: 11.41167 Test: 0.51025
Epoch: 4160 Train: 11.02492 Test: 0.33711
Epoch: 4240 Train: 11.22217 Test: 0.42657
Epoch: 4320 Train: 11.26195 Test: 1.00813
Epoch: 4400 Train: 10.58225 Test: 0.43618
Epoch: 4480 Train: 11.23822 Test: 1.01281
Epoch: 4560 Train: 10.42997 Test: 0.44454
Epoch 4640: New minimal relative error: 2.58%, model saved.
Epoch: 4640 Train: 10.60598 Test: 0.53673
Epoch: 4720 Train: 10.27724 Test: 0.54075
Epoch 4800: New minimal relative error: 1.84%, model saved.
Epoch: 4800 Train: 9.76931 Test: 0.30388
Epoch: 4880 Train: 10.48485 Test: 0.38674
Epoch: 4960 Train: 9.68398 Test: 0.28735
Epoch: 5040 Train: 9.62247 Test: 0.28235
Epoch: 5120 Train: 9.50595 Test: 0.31421
Epoch: 5200 Train: 9.61270 Test: 0.32890
Epoch: 5280 Train: 9.49384 Test: 0.32674
Epoch: 5360 Train: 9.29930 Test: 0.26099
Epoch: 5440 Train: 9.61471 Test: 0.41390
Epoch: 5520 Train: 9.46345 Test: 0.34121
Epoch: 5600 Train: 9.18859 Test: 0.25666
Epoch: 5680 Train: 9.63918 Test: 0.50005
Epoch: 5760 Train: 9.14504 Test: 0.25930
Epoch: 5840 Train: 10.44756 Test: 0.91095
Epoch: 5920 Train: 9.16517 Test: 0.25822
Epoch: 6000 Train: 9.23028 Test: 0.25598
Epoch: 6080 Train: 9.27023 Test: 0.26634
Epoch: 6160 Train: 9.49557 Test: 0.51189
Epoch: 6240 Train: 9.28326 Test: 0.32798
Epoch: 6320 Train: 9.10917 Test: 0.35318
Epoch: 6400 Train: 9.12809 Test: 0.30068
Epoch: 6480 Train: 10.10054 Test: 1.42292
Epoch: 6560 Train: 9.09324 Test: 0.27935
Epoch: 6640 Train: 10.54404 Test: 1.28832
Epoch: 6720 Train: 9.26532 Test: 0.32044
Epoch: 6800 Train: 9.21102 Test: 0.36369
Epoch: 6880 Train: 9.06748 Test: 0.26923
Epoch: 6960 Train: 9.22864 Test: 0.35652
Epoch: 7040 Train: 8.83270 Test: 0.26096
Epoch: 7120 Train: 8.88519 Test: 0.28258
Epoch: 7200 Train: 8.85650 Test: 0.36102
Epoch: 7280 Train: 8.81057 Test: 0.29884
Epoch: 7360 Train: 8.68718 Test: 0.27431
Epoch: 7440 Train: 8.57749 Test: 0.27702
Epoch: 7520 Train: 8.52060 Test: 0.25866
Epoch: 7600 Train: 8.68130 Test: 0.31035
Epoch: 7680 Train: 8.50768 Test: 0.26517
Epoch: 7760 Train: 8.61236 Test: 0.26521
Epoch: 7840 Train: 8.45628 Test: 0.25676
Epoch: 7920 Train: 8.45387 Test: 0.26489
Epoch: 7999 Train: 8.27557 Test: 0.26954
Training Loss: tensor(8.2756)
Test Loss: tensor(0.2695)
Learned LE: [  0.810879     0.01486255 -14.488949  ]
True LE: [ 8.4429753e-01  3.6374195e-03 -1.4521420e+01]
Relative Error: [2.164619   1.9653612  1.9435565  1.473601   1.2693342  1.3591437
 1.9148471  2.7725992  2.9868863  2.673595   2.6167088  2.4322004
 1.5639107  1.3950586  1.8479125  2.4440522  3.1199174  3.7322495
 4.114435   4.3703513  4.5307465  4.633099   4.6816444  4.8486357
 5.1173134  5.38293    5.5547585  5.6420965  5.8184342  5.654594
 5.334721   5.044368   4.9435415  4.944178   4.4871926  4.2162085
 4.1569333  4.1200757  3.8943994  3.5254664  3.0300138  3.0351834
 2.8678524  2.5121238  2.0206141  1.5914463  1.4062252  1.7714357
 2.3704388  2.4577792  2.8816926  3.576367   3.2619317  3.226709
 3.3187706  3.1913583  3.1104586  3.1295505  3.0443008  2.8441658
 2.6421099  2.418481   2.0261812  1.7135674  1.6482201  1.130411
 0.8497106  0.9636961  1.4777398  2.3466756  2.815899   2.470883
 2.4029603  2.5024827  1.593757   1.2962991  1.6686976  2.218401
 2.7990062  3.3674386  3.844775   4.0765104  4.1670237  4.1643367
 4.118149   4.2541733  4.62065    4.8005037  4.990952   5.138563
 5.20527    4.9123826  4.6075864  4.220372   4.1041245  4.1868634
 3.761516   3.4205258  3.459321   3.4832969  3.37164    3.1104648
 2.6900961  2.7055192  2.5838406  2.2733746  1.8668765  1.4332256
 1.0897008  1.395918   2.1460903  2.449177   2.797122   3.5258849
 3.0571098  2.9151013  3.0481305  3.0709445  2.8476686  2.9358993
 2.9223793  2.8097723  2.5871787  2.3403077  2.009888   1.5702296
 1.3779255  0.87879103 0.5269417  0.56275517 0.9928289  1.8744847
 2.474399   2.275712   2.1482139  2.2026248  1.7220573  1.2790037
 1.5404102  2.015776   2.4826543  3.0020251  3.4367616  3.7902112
 3.8785548  3.8253202  3.6642063  3.7693653  4.0645833  4.323312
 4.4438744  4.555868   4.6735983  4.2553477  3.8820357  3.4125154
 3.211479   3.3464658  3.0637374  2.716342   2.8203228  2.8998072
 2.8612676  2.644435   2.3002605  2.3210845  2.3006616  2.056404
 1.7357455  1.3252267  0.8664073  1.0149261  1.8069733  2.3971927
 2.8197305  3.468158   3.014678   2.7631545  2.8491144  3.0410535
 2.8403213  2.7999773  2.8320525  2.7967224  2.633782   2.367492
 2.0036802  1.5465112  1.1909859  0.6926109  0.37607235 0.22096588
 0.4915392  1.3208814  2.073323   2.0357223  1.8920381  1.9892993
 2.0724454  1.4106537  1.3898485  1.7818623  2.2411878  2.7122989
 3.117118   3.4295506  3.6943681  3.6937602  3.551617   3.5649107
 3.6941056  4.077635   4.145558   4.177979   4.131112   3.8405914
 3.3045392  2.851358   2.5017402  2.6174166  2.4579616  2.2120779
 2.1810591  2.351991   2.4302654  2.3776505  2.041599   1.9071016
 2.0110278  1.8598487  1.6895698  1.3361404  0.7706353  0.58258784
 1.4566591  2.0521898  2.706837   3.4698534  3.1119256  2.712483
 2.773804   2.9419076  2.9425073  2.7176998  2.8168702  2.847459
 2.7233715  2.5143921  2.1222825  1.6558975  1.1556367  0.5901182
 0.4062438  0.30565718 0.09076173 0.7852675  1.7056173  1.9488571
 1.7080065  1.7233982  2.033167   1.6993526  1.382642   1.6194197
 2.031961   2.4202087  2.8161244  3.1395168  3.383849   3.556511
 3.422461   3.3865716  3.5029883  3.7400503  4.0340466  3.9559464
 3.8718183  3.6016788  3.2142076  2.8177674  2.1220896  1.8713001
 1.7648286  1.5925484  1.4788849  1.642165   1.8852193  2.0012727
 1.8385684  1.6523304  1.7423159  1.6439149  1.5919427  1.3411199
 0.82438517 0.22015646 1.0136857  1.6259819  2.4514172  3.289605
 3.3383467  2.762088   2.3465054  2.2712102  2.4880846  2.4607646
 2.6477635  2.8893123  2.8414018  2.6820111  2.3731976  1.8919756
 1.2944381  0.65245086 0.47906604 0.50853574 0.32979226 0.27259248
 1.2083322  1.6206714  1.586573   1.5045359  1.7224284  1.9972595
 1.5133187  1.5692238  1.8366773  2.0902314  2.4102283  2.8060448
 3.091841   3.1977553  3.2512527  3.1431057  3.2438107  3.390364
 3.8337038  4.026257   3.8535805  3.6131933  3.2258756  3.0738819
 2.6061225  1.7078882  1.186135   0.927568   0.8504051  0.7559307
 1.153564   1.4597441  1.512224   1.4033151  1.4963322  1.4395962
 1.3030256  1.1839299  0.8063717  0.2741169  0.80694723 1.3864485
 1.8795953  2.7693284  3.3668344  2.5144877  2.038979   1.8502116
 1.8263519  1.9874566  1.9636109  2.2973943  2.7525182  2.8430123
 2.649633   2.2334714  1.5637288  0.8891077  0.42160136 0.63162583
 0.62415767 0.27030796 0.58660144 1.4033284  1.5253286  1.4888967
 1.6317861  2.0186112  1.9474195  1.6288085  1.4611119  1.6072403
 1.8398201  2.218529   2.482012   2.6749778  2.7498882  2.808646
 2.9674604  3.028124   3.3483841  3.7279491 ]
