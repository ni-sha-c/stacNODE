time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 101.31%, model saved.
Epoch: 0 Train: 3918.55322 Test: 4226.64941
Epoch 100: New minimal relative error: 35.58%, model saved.
Epoch: 100 Train: 75.10994 Test: 63.45167
Epoch: 200 Train: 67.61169 Test: 35.69569
Epoch: 300 Train: 8.56135 Test: 7.04435
Epoch: 400 Train: 20.79428 Test: 20.70270
Epoch 500: New minimal relative error: 18.32%, model saved.
Epoch: 500 Train: 13.71902 Test: 10.46299
Epoch: 600 Train: 21.55000 Test: 21.49857
Epoch 700: New minimal relative error: 18.08%, model saved.
Epoch: 700 Train: 11.79029 Test: 13.25050
Epoch: 800 Train: 21.09899 Test: 24.05581
Epoch: 900 Train: 3.51188 Test: 3.70974
Epoch 1000: New minimal relative error: 15.20%, model saved.
Epoch: 1000 Train: 6.76383 Test: 9.35888
Epoch: 1100 Train: 8.96233 Test: 8.84490
Epoch: 1200 Train: 7.10016 Test: 6.47012
Epoch 1300: New minimal relative error: 11.32%, model saved.
Epoch: 1300 Train: 4.35815 Test: 4.54983
Epoch: 1400 Train: 2.63505 Test: 2.85769
Epoch 1500: New minimal relative error: 10.44%, model saved.
Epoch: 1500 Train: 0.70047 Test: 0.80516
Epoch: 1600 Train: 4.91846 Test: 4.97287
Epoch: 1700 Train: 3.38555 Test: 3.85490
Epoch 1800: New minimal relative error: 7.78%, model saved.
Epoch: 1800 Train: 1.28563 Test: 1.65792
Epoch: 1900 Train: 2.46269 Test: 4.74223
Epoch: 2000 Train: 0.76848 Test: 0.92274
Epoch: 2100 Train: 4.39728 Test: 3.66574
Epoch: 2200 Train: 9.33124 Test: 9.34673
Epoch: 2300 Train: 1.50009 Test: 1.62922
Epoch: 2400 Train: 4.06726 Test: 5.22025
Epoch: 2500 Train: 2.07138 Test: 2.00348
Epoch: 2600 Train: 0.42815 Test: 0.31009
Epoch: 2700 Train: 0.40379 Test: 0.49964
Epoch: 2800 Train: 0.71662 Test: 0.98234
Epoch: 2900 Train: 2.89667 Test: 2.43067
Epoch: 3000 Train: 3.79847 Test: 1.78047
Epoch: 3100 Train: 1.06575 Test: 1.01878
Epoch 3200: New minimal relative error: 4.44%, model saved.
Epoch: 3200 Train: 0.57790 Test: 0.65836
Epoch: 3300 Train: 0.67238 Test: 0.79566
Epoch: 3400 Train: 1.55266 Test: 1.68849
Epoch: 3500 Train: 0.80485 Test: 0.93934
Epoch: 3600 Train: 1.32391 Test: 1.24028
Epoch: 3700 Train: 0.25841 Test: 0.31012
Epoch: 3800 Train: 0.80647 Test: 0.54205
Epoch: 3900 Train: 0.43178 Test: 0.56267
Epoch: 4000 Train: 0.99243 Test: 0.93285
Epoch: 4100 Train: 0.46868 Test: 0.46452
Epoch: 4200 Train: 0.73448 Test: 0.62674
Epoch: 4300 Train: 4.82826 Test: 5.71999
Epoch: 4400 Train: 0.13009 Test: 0.14808
Epoch: 4500 Train: 0.14386 Test: 0.16984
Epoch 4600: New minimal relative error: 3.38%, model saved.
Epoch: 4600 Train: 0.59959 Test: 0.74297
Epoch 4700: New minimal relative error: 3.02%, model saved.
Epoch: 4700 Train: 0.08677 Test: 0.11298
Epoch 4800: New minimal relative error: 1.88%, model saved.
Epoch: 4800 Train: 0.08216 Test: 0.10822
Epoch: 4900 Train: 0.62632 Test: 0.88090
Epoch: 5000 Train: 1.64501 Test: 1.98816
Epoch: 5100 Train: 0.61711 Test: 0.88873
Epoch: 5200 Train: 0.08929 Test: 0.12599
Epoch: 5300 Train: 0.07128 Test: 0.09740
Epoch: 5400 Train: 0.35021 Test: 0.51930
Epoch: 5500 Train: 0.10617 Test: 0.14166
Epoch: 5600 Train: 0.21141 Test: 0.29039
Epoch: 5700 Train: 0.12123 Test: 0.10710
Epoch: 5800 Train: 0.12542 Test: 0.13897
Epoch: 5900 Train: 0.12038 Test: 0.14872
Epoch: 6000 Train: 0.06906 Test: 0.09673
Epoch: 6100 Train: 0.06831 Test: 0.09720
Epoch: 6200 Train: 0.06525 Test: 0.09922
Epoch: 6300 Train: 0.05709 Test: 0.08575
Epoch: 6400 Train: 0.10833 Test: 0.15254
Epoch: 6500 Train: 0.06197 Test: 0.08556
Epoch: 6600 Train: 0.05859 Test: 0.08561
Epoch: 6700 Train: 0.06560 Test: 0.09577
Epoch: 6800 Train: 0.10081 Test: 0.14367
Epoch: 6900 Train: 2.10903 Test: 2.00057
Epoch: 7000 Train: 0.42448 Test: 0.75045
Epoch: 7100 Train: 0.08495 Test: 0.11831
Epoch: 7200 Train: 0.15487 Test: 0.21215
Epoch: 7300 Train: 0.07369 Test: 0.10203
Epoch: 7400 Train: 0.06287 Test: 0.09145
Epoch: 7500 Train: 0.15456 Test: 0.23228
Epoch: 7600 Train: 0.04776 Test: 0.07555
Epoch: 7700 Train: 0.30874 Test: 0.38255
Epoch: 7800 Train: 1.41096 Test: 1.36088
Epoch: 7900 Train: 0.12196 Test: 0.17207
Epoch: 8000 Train: 0.14546 Test: 0.13567
Epoch: 8100 Train: 0.05697 Test: 0.10342
Epoch: 8200 Train: 0.08014 Test: 0.12317
Epoch: 8300 Train: 0.04589 Test: 0.06828
Epoch: 8400 Train: 1.12436 Test: 1.41630
Epoch: 8500 Train: 1.54162 Test: 1.62179
Epoch: 8600 Train: 0.05618 Test: 0.08835
Epoch: 8700 Train: 0.09379 Test: 0.13193
Epoch: 8800 Train: 0.10702 Test: 0.15705
Epoch: 8900 Train: 0.03686 Test: 0.06287
Epoch: 9000 Train: 0.12399 Test: 0.16376
Epoch: 9100 Train: 0.35349 Test: 0.47844
Epoch: 9200 Train: 0.04317 Test: 0.06955
Epoch: 9300 Train: 0.96484 Test: 1.20401
Epoch: 9400 Train: 0.03399 Test: 0.05968
Epoch: 9500 Train: 0.03411 Test: 0.06082
Epoch: 9600 Train: 0.03363 Test: 0.05955
Epoch: 9700 Train: 0.25747 Test: 0.33752
Epoch: 9800 Train: 0.10364 Test: 0.12413
Epoch: 9900 Train: 0.03646 Test: 0.06574
Epoch: 9999 Train: 0.03401 Test: 0.05922
Training Loss: tensor(0.0340)
Test Loss: tensor(0.0592)
Learned LE: [ 0.7932067   0.04501499 -3.154251  ]
True LE: [ 8.616853e-01  5.303202e-03 -1.454124e+01]
Relative Error: [0.7714942  0.5656241  0.70438725 0.8951631  1.131109   1.3100337
 1.5562332  1.2638906  1.1748139  1.280935   1.4477603  1.927105
 2.4718707  3.5589437  3.5554652  1.3500874  0.93914604 0.9641144
 0.4213257  0.82810205 0.95733714 0.97100455 0.94933176 0.8765284
 0.68851566 0.48361203 0.7112127  1.0089875  0.6300843  0.2549445
 0.4975995  0.7173526  0.76265603 0.50305426 0.26206183 0.4820207
 0.7299711  1.0889293  1.0348653  0.69752735 0.75544626 0.72377235
 0.36336702 0.32811844 0.49635735 0.7802917  0.6279746  0.36522582
 0.26083818 0.17646591 0.1519113  0.29306585 0.4036521  0.44858193
 0.41895598 0.5553362  0.33309    0.2576661  0.43499586 0.56823325
 0.75523907 0.82273483 0.93157005 0.5835358  0.5985668  0.8688785
 0.9947244  1.238237   1.619846   1.2164955  1.0394704  1.0180537
 1.333139   1.9258407  2.3848205  3.083021   3.162429   1.0841656
 0.82346815 0.92484725 0.29335535 0.7786867  0.7327328  0.8121279
 0.9679582  1.0253596  0.84654754 0.43983772 0.5118893  0.9542275
 0.66120213 0.4315922  0.2598974  0.32778275 0.52797693 0.5382801
 0.23616582 0.3073498  0.7455946  1.1397189  1.076629   0.81790423
 0.5930189  0.6653665  0.5642457  0.4540797  0.35942605 0.7001655
 0.7703799  0.31214207 0.22740334 0.2956819  0.27567607 0.33161813
 0.59242845 0.76546013 0.5843577  0.40845257 0.53942657 0.35683188
 0.4123223  0.5837723  0.6139798  0.7250725  0.7751577  0.7991645
 0.63290054 0.74684006 0.90906787 1.170256   1.5949411  1.4447567
 1.0955038  0.92814636 1.0142431  1.9309914  1.9747541  2.5123038
 3.0191483  1.1181059  0.8816221  0.8670115  0.47450447 0.34629858
 0.59467614 0.67363155 0.9053792  1.0568086  1.148316   0.813936
 0.29303098 0.64348525 0.81939137 0.47215644 0.36903796 0.34671748
 0.19711615 0.3624788  0.39312664 0.12049644 0.5672762  1.0868442
 1.1693616  1.0085232  0.52603114 0.48866543 0.5650257  0.63320035
 0.466805   0.4475635  0.72334516 0.515277   0.26303113 0.44284108
 0.5025887  0.5123478  0.7023597  0.8104162  0.6749058  0.43314254
 0.57163954 0.6339625  0.40369415 0.62299097 0.5915386  0.46238393
 0.7567322  0.8769373  0.81383055 0.65963805 0.8893216  1.0538774
 1.4361496  1.6987357  1.402201   1.1546316  0.874931   1.3941001
 1.7706401  1.8434048  2.547971   1.9997802  1.0351702  0.93139935
 1.0045878  0.61208403 0.55800724 0.69337094 0.72252357 0.8854752
 0.8553391  0.90480345 0.5832985  0.20532762 0.72109663 0.5936055
 0.45863023 0.62679315 0.43254915 0.22599599 0.31128868 0.3876133
 0.30946478 0.9663619  1.1922554  0.9467322  0.784996   0.29947466
 0.6620807  0.6998037  0.8470469  0.37676483 0.42694992 0.46340021
 0.32500988 0.55628175 0.70090324 0.6874716  0.4962928  0.61195207
 0.4724444  0.20071675 0.2770261  0.5098044  0.6565105  0.5806121
 0.54288554 0.36104882 0.40407792 0.81051534 0.9780059  0.7409767
 0.59054464 0.90159637 1.2322379  1.5649239  1.7995572  1.4278492
 1.1628196  0.7323924  1.2522193  1.6067817  1.5525956  2.0411835
 0.8064434  1.2032913  1.0329189  1.3797405  0.28991136 0.42821673
 0.38859692 0.48808718 0.5730134  0.53005207 0.592085   0.38479626
 0.25305593 0.70748985 0.49921638 0.35192692 0.8362768  0.5573241
 0.24884443 0.37419564 0.57904756 0.41643795 1.1903033  0.9218311
 0.5396976  0.29222748 0.29491162 0.29072097 0.56945294 0.99430627
 0.2775925  0.309432   0.37260467 0.6785046  0.6747223  0.7508088
 0.7208336  0.52489924 0.4353684  0.29385117 0.09117336 0.60187805
 0.7856121  0.7645577  0.64015454 0.32975182 0.1878467  0.47598654
 0.8005184  0.98835284 0.6983719  0.48145753 0.91343296 1.3991864
 1.7402148  2.1207821  1.672598   1.2935971  0.62331736 0.8828056
 1.3598107  1.2831984  1.2619153  1.3622669  1.4966238  1.1262579
 1.0882193  0.30989763 0.36522263 0.23365456 0.35183606 0.24191667
 0.4203927  0.49811885 0.48009536 0.3782823  0.5336488  0.37603614
 0.17735136 0.6474052  0.81339085 0.34176606 0.43612587 0.71343994
 0.4818528  0.7563804  0.5758579  0.49867794 0.13046065 0.5419272
 0.2244271  0.56181026 0.8881678  0.2744636  0.37100193 0.70346576
 1.0075091  0.86041176 0.76670027 0.75383604 0.5612164  0.49043885
 0.27984762 0.34631255 0.9117915  1.1287233  0.87854254 0.62649155
 0.15329073 0.26083553 0.6974831  1.0274497  1.1459383  0.966141
 0.5140961  0.69063586 1.109099   1.6080565  2.2761724  2.2846785
 1.1087916  0.7577065  1.1510239  1.6063144  1.5663656  1.0144182
 1.6127665  1.9535459  1.4647844  1.0129567  0.7351673  0.23134048
 0.27319643 0.3595408  0.74327236 0.8707598 ]
