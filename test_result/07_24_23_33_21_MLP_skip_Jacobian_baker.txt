time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 6000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 512
n_layers: 4
reg_param: 700.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 1003.236450195 Test: 19.950225830
Epoch 0: New minimal relative error: 19.95%, model saved.
Epoch: 60 Train: 14.416166306 Test: 9.424107552
Epoch 60: New minimal relative error: 9.42%, model saved.
Epoch: 120 Train: 16.501487732 Test: 7.582591534
Epoch 120: New minimal relative error: 7.58%, model saved.
Epoch: 180 Train: 12.129478455 Test: 7.719964981
Epoch: 240 Train: 11.044159889 Test: 7.033852100
Epoch 240: New minimal relative error: 7.03%, model saved.
Epoch: 300 Train: 9.858986855 Test: 6.495704174
Epoch 300: New minimal relative error: 6.50%, model saved.
Epoch: 360 Train: 9.955778122 Test: 6.535118103
Epoch: 420 Train: 9.859878540 Test: 5.998013020
Epoch 420: New minimal relative error: 6.00%, model saved.
Epoch: 480 Train: 8.802309990 Test: 6.018399715
Epoch: 540 Train: 9.705656052 Test: 5.912765980
Epoch 540: New minimal relative error: 5.91%, model saved.
Epoch: 600 Train: 8.861696243 Test: 6.069061756
Epoch: 660 Train: 8.449583054 Test: 6.029184818
Epoch: 720 Train: 8.755344391 Test: 6.003282070
Epoch: 780 Train: 8.483450890 Test: 6.034673214
Epoch: 840 Train: 8.365893364 Test: 6.020799160
Epoch: 900 Train: 8.409997940 Test: 6.062778473
Epoch: 960 Train: 9.249539375 Test: 6.103772163
Epoch: 1020 Train: 8.427997589 Test: 6.053465366
Epoch: 1080 Train: 8.423795700 Test: 6.055777073
Epoch: 1140 Train: 8.563095093 Test: 6.104764462
Epoch: 1200 Train: 8.218518257 Test: 6.045611382
Epoch: 1260 Train: 8.195619583 Test: 6.036091805
Epoch: 1320 Train: 8.565498352 Test: 6.034009933
Epoch: 1380 Train: 8.681823730 Test: 6.033520222
Epoch: 1440 Train: 8.405252457 Test: 6.040506840
Epoch: 1500 Train: 8.358928680 Test: 6.054715633
Epoch: 1560 Train: 8.229738235 Test: 6.051529408
Epoch: 1620 Train: 8.288785934 Test: 6.067387104
Epoch: 1680 Train: 8.121851921 Test: 6.019168854
Epoch: 1740 Train: 8.322686195 Test: 6.000552654
Epoch: 1800 Train: 8.444929123 Test: 6.013303280
Epoch: 1860 Train: 8.435098648 Test: 6.011747837
Epoch: 1920 Train: 8.129656792 Test: 6.021727085
Epoch: 1980 Train: 8.030691147 Test: 6.012663841
Epoch: 2040 Train: 8.242733002 Test: 6.028003216
Epoch: 2100 Train: 8.084712982 Test: 6.022688866
Epoch: 2160 Train: 8.190036774 Test: 6.044896126
Epoch: 2220 Train: 8.233678818 Test: 6.060721874
Epoch: 2280 Train: 8.327847481 Test: 6.066478729
Epoch: 2340 Train: 8.294860840 Test: 6.077059269
Epoch: 2400 Train: 8.327711105 Test: 6.081235409
Epoch: 2460 Train: 8.236717224 Test: 6.075980186
Epoch: 2520 Train: 8.280995369 Test: 6.088997841
Epoch: 2580 Train: 8.234001160 Test: 6.068584442
Epoch: 2640 Train: 8.234085083 Test: 6.063768864
Epoch: 2700 Train: 8.236450195 Test: 6.070646763
Epoch: 2760 Train: 8.287071228 Test: 6.075780869
Epoch: 2820 Train: 8.232833862 Test: 6.078115940
Epoch: 2880 Train: 8.259626389 Test: 6.073909283
Epoch: 2940 Train: 8.235043526 Test: 6.063647747
Epoch: 3000 Train: 8.048452377 Test: 6.046313763
Epoch: 3060 Train: 8.106386185 Test: 6.062809944
Epoch: 3120 Train: 8.217823029 Test: 6.068049431
Epoch: 3180 Train: 8.142436028 Test: 6.056883812
Epoch: 3240 Train: 8.188294411 Test: 6.046327591
Epoch: 3300 Train: 8.275920868 Test: 6.029599190
Epoch: 3360 Train: 8.155441284 Test: 6.017004967
Epoch: 3420 Train: 8.118803978 Test: 6.030620575
Epoch: 3480 Train: 7.992426872 Test: 6.016611099
Epoch: 3540 Train: 7.979451656 Test: 6.025691509
Epoch: 3600 Train: 8.206039429 Test: 6.029701233
Epoch: 3660 Train: 8.139542580 Test: 6.032500744
Epoch: 3720 Train: 8.072433472 Test: 6.037693024
Epoch: 3780 Train: 8.086340904 Test: 6.038859844
Epoch: 3840 Train: 8.186486244 Test: 6.044426918
Epoch: 3900 Train: 8.337385178 Test: 6.062596798
Epoch: 3960 Train: 8.601582527 Test: 6.062452793
Epoch: 4020 Train: 8.343988419 Test: 6.063281536
Epoch: 4080 Train: 8.338779449 Test: 6.070645809
Epoch: 4140 Train: 8.428099632 Test: 6.086651325
Epoch: 4200 Train: 8.823970795 Test: 6.096811771
Epoch: 4260 Train: 8.267568588 Test: 6.089685440
Epoch: 4320 Train: 8.245503426 Test: 6.082257271
Epoch: 4380 Train: 8.250075340 Test: 6.084069729
Epoch: 4440 Train: 8.233683586 Test: 6.080880165
Epoch: 4500 Train: 8.235223770 Test: 6.077692032
Epoch: 4560 Train: 8.258073807 Test: 6.077980518
Epoch: 4620 Train: 8.139037132 Test: 6.071694374
Epoch: 4680 Train: 8.203087807 Test: 6.070415974
Epoch: 4740 Train: 7.980996132 Test: 6.059972763
Epoch: 4800 Train: 8.095244408 Test: 6.060532570
Epoch: 4860 Train: 8.102373123 Test: 6.066781521
Epoch: 4920 Train: 8.008959770 Test: 6.077390194
Epoch: 4980 Train: 8.000436783 Test: 6.084603786
Epoch: 5040 Train: 8.139291763 Test: 6.094595432
Epoch: 5100 Train: 8.337944984 Test: 6.089865685
Epoch: 5160 Train: 8.695274353 Test: 6.031058788
Epoch: 5220 Train: 8.340991974 Test: 6.066144466
Epoch: 5280 Train: 8.249500275 Test: 6.039799690
Epoch: 5340 Train: 8.072298050 Test: 6.023756504
Epoch: 5400 Train: 8.094467163 Test: 6.028736115
Epoch: 5460 Train: 8.180348396 Test: 6.033071995
Epoch: 5520 Train: 8.244056702 Test: 6.040627956
Epoch: 5580 Train: 8.415449142 Test: 6.039867878
Epoch: 5640 Train: 8.503182411 Test: 6.052465439
Epoch: 5700 Train: 8.545900345 Test: 6.063418865
Epoch: 5760 Train: 8.407371521 Test: 6.059257507
Epoch: 5820 Train: 8.277902603 Test: 6.055957794
Epoch: 5880 Train: 8.162342072 Test: 6.054444790
Epoch: 5940 Train: 8.217918396 Test: 6.047896862
Epoch: 5999 Train: 8.040248871 Test: 6.051691055
Training Loss: tensor(8.0402)
Test Loss: tensor(6.0517)
True Mean x: tensor(3.4447, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.5065, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(nan, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0029)
Jacobian term Test Loss: tensor(6.6605e-05)
Learned LE: [1.7037654  0.32290253]
True LE: tensor([ 0.6932, -0.7446], dtype=torch.float64)
