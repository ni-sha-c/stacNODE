time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.19%, model saved.
Epoch: 0 Train: 9367.31641 Test: 4104.77539
Epoch 100: New minimal relative error: 95.90%, model saved.
Epoch: 100 Train: 2637.72412 Test: 1161.20386
Epoch: 200 Train: 2495.41895 Test: 1051.70435
Epoch: 300 Train: 1519.80469 Test: 407.47919
Epoch: 400 Train: 557.06158 Test: 113.01243
Epoch 500: New minimal relative error: 36.66%, model saved.
Epoch: 500 Train: 340.01233 Test: 41.44181
Epoch 600: New minimal relative error: 32.80%, model saved.
Epoch: 600 Train: 342.22385 Test: 100.22654
Epoch 700: New minimal relative error: 26.47%, model saved.
Epoch: 700 Train: 178.04736 Test: 19.30913
Epoch 800: New minimal relative error: 23.96%, model saved.
Epoch: 800 Train: 124.44983 Test: 16.46063
Epoch: 900 Train: 93.63510 Test: 12.71143
Epoch: 1000 Train: 92.42047 Test: 14.13947
Epoch: 1100 Train: 60.86116 Test: 10.23010
Epoch: 1200 Train: 60.54691 Test: 9.30074
Epoch 1300: New minimal relative error: 12.00%, model saved.
Epoch: 1300 Train: 62.04654 Test: 15.07876
Epoch 1400: New minimal relative error: 9.54%, model saved.
Epoch: 1400 Train: 65.60178 Test: 12.71400
Epoch: 1500 Train: 43.20275 Test: 2.62513
Epoch: 1600 Train: 60.24968 Test: 18.76068
Epoch: 1700 Train: 52.30814 Test: 11.92467
Epoch 1800: New minimal relative error: 4.42%, model saved.
Epoch: 1800 Train: 37.99055 Test: 2.15962
Epoch: 1900 Train: 36.22800 Test: 4.17931
Epoch: 2000 Train: 33.24907 Test: 3.86854
Epoch: 2100 Train: 36.47322 Test: 7.34870
Epoch: 2200 Train: 35.58935 Test: 6.29586
Epoch 2300: New minimal relative error: 4.34%, model saved.
Epoch: 2300 Train: 25.25886 Test: 1.58661
Epoch: 2400 Train: 28.00643 Test: 5.27713
Epoch: 2500 Train: 39.72023 Test: 12.38750
Epoch: 2600 Train: 31.50738 Test: 8.67930
Epoch: 2700 Train: 26.93386 Test: 2.09214
Epoch: 2800 Train: 23.95862 Test: 4.76780
Epoch: 2900 Train: 28.29778 Test: 8.16043
Epoch: 3000 Train: 18.41211 Test: 1.12547
Epoch: 3100 Train: 26.78035 Test: 5.42244
Epoch 3200: New minimal relative error: 2.82%, model saved.
Epoch: 3200 Train: 17.36281 Test: 1.02457
Epoch: 3300 Train: 18.32734 Test: 2.68157
Epoch: 3400 Train: 18.56143 Test: 1.73640
Epoch: 3500 Train: 17.50431 Test: 1.70866
Epoch: 3600 Train: 19.18318 Test: 3.11407
Epoch: 3700 Train: 19.39251 Test: 3.03478
Epoch: 3800 Train: 19.70089 Test: 2.59833
Epoch: 3900 Train: 15.96050 Test: 1.58069
Epoch: 4000 Train: 14.96557 Test: 0.70949
Epoch: 4100 Train: 15.14138 Test: 1.92048
Epoch: 4200 Train: 19.97605 Test: 5.35803
Epoch: 4300 Train: 17.62459 Test: 3.50968
Epoch: 4400 Train: 15.20879 Test: 1.39446
Epoch: 4500 Train: 13.23411 Test: 0.82746
Epoch: 4600 Train: 20.60118 Test: 5.06383
Epoch: 4700 Train: 13.56036 Test: 1.83805
Epoch: 4800 Train: 14.85812 Test: 0.94813
Epoch: 4900 Train: 13.83845 Test: 2.31681
Epoch: 5000 Train: 14.86650 Test: 3.01504
Epoch: 5100 Train: 12.95651 Test: 1.37857
Epoch 5200: New minimal relative error: 2.24%, model saved.
Epoch: 5200 Train: 14.82175 Test: 2.25677
Epoch: 5300 Train: 14.60560 Test: 0.95997
Epoch: 5400 Train: 11.76569 Test: 0.98334
Epoch: 5500 Train: 11.30359 Test: 0.37253
Epoch: 5600 Train: 12.57383 Test: 1.89067
Epoch: 5700 Train: 14.06314 Test: 1.61465
Epoch 5800: New minimal relative error: 1.95%, model saved.
Epoch: 5800 Train: 11.16089 Test: 0.33746
Epoch: 5900 Train: 12.05657 Test: 0.87706
Epoch: 6000 Train: 14.66633 Test: 3.03823
Epoch 6100: New minimal relative error: 1.91%, model saved.
Epoch: 6100 Train: 10.55627 Test: 0.41607
Epoch: 6200 Train: 10.19990 Test: 0.35591
Epoch: 6300 Train: 11.63196 Test: 1.74159
Epoch: 6400 Train: 10.87948 Test: 1.97384
Epoch: 6500 Train: 10.74249 Test: 1.57773
Epoch: 6600 Train: 14.52278 Test: 4.72905
Epoch: 6700 Train: 8.90243 Test: 0.22315
Epoch: 6800 Train: 9.08013 Test: 0.26407
Epoch: 6900 Train: 9.34310 Test: 1.01430
Epoch: 7000 Train: 10.02958 Test: 0.77375
Epoch: 7100 Train: 8.89662 Test: 0.34407
Epoch: 7200 Train: 9.13227 Test: 0.54727
Epoch: 7300 Train: 9.66376 Test: 1.60672
Epoch: 7400 Train: 8.40002 Test: 0.37920
Epoch: 7500 Train: 8.61956 Test: 0.77600
Epoch: 7600 Train: 12.06674 Test: 1.40039
Epoch: 7700 Train: 7.94298 Test: 0.34356
Epoch: 7800 Train: 8.01394 Test: 0.36209
Epoch: 7900 Train: 8.97528 Test: 0.86978
Epoch 8000: New minimal relative error: 1.02%, model saved.
Epoch: 8000 Train: 7.86584 Test: 0.20272
Epoch: 8100 Train: 7.88921 Test: 0.20231
Epoch 8200: New minimal relative error: 0.90%, model saved.
Epoch: 8200 Train: 7.94137 Test: 0.20517
Epoch: 8300 Train: 8.15255 Test: 0.37728
Epoch: 8400 Train: 7.74641 Test: 0.36230
Epoch: 8500 Train: 7.58205 Test: 0.23542
Epoch: 8600 Train: 7.50209 Test: 0.26601
Epoch: 8700 Train: 7.43390 Test: 0.24492
Epoch: 8800 Train: 7.44650 Test: 0.23127
Epoch: 8900 Train: 9.60959 Test: 1.50857
Epoch: 9000 Train: 7.20124 Test: 0.33084
Epoch: 9100 Train: 8.61106 Test: 0.79146
Epoch: 9200 Train: 7.11731 Test: 0.22006
Epoch: 9300 Train: 7.14181 Test: 0.40751
Epoch: 9400 Train: 7.11337 Test: 0.20746
Epoch: 9500 Train: 8.04067 Test: 1.46826
Epoch: 9600 Train: 7.16736 Test: 0.54033
Epoch: 9700 Train: 6.78522 Test: 0.24054
Epoch: 9800 Train: 6.75811 Test: 0.20911
Epoch: 9900 Train: 7.03194 Test: 0.26385
Epoch: 9999 Train: 8.20569 Test: 0.78296
Training Loss: tensor(8.2057)
Test Loss: tensor(0.7830)
Learned LE: [  0.88856983  -0.03341175 -14.483409  ]
True LE: [ 8.6566323e-01 -2.7890876e-03 -1.4537115e+01]
Relative Error: [0.6118769  0.48062494 0.42134348 0.49241805 0.6837715  0.86596245
 0.85155696 0.8345905  0.76088065 0.63932264 0.5695516  0.45781764
 0.38921338 0.42605913 0.4784809  0.5710534  0.6310609  0.59266216
 0.48396274 0.29481393 0.18435608 0.12339685 0.2144367  0.4857549
 0.69139826 0.70867324 0.5709935  0.31537858 0.29588422 0.41636372
 0.35182124 0.38744012 0.3883458  0.2950719  0.20861767 0.15140644
 0.14595526 0.30710027 0.35815746 0.3862911  0.5795315  0.6689788
 0.44358307 0.44529754 0.57294583 0.53369623 0.67986876 0.83317506
 0.6916973  0.41289982 0.3317551  0.41663525 0.47149318 0.44819158
 0.44337398 0.44450542 0.43663228 0.53437793 0.74483895 0.96179515
 0.63392407 0.44904897 0.42963076 0.32132298 0.30978042 0.45210296
 0.679376   0.8151366  0.7805813  0.7743638  0.76423985 0.6141905
 0.61195254 0.5326303  0.4658787  0.33505386 0.31739807 0.45558935
 0.57374483 0.55299455 0.37084126 0.27889806 0.1296049  0.03397783
 0.16158023 0.32796568 0.50916255 0.5848183  0.4777933  0.23932242
 0.24147797 0.4914365  0.4995816  0.40496448 0.43175703 0.37881696
 0.31299278 0.25373375 0.13035911 0.2797855  0.34159672 0.44013327
 0.5595371  0.60285914 0.49827528 0.28771242 0.43316618 0.40025237
 0.5848399  0.8065366  0.7232559  0.47355965 0.29509652 0.35642752
 0.31946608 0.25608447 0.35054377 0.4312987  0.33721632 0.4511714
 0.73420846 0.8808078  0.57213664 0.3525682  0.29307052 0.27595755
 0.24585125 0.370183   0.51757777 0.6603405  0.73432565 0.77961856
 0.71057934 0.5851418  0.52093244 0.49180794 0.42723525 0.4002917
 0.33359662 0.41125858 0.45464775 0.439308   0.3219076  0.3007606
 0.21034358 0.0914321  0.17900416 0.19683193 0.27081123 0.4614144
 0.45372787 0.22548805 0.18623495 0.55784523 0.61388475 0.48133278
 0.5591769  0.47322768 0.44112486 0.3543155  0.21885121 0.16168551
 0.29177096 0.44361055 0.43799233 0.36074054 0.4486015  0.25818896
 0.29143712 0.30749172 0.48972863 0.79261374 0.7404304  0.5438342
 0.30877936 0.34801328 0.1940059  0.12617247 0.22765863 0.29990944
 0.26281226 0.31668094 0.5824486  0.75758886 0.52182364 0.3176402
 0.27141845 0.3114586  0.21384004 0.28931653 0.42107004 0.47851956
 0.5733368  0.67483324 0.61874783 0.54970205 0.423606   0.37362552
 0.25112194 0.38414347 0.4899314  0.53605497 0.515344   0.5080616
 0.36591917 0.21155642 0.2447797  0.22491528 0.10059522 0.28392565
 0.14895502 0.31552905 0.436211   0.3285034  0.12150783 0.502536
 0.5608784  0.5607862  0.7132002  0.64655095 0.6068993  0.5091378
 0.3404507  0.21428682 0.29800475 0.381141   0.4764879  0.38861787
 0.2820525  0.24856417 0.222465   0.2911428  0.3776601  0.7241747
 0.7958358  0.6498921  0.41399607 0.39056814 0.14414743 0.03715383
 0.18265744 0.28525883 0.29462257 0.18863085 0.3519415  0.6101835
 0.53244525 0.32869685 0.22554293 0.26287922 0.2258722  0.2515751
 0.3548912  0.4102534  0.3937939  0.4761864  0.57081395 0.5525659
 0.4238766  0.20043217 0.30841416 0.4861579  0.65818983 0.6468044
 0.64842266 0.5503121  0.5304313  0.42422727 0.34592304 0.27020302
 0.19061914 0.14142163 0.33844867 0.08495618 0.3202738  0.35300007
 0.20536242 0.31877777 0.51072866 0.60668725 0.7179581  0.75352156
 0.7051918  0.7275988  0.5223533  0.2906768  0.21707605 0.2958442
 0.45810187 0.52166253 0.3215495  0.3146599  0.22851017 0.16072606
 0.27735692 0.5602317  0.8230526  0.7379899  0.5546668  0.548104
 0.18504329 0.12181328 0.2423488  0.33496845 0.26063287 0.1846045
 0.1418221  0.4614652  0.6064737  0.40833732 0.17417747 0.16400035
 0.24368069 0.2744412  0.31274354 0.32510337 0.31841147 0.3558185
 0.45512328 0.51013553 0.42111868 0.30032352 0.39010146 0.5430564
 0.69243383 0.7497552  0.68990743 0.7158156  0.55370224 0.48730657
 0.47838703 0.48600435 0.28347975 0.21554005 0.18027663 0.24009815
 0.11694976 0.31240866 0.3019421  0.21213914 0.448228   0.5454409
 0.7248143  0.78291523 0.68091613 0.7510021  0.7280868  0.44138968
 0.22403693 0.17772253 0.31861514 0.46639583 0.40650567 0.25133592
 0.22066604 0.14932796 0.26911435 0.45588052 0.72584015 0.81307125
 0.6268116  0.5943007  0.41769734 0.05081278 0.2740529  0.3863063
 0.3073289  0.1964289  0.11271866 0.24024495 0.6220789  0.5190276
 0.29850283 0.17956953 0.23988941 0.3182045  0.34478617 0.32524434
 0.23983464 0.31647784 0.3579899  0.4178876  0.37753022 0.50232315
 0.4912863  0.57647234 0.645196   0.73898745 0.7119845  0.7212372
 0.6282618  0.44299826 0.42587128 0.50626194 0.3929453  0.3400188
 0.31262004 0.24118416 0.09022923 0.17030284]
