time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.84%, model saved.
Epoch: 0 Train: 31637.35547 Test: 4162.59033
Epoch 80: New minimal relative error: 76.22%, model saved.
Epoch: 80 Train: 4971.42529 Test: 610.61127
Epoch 160: New minimal relative error: 33.97%, model saved.
Epoch: 160 Train: 272.88406 Test: 14.91115
Epoch: 240 Train: 1189.55725 Test: 601.77332
Epoch 320: New minimal relative error: 9.51%, model saved.
Epoch: 320 Train: 38.71916 Test: 19.50321
Epoch 400: New minimal relative error: 5.69%, model saved.
Epoch: 400 Train: 13.73437 Test: 0.16124
Epoch: 480 Train: 24.07171 Test: 1.80408
Epoch: 560 Train: 35.74392 Test: 6.70270
Epoch: 640 Train: 62.01773 Test: 15.03748
Epoch: 720 Train: 28.77798 Test: 21.94681
Epoch: 800 Train: 3.75940 Test: 4.05688
Epoch: 880 Train: 39.01376 Test: 9.43077
Epoch: 960 Train: 6.08561 Test: 0.38932
Epoch: 1040 Train: 15.13968 Test: 2.28066
Epoch: 1120 Train: 27.33960 Test: 11.24171
Epoch: 1200 Train: 20.77204 Test: 12.10017
Epoch: 1280 Train: 17.80941 Test: 3.71128
Epoch: 1360 Train: 1.72287 Test: 0.08006
Epoch 1440: New minimal relative error: 0.46%, model saved.
Epoch: 1440 Train: 2.53429 Test: 0.01902
Epoch: 1520 Train: 18.02390 Test: 4.89626
Epoch: 1600 Train: 13.01315 Test: 7.79458
Epoch: 1680 Train: 8.83496 Test: 1.41300
Epoch: 1760 Train: 4.93436 Test: 2.78985
Epoch: 1840 Train: 13.10127 Test: 7.17003
Epoch: 1920 Train: 16.88430 Test: 6.55310
Epoch: 2000 Train: 1.80968 Test: 0.03707
Epoch: 2080 Train: 0.37312 Test: 0.10207
Epoch: 2160 Train: 5.21259 Test: 2.68923
Epoch: 2240 Train: 2.31187 Test: 1.08959
Epoch: 2320 Train: 14.36104 Test: 5.93407
Epoch: 2400 Train: 0.65730 Test: 0.31511
Epoch: 2480 Train: 1.34190 Test: 0.16531
Epoch: 2560 Train: 6.65690 Test: 3.40379
Epoch: 2640 Train: 16.73108 Test: 8.02194
Epoch: 2720 Train: 0.53963 Test: 0.19512
Epoch: 2800 Train: 7.12469 Test: 3.84662
Epoch: 2880 Train: 14.42685 Test: 4.58439
Epoch: 2960 Train: 4.04218 Test: 1.81218
Epoch: 3040 Train: 11.63940 Test: 4.49958
Epoch: 3120 Train: 48.47891 Test: 16.21167
Epoch: 3200 Train: 0.24112 Test: 0.05062
Epoch: 3280 Train: 2.22603 Test: 0.29068
Epoch: 3360 Train: 2.88488 Test: 1.47385
Epoch: 3440 Train: 2.89817 Test: 1.71919
Epoch: 3520 Train: 1.28298 Test: 0.57953
Epoch: 3600 Train: 0.49770 Test: 0.13464
Epoch: 3680 Train: 0.35724 Test: 0.05138
Epoch: 3760 Train: 7.73633 Test: 3.51146
Epoch: 3840 Train: 1.71407 Test: 0.71656
Epoch: 3920 Train: 1.89842 Test: 0.81297
Epoch: 4000 Train: 0.15244 Test: 0.04432
Epoch: 4080 Train: 0.45508 Test: 0.07900
Epoch: 4160 Train: 3.38011 Test: 1.35193
Epoch: 4240 Train: 0.17562 Test: 0.08862
Epoch: 4320 Train: 0.49981 Test: 0.20838
Epoch: 4400 Train: 0.69305 Test: 0.28007
Epoch: 4480 Train: 0.95903 Test: 0.20470
Epoch: 4560 Train: 0.94303 Test: 0.38383
Epoch: 4640 Train: 2.15449 Test: 0.82290
Epoch: 4720 Train: 2.59425 Test: 0.60606
Epoch: 4800 Train: 1.93695 Test: 0.77366
Epoch: 4880 Train: 3.34621 Test: 1.31823
Epoch: 4960 Train: 2.94388 Test: 0.74045
Epoch: 5040 Train: 1.71869 Test: 0.46411
Epoch: 5120 Train: 0.66637 Test: 0.23581
Epoch: 5200 Train: 1.96422 Test: 0.97785
Epoch: 5280 Train: 4.43450 Test: 0.38879
Epoch 5360: New minimal relative error: 0.37%, model saved.
Epoch: 5360 Train: 0.04927 Test: 0.00626
Epoch: 5440 Train: 0.28286 Test: 0.02795
Epoch: 5520 Train: 0.45899 Test: 0.17887
Epoch: 5600 Train: 0.92439 Test: 0.36656
Epoch: 5680 Train: 3.26732 Test: 1.19373
Epoch: 5760 Train: 3.68064 Test: 1.11892
Epoch: 5840 Train: 1.43354 Test: 0.39785
Epoch: 5920 Train: 4.23376 Test: 1.21659
Epoch: 6000 Train: 1.00386 Test: 0.37858
Epoch: 6080 Train: 1.21579 Test: 0.43218
Epoch: 6160 Train: 1.37621 Test: 0.53590
Epoch: 6240 Train: 0.52827 Test: 0.15889
Epoch: 6320 Train: 2.54056 Test: 1.21404
Epoch: 6400 Train: 0.07449 Test: 0.02735
Epoch: 6480 Train: 0.30007 Test: 0.12973
Epoch: 6560 Train: 1.15048 Test: 0.46547
Epoch: 6640 Train: 0.30887 Test: 0.07819
Epoch: 6720 Train: 0.15361 Test: 0.04663
Epoch: 6800 Train: 0.60004 Test: 0.24736
Epoch: 6880 Train: 2.84128 Test: 1.40609
Epoch: 6960 Train: 0.03892 Test: 0.00644
Epoch: 7040 Train: 0.10651 Test: 0.04271
Epoch: 7120 Train: 0.47899 Test: 0.17751
Epoch: 7200 Train: 2.65844 Test: 0.80969
Epoch: 7280 Train: 1.66247 Test: 0.68466
Epoch: 7360 Train: 2.21336 Test: 0.90540
Epoch: 7440 Train: 1.53985 Test: 0.42166
Epoch: 7520 Train: 0.48996 Test: 0.16022
Epoch: 7600 Train: 0.79920 Test: 0.14937
Epoch: 7680 Train: 0.15132 Test: 0.05430
Epoch: 7760 Train: 0.44485 Test: 0.12670
Epoch: 7840 Train: 0.11833 Test: 0.04928
Epoch 7920: New minimal relative error: 0.22%, model saved.
Epoch: 7920 Train: 0.01836 Test: 0.00200
Epoch: 7999 Train: 0.40658 Test: 0.17441
Training Loss: tensor(0.4066)
Test Loss: tensor(0.1744)
Learned LE: [  0.8700715   -0.03105616 -14.557338  ]
True LE: [ 8.7585020e-01  4.4326589e-04 -1.4548204e+01]
Relative Error: [7.364628  7.448223  7.5322046 7.6112432 7.679127  7.7294917 7.7559752
 7.7527084 7.714843  7.639476  7.5273104 7.3808007 7.2059064 7.0099635
 6.8013034 6.5877943 6.376204  6.172249  5.980393  5.804618  5.6487246
 5.5159245 5.4096045 5.332707  5.287426  5.275799  5.298158  5.3541775
 5.4424806 5.5600176 5.7039785 5.8715835 6.060108  6.2688904 6.497524
 6.744626  7.0070915 7.277258  7.5421834 7.784111  7.983496  8.122382
 8.188924  8.180248  8.102126  7.9669394 7.792283  7.595551  7.393252
 7.199004  7.022733  6.8704686 6.745562  6.648766  6.579602  6.536808
 6.518422  6.5224557 6.5464816 6.5880103 6.6438923 6.7113204 6.7861276
 6.864048  6.940508  7.0099907 7.0669746 7.1058035 7.120967  7.1073055
 7.0610175 6.980485  6.8662906 6.721507  6.5519786 6.364613  6.1669993
 5.9663453 5.768409  5.578028  5.399166  5.235589  5.0903974 4.9673715
 4.8697577 4.800544  4.762325  4.75673   4.7843943 4.844333  4.9346547
 5.0518794 5.192526  5.3525457 5.530005  5.7242827 5.9359465 6.165983
 6.4124403 6.6690407 6.9230614 7.1566434 7.3488636 7.4809337 7.540804
 7.5253615 7.441312  7.3026547 7.127154  6.9331245 6.7371664 6.552096
 6.3873267 6.247819  6.135821  6.05148   5.993576  5.960406  5.9499364
 5.960059  5.9881663 6.031834  6.088452  6.1542416 6.2257743 6.298417
 6.36782   6.4287252 6.4762034 6.505122  6.5107694 6.4890018 6.4367285
 6.3529596 6.238709  6.0972357 5.9338293 5.7553663 5.5687075 5.380381
 5.1951103 5.0172925 4.85022   4.697243  4.5615478 4.446939  4.3566723
 4.294239  4.261885  4.261531  4.293519  4.356951  4.4489017 4.5656343
 4.702351  4.8546247 5.0200367 5.198522  5.392501  5.6046286 5.834887
 6.078225  6.32263   6.5491147 6.735796  6.8627543 6.916643  6.8949394
 6.805318  6.66294   6.4864173 6.2949905 6.1052446 5.929506  5.776023
 5.648997  5.549363  5.47666   5.4290304 5.4045463 5.40089   5.4159055
 5.44746   5.4929876 5.549568  5.6139216 5.682193  5.75025   5.813419
 5.8670483 5.906249  5.9270654 5.9250913 5.8971925 5.8411994 5.756306
 5.6436157 5.5064416 5.3499794 5.180575  5.004493  4.8276567 4.654318
 4.4880075 4.3317547 4.1883097 4.061075  3.9536674 3.869817  3.8127434
 3.7852607 3.7890687 3.824775  3.8908315 3.9842923 4.10024   4.233
 4.377155  4.530009  4.692162  4.8672633 5.0603313 5.2736907 5.5039835
 5.7393217 5.9602823 6.1431093 6.266198  6.315238  6.287837  6.192658
 6.0463834 5.868993  5.6801715 5.496628  5.3302083 5.188078  5.073138
 4.9855065 4.923513  4.8851004 4.8680534 4.8701053 4.889201  4.923103
 4.969637  5.0260363 5.0889025 5.1546044 5.218528  5.2765064 5.323978
 5.356697  5.3708673 5.363378  5.3314805 5.2737184 5.1895175 5.0800095
 4.9483147 4.7991796 4.638591  4.4727736 4.306832  4.144688  3.9891694
 3.842708  3.7079647 3.5882485 3.4872532 3.4086666 3.3560255 3.3321135
 3.3390276 3.3770752 3.4450798 3.5395682 3.6548893 3.7838023 3.9201562
 4.0604935 4.2054663 4.3606453 4.5334225 4.7290487 4.945703  5.17244
 5.388775  5.5692625 5.6896434 5.7351665 5.7025633 5.6023316 5.4522815
 5.274067  5.0881734 4.911172  4.7542    4.6234245 4.5202384 4.443742
 4.3916526 4.361478  4.3507094 4.3571177 4.3789673 4.4143248 4.4612136
 4.516839  4.5783715 4.6417146 4.702518  4.7563934 4.799064  4.8267083
 4.8362074 4.825008  4.7913384 4.733877  4.652137  4.547141  4.4215693
 4.280121  4.1282806 3.972099  3.816504  3.66492   3.5197098 3.3825307
 3.2559521 3.1430104 3.0475612 2.973401  2.924147  2.9026623 2.9111228
 2.9502933 3.0190873 3.1139688 3.2283776 3.3542867 3.483728  3.6119013
 3.7396698 3.8740945 4.0250497 4.201239  4.4033127 4.6209865 4.8332524
 5.012653  5.132296  5.175167  5.1386423 5.033673  4.8804297 4.7019315
 4.5193324 4.3493495 4.20224   4.0826335 3.9909482 3.9248288 3.881614
 3.8583808 3.852447  3.8618689 3.885211  3.9207742 3.9670138 4.0215273
 4.0814247 4.142772  4.2013125 4.252396  4.2918468 4.316205  4.3227453
 4.309931  4.276101  4.2207456 4.1431704 4.0440154 3.9256825 3.7920923
 3.6487367 3.5018158 3.3560529 3.214691  3.0791786 2.9510403 2.8323007
 2.7258658 2.635414  2.5649505 2.518133  2.4976687 2.5061681 2.5446765
 2.6124823]
