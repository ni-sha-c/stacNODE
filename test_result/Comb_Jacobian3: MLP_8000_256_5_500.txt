time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.92%, model saved.
Epoch: 0 Train: 32102.70312 Test: 4031.44946
Epoch 80: New minimal relative error: 92.76%, model saved.
Epoch: 80 Train: 6235.38184 Test: 1173.53357
Epoch: 160 Train: 2865.22021 Test: 689.84406
Epoch 240: New minimal relative error: 58.13%, model saved.
Epoch: 240 Train: 244.89069 Test: 212.19177
Epoch 320: New minimal relative error: 48.89%, model saved.
Epoch: 320 Train: 137.51538 Test: 12.18637
Epoch 400: New minimal relative error: 22.98%, model saved.
Epoch: 400 Train: 51.53266 Test: 23.63696
Epoch 480: New minimal relative error: 21.81%, model saved.
Epoch: 480 Train: 22.97240 Test: 4.48070
Epoch: 560 Train: 61.43502 Test: 40.74474
Epoch: 640 Train: 15.07797 Test: 1.73081
Epoch: 720 Train: 39.55244 Test: 7.10496
Epoch: 800 Train: 23.35732 Test: 6.47205
Epoch 880: New minimal relative error: 10.47%, model saved.
Epoch: 880 Train: 37.43399 Test: 1.00975
Epoch: 960 Train: 14.99531 Test: 5.75135
Epoch: 1040 Train: 4.58843 Test: 0.08765
Epoch: 1120 Train: 24.17115 Test: 4.38563
Epoch: 1200 Train: 4.63911 Test: 0.97654
Epoch 1280: New minimal relative error: 7.03%, model saved.
Epoch: 1280 Train: 3.45594 Test: 0.28711
Epoch: 1360 Train: 9.35904 Test: 2.94233
Epoch 1440: New minimal relative error: 1.42%, model saved.
Epoch: 1440 Train: 2.74675 Test: 0.10978
Epoch: 1520 Train: 5.94298 Test: 3.24397
Epoch: 1600 Train: 3.73060 Test: 0.30962
Epoch: 1680 Train: 16.40765 Test: 6.05955
Epoch: 1760 Train: 1.96643 Test: 0.03246
Epoch: 1840 Train: 2.72388 Test: 0.14484
Epoch: 1920 Train: 9.73912 Test: 3.78476
Epoch: 2000 Train: 3.69604 Test: 1.02338
Epoch: 2080 Train: 31.22042 Test: 10.53592
Epoch 2160: New minimal relative error: 0.79%, model saved.
Epoch: 2160 Train: 1.42723 Test: 0.03466
Epoch: 2240 Train: 3.85476 Test: 2.87402
Epoch: 2320 Train: 1.35335 Test: 0.15120
Epoch: 2400 Train: 5.44174 Test: 3.16593
Epoch: 2480 Train: 1.12031 Test: 0.00749
Epoch: 2560 Train: 1.27948 Test: 0.18585
Epoch: 2640 Train: 1.71593 Test: 0.34405
Epoch: 2720 Train: 1.20912 Test: 0.12273
Epoch: 2800 Train: 1.00567 Test: 0.06756
Epoch: 2880 Train: 0.89608 Test: 0.01652
Epoch: 2960 Train: 1.79113 Test: 0.37647
Epoch: 3040 Train: 6.09700 Test: 3.80448
Epoch: 3120 Train: 22.15965 Test: 14.02458
Epoch: 3200 Train: 4.60680 Test: 2.28516
Epoch: 3280 Train: 0.74714 Test: 0.01442
Epoch: 3360 Train: 1.45343 Test: 0.40172
Epoch: 3440 Train: 1.38714 Test: 0.42641
Epoch: 3520 Train: 7.00873 Test: 2.91657
Epoch: 3600 Train: 0.74205 Test: 0.05945
Epoch: 3680 Train: 2.10991 Test: 1.18517
Epoch: 3760 Train: 3.15521 Test: 1.11393
Epoch: 3840 Train: 0.91206 Test: 0.21609
Epoch: 3920 Train: 0.70304 Test: 0.03763
Epoch: 4000 Train: 0.97857 Test: 0.29462
Epoch: 4080 Train: 6.04100 Test: 3.78700
Epoch: 4160 Train: 1.36295 Test: 0.60325
Epoch: 4240 Train: 3.94892 Test: 0.89596
Epoch: 4320 Train: 0.49563 Test: 0.01940
Epoch: 4400 Train: 0.88859 Test: 0.20754
Epoch: 4480 Train: 0.94397 Test: 0.91247
Epoch: 4560 Train: 1.82440 Test: 1.01417
Epoch: 4640 Train: 0.43668 Test: 0.01313
Epoch: 4720 Train: 0.76282 Test: 0.18146
Epoch: 4800 Train: 4.37104 Test: 2.59747
Epoch: 4880 Train: 0.63775 Test: 0.18867
Epoch: 4960 Train: 0.56924 Test: 0.09091
Epoch: 5040 Train: 2.29706 Test: 1.04989
Epoch: 5120 Train: 12.45383 Test: 9.93607
Epoch: 5200 Train: 0.36301 Test: 0.00435
Epoch: 5280 Train: 0.37027 Test: 0.02061
Epoch: 5360 Train: 0.34734 Test: 0.01299
Epoch: 5440 Train: 0.74543 Test: 0.14441
Epoch: 5520 Train: 0.37820 Test: 0.04572
Epoch: 5600 Train: 0.36489 Test: 0.01882
Epoch: 5680 Train: 0.30301 Test: 0.00224
Epoch: 5760 Train: 1.30803 Test: 0.46765
Epoch: 5840 Train: 0.29999 Test: 0.00471
Epoch: 5920 Train: 8.64169 Test: 4.28848
Epoch: 6000 Train: 3.94702 Test: 1.96479
Epoch: 6080 Train: 0.27509 Test: 0.00301
Epoch: 6160 Train: 0.61110 Test: 0.24174
Epoch: 6240 Train: 7.03786 Test: 3.23834
Epoch: 6320 Train: 0.26052 Test: 0.00341
Epoch: 6400 Train: 0.36727 Test: 0.05989
Epoch: 6480 Train: 0.28715 Test: 0.02195
Epoch: 6560 Train: 0.24875 Test: 0.00611
Epoch: 6640 Train: 0.31310 Test: 0.02414
Epoch: 6720 Train: 0.60035 Test: 0.09539
Epoch 6800: New minimal relative error: 0.58%, model saved.
Epoch: 6800 Train: 0.27032 Test: 0.02296
Epoch: 6880 Train: 0.32681 Test: 0.06473
Epoch: 6960 Train: 0.24044 Test: 0.01439
Epoch: 7040 Train: 0.39111 Test: 0.15847
Epoch: 7120 Train: 0.26056 Test: 0.03372
Epoch: 7200 Train: 0.22652 Test: 0.00733
Epoch: 7280 Train: 0.20973 Test: 0.00587
Epoch: 7360 Train: 0.68427 Test: 0.20593
Epoch: 7440 Train: 1.63491 Test: 0.97182
Epoch: 7520 Train: 0.19608 Test: 0.00195
Epoch: 7600 Train: 0.36649 Test: 0.09168
Epoch: 7680 Train: 0.34319 Test: 0.11386
Epoch: 7760 Train: 0.18789 Test: 0.00284
Epoch: 7840 Train: 0.54840 Test: 0.13829
Epoch: 7920 Train: 0.83268 Test: 0.47047
Epoch 7999: New minimal relative error: 0.38%, model saved.
Epoch: 7999 Train: 0.18415 Test: 0.00602
Training Loss: tensor(0.1842)
Test Loss: tensor(0.0060)
Learned LE: [ 8.5816503e-01 -8.6274715e-03 -1.4526561e+01]
True LE: [ 8.6515564e-01  2.8014877e-03 -1.4542566e+01]
Relative Error: [0.40843347 0.39293906 0.37546968 0.35629597 0.33548126 0.31298307
 0.28923967 0.26438046 0.23869015 0.21210049 0.18479602 0.15646113
 0.12754023 0.09874552 0.07365334 0.06048122 0.06260441 0.06683234
 0.06095191 0.05038746 0.05409942 0.06765084 0.07769069 0.08903674
 0.11352509 0.15688744 0.214379   0.27436596 0.32536504 0.3624235
 0.38757852 0.40654427 0.4230788  0.4381934  0.44947696 0.45321515
 0.44728997 0.43088785 0.40638623 0.3780754  0.35132718 0.33074054
 0.31884348 0.3168392  0.3227391  0.33023918 0.33141857 0.3306155
 0.3457702  0.38073966 0.41014445 0.40846035 0.3872492  0.37482184
 0.38513643 0.4102359  0.43020457 0.4341605  0.4270446  0.4175087
 0.4087116  0.39980257 0.38954568 0.3770949  0.36232653 0.34514588
 0.325554   0.30397773 0.28073674 0.25661337 0.23182006 0.20680581
 0.18203573 0.15750046 0.1336743  0.11066985 0.0898264  0.0746843
 0.06924675 0.06964476 0.0679237  0.06630349 0.07532038 0.08719828
 0.09213457 0.09540728 0.11086124 0.14536779 0.19416298 0.24622516
 0.29145706 0.32429847 0.34698734 0.3656977  0.3843638  0.40261447
 0.41718808 0.42325208 0.41774726 0.3998908  0.3723731  0.34082812
 0.3120844  0.2914363  0.280763   0.28022194 0.2891843  0.30330968
 0.311655   0.3120389  0.32325223 0.35756162 0.38928202 0.38828805
 0.36749917 0.35907453 0.37245157 0.39609495 0.4105294  0.4085646
 0.39837685 0.3897568  0.3841325  0.3794105  0.37318265 0.3643083
 0.35224038 0.33689478 0.3183079  0.29713115 0.27417806 0.2502428
 0.22614366 0.20242354 0.1796398  0.1585669  0.1395886  0.12312526
 0.10829883 0.09483178 0.08503077 0.08067451 0.0805676  0.08542558
 0.09873729 0.11135338 0.11239836 0.10678176 0.1108124  0.13447428
 0.1730561  0.216727   0.2563315  0.28608096 0.30718473 0.32631862
 0.34767336 0.3699327  0.38851956 0.39774567 0.39370778 0.37532794
 0.34537774 0.31040975 0.27892956 0.25750265 0.24850343 0.249849
 0.25995827 0.27866122 0.29584953 0.30032638 0.3066751  0.33682284
 0.37089142 0.3716443  0.35096082 0.34520558 0.36041403 0.3805312
 0.3885642  0.380877   0.3686774  0.36207983 0.36085486 0.36078992
 0.3592348  0.3542012  0.34493312 0.33115357 0.31330466 0.2924547
 0.26959047 0.24572198 0.22184397 0.19897711 0.17767088 0.15917791
 0.14452577 0.13377416 0.12580596 0.11741167 0.10794365 0.10035376
 0.09885236 0.10633086 0.1229864  0.13796711 0.13826576 0.12529963
 0.11594068 0.12603271 0.15251337 0.18679771 0.22134124 0.24855807
 0.26875603 0.28860825 0.31252468 0.3385966  0.36159152 0.37498972
 0.3738053  0.3562913  0.32530102 0.28804237 0.253401   0.22972138
 0.22091252 0.2243276  0.23537336 0.25476277 0.28004098 0.29352733
 0.29586467 0.31729448 0.35303816 0.3583697  0.3371217  0.3324619
 0.34875003 0.36444443 0.36549616 0.35245785 0.338649   0.3344265
 0.3379491  0.34334874 0.3467333  0.34583154 0.33930442 0.3271137
 0.31011927 0.28958634 0.26669014 0.2428344  0.219067   0.1964539
 0.17608316 0.15910526 0.1470096  0.14078942 0.13916112 0.1380592
 0.13360876 0.12617882 0.12220068 0.12863845 0.1466978  0.1655046
 0.16874027 0.15171444 0.12927686 0.12282276 0.13544825 0.15802445
 0.18670821 0.21272863 0.23226859 0.25228754 0.27748176 0.30646884
 0.33330378 0.35106727 0.35420695 0.33950454 0.30932155 0.2713705
 0.23592599 0.21012588 0.19808453 0.20111562 0.21433721 0.2327616
 0.2605493  0.28640333 0.29139438 0.30005845 0.3331236  0.3468428
 0.32596263 0.31951362 0.3371845  0.3490793  0.34278256 0.32462478
 0.30884615 0.30682963 0.31490818 0.32585907 0.33444574 0.33765677
 0.33409706 0.3237787  0.30792603 0.28786016 0.26510918 0.24113423
 0.21712044 0.19454081 0.17433722 0.15765154 0.1462635  0.14191851
 0.14501335 0.15123442 0.15521836 0.15312394 0.14914052 0.15228812
 0.16877069 0.19115724 0.20103489 0.18623132 0.15449229 0.12925877
 0.12481076 0.13324724 0.1536748  0.17848605 0.19828358 0.21711555
 0.24153036 0.27085346 0.3002254  0.322912   0.33196256 0.32235298
 0.29419094 0.25600645 0.22085707 0.1968144  0.18287271 0.18004239
 0.19287501 0.21392207 0.23914932 0.2722016  0.29037082 0.28880623
 0.30924228 0.3334726  0.31725648 0.30461618 0.32402942 0.33597535
 0.3232497  0.2992576  0.28039476 0.27840057 0.29016367 0.3059783
 0.31983495 0.3277215  0.32767308 0.31980047 0.30550757 0.28652826
 0.26416048 0.24007003 0.21564399 0.19256337 0.17197855 0.15494101
 0.14273766 0.13729504 0.14061989 0.15178394 0.16469651 0.17299421
 0.1746051  0.17630099 0.18835567 0.21162753 0.23046102 0.22502886
 0.19282717 0.15148067 0.12537718 0.11683299]
