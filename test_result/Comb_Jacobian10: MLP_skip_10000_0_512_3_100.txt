time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 98.62%, model saved.
Epoch: 0 Train: 9207.76172 Test: 3921.07104
Epoch: 100 Train: 2361.84570 Test: 996.00385
Epoch: 200 Train: 1176.56335 Test: 340.50070
Epoch 300: New minimal relative error: 61.59%, model saved.
Epoch: 300 Train: 483.24997 Test: 87.84695
Epoch 400: New minimal relative error: 31.90%, model saved.
Epoch: 400 Train: 212.39136 Test: 26.69133
Epoch 500: New minimal relative error: 31.51%, model saved.
Epoch: 500 Train: 141.84291 Test: 20.22341
Epoch 600: New minimal relative error: 25.04%, model saved.
Epoch: 600 Train: 101.36238 Test: 8.57880
Epoch 700: New minimal relative error: 6.33%, model saved.
Epoch: 700 Train: 81.11192 Test: 6.85109
Epoch: 800 Train: 65.46302 Test: 15.22927
Epoch: 900 Train: 54.18784 Test: 3.22417
Epoch: 1000 Train: 69.29423 Test: 27.35139
Epoch: 1100 Train: 71.12646 Test: 8.97696
Epoch: 1200 Train: 37.25476 Test: 3.54921
Epoch: 1300 Train: 52.37555 Test: 19.65740
Epoch: 1400 Train: 35.46532 Test: 2.96131
Epoch: 1500 Train: 42.73127 Test: 3.82669
Epoch 1600: New minimal relative error: 1.85%, model saved.
Epoch: 1600 Train: 24.21843 Test: 0.76297
Epoch: 1700 Train: 23.38238 Test: 0.82927
Epoch: 1800 Train: 22.91936 Test: 1.27220
Epoch 1900: New minimal relative error: 1.60%, model saved.
Epoch: 1900 Train: 21.81828 Test: 0.67107
Epoch: 2000 Train: 27.09994 Test: 9.72140
Epoch: 2100 Train: 18.78077 Test: 0.47857
Epoch: 2200 Train: 18.68574 Test: 0.50296
Epoch: 2300 Train: 22.37168 Test: 2.98231
Epoch: 2400 Train: 20.70676 Test: 6.09222
Epoch: 2500 Train: 23.58081 Test: 1.80230
Epoch: 2600 Train: 15.06817 Test: 0.31232
Epoch 2700: New minimal relative error: 1.34%, model saved.
Epoch: 2700 Train: 14.63646 Test: 0.30671
Epoch: 2800 Train: 14.78853 Test: 0.41597
Epoch: 2900 Train: 14.16111 Test: 2.86441
Epoch: 3000 Train: 17.50901 Test: 5.25482
Epoch: 3100 Train: 12.47438 Test: 0.49100
Epoch: 3200 Train: 15.20445 Test: 0.65654
Epoch: 3300 Train: 11.55986 Test: 0.19125
Epoch 3400: New minimal relative error: 1.30%, model saved.
Epoch: 3400 Train: 12.23635 Test: 0.24833
Epoch: 3500 Train: 21.27634 Test: 8.02784
Epoch: 3600 Train: 13.27937 Test: 2.16579
Epoch: 3700 Train: 10.84276 Test: 0.63490
Epoch: 3800 Train: 15.30382 Test: 2.26368
Epoch 3900: New minimal relative error: 1.24%, model saved.
Epoch: 3900 Train: 9.46481 Test: 0.12922
Epoch: 4000 Train: 9.72629 Test: 0.14068
Epoch: 4100 Train: 9.18859 Test: 0.13836
Epoch: 4200 Train: 9.41245 Test: 0.29086
Epoch: 4300 Train: 9.20417 Test: 0.61525
Epoch: 4400 Train: 10.29554 Test: 0.38407
Epoch: 4500 Train: 8.36437 Test: 0.10513
Epoch: 4600 Train: 12.15945 Test: 4.24036
Epoch: 4700 Train: 8.17120 Test: 0.09975
Epoch 4800: New minimal relative error: 0.97%, model saved.
Epoch: 4800 Train: 8.73147 Test: 0.11377
Epoch: 4900 Train: 7.74020 Test: 0.09460
Epoch: 5000 Train: 7.60612 Test: 0.10810
Epoch 5100: New minimal relative error: 0.96%, model saved.
Epoch: 5100 Train: 7.44081 Test: 0.09615
Epoch: 5200 Train: 7.47545 Test: 0.09180
Epoch: 5300 Train: 7.78658 Test: 0.23572
Epoch: 5400 Train: 7.25126 Test: 0.23464
Epoch: 5500 Train: 6.82422 Test: 0.16671
Epoch: 5600 Train: 9.68293 Test: 2.89290
Epoch: 5700 Train: 7.37218 Test: 1.11291
Epoch: 5800 Train: 6.64871 Test: 0.07731
Epoch: 5900 Train: 6.91328 Test: 0.34361
Epoch: 6000 Train: 6.53299 Test: 0.11699
Epoch: 6100 Train: 7.96282 Test: 1.50581
Epoch: 6200 Train: 6.43291 Test: 0.19524
Epoch: 6300 Train: 6.17396 Test: 0.07977
Epoch: 6400 Train: 6.07773 Test: 0.05549
Epoch: 6500 Train: 7.59323 Test: 1.90282
Epoch: 6600 Train: 6.05989 Test: 0.33016
Epoch 6700: New minimal relative error: 0.87%, model saved.
Epoch: 6700 Train: 5.83248 Test: 0.05427
Epoch: 6800 Train: 7.54279 Test: 1.47132
Epoch: 6900 Train: 5.90627 Test: 0.19741
Epoch: 7000 Train: 5.67122 Test: 0.12822
Epoch: 7100 Train: 6.30242 Test: 0.36111
Epoch 7200: New minimal relative error: 0.50%, model saved.
Epoch: 7200 Train: 5.56466 Test: 0.05107
Epoch: 7300 Train: 5.59034 Test: 0.05673
Epoch: 7400 Train: 5.71852 Test: 0.26027
Epoch: 7500 Train: 6.19584 Test: 0.32484
Epoch: 7600 Train: 5.54008 Test: 0.05080
Epoch: 7700 Train: 5.80794 Test: 0.31530
Epoch: 7800 Train: 5.59623 Test: 0.12945
Epoch: 7900 Train: 5.63001 Test: 0.17685
Epoch: 8000 Train: 5.38209 Test: 0.12008
Epoch: 8100 Train: 5.29852 Test: 0.04718
Epoch: 8200 Train: 5.35618 Test: 0.15767
Epoch: 8300 Train: 5.13835 Test: 0.05123
Epoch: 8400 Train: 5.22740 Test: 0.12954
Epoch: 8500 Train: 5.33527 Test: 0.18801
Epoch: 8600 Train: 5.24760 Test: 0.29239
Epoch: 8700 Train: 5.04761 Test: 0.17220
Epoch: 8800 Train: 4.96868 Test: 0.05453
Epoch: 8900 Train: 5.04133 Test: 0.16260
Epoch: 9000 Train: 5.47150 Test: 0.37142
Epoch: 9100 Train: 5.13763 Test: 0.13350
Epoch: 9200 Train: 4.90854 Test: 0.09843
Epoch: 9300 Train: 5.06333 Test: 0.18531
Epoch: 9400 Train: 4.78854 Test: 0.03995
Epoch: 9500 Train: 4.81600 Test: 0.06313
Epoch: 9600 Train: 4.65291 Test: 0.03437
Epoch: 9700 Train: 5.09547 Test: 0.26609
Epoch: 9800 Train: 4.78240 Test: 0.06665
Epoch: 9900 Train: 4.66602 Test: 0.03389
Epoch: 9999 Train: 4.79231 Test: 0.19598
Training Loss: tensor(4.7923)
Test Loss: tensor(0.1960)
Learned LE: [  0.920582    -0.02755254 -14.563744  ]
True LE: [ 8.8240355e-01  1.2684742e-02 -1.4568612e+01]
Relative Error: [2.2236657  2.260923   2.1503081  2.084328   1.959968   1.6337942
 1.098444   0.81013006 1.0861878  1.5968857  2.015789   2.1196444
 2.3293545  2.398828   2.6231346  2.7138336  2.1819425  1.8541071
 1.7590036  1.4247669  1.2644389  1.3188037  1.4729689  1.2773165
 1.0035696  1.1758643  1.5194424  1.8739253  2.2827616  2.6816576
 2.5174286  2.516126   2.328314   2.1649861  1.9811769  1.7226617
 1.3955625  0.9333024  1.0132737  2.360851   3.2141614  3.8579862
 4.1858788  3.9762588  3.4319406  2.7681491  2.414841   2.1053817
 1.7929032  1.596668   1.6504152  1.5337597  1.423755   1.6189376
 1.6574172  1.558501   1.5254331  1.4452455  1.3775555  1.4130136
 1.5368277  1.7220292  1.8368762  1.9647287  1.9640518  1.9161793
 1.8226573  1.51833    1.0469774  0.7009926  0.82406175 1.1807846
 1.6860121  1.8489817  1.9861887  2.0966408  2.1844256  2.6809084
 2.31598    1.9229188  1.713473   1.463681   1.2550989  1.2859325
 1.444956   1.3137481  1.1090503  1.0621759  1.2628422  1.6815206
 2.1583052  2.665577   2.7162974  2.6896195  2.3575869  2.111234
 2.0196424  1.7541411  1.6977555  1.1705426  0.7058029  1.8408815
 2.7828374  3.539349   3.858852   3.6142414  3.166596   2.563995
 2.1116505  1.8326752  1.5417035  1.322483   1.3536887  1.3177371
 1.1578325  1.366496   1.4962875  1.3568894  1.3201497  1.2379367
 1.1506953  1.103432   1.1402736  1.3979721  1.6101003  1.6186213
 1.7438568  1.711215   1.675822   1.4358301  1.0655732  0.6712457
 0.5654481  0.8948243  1.2707746  1.605014   1.672968   1.7187086
 1.8911355  2.312803   2.4237792  1.8904735  1.4169619  1.2102176
 1.2962714  1.2894979  1.4044892  1.3104533  1.1370493  1.0852969
 1.149466   1.4065404  1.9598994  2.561676   2.8226109  2.5821497
 2.3592532  2.0145905  1.9277722  1.6999191  1.7317315  1.4597819
 0.80921304 1.2176716  2.2419243  3.0594394  3.401153   3.2624092
 2.9508536  2.4991207  1.9015477  1.5973694  1.2896535  1.075261
 1.1097049  1.0160464  0.9277277  1.123582   1.3560178  1.1797684
 1.0805073  1.0592103  1.0578122  0.9396803  0.8134234  1.0565681
 1.3128929  1.365263   1.3333472  1.4385481  1.4637115  1.3690392
 1.0631902  0.78257716 0.49863094 0.54692346 0.94737476 1.2801247
 1.3784547  1.3878688  1.5534381  1.7974966  2.1205802  1.7504302
 1.096878   0.846028   1.0451051  1.2523881  1.3332242  1.3428835
 1.1020701  1.0646553  1.0906993  1.2867565  1.6780877  2.3311296
 2.7428713  2.6361053  2.370928   2.0883462  1.9209374  1.770884
 1.624421   1.7865396  1.1378838  0.78503954 1.5047902  2.412305
 2.8467953  2.8465006  2.6958458  2.4539704  1.9364599  1.4413756
 1.1631836  0.88071376 0.8939915  0.8078408  0.7012566  0.9181193
 1.205104   1.164637   0.94937617 0.9294088  0.9719494  0.9960219
 0.63729846 0.62648475 0.9298768  1.0522637  1.074978   1.076722
 1.2030233  1.2429022  1.0300715  0.8060263  0.6462604  0.50206995
 0.5659822  0.86541307 1.1571934  1.1003875  1.0886017  1.5052023
 1.8655362  1.5366335  0.9809113  0.5098419  0.784468   1.0764343
 1.2316389  1.2526125  1.1116725  0.9820407  0.98621774 1.1374241
 1.4821509  2.051684   2.5051506  2.5403068  2.3156066  2.059705
 1.8384553  1.7190672  1.5931314  1.7524874  1.6484059  0.942997
 0.9197444  1.5764959  2.1516125  2.337161   2.3969162  2.3097343
 1.9307001  1.4017252  1.1108804  0.7732503  0.7310559  0.7543638
 0.5963933  0.67381483 1.0112493  1.1334732  0.98212856 0.9145512
 0.9392853  0.9757019  0.75400525 0.4900264  0.4302167  0.58034855
 0.74957263 0.8214361  0.9420975  1.0371615  1.0878347  0.8312624
 0.68643254 0.5707815  0.47694123 0.5627952  0.70678544 0.76025355
 0.76751655 1.1600416  1.6201788  1.3489486  0.9899975  0.49768925
 0.46364856 0.8579894  1.0626895  1.172019   1.0461124  0.8795737
 0.8539402  0.95762926 1.1893071  1.7040735  2.1086442  2.2479596
 2.1926417  2.0316546  1.7989559  1.576714   1.5407377  1.4104524
 1.7330533  1.4966817  0.8311298  0.9301417  1.4163834  1.6611578
 1.9462295  2.081398   1.8990735  1.5183654  1.0147347  0.7836868
 0.6148625  0.74139535 0.59652513 0.35293987 0.829227   0.94482034
 1.0085322  0.9340558  0.9353376  0.9062387  0.8153771  0.64174896
 0.46810535 0.55306184 0.67285573 0.7191848  0.7979082  0.89532834
 0.87966293 0.86691797 0.75930274 0.65688986 0.5687814  0.42504975
 0.43048584 0.39321128 0.44720173 0.8168414  1.2548203  1.3730865
 0.9178963  0.64901054 0.26657316 0.6592875  0.93003875 0.97362196
 1.0679936  0.8471463  0.72650725 0.7402749 ]
