time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.10%, model saved.
Epoch: 0 Train: 32583.21289 Test: 4294.69092
Epoch 100: New minimal relative error: 93.90%, model saved.
Epoch: 100 Train: 8489.01172 Test: 1391.73242
Epoch: 200 Train: 7505.71582 Test: 1340.03418
Epoch: 300 Train: 7511.82471 Test: 1056.78638
Epoch: 400 Train: 5379.13916 Test: 702.10992
Epoch: 500 Train: 4017.03394 Test: 376.68430
Epoch: 600 Train: 2219.87500 Test: 127.17056
Epoch 700: New minimal relative error: 68.26%, model saved.
Epoch: 700 Train: 958.09430 Test: 62.65704
Epoch 800: New minimal relative error: 30.11%, model saved.
Epoch: 800 Train: 576.80676 Test: 24.63905
Epoch 900: New minimal relative error: 19.28%, model saved.
Epoch: 900 Train: 479.31442 Test: 14.81024
Epoch: 1000 Train: 350.67654 Test: 11.31189
Epoch: 1100 Train: 314.83850 Test: 5.08015
Epoch: 1200 Train: 266.37363 Test: 4.57144
Epoch: 1300 Train: 233.31758 Test: 6.17455
Epoch 1400: New minimal relative error: 9.19%, model saved.
Epoch: 1400 Train: 217.00726 Test: 2.75449
Epoch: 1500 Train: 179.89479 Test: 2.18043
Epoch 1600: New minimal relative error: 4.99%, model saved.
Epoch: 1600 Train: 175.00879 Test: 1.86487
Epoch: 1700 Train: 168.91478 Test: 1.76679
Epoch: 1800 Train: 226.35681 Test: 27.13295
Epoch: 1900 Train: 148.59312 Test: 1.62769
Epoch: 2000 Train: 138.06992 Test: 1.90858
Epoch: 2100 Train: 179.59900 Test: 5.05087
Epoch: 2200 Train: 158.30669 Test: 23.82006
Epoch: 2300 Train: 133.63791 Test: 12.49776
Epoch: 2400 Train: 121.14284 Test: 4.31102
Epoch: 2500 Train: 116.99906 Test: 2.05678
Epoch: 2600 Train: 115.28069 Test: 2.01139
Epoch: 2700 Train: 106.25206 Test: 1.56550
Epoch: 2800 Train: 99.85435 Test: 0.93300
Epoch: 2900 Train: 173.33098 Test: 2.56794
Epoch: 3000 Train: 118.11308 Test: 5.71513
Epoch: 3100 Train: 105.68924 Test: 2.61011
Epoch: 3200 Train: 96.50908 Test: 8.68422
Epoch: 3300 Train: 105.16424 Test: 7.62216
Epoch: 3400 Train: 78.14444 Test: 0.65643
Epoch: 3500 Train: 103.63492 Test: 16.48105
Epoch: 3600 Train: 72.84286 Test: 0.71467
Epoch 3700: New minimal relative error: 3.00%, model saved.
Epoch: 3700 Train: 66.06298 Test: 0.44050
Epoch: 3800 Train: 62.28376 Test: 0.40468
Epoch: 3900 Train: 61.85715 Test: 0.39035
Epoch: 4000 Train: 58.64688 Test: 0.27894
Epoch: 4100 Train: 60.39920 Test: 2.07394
Epoch: 4200 Train: 60.56511 Test: 5.33969
Epoch: 4300 Train: 59.63226 Test: 0.55723
Epoch: 4400 Train: 72.37520 Test: 2.08209
Epoch: 4500 Train: 59.48410 Test: 0.30648
Epoch: 4600 Train: 64.03038 Test: 0.59328
Epoch: 4700 Train: 152.96417 Test: 26.40465
Epoch: 4800 Train: 52.36134 Test: 0.20575
Epoch 4900: New minimal relative error: 2.35%, model saved.
Epoch: 4900 Train: 48.49541 Test: 0.15766
Epoch: 5000 Train: 45.77567 Test: 0.15261
Epoch: 5100 Train: 48.01323 Test: 2.10138
Epoch: 5200 Train: 49.38691 Test: 0.87947
Epoch: 5300 Train: 46.67300 Test: 0.23541
Epoch: 5400 Train: 53.98090 Test: 2.71157
Epoch: 5500 Train: 48.12439 Test: 2.46718
Epoch: 5600 Train: 49.63714 Test: 0.89679
Epoch: 5700 Train: 45.57770 Test: 0.16956
Epoch: 5800 Train: 46.15379 Test: 0.20051
Epoch: 5900 Train: 48.68176 Test: 0.26191
Epoch: 6000 Train: 47.78829 Test: 0.34935
Epoch: 6100 Train: 50.93930 Test: 3.58469
Epoch: 6200 Train: 45.48742 Test: 0.29739
Epoch: 6300 Train: 50.53030 Test: 1.65799
Epoch: 6400 Train: 45.96197 Test: 0.69500
Epoch: 6500 Train: 44.48780 Test: 0.78786
Epoch: 6600 Train: 38.57879 Test: 0.23482
Epoch: 6700 Train: 37.76207 Test: 0.18592
Epoch 6800: New minimal relative error: 2.35%, model saved.
Epoch: 6800 Train: 38.11673 Test: 0.13024
Epoch: 6900 Train: 44.21323 Test: 1.05705
Epoch: 7000 Train: 35.87952 Test: 0.11782
Epoch: 7100 Train: 36.64993 Test: 0.21644
Epoch: 7200 Train: 41.57504 Test: 1.85813
Epoch: 7300 Train: 37.72538 Test: 0.74114
Epoch: 7400 Train: 36.64403 Test: 0.49629
Epoch 7500: New minimal relative error: 2.22%, model saved.
Epoch: 7500 Train: 35.55804 Test: 0.13394
Epoch: 7600 Train: 36.52732 Test: 2.53119
Epoch: 7700 Train: 32.04774 Test: 0.10109
Epoch: 7800 Train: 35.07901 Test: 0.15236
Epoch: 7900 Train: 33.97321 Test: 0.11700
Epoch: 8000 Train: 31.17489 Test: 0.27283
Epoch: 8100 Train: 33.16414 Test: 0.13716
Epoch: 8200 Train: 36.77901 Test: 0.18755
Epoch: 8300 Train: 35.22387 Test: 3.99972
Epoch: 8400 Train: 28.75064 Test: 0.08515
Epoch 8500: New minimal relative error: 1.56%, model saved.
Epoch: 8500 Train: 30.37620 Test: 0.08077
Epoch: 8600 Train: 26.58882 Test: 0.06362
Epoch 8700: New minimal relative error: 1.43%, model saved.
Epoch: 8700 Train: 26.44850 Test: 0.06160
Epoch: 8800 Train: 26.61331 Test: 0.18720
Epoch: 8900 Train: 28.84846 Test: 0.09969
Epoch: 9000 Train: 26.54819 Test: 0.07935
Epoch: 9100 Train: 25.66137 Test: 0.08641
Epoch: 9200 Train: 29.08591 Test: 0.65221
Epoch: 9300 Train: 28.23962 Test: 0.13229
Epoch: 9400 Train: 28.93747 Test: 0.10169
Epoch: 9500 Train: 25.36372 Test: 0.06392
Epoch: 9600 Train: 23.78016 Test: 0.07793
Epoch: 9700 Train: 22.65279 Test: 0.03449
Epoch: 9800 Train: 22.77961 Test: 0.10800
Epoch: 9900 Train: 22.78920 Test: 0.04763
Epoch: 9999 Train: 23.54922 Test: 0.07445
Training Loss: tensor(23.5492)
Test Loss: tensor(0.0745)
Learned LE: [  0.82937      0.03047382 -14.528396  ]
True LE: [ 8.6601180e-01 -6.5042251e-03 -1.4541409e+01]
Relative Error: [2.6073256  2.8607006  3.1687493  3.2205992  3.3275585  3.2894578
 2.9434316  2.5578048  2.321391   1.9812067  1.6924592  1.6140878
 1.6906036  1.6819786  1.6474658  1.5102044  1.3226801  1.0700196
 0.99443483 0.7693132  0.3831347  0.42578828 1.0579252  1.868177
 2.8454976  3.0194342  2.7195108  2.007561   1.3611042  1.0880748
 1.299435   1.520691   1.4915676  1.5702648  1.629325   1.4997786
 1.0958991  0.6845349  1.0265256  1.9796828  2.5422108  3.2931366
 4.1636286  4.9706593  5.752061   5.7124286  5.4853687  5.1565604
 4.8662024  4.596771   4.38024    4.222705   4.1694202  3.9793777
 3.7958329  3.7502003  3.7084792  3.5400803  3.4144752  3.1913955
 2.8785305  2.5863693  2.4643793  2.7494266  3.1764507  3.3681598
 3.4473433  3.4010363  3.0392194  2.529497   2.0575433  1.6573064
 1.4358981  1.3477349  1.4078683  1.4441663  1.2397723  1.1815952
 1.1014985  0.99412084 0.9454389  0.7483255  0.40048972 0.41502517
 1.0276797  1.8709518  2.8256087  2.9813821  2.4748669  1.6646757
 0.99911374 0.84218466 1.177114   1.5093974  1.5304823  1.601997
 1.7573142  1.6649693  1.3128995  0.73621297 0.76034665 1.7232146
 2.5945225  3.290307   4.047811   4.570579   5.2975483  5.649921
 5.3514166  4.8798246  4.626294   4.305895   4.080177   3.9031003
 3.6778266  3.5139687  3.3688478  3.3953655  3.5423453  3.6804101
 3.6607492  3.3576875  2.8989995  2.2150655  2.0811849  2.5433395
 3.1001272  3.5352328  3.6103654  3.4757383  3.1417994  2.6706517
 2.1846747  1.5521448  1.2074703  1.1630034  1.1310992  1.1766658
 1.0193673  0.8464709  0.8633496  0.8693549  0.85401833 0.70360994
 0.38981253 0.32471532 0.9499399  1.7981067  2.7571163  2.9220464
 2.1351304  1.3861816  0.7827149  0.56888175 1.0293207  1.3657225
 1.5038663  1.5643752  1.798895   1.7725031  1.4914339  0.87218946
 0.5589121  1.3597763  2.5715885  3.3323843  3.7538583  4.199789
 4.8341594  5.640833   5.1144304  4.5492225  4.3105965  4.014503
 3.7727106  3.483801   3.1646478  3.0461817  2.9725902  3.0720935
 3.2908444  3.6373165  3.8253672  3.326929   2.329556   1.6925184
 1.713274   2.2730443  2.9904838  3.5522983  3.9427836  3.9854472
 3.5676475  2.9325712  2.4644895  1.838695   1.3651553  1.1154088
 1.0319068  0.9514318  0.8140279  0.6490663  0.58355    0.6749176
 0.71472144 0.7289965  0.5029426  0.17961836 0.7708806  1.623964
 2.6238086  2.7506316  1.8502884  1.177118   0.6610433  0.34639615
 0.765272   1.1432612  1.5779287  1.7803427  2.130306   2.1595857
 1.8502507  1.0572767  0.45796135 1.0416682  2.1912794  3.348642
 3.5347033  3.8768072  4.43569    5.2389235  4.9568505  4.3607006
 3.9856038  3.717008   3.3511355  3.08442    2.69872    2.5825322
 2.5391846  2.694097   3.0088744  3.4418523  3.5915337  2.812002
 1.9775307  1.3383088  1.4033141  1.9932661  2.794259   3.5439749
 4.134085   4.379067   4.072951   3.4118743  2.7926984  2.1133263
 1.6251848  1.3421483  1.1825986  0.97433513 0.67325896 0.50105643
 0.32885957 0.5329601  0.65901357 0.8837324  0.79090464 0.37232846
 0.3962229  1.2478316  2.3130155  2.4437137  1.6475254  0.982476
 0.5747243  0.31810355 0.4533002  1.1539493  1.6629487  1.9408457
 2.3127997  2.5549686  2.390922   1.6925294  0.6210869  0.79906315
 1.8558784  2.9455488  3.383418   3.6403482  4.1113024  4.65353
 4.914717   4.3086524  3.7982783  3.4133441  2.9962552  2.7436712
 2.3824825  2.264018   2.2575185  2.404202   2.696706   3.1380434
 2.9647968  2.3558395  1.7188771  1.1801507  1.1218691  1.7045002
 2.4808009  3.347997   4.021741   4.54697    4.4716687  3.912691
 3.2283247  2.4153562  1.8797433  1.566338   1.3438919  1.2178454
 0.85544896 0.47774288 0.3136005  0.50097686 0.8296011  1.0183139
 0.999878   0.6755981  0.1036703  0.7315422  1.8115463  2.0298667
 1.5005867  0.86873585 0.432183   0.25704125 0.4761734  1.1573232
 1.6738838  1.9917574  2.3439736  2.7715054  2.7859607  2.3075666
 1.3650433  0.35237834 1.5514343  2.423132   3.223526   3.465971
 3.7468243  4.108987   4.55618    4.329282   3.65053    3.1433961
 2.7759774  2.4708343  2.089254   1.9668669  2.0133457  2.183526
 2.525542   2.6954515  2.4813173  1.877177   1.5223459  1.1150608
 0.9123597  1.3113452  2.0510216  2.9034193  3.6820133  4.3194327
 4.632334   4.319277   3.6628392  2.8721418  2.1664355  1.7321998
 1.5563102  1.306342   1.174837   0.7809119  0.39259115 0.6673664
 0.77093744 0.99212396 1.0954331  0.9105664  0.46934256 0.3249327
 1.1692555  1.7346373  1.354841   0.7284363 ]
