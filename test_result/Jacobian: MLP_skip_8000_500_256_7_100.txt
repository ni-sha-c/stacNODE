time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.19%, model saved.
Epoch: 0 Train: 9469.12402 Test: 4132.62695
Epoch: 80 Train: 2708.92822 Test: 1215.62134
Epoch: 160 Train: 2639.74634 Test: 1141.04761
Epoch: 240 Train: 2525.96094 Test: 1032.22485
Epoch 320: New minimal relative error: 95.12%, model saved.
Epoch: 320 Train: 2213.16528 Test: 1301.73853
Epoch: 400 Train: 735.66632 Test: 144.89598
Epoch 480: New minimal relative error: 50.13%, model saved.
Epoch: 480 Train: 362.07980 Test: 64.63744
Epoch 560: New minimal relative error: 30.58%, model saved.
Epoch: 560 Train: 262.94879 Test: 46.02210
Epoch: 640 Train: 186.12277 Test: 27.91555
Epoch: 720 Train: 123.06590 Test: 29.59683
Epoch 800: New minimal relative error: 24.04%, model saved.
Epoch: 800 Train: 92.46111 Test: 12.81283
Epoch 880: New minimal relative error: 21.68%, model saved.
Epoch: 880 Train: 90.37480 Test: 6.46490
Epoch: 960 Train: 92.88628 Test: 18.32425
Epoch 1040: New minimal relative error: 9.92%, model saved.
Epoch: 1040 Train: 64.81617 Test: 6.38332
Epoch: 1120 Train: 58.40094 Test: 10.55684
Epoch: 1200 Train: 52.15103 Test: 11.54865
Epoch: 1280 Train: 42.37184 Test: 2.44309
Epoch: 1360 Train: 38.31003 Test: 1.82661
Epoch: 1440 Train: 53.52632 Test: 18.41068
Epoch: 1520 Train: 41.30091 Test: 9.74637
Epoch: 1600 Train: 38.52070 Test: 10.43200
Epoch: 1680 Train: 31.43780 Test: 5.71556
Epoch: 1760 Train: 30.55186 Test: 1.80230
Epoch 1840: New minimal relative error: 8.20%, model saved.
Epoch: 1840 Train: 28.74231 Test: 1.57839
Epoch: 1920 Train: 34.67072 Test: 4.83781
Epoch: 2000 Train: 28.29219 Test: 4.66766
Epoch: 2080 Train: 27.99678 Test: 1.31820
Epoch: 2160 Train: 46.85893 Test: 9.43706
Epoch: 2240 Train: 29.76393 Test: 4.77835
Epoch: 2320 Train: 23.60879 Test: 1.08668
Epoch 2400: New minimal relative error: 7.55%, model saved.
Epoch: 2400 Train: 22.90968 Test: 2.86066
Epoch: 2480 Train: 20.14618 Test: 1.85726
Epoch: 2560 Train: 21.40767 Test: 4.11314
Epoch: 2640 Train: 24.68229 Test: 3.71140
Epoch: 2720 Train: 17.76858 Test: 0.54016
Epoch: 2800 Train: 19.01051 Test: 1.68553
Epoch: 2880 Train: 17.95571 Test: 1.91087
Epoch: 2960 Train: 20.69677 Test: 5.71300
Epoch: 3040 Train: 21.42724 Test: 2.30608
Epoch 3120: New minimal relative error: 3.42%, model saved.
Epoch: 3120 Train: 17.30051 Test: 0.73290
Epoch: 3200 Train: 17.72090 Test: 1.30735
Epoch: 3280 Train: 16.85455 Test: 1.99054
Epoch: 3360 Train: 15.13493 Test: 1.10233
Epoch: 3440 Train: 14.82683 Test: 0.40410
Epoch: 3520 Train: 16.41474 Test: 3.16160
Epoch: 3600 Train: 14.63875 Test: 0.73831
Epoch: 3680 Train: 20.07651 Test: 2.53010
Epoch: 3760 Train: 14.40263 Test: 0.39744
Epoch: 3840 Train: 14.06117 Test: 0.49544
Epoch: 3920 Train: 16.00888 Test: 1.38317
Epoch: 4000 Train: 13.57896 Test: 0.72203
Epoch: 4080 Train: 13.57254 Test: 1.02534
Epoch: 4160 Train: 16.24432 Test: 2.65352
Epoch 4240: New minimal relative error: 3.05%, model saved.
Epoch: 4240 Train: 12.94699 Test: 0.36083
Epoch: 4320 Train: 13.06643 Test: 0.51212
Epoch: 4400 Train: 17.33124 Test: 1.96784
Epoch: 4480 Train: 17.62882 Test: 2.15560
Epoch: 4560 Train: 12.79021 Test: 0.34935
Epoch: 4640 Train: 15.74975 Test: 1.53620
Epoch: 4720 Train: 13.25553 Test: 0.54326
Epoch: 4800 Train: 12.46510 Test: 0.39570
Epoch: 4880 Train: 14.71397 Test: 1.06432
Epoch: 4960 Train: 12.95263 Test: 1.73318
Epoch: 5040 Train: 12.76641 Test: 0.50395
Epoch: 5120 Train: 15.54231 Test: 2.12971
Epoch: 5200 Train: 11.94539 Test: 0.30587
Epoch: 5280 Train: 11.64750 Test: 0.34914
Epoch: 5360 Train: 11.64897 Test: 0.38031
Epoch: 5440 Train: 12.78693 Test: 1.59834
Epoch: 5520 Train: 12.22351 Test: 0.47461
Epoch 5600: New minimal relative error: 2.39%, model saved.
Epoch: 5600 Train: 11.81676 Test: 0.71002
Epoch: 5680 Train: 14.75995 Test: 1.37050
Epoch: 5760 Train: 12.22977 Test: 0.33742
Epoch: 5840 Train: 12.41914 Test: 1.31077
Epoch: 5920 Train: 12.11385 Test: 1.30200
Epoch: 6000 Train: 11.67762 Test: 0.38804
Epoch: 6080 Train: 10.91045 Test: 0.42091
Epoch: 6160 Train: 11.46463 Test: 0.34251
Epoch: 6240 Train: 12.35274 Test: 0.66838
Epoch: 6320 Train: 11.29802 Test: 1.03041
Epoch: 6400 Train: 10.91238 Test: 0.78122
Epoch: 6480 Train: 11.66749 Test: 0.40343
Epoch: 6560 Train: 10.81243 Test: 0.33296
Epoch: 6640 Train: 10.86054 Test: 1.08998
Epoch: 6720 Train: 10.22264 Test: 0.22796
Epoch: 6800 Train: 10.71927 Test: 0.33065
Epoch: 6880 Train: 9.88642 Test: 0.45508
Epoch: 6960 Train: 9.81236 Test: 0.22809
Epoch: 7040 Train: 11.53208 Test: 0.65166
Epoch: 7120 Train: 9.79184 Test: 0.40420
Epoch: 7200 Train: 10.00115 Test: 0.72541
Epoch: 7280 Train: 9.67558 Test: 0.36101
Epoch: 7360 Train: 9.89098 Test: 0.89339
Epoch: 7440 Train: 11.98538 Test: 1.10118
Epoch: 7520 Train: 9.17444 Test: 0.26969
Epoch: 7600 Train: 8.96400 Test: 0.34080
Epoch: 7680 Train: 8.97524 Test: 0.20351
Epoch 7760: New minimal relative error: 1.84%, model saved.
Epoch: 7760 Train: 9.37633 Test: 0.27054
Epoch: 7840 Train: 8.94064 Test: 0.21352
Epoch: 7920 Train: 8.77637 Test: 0.23054
Epoch: 7999 Train: 8.77217 Test: 0.22138
Training Loss: tensor(8.7722)
Test Loss: tensor(0.2214)
Learned LE: [  0.8127271    0.06439511 -14.522388  ]
True LE: [ 8.6566323e-01 -2.7890876e-03 -1.4537115e+01]
Relative Error: [1.1295474  1.2836999  1.3985479  1.3868219  1.402994   1.4175591
 1.430166   1.4377438  1.4964042  1.5313693  1.571488   1.4527646
 1.2451459  1.1328281  1.1123774  1.1672696  1.2890126  1.5152583
 1.7191788  1.7342126  1.1612381  1.0128837  0.94068897 0.7606497
 0.5057153  0.19528371 0.46859187 0.7042785  0.82166475 1.196059
 1.5685526  1.8300036  2.0195582  2.0100856  1.8250108  1.720128
 1.5985316  1.331737   0.78229094 0.42088595 0.4126049  0.5712604
 0.852972   1.2519205  1.4688203  1.5209413  1.4831567  1.5732601
 1.7723176  1.5935445  1.5950335  1.507866   1.5243229  1.6198987
 1.7387515  2.179327   2.4464955  2.5904644  2.147779   1.6205626
 1.1620779  0.94971496 1.0845178  1.2699742  1.3807172  1.3645259
 1.2879859  1.2522537  1.260394   1.3448877  1.389669   1.4004114
 1.4796376  1.5225581  1.2604653  1.0614424  0.98970777 0.986459
 1.167995   1.3731756  1.586271   1.6296496  1.105157   0.8250907
 0.8435665  0.7700726  0.53396344 0.16638462 0.38661698 0.6191891
 0.8292067  1.1793492  1.513042   1.7963814  2.018869   2.018697
 1.8585712  1.6528752  1.4450845  1.163789   0.6969331  0.41938898
 0.3678986  0.462938   0.70434266 1.0150543  1.2882544  1.3971704
 1.3716838  1.4164464  1.5558267  1.3970721  1.3941367  1.356705
 1.3439854  1.2930393  1.3807906  1.7836055  1.8537343  2.010924
 1.9579222  1.4234929  0.9738967  0.937723   1.076761   1.2546269
 1.3423296  1.3183702  1.2776079  1.1719515  1.1703533  1.239297
 1.3390216  1.3985461  1.3959224  1.4235688  1.3365142  1.0745963
 0.9322266  0.9018271  0.97960865 1.147901   1.3679572  1.4607072
 0.99622244 0.59532106 0.7283784  0.7503956  0.4894553  0.19421068
 0.35582474 0.5913578  0.7945927  0.95471257 1.2480572  1.5571876
 1.8071738  1.8666794  1.8171214  1.6590546  1.3277483  0.9815365
 0.5454722  0.38117942 0.33299685 0.33038062 0.6335489  1.0323303
 1.4228598  1.5642885  1.437184   1.2841144  1.3355623  1.2374532
 1.1492908  1.2513789  1.1371983  1.0263091  1.1087705  1.4068053
 1.3670319  1.478592   1.7792778  1.2971058  0.83877844 0.93464863
 1.1621144  1.3193865  1.3539783  1.3477046  1.2758049  1.1621386
 1.1355788  1.2261411  1.2801228  1.3941004  1.3518904  1.285841
 1.2630439  1.0528172  0.81397474 0.8185405  0.8834364  0.95013577
 1.101232   1.1871898  0.8046693  0.5441294  0.6237395  0.6938477
 0.49315864 0.30370688 0.33684272 0.49319673 0.6536719  0.81953293
 1.0471481  1.3295169  1.6130278  1.7060857  1.6919122  1.5127453
 1.2603041  0.85473883 0.3378137  0.36326805 0.35711873 0.29151097
 0.6524659  1.0605835  1.4870744  1.7029569  1.6478511  1.3491379
 1.1906475  1.1486723  0.92513126 0.93965447 1.0151495  0.8412189
 0.8931812  1.0526865  0.9743603  1.0066559  1.3776513  1.2707188
 0.75959104 0.7294377  1.0334026  1.1892647  1.2222502  1.2161795
 1.2197508  1.2008954  1.197865   1.2733722  1.2932805  1.2396728
 1.2838724  1.1686463  1.0335119  0.9570678  0.7194775  0.6923936
 0.69773376 0.8398063  0.9777312  0.96978563 0.59153414 0.4350625
 0.54602015 0.6171231  0.48307472 0.3918902  0.24457122 0.3443887
 0.525204   0.6772929  0.8670648  1.1181254  1.399659   1.5606529
 1.6070065  1.458336   1.1603391  0.81285954 0.3293228  0.42764363
 0.23564585 0.2797786  0.61155635 0.96961063 1.3473333  1.7855164
 1.7894853  1.4789805  1.1934146  0.83153343 0.7615636  0.6993578
 0.8190135  0.69668293 0.7205071  0.7629172  0.69727945 0.67940086
 0.9668102  1.1264404  0.69601995 0.4964504  0.84960324 1.0752591
 1.0454977  0.9565021  0.8952212  1.0319538  1.1841838  1.3049855
 1.3404666  1.206005   1.0796046  1.0241438  0.84307057 0.6779099
 0.6308338  0.64937454 0.60643375 0.6586487  0.8653502  1.0627118
 0.57562196 0.2612884  0.36490345 0.495661   0.53355235 0.46671987
 0.23148267 0.17000109 0.36642587 0.5002627  0.627583   0.858035
 1.132542   1.3247969  1.462273   1.4695299  1.2864372  0.9101404
 0.4565232  0.28460586 0.24838838 0.38225698 0.60378134 0.82205695
 1.0818627  1.6127868  1.7781595  1.6147414  1.2359664  0.76585984
 0.50621617 0.43334222 0.5815056  0.57097805 0.5600873  0.5848973
 0.5910137  0.63261026 0.66423297 0.806403   0.6877643  0.28893703
 0.6644361  0.9484159  0.98248535 0.82808    0.71090007 0.8122184
 0.9945417  1.1764498  1.3074911  1.2528287  1.0217618  0.8801088
 0.7046549  0.51285315 0.5255824  0.57039744 0.5846074  0.59131616
 0.65304816 0.9373713  0.90738523 0.24149665 0.17025027 0.33440816
 0.3796207  0.4536494  0.43948996 0.25423214]
