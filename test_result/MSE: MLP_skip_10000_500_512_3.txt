time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.28%, model saved.
Epoch: 0 Train: 3776.36084 Test: 4124.71045
Epoch 100: New minimal relative error: 46.06%, model saved.
Epoch: 100 Train: 78.82205 Test: 87.94718
Epoch 200: New minimal relative error: 31.24%, model saved.
Epoch: 200 Train: 14.08784 Test: 15.93799
Epoch: 300 Train: 17.51137 Test: 25.39282
Epoch 400: New minimal relative error: 10.20%, model saved.
Epoch: 400 Train: 4.88812 Test: 4.51508
Epoch: 500 Train: 3.90301 Test: 3.63236
Epoch: 600 Train: 4.00232 Test: 4.62628
Epoch: 700 Train: 3.20048 Test: 2.82236
Epoch: 800 Train: 35.57742 Test: 37.30162
Epoch: 900 Train: 2.02390 Test: 1.82753
Epoch 1000: New minimal relative error: 8.87%, model saved.
Epoch: 1000 Train: 1.85515 Test: 1.66737
Epoch: 1100 Train: 8.41858 Test: 10.28389
Epoch: 1200 Train: 12.50267 Test: 6.67571
Epoch: 1300 Train: 1.15416 Test: 1.03552
Epoch: 1400 Train: 1.62912 Test: 1.94984
Epoch: 1500 Train: 7.23770 Test: 5.29422
Epoch: 1600 Train: 1.18624 Test: 0.87710
Epoch 1700: New minimal relative error: 8.12%, model saved.
Epoch: 1700 Train: 0.87388 Test: 0.75267
Epoch: 1800 Train: 1.01421 Test: 0.79538
Epoch: 1900 Train: 1.87885 Test: 2.31658
Epoch: 2000 Train: 1.16029 Test: 1.34808
Epoch: 2100 Train: 1.22991 Test: 1.74735
Epoch: 2200 Train: 0.68487 Test: 0.68583
Epoch: 2300 Train: 0.52763 Test: 0.45823
Epoch: 2400 Train: 0.79029 Test: 0.89076
Epoch: 2500 Train: 0.98794 Test: 0.66787
Epoch: 2600 Train: 1.54955 Test: 0.93579
Epoch 2700: New minimal relative error: 6.19%, model saved.
Epoch: 2700 Train: 0.44138 Test: 0.41436
Epoch: 2800 Train: 0.38252 Test: 0.36126
Epoch: 2900 Train: 0.33658 Test: 0.29927
Epoch: 3000 Train: 0.32693 Test: 0.30634
Epoch: 3100 Train: 0.33193 Test: 0.29459
Epoch: 3200 Train: 0.63409 Test: 0.73105
Epoch: 3300 Train: 0.53429 Test: 0.49015
Epoch: 3400 Train: 1.14561 Test: 1.01659
Epoch: 3500 Train: 0.31122 Test: 0.30322
Epoch: 3600 Train: 0.34995 Test: 0.40743
Epoch: 3700 Train: 1.16139 Test: 1.05155
Epoch: 3800 Train: 0.24809 Test: 0.21281
Epoch: 3900 Train: 0.94365 Test: 0.70634
Epoch: 4000 Train: 0.31995 Test: 0.27822
Epoch: 4100 Train: 0.30391 Test: 0.28992
Epoch: 4200 Train: 0.20653 Test: 0.19768
Epoch: 4300 Train: 0.22248 Test: 0.21164
Epoch: 4400 Train: 0.23988 Test: 0.26675
Epoch: 4500 Train: 0.22348 Test: 0.21111
Epoch: 4600 Train: 0.18701 Test: 0.18582
Epoch: 4700 Train: 0.77651 Test: 0.82893
Epoch: 4800 Train: 0.19961 Test: 0.20213
Epoch: 4900 Train: 0.17759 Test: 0.16090
Epoch: 5000 Train: 0.25157 Test: 0.23076
Epoch: 5100 Train: 0.56375 Test: 0.42187
Epoch: 5200 Train: 0.15195 Test: 0.14798
Epoch: 5300 Train: 0.16988 Test: 0.17229
Epoch 5400: New minimal relative error: 5.74%, model saved.
Epoch: 5400 Train: 0.13889 Test: 0.13887
Epoch: 5500 Train: 0.23705 Test: 0.22968
Epoch: 5600 Train: 0.43213 Test: 0.26742
Epoch 5700: New minimal relative error: 4.93%, model saved.
Epoch: 5700 Train: 0.36842 Test: 0.32597
Epoch: 5800 Train: 0.35639 Test: 0.38304
Epoch: 5900 Train: 0.40495 Test: 0.39915
Epoch: 6000 Train: 0.13478 Test: 0.14936
Epoch: 6100 Train: 0.50962 Test: 0.54404
Epoch: 6200 Train: 0.11748 Test: 0.12186
Epoch: 6300 Train: 0.18326 Test: 0.17954
Epoch: 6400 Train: 0.24088 Test: 0.18149
Epoch: 6500 Train: 0.19287 Test: 0.21596
Epoch: 6600 Train: 0.19991 Test: 0.20176
Epoch: 6700 Train: 0.10669 Test: 0.11157
Epoch: 6800 Train: 0.10448 Test: 0.10742
Epoch: 6900 Train: 0.10560 Test: 0.10610
Epoch: 7000 Train: 0.12164 Test: 0.13057
Epoch: 7100 Train: 0.10681 Test: 0.11660
Epoch: 7200 Train: 0.28056 Test: 0.29754
Epoch: 7300 Train: 0.19898 Test: 0.23281
Epoch: 7400 Train: 0.12889 Test: 0.12556
Epoch: 7500 Train: 0.09938 Test: 0.10919
Epoch: 7600 Train: 0.12417 Test: 0.15202
Epoch 7700: New minimal relative error: 3.99%, model saved.
Epoch: 7700 Train: 0.08992 Test: 0.09582
Epoch: 7800 Train: 0.15344 Test: 0.13885
Epoch 7900: New minimal relative error: 3.16%, model saved.
Epoch: 7900 Train: 0.09813 Test: 0.10625
Epoch: 8000 Train: 0.08830 Test: 0.09258
Epoch: 8100 Train: 0.08600 Test: 0.09337
Epoch: 8200 Train: 0.08874 Test: 0.09200
Epoch: 8300 Train: 0.13198 Test: 0.12190
Epoch: 8400 Train: 0.12045 Test: 0.12829
Epoch: 8500 Train: 0.07980 Test: 0.08546
Epoch: 8600 Train: 0.44139 Test: 0.49906
Epoch: 8700 Train: 0.07748 Test: 0.08312
Epoch: 8800 Train: 0.07722 Test: 0.08200
Epoch: 8900 Train: 0.08645 Test: 0.10094
Epoch: 9000 Train: 0.07662 Test: 0.08122
Epoch: 9100 Train: 0.07371 Test: 0.07926
Epoch: 9200 Train: 0.07378 Test: 0.08338
Epoch: 9300 Train: 0.07581 Test: 0.08418
Epoch: 9400 Train: 0.16876 Test: 0.19744
Epoch: 9500 Train: 0.08377 Test: 0.08597
Epoch: 9600 Train: 0.07211 Test: 0.07934
Epoch: 9700 Train: 0.07012 Test: 0.07503
Epoch: 9800 Train: 0.12072 Test: 0.17002
Epoch: 9900 Train: 0.06709 Test: 0.07256
Epoch: 9999 Train: 0.06715 Test: 0.07308
Training Loss: tensor(0.0671)
Test Loss: tensor(0.0731)
Learned LE: [ 0.8226617   0.03977243 -4.3965645 ]
True LE: [ 8.5975873e-01  6.8248482e-03 -1.4559448e+01]
Relative Error: [0.62281924 0.6410024  0.79546404 1.0450981  0.84948605 0.8126365
 0.7262555  0.49112812 0.5686483  0.19353926 0.7214355  1.1181725
 1.3771698  1.3709553  1.4589359  1.4417958  1.6682936  1.4520057
 0.80107653 0.55695164 0.5194503  0.52189165 0.30718324 0.2755569
 0.37708977 0.41383538 0.48321918 0.70596737 0.7844306  0.9620895
 1.2619944  1.568676   1.3386686  1.1080673  1.1116703  0.82923967
 0.5476089  0.44651538 0.21670125 0.7432444  0.34387934 0.26207006
 0.6856227  0.845326   1.0145037  1.1270767  0.9540203  1.0236646
 1.0568659  1.2073519  1.0080411  0.5175694  0.549316   0.62883437
 0.7429311  0.8827693  0.90835065 0.87911254 0.8399844  0.7150288
 0.5775134  0.36158854 0.30158126 0.4765031  0.6971349  1.0042722
 0.7475311  0.67643666 0.7066026  0.43278012 0.44228876 0.18901397
 0.6577305  1.0693979  1.332544   1.3369347  1.3873206  1.2284315
 1.3114845  1.0853616  0.6252452  0.72882104 0.5296136  1.0329903
 0.9223984  0.7900687  0.66304964 0.6432896  0.5817468  0.7195878
 0.7423191  0.8024813  1.0082786  1.2694045  1.2113072  0.82783335
 0.671679   0.754523   0.6551678  0.3862705  0.21588157 0.4884569
 0.3859744  0.21395649 0.49438024 0.77637035 0.9077554  1.0282524
 0.89212424 0.79927015 0.6830855  0.68654513 0.91006744 0.6198874
 0.5125147  0.60284126 0.674328   0.7619798  0.86656165 0.91420346
 0.861237   0.70631284 0.5710942  0.5244918  0.44789213 0.3711926
 0.5593731  0.8902253  0.70232576 0.57238567 0.60399395 0.5751082
 0.39365754 0.4715601  0.32281938 0.81717306 1.1082149  1.303797
 1.3225908  1.1712071  1.0810812  0.835908   0.8220384  0.61882114
 0.61201954 1.1318196  1.5393428  1.1409087  0.69395256 0.55351454
 0.4620796  0.5307457  0.6413269  0.7278003  0.92124194 1.1614164
 1.2139914  0.8475277  0.43195263 0.5560754  0.6414677  0.6191706
 0.4512185  0.1119653  0.4555069  0.2051265  0.39328867 0.69878316
 0.91876334 0.96558046 0.8901962  0.7042366  0.47265524 0.26712498
 0.6323774  0.6819212  0.4887342  0.54094076 0.628319   0.6535095
 0.7443034  0.7777237  0.7359552  0.65029335 0.5696061  0.5328131
 0.67680305 0.6176307  0.54283524 0.72714126 0.7358244  0.5803161
 0.5331931  0.62764335 0.5162255  0.5252715  0.1309981  0.601222
 0.8998163  1.045982   1.1413066  1.2184227  0.9952464  0.77478105
 1.0908585  0.66879636 0.6197976  1.2049932  1.3410475  1.2824218
 0.59483206 0.35664636 0.32750615 0.27174324 0.25820994 0.33348888
 0.6522473  1.0368762  0.958266   1.0991507  0.7907192  0.167003
 0.68991256 0.72993124 0.7214058  0.5685812  0.25256708 0.27032298
 0.37335396 0.5308548  0.92718273 1.002299   0.9199409  0.7838314
 0.48182195 0.269362   0.33402735 0.5028543  0.5146158  0.42378148
 0.48585954 0.48366097 0.5252375  0.59914047 0.6233691  0.6687518
 0.67568064 0.36594427 0.32506374 0.59063584 0.64411116 0.662707
 0.72598815 0.7242438  0.71060497 0.5422738  0.6135499  0.5294709
 0.25234747 0.33880663 0.87658745 0.96949583 0.93842083 1.1042553
 0.9217552  0.8164264  1.1964921  0.72910124 0.5148721  1.0050459
 1.1587491  0.94818556 0.6204644  0.30260655 0.3746612  0.41893116
 0.40003514 0.40871775 0.20814934 0.51000774 0.75011754 0.7524832
 0.93249494 0.9228196  0.16716136 0.62854946 0.8453333  0.8050423
 0.43437234 0.32542938 0.24245001 0.31245956 0.6018402  1.0177636
 1.0505567  0.82470906 0.67546326 0.38912722 0.28290713 0.3192796
 0.5569163  0.4989794  0.5406568  0.45051345 0.37628123 0.42552996
 0.5295892  0.6760597  0.8554104  0.54776603 0.3437652  0.15805808
 0.21702205 0.6265837  0.7278381  0.5263525  0.7454293  0.8536244
 0.6422588  0.57168484 0.5833936  0.43871245 0.8049078  1.1914955
 1.0840929  0.9955711  1.0443518  0.7677299  1.2710357  0.89051473
 0.5547085  0.6885062  0.81109965 0.72085553 0.32777977 0.24926212
 0.3058187  0.426532   0.6111881  0.80334234 0.7413463  0.40567172
 0.26887885 0.37038326 0.64217544 0.8180108  1.0502822  0.32868138
 0.58127034 0.73793525 0.6601497  0.3906482  0.2665698  0.3174975
 0.27993855 0.7278143  1.0634934  0.8428488  0.6532866  0.44025397
 0.46303886 0.3303034  0.29980445 0.74211836 0.7392517  0.59141666
 0.3567063  0.28000018 0.37479138 0.6564942  0.95024335 0.75768405
 0.5540015  0.52694935 0.46979195 0.42188606 0.6886183  0.61346835
 0.3032736  0.6754277  1.1615021  0.9878657  0.6829321  0.71217555
 0.77193826 1.2487873  1.4595498  1.1432896  1.0961175  0.8855938
 0.9550154  1.2099996  0.71637905 0.6165536  0.51916    0.42162672
 0.2668306  0.540751   0.3793093  0.51676154]
