time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.91%, model saved.
Epoch: 0 Train: 60835.74609 Test: 3975.42749
Epoch: 80 Train: 14166.15039 Test: 1603.00171
Epoch: 160 Train: 13651.31152 Test: 1128.37891
Epoch 240: New minimal relative error: 90.32%, model saved.
Epoch: 240 Train: 13753.12988 Test: 1280.26990
Epoch 320: New minimal relative error: 85.46%, model saved.
Epoch: 320 Train: 13582.33887 Test: 1269.83154
Epoch 400: New minimal relative error: 51.62%, model saved.
Epoch: 400 Train: 13201.03809 Test: 1393.75073
Epoch: 480 Train: 12881.54492 Test: 1378.17468
Epoch: 560 Train: 14327.51758 Test: 1330.54126
Epoch: 640 Train: 11086.61426 Test: 1111.27844
Epoch: 720 Train: 12797.18359 Test: 1415.14673
Epoch: 800 Train: 10704.76953 Test: 1113.12561
Epoch: 880 Train: 10902.03223 Test: 852.55084
Epoch: 960 Train: 9509.05469 Test: 814.69751
Epoch: 1040 Train: 9541.69238 Test: 738.52240
Epoch: 1120 Train: 10052.02539 Test: 995.86261
Epoch: 1200 Train: 8121.25439 Test: 486.11493
Epoch: 1280 Train: 8867.27148 Test: 647.17865
Epoch: 1360 Train: 6295.16357 Test: 272.83917
Epoch: 1440 Train: 5682.91602 Test: 313.61920
Epoch: 1520 Train: 3387.28833 Test: 203.01846
Epoch: 1600 Train: 1427.14746 Test: 187.02753
Epoch 1680: New minimal relative error: 13.42%, model saved.
Epoch: 1680 Train: 1101.22302 Test: 35.46310
Epoch 1760: New minimal relative error: 7.53%, model saved.
Epoch: 1760 Train: 487.55148 Test: 6.30764
Epoch: 1840 Train: 375.01611 Test: 4.47520
Epoch 1920: New minimal relative error: 6.54%, model saved.
Epoch: 1920 Train: 302.52368 Test: 7.29563
Epoch 2000: New minimal relative error: 2.98%, model saved.
Epoch: 2000 Train: 275.74789 Test: 6.33254
Epoch: 2080 Train: 224.26361 Test: 4.64714
Epoch: 2160 Train: 211.89708 Test: 3.46690
Epoch: 2240 Train: 212.62607 Test: 4.51829
Epoch: 2320 Train: 193.25961 Test: 2.05682
Epoch: 2400 Train: 195.56018 Test: 2.80692
Epoch: 2480 Train: 190.78938 Test: 6.53211
Epoch: 2560 Train: 154.50032 Test: 2.26572
Epoch: 2640 Train: 155.69032 Test: 3.35489
Epoch: 2720 Train: 150.78638 Test: 1.48496
Epoch: 2800 Train: 153.44110 Test: 5.21560
Epoch: 2880 Train: 141.19475 Test: 2.78028
Epoch: 2960 Train: 158.35802 Test: 2.14563
Epoch: 3040 Train: 137.73129 Test: 1.32325
Epoch: 3120 Train: 113.89989 Test: 2.23510
Epoch: 3200 Train: 124.86598 Test: 6.99607
Epoch: 3280 Train: 157.08844 Test: 8.65584
Epoch: 3360 Train: 112.03513 Test: 1.21754
Epoch: 3440 Train: 127.51279 Test: 2.73423
Epoch: 3520 Train: 118.63663 Test: 2.68169
Epoch: 3600 Train: 113.70450 Test: 2.86514
Epoch: 3680 Train: 97.95074 Test: 1.26065
Epoch: 3760 Train: 107.18966 Test: 0.83417
Epoch: 3840 Train: 96.06889 Test: 0.99698
Epoch: 3920 Train: 95.02013 Test: 0.75774
Epoch 4000: New minimal relative error: 2.26%, model saved.
Epoch: 4000 Train: 92.02517 Test: 0.68995
Epoch: 4080 Train: 92.30917 Test: 0.77857
Epoch: 4160 Train: 134.64861 Test: 7.65687
Epoch: 4240 Train: 99.99815 Test: 1.92714
Epoch: 4320 Train: 89.23253 Test: 0.57805
Epoch: 4400 Train: 109.94861 Test: 3.71654
Epoch: 4480 Train: 83.49178 Test: 0.52395
Epoch: 4560 Train: 77.46966 Test: 0.84842
Epoch: 4640 Train: 82.83398 Test: 0.93540
Epoch: 4720 Train: 76.64651 Test: 2.54106
Epoch: 4800 Train: 81.99020 Test: 3.21568
Epoch: 4880 Train: 69.76588 Test: 0.84204
Epoch: 4960 Train: 70.91557 Test: 0.48903
Epoch: 5040 Train: 66.17564 Test: 0.42831
Epoch: 5120 Train: 77.70036 Test: 0.66431
Epoch: 5200 Train: 74.04743 Test: 0.68100
Epoch: 5280 Train: 71.91654 Test: 3.20876
Epoch 5360: New minimal relative error: 1.87%, model saved.
Epoch: 5360 Train: 64.56160 Test: 0.45337
Epoch: 5440 Train: 66.50919 Test: 0.40495
Epoch: 5520 Train: 66.29578 Test: 2.93601
Epoch: 5600 Train: 60.44435 Test: 0.40742
Epoch: 5680 Train: 68.36761 Test: 2.16960
Epoch: 5760 Train: 58.59179 Test: 0.33420
Epoch: 5840 Train: 60.12662 Test: 0.39308
Epoch: 5920 Train: 68.93886 Test: 2.46484
Epoch: 6000 Train: 65.65953 Test: 1.63637
Epoch: 6080 Train: 58.74345 Test: 1.17725
Epoch: 6160 Train: 58.01285 Test: 1.07400
Epoch: 6240 Train: 58.60889 Test: 0.48535
Epoch: 6320 Train: 62.93955 Test: 2.52245
Epoch: 6400 Train: 55.50949 Test: 0.37280
Epoch: 6480 Train: 55.99457 Test: 0.37064
Epoch: 6560 Train: 56.99437 Test: 0.96131
Epoch: 6640 Train: 57.01635 Test: 0.55955
Epoch: 6720 Train: 55.21217 Test: 0.71857
Epoch: 6800 Train: 53.47463 Test: 0.57498
Epoch: 6880 Train: 54.41088 Test: 0.42458
Epoch: 6960 Train: 59.34437 Test: 2.63128
Epoch: 7040 Train: 70.51363 Test: 1.76433
Epoch: 7120 Train: 56.02746 Test: 0.35823
Epoch: 7200 Train: 57.66418 Test: 0.35785
Epoch: 7280 Train: 59.81981 Test: 0.52670
Epoch 7360: New minimal relative error: 1.69%, model saved.
Epoch: 7360 Train: 53.94990 Test: 0.42643
Epoch 7440: New minimal relative error: 1.65%, model saved.
Epoch: 7440 Train: 59.43737 Test: 0.38826
Epoch: 7520 Train: 56.51832 Test: 0.38094
Epoch 7600: New minimal relative error: 1.56%, model saved.
Epoch: 7600 Train: 56.74526 Test: 0.37042
Epoch: 7680 Train: 53.23933 Test: 0.79011
Epoch 7760: New minimal relative error: 0.71%, model saved.
Epoch: 7760 Train: 48.72228 Test: 0.28258
Epoch: 7840 Train: 46.81359 Test: 0.52839
Epoch: 7920 Train: 46.71329 Test: 0.41053
Epoch: 7999 Train: 47.66540 Test: 0.31640
Training Loss: tensor(47.6654)
Test Loss: tensor(0.3164)
Learned LE: [ 8.9033043e-01 -6.4148777e-03 -1.4555947e+01]
True LE: [ 8.6872905e-01  1.1427456e-02 -1.4555925e+01]
Relative Error: [0.39583495 0.47400182 0.25978595 0.2395018  0.36477348 0.39553377
 0.38356647 0.4892418  0.44851038 0.40517724 0.5092426  0.7947254
 0.91879785 0.89414597 0.7423013  0.6503926  0.84900486 0.9384185
 0.98031163 1.0024192  0.98919225 1.1174383  1.0656822  0.5421638
 0.34549117 0.34342125 0.3694707  0.3775195  0.5733327  0.66495675
 0.71070516 0.5576262  0.618762   0.72532624 0.74797446 0.5901932
 0.42695132 0.17922522 0.1527221  0.17848928 0.30849823 0.3322282
 0.34060696 0.22958198 0.17900495 0.23439263 0.20987792 0.3130154
 0.14513959 0.32537025 0.472617   0.30099803 0.11155843 0.33212045
 0.13749662 0.15734279 0.20267956 0.1995065  0.24602187 0.2583533
 0.29323894 0.45724162 0.48260546 0.5145578  0.40605822 0.28160322
 0.2865657  0.28470898 0.319268   0.24251163 0.22560996 0.21909757
 0.4059838  0.6162358  0.8163159  0.93643486 0.9361204  0.9329448
 0.8511396  0.945803   0.98445797 0.8985874  0.93090487 0.91183835
 0.96802825 0.9914908  0.57182384 0.40166205 0.38280767 0.32495487
 0.36801305 0.60128397 0.68262726 0.6633565  0.6164521  0.6351846
 0.6553881  0.5344407  0.41046426 0.369839   0.20152251 0.15704824
 0.18924728 0.26883182 0.30684462 0.3201051  0.2047295  0.26714587
 0.2125354  0.21474713 0.36753517 0.2256531  0.28209618 0.40486407
 0.26913512 0.23838766 0.43300134 0.25167283 0.20858078 0.25753525
 0.2442018  0.24032037 0.25932947 0.40161967 0.54543793 0.50136894
 0.49040672 0.38905567 0.2894977  0.27888992 0.24096759 0.2861942
 0.14947498 0.12629424 0.21856737 0.52992564 0.67620826 0.815701
 0.84486425 0.82625335 0.8177118  0.7814941  0.8651415  0.8503548
 0.73675567 0.78354615 0.6837262  0.74327177 0.9738204  0.69298464
 0.47169006 0.3805623  0.276144   0.42395708 0.61019355 0.620645
 0.59384716 0.70162094 0.6771936  0.54104465 0.43018252 0.32476732
 0.3733519  0.32003638 0.30769843 0.21996023 0.21736439 0.23938797
 0.2643682  0.30224496 0.26971462 0.20436728 0.16908191 0.25274414
 0.29534033 0.10662057 0.3482882  0.28500977 0.3890118  0.50650203
 0.34142238 0.2539596  0.2959168  0.2944701  0.20946203 0.21352354
 0.43638322 0.42369777 0.42127073 0.44557667 0.21890384 0.23756036
 0.26657376 0.2310685  0.14789487 0.15924223 0.29504776 0.45166728
 0.5325218  0.5894462  0.6982653  0.7472754  0.72433174 0.6245742
 0.4503221  0.5767409  0.5588973  0.5207121  0.5535622  0.63166726
 0.54221034 0.6919681  0.58259946 0.44334686 0.38185927 0.35104296
 0.4906988  0.6415059  0.5886366  0.54650426 0.6172932  0.6589316
 0.46469894 0.38099188 0.23193893 0.18636179 0.38888395 0.44765648
 0.447738   0.28489187 0.15458387 0.27305374 0.3337417  0.3333241
 0.23159862 0.19267087 0.11037318 0.30860785 0.05495395 0.2724402
 0.37613297 0.37935174 0.48508272 0.43020207 0.29683653 0.32883137
 0.35025692 0.24941982 0.20547523 0.2702314  0.28253907 0.37595284
 0.39397785 0.20549396 0.1997584  0.25654316 0.18725865 0.11640312
 0.24966383 0.28144157 0.32088485 0.25979072 0.3978288  0.4669822
 0.48654693 0.44561318 0.38039848 0.1782865  0.18336697 0.25386378
 0.36151606 0.28070313 0.4677264  0.46389848 0.39026976 0.3015353
 0.28595164 0.38857582 0.53383356 0.54312754 0.66541636 0.6459678
 0.5115203  0.48391563 0.54329365 0.40894145 0.41141325 0.18331
 0.12441569 0.40802962 0.5465196  0.37802914 0.24978511 0.18842918
 0.37829706 0.45019585 0.4930766  0.5751218  0.30061567 0.23579647
 0.0803447  0.09641866 0.18714412 0.40464756 0.32348976 0.29740486
 0.3438689  0.4333362  0.36971906 0.3476343  0.25499713 0.19428603
 0.20636763 0.26229978 0.29432416 0.3568286  0.24773318 0.17521724
 0.12089859 0.12678275 0.17498142 0.24781203 0.3765228  0.4097241
 0.25462314 0.26905918 0.2777389  0.24425991 0.21168648 0.28655314
 0.32108995 0.22474866 0.22984725 0.2730631  0.31230548 0.28716415
 0.36525443 0.30887777 0.2989168  0.26623935 0.07415019 0.28961954
 0.48410386 0.6632625  0.67712593 0.66517407 0.50941265 0.38860437
 0.32678688 0.32456896 0.33225188 0.2459598  0.2805314  0.5356315
 0.4738017  0.4009899  0.21426451 0.3250508  0.53473043 0.6107932
 0.602893   0.5729643  0.42763427 0.25385326 0.07061408 0.08772669
 0.2682096  0.29839265 0.17736426 0.10417569 0.31314644 0.36958343
 0.35092622 0.18663526 0.15340742 0.1466907  0.17218265 0.20111033
 0.29680037 0.35867983 0.24466857 0.12089786 0.02867834 0.12105858
 0.190937   0.25566798 0.26947486 0.2506368  0.2405579  0.26501775
 0.21092553 0.11747818 0.27425605 0.57211834 0.6252163  0.57236737
 0.5926064  0.64969957 0.6172145  0.5624518 ]
