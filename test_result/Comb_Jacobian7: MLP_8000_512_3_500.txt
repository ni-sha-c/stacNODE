time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.37%, model saved.
Epoch: 0 Train: 31128.09766 Test: 4145.51904
Epoch: 80 Train: 4008.84473 Test: 762.72382
Epoch 160: New minimal relative error: 38.49%, model saved.
Epoch: 160 Train: 340.28082 Test: 36.08759
Epoch 240: New minimal relative error: 30.10%, model saved.
Epoch: 240 Train: 114.80956 Test: 29.11759
Epoch 320: New minimal relative error: 9.60%, model saved.
Epoch: 320 Train: 42.16520 Test: 8.10435
Epoch: 400 Train: 25.85092 Test: 2.80878
Epoch 480: New minimal relative error: 9.03%, model saved.
Epoch: 480 Train: 22.32492 Test: 11.04748
Epoch: 560 Train: 22.59041 Test: 10.21692
Epoch: 640 Train: 18.90254 Test: 3.20271
Epoch: 720 Train: 22.59890 Test: 11.99198
Epoch 800: New minimal relative error: 3.25%, model saved.
Epoch: 800 Train: 6.09118 Test: 0.12121
Epoch: 880 Train: 7.74781 Test: 2.03267
Epoch: 960 Train: 12.82640 Test: 2.21688
Epoch: 1040 Train: 17.53411 Test: 8.88026
Epoch: 1120 Train: 5.83320 Test: 1.74189
Epoch: 1200 Train: 46.70417 Test: 27.77923
Epoch: 1280 Train: 4.01760 Test: 0.81332
Epoch: 1360 Train: 2.65477 Test: 0.03074
Epoch: 1440 Train: 2.92944 Test: 0.03358
Epoch: 1520 Train: 5.06522 Test: 2.31605
Epoch: 1600 Train: 6.20328 Test: 1.43410
Epoch: 1680 Train: 4.87954 Test: 2.01031
Epoch: 1760 Train: 5.05683 Test: 2.00846
Epoch: 1840 Train: 2.98928 Test: 0.50860
Epoch 1920: New minimal relative error: 2.11%, model saved.
Epoch: 1920 Train: 1.65310 Test: 0.09680
Epoch 2000: New minimal relative error: 1.93%, model saved.
Epoch: 2000 Train: 1.51946 Test: 0.03116
Epoch: 2080 Train: 2.13217 Test: 0.60379
Epoch: 2160 Train: 1.40644 Test: 0.05369
Epoch: 2240 Train: 2.45819 Test: 0.82091
Epoch: 2320 Train: 4.77897 Test: 2.88093
Epoch 2400: New minimal relative error: 0.92%, model saved.
Epoch: 2400 Train: 1.12664 Test: 0.01984
Epoch: 2480 Train: 3.14825 Test: 0.54676
Epoch: 2560 Train: 1.19374 Test: 0.13240
Epoch: 2640 Train: 1.00865 Test: 0.03752
Epoch: 2720 Train: 1.57558 Test: 0.12388
Epoch: 2800 Train: 0.87119 Test: 0.00931
Epoch: 2880 Train: 0.94651 Test: 0.04179
Epoch: 2960 Train: 1.71615 Test: 0.47785
Epoch: 3040 Train: 7.28768 Test: 5.76092
Epoch: 3120 Train: 0.75997 Test: 0.01322
Epoch: 3200 Train: 2.09899 Test: 0.45958
Epoch: 3280 Train: 1.02979 Test: 0.04312
Epoch: 3360 Train: 6.44770 Test: 1.41897
Epoch: 3440 Train: 1.41348 Test: 0.40556
Epoch: 3520 Train: 1.29861 Test: 0.26330
Epoch: 3600 Train: 1.49424 Test: 0.57727
Epoch: 3680 Train: 3.24007 Test: 0.43842
Epoch: 3760 Train: 2.53396 Test: 2.45237
Epoch: 3840 Train: 0.52427 Test: 0.00561
Epoch: 3920 Train: 2.91836 Test: 1.00449
Epoch: 4000 Train: 1.15681 Test: 0.46511
Epoch: 4080 Train: 1.89227 Test: 0.85834
Epoch: 4160 Train: 4.56595 Test: 2.08002
Epoch: 4240 Train: 0.50521 Test: 0.05346
Epoch: 4320 Train: 0.47086 Test: 0.02933
Epoch: 4400 Train: 0.94188 Test: 0.27995
Epoch: 4480 Train: 1.41285 Test: 0.30649
Epoch: 4560 Train: 3.35726 Test: 1.92818
Epoch: 4640 Train: 1.23448 Test: 0.21913
Epoch: 4720 Train: 0.90604 Test: 0.41084
Epoch: 4800 Train: 0.35717 Test: 0.00471
Epoch: 4880 Train: 0.38892 Test: 0.01326
Epoch: 4960 Train: 4.17566 Test: 2.06277
Epoch: 5040 Train: 0.33712 Test: 0.01243
Epoch: 5120 Train: 0.37122 Test: 0.03788
Epoch: 5200 Train: 0.31909 Test: 0.00813
Epoch: 5280 Train: 1.10181 Test: 0.60716
Epoch: 5360 Train: 2.48680 Test: 1.42795
Epoch: 5440 Train: 1.48169 Test: 1.00038
Epoch 5520: New minimal relative error: 0.68%, model saved.
Epoch: 5520 Train: 0.28147 Test: 0.00456
Epoch: 5600 Train: 6.46006 Test: 2.04570
Epoch: 5680 Train: 0.72619 Test: 0.43278
Epoch: 5760 Train: 0.26389 Test: 0.00624
Epoch: 5840 Train: 0.24934 Test: 0.00251
Epoch: 5920 Train: 0.31735 Test: 0.07549
Epoch: 6000 Train: 0.23807 Test: 0.00237
Epoch: 6080 Train: 3.46755 Test: 1.70282
Epoch: 6160 Train: 0.23294 Test: 0.00670
Epoch: 6240 Train: 0.23843 Test: 0.01392
Epoch 6320: New minimal relative error: 0.57%, model saved.
Epoch: 6320 Train: 0.21690 Test: 0.00233
Epoch: 6400 Train: 0.91264 Test: 0.58523
Epoch: 6480 Train: 0.20765 Test: 0.00231
Epoch: 6560 Train: 0.23410 Test: 0.01499
Epoch: 6640 Train: 0.36295 Test: 0.10865
Epoch: 6720 Train: 0.23436 Test: 0.03522
Epoch: 6800 Train: 2.49061 Test: 1.23311
Epoch 6880: New minimal relative error: 0.35%, model saved.
Epoch: 6880 Train: 0.18612 Test: 0.00233
Epoch: 6960 Train: 0.35608 Test: 0.04971
Epoch: 7040 Train: 0.17830 Test: 0.00225
Epoch: 7120 Train: 0.21367 Test: 0.00594
Epoch: 7200 Train: 0.17129 Test: 0.00223
Epoch: 7280 Train: 2.14127 Test: 0.26466
Epoch: 7360 Train: 0.16551 Test: 0.00232
Epoch 7440: New minimal relative error: 0.26%, model saved.
Epoch: 7440 Train: 0.16166 Test: 0.00220
Epoch: 7520 Train: 0.81630 Test: 0.21575
Epoch: 7600 Train: 0.15600 Test: 0.00228
Epoch: 7680 Train: 0.15262 Test: 0.00218
Epoch: 7760 Train: 0.19548 Test: 0.00834
Epoch: 7840 Train: 0.14716 Test: 0.00217
Epoch: 7920 Train: 0.40681 Test: 0.11684
Epoch: 7999 Train: 0.14214 Test: 0.00221
Training Loss: tensor(0.1421)
Test Loss: tensor(0.0022)
Learned LE: [ 8.7255228e-01 -7.7745775e-03 -1.4542863e+01]
True LE: [ 8.6779112e-01  4.4746112e-04 -1.4542517e+01]
Relative Error: [0.04137836 0.05322915 0.06821045 0.08480088 0.09766809 0.10097383
 0.09421375 0.08256892 0.07317618 0.06650792 0.05745143 0.04642823
 0.04000081 0.03894021 0.03848722 0.03599813 0.03042722 0.0221883
 0.01513609 0.01717087 0.0256646  0.03469152 0.04354539 0.05251786
 0.06185156 0.07174926 0.08337318 0.09316026 0.09401397 0.09045603
 0.08779642 0.09563465 0.10872666 0.11692136 0.11963085 0.10226301
 0.08849365 0.09209435 0.09749592 0.09662641 0.09778123 0.10915527
 0.12297124 0.13069065 0.13183784 0.12848668 0.12265957 0.11484461
 0.10518837 0.09394003 0.08197448 0.06942578 0.05717871 0.04545782
 0.03503688 0.02798807 0.02479721 0.02068167 0.01151741 0.00931542
 0.02241813 0.03525039 0.04640144 0.05551227 0.06667192 0.08131764
 0.09399136 0.09846689 0.09338821 0.08328024 0.07515134 0.06921425
 0.05955513 0.04734564 0.04022555 0.0393131  0.03972678 0.03784601
 0.03199177 0.02237074 0.01314934 0.01530239 0.02522517 0.0345293
 0.04269734 0.05139462 0.06057934 0.06897125 0.07659867 0.08537792
 0.08873783 0.0845127  0.07942761 0.08094542 0.09417356 0.1052378
 0.11068969 0.09235962 0.07729249 0.08081279 0.08277683 0.07858862
 0.08162272 0.09634966 0.10931123 0.11433833 0.11338188 0.11006741
 0.10537446 0.09876845 0.09009194 0.07992126 0.06923932 0.05886653
 0.04974762 0.04176737 0.0345745  0.02798216 0.02292171 0.01872178
 0.01057293 0.00590959 0.02320331 0.03922176 0.0517588  0.0595722
 0.06601981 0.07720251 0.0890523  0.09509651 0.09254871 0.0849534
 0.07854163 0.0741355  0.06488829 0.05256547 0.04494393 0.04288356
 0.04232103 0.04025717 0.03460483 0.02478407 0.01469273 0.01530323
 0.02480187 0.03407991 0.04119727 0.04761694 0.05593362 0.06557865
 0.07225295 0.07682399 0.08188236 0.07807542 0.07174752 0.06726487
 0.07728453 0.09120257 0.10173158 0.08499587 0.06589872 0.06825086
 0.06704082 0.06026201 0.06493515 0.08080588 0.09147547 0.09390315
 0.09273353 0.09147515 0.08926785 0.0846646  0.07710448 0.06746677
 0.05697702 0.04708586 0.03906196 0.03371057 0.03026348 0.0269677
 0.02238511 0.01687731 0.01008531 0.00265765 0.02115218 0.04044979
 0.05615432 0.06556778 0.06834166 0.07380002 0.08382866 0.09059462
 0.0910605  0.08677778 0.08315773 0.08080999 0.07314716 0.06135447
 0.05245823 0.04686325 0.04327142 0.04133679 0.03690299 0.02796796
 0.02001664 0.02149709 0.02892011 0.03674975 0.04297707 0.04678293
 0.05037226 0.05805617 0.06750698 0.0712592  0.07279429 0.07244743
 0.06346494 0.05698233 0.05837486 0.07393344 0.09049567 0.08194374
 0.05545996 0.05511665 0.05170359 0.04260663 0.04768793 0.06282421
 0.07099579 0.0727046  0.0738458  0.0770971  0.07929546 0.07758945
 0.07161038 0.06261828 0.05188864 0.04098466 0.03164693 0.02587452
 0.02392869 0.02379148 0.0223265  0.01772759 0.01058171 0.00469835
 0.01764482 0.03812721 0.05737874 0.0714449  0.07563792 0.07399845
 0.07926736 0.08575662 0.08837239 0.0879574  0.08777532 0.08828516
 0.08312757 0.07252655 0.06197849 0.05049758 0.04137582 0.03943899
 0.03653273 0.02867212 0.02452944 0.02942076 0.03745566 0.04410375
 0.04772796 0.04931593 0.04954555 0.05012796 0.05653533 0.0657375
 0.06590864 0.06449932 0.05801134 0.04766018 0.04254768 0.05264251
 0.07274741 0.08166751 0.04986769 0.04194812 0.03805655 0.02689993
 0.03089328 0.0438245  0.05071022 0.05332651 0.05922916 0.06937094
 0.07707722 0.07869209 0.07440763 0.06638105 0.05609485 0.04505529
 0.03437819 0.02584225 0.02169728 0.02180139 0.02274305 0.02127583
 0.01613278 0.01051671 0.01571509 0.03305278 0.05438612 0.0729337
 0.08436814 0.08248401 0.07831072 0.08217418 0.08477594 0.08684721
 0.09043027 0.09471749 0.09325001 0.08490799 0.07398694 0.05677757
 0.03824822 0.03386175 0.03301131 0.02685822 0.02459388 0.03120268
 0.0421755  0.05254924 0.0574269  0.05512618 0.04964511 0.0460479
 0.04501015 0.05043952 0.05965492 0.05666534 0.05268214 0.04101299
 0.0323688  0.03086582 0.04774945 0.07334191 0.05655116 0.03025282
 0.02722738 0.01452874 0.01760375 0.02788401 0.03301905 0.03725554
 0.04862637 0.06676584 0.07947666 0.08287714 0.07891759 0.07094397
 0.06173961 0.05271437 0.04433369 0.03639756 0.02954497 0.02572627
 0.02566038 0.02650432 0.02481457 0.02053107 0.01936187 0.02878815
 0.04787591 0.06914771 0.08649424 0.09533879 0.08823112 0.08197311
 0.08321477 0.08369507 0.08901148 0.09770904 0.10144551 0.09658033
 0.08805763 0.06977644 0.0406527  0.02556691 0.02728453 0.0244893
 0.02184707 0.02625877 0.03824778 0.05300705 0.0638648  0.06596251
 0.05854125 0.0460934  0.03801071 0.03606267]
