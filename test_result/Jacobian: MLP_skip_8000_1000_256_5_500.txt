time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.90%, model saved.
Epoch: 0 Train: 31945.15234 Test: 3798.29248
Epoch: 80 Train: 9118.93359 Test: 1435.03418
Epoch 160: New minimal relative error: 75.12%, model saved.
Epoch: 160 Train: 8637.46777 Test: 1263.60706
Epoch 240: New minimal relative error: 68.78%, model saved.
Epoch: 240 Train: 7672.15234 Test: 1107.68433
Epoch: 320 Train: 8376.25098 Test: 1128.12720
Epoch: 400 Train: 7742.97363 Test: 1118.12610
Epoch: 480 Train: 7543.58447 Test: 1039.15625
Epoch 560: New minimal relative error: 60.94%, model saved.
Epoch: 560 Train: 5350.50391 Test: 631.26312
Epoch: 640 Train: 4625.94434 Test: 668.39276
Epoch: 720 Train: 2361.19678 Test: 174.85085
Epoch 800: New minimal relative error: 47.47%, model saved.
Epoch: 800 Train: 1463.96423 Test: 73.89632
Epoch: 880 Train: 1073.19165 Test: 160.49394
Epoch 960: New minimal relative error: 23.28%, model saved.
Epoch: 960 Train: 714.33832 Test: 22.47669
Epoch: 1040 Train: 588.95868 Test: 15.00456
Epoch 1120: New minimal relative error: 11.02%, model saved.
Epoch: 1120 Train: 520.94678 Test: 12.16383
Epoch 1200: New minimal relative error: 10.49%, model saved.
Epoch: 1200 Train: 463.95020 Test: 9.33000
Epoch: 1280 Train: 409.25797 Test: 13.86852
Epoch: 1360 Train: 377.20181 Test: 7.49138
Epoch: 1440 Train: 383.07278 Test: 43.10464
Epoch: 1520 Train: 325.83081 Test: 5.85940
Epoch: 1600 Train: 296.93546 Test: 4.52772
Epoch 1680: New minimal relative error: 9.47%, model saved.
Epoch: 1680 Train: 278.11768 Test: 4.18262
Epoch: 1760 Train: 254.36017 Test: 5.85618
Epoch 1840: New minimal relative error: 9.23%, model saved.
Epoch: 1840 Train: 254.91272 Test: 3.81068
Epoch: 1920 Train: 261.11456 Test: 15.81250
Epoch: 2000 Train: 229.38637 Test: 4.42407
Epoch 2080: New minimal relative error: 5.40%, model saved.
Epoch: 2080 Train: 214.80128 Test: 2.89088
Epoch: 2160 Train: 204.37111 Test: 3.58115
Epoch: 2240 Train: 224.48750 Test: 3.57505
Epoch: 2320 Train: 200.84517 Test: 2.63045
Epoch: 2400 Train: 188.04396 Test: 4.95512
Epoch: 2480 Train: 160.95670 Test: 1.70260
Epoch: 2560 Train: 150.53531 Test: 2.11851
Epoch: 2640 Train: 143.51488 Test: 2.27723
Epoch 2720: New minimal relative error: 4.28%, model saved.
Epoch: 2720 Train: 134.59683 Test: 1.27078
Epoch: 2800 Train: 135.98343 Test: 1.47999
Epoch: 2880 Train: 128.67213 Test: 1.17792
Epoch: 2960 Train: 128.26711 Test: 1.24059
Epoch: 3040 Train: 122.21780 Test: 1.18248
Epoch: 3120 Train: 118.84354 Test: 3.18175
Epoch: 3200 Train: 107.47311 Test: 0.97953
Epoch: 3280 Train: 107.26244 Test: 1.75852
Epoch: 3360 Train: 105.46150 Test: 0.97480
Epoch: 3440 Train: 106.19914 Test: 0.98289
Epoch: 3520 Train: 102.09707 Test: 0.91484
Epoch: 3600 Train: 113.92216 Test: 1.51017
Epoch 3680: New minimal relative error: 4.25%, model saved.
Epoch: 3680 Train: 106.60591 Test: 0.93333
Epoch: 3760 Train: 103.02274 Test: 1.02932
Epoch: 3840 Train: 105.75980 Test: 1.06108
Epoch: 3920 Train: 105.62571 Test: 1.10770
Epoch: 4000 Train: 104.58901 Test: 1.39182
Epoch: 4080 Train: 102.61172 Test: 1.09541
Epoch: 4160 Train: 98.29189 Test: 2.06292
Epoch: 4240 Train: 94.03650 Test: 0.85299
Epoch 4320: New minimal relative error: 3.03%, model saved.
Epoch: 4320 Train: 98.96893 Test: 1.11746
Epoch: 4400 Train: 89.45276 Test: 0.94561
Epoch: 4480 Train: 85.31905 Test: 0.69346
Epoch: 4560 Train: 82.73537 Test: 0.62286
Epoch: 4640 Train: 78.79565 Test: 0.81048
Epoch: 4720 Train: 78.02471 Test: 0.61802
Epoch: 4800 Train: 79.29356 Test: 0.66500
Epoch: 4880 Train: 76.85337 Test: 0.85805
Epoch: 4960 Train: 79.17392 Test: 4.55228
Epoch: 5040 Train: 73.41763 Test: 0.59201
Epoch: 5120 Train: 71.74267 Test: 0.62323
Epoch: 5200 Train: 70.79648 Test: 0.59116
Epoch: 5280 Train: 70.40941 Test: 0.56573
Epoch: 5360 Train: 67.53520 Test: 0.69959
Epoch: 5440 Train: 64.95418 Test: 0.49147
Epoch: 5520 Train: 66.83234 Test: 0.69314
Epoch: 5600 Train: 65.24417 Test: 0.50675
Epoch: 5680 Train: 63.27548 Test: 0.47006
Epoch: 5760 Train: 68.68421 Test: 0.57675
Epoch: 5840 Train: 75.73168 Test: 0.69295
Epoch: 5920 Train: 68.25964 Test: 0.62243
Epoch: 6000 Train: 65.27892 Test: 0.57844
Epoch: 6080 Train: 62.06258 Test: 0.89131
Epoch: 6160 Train: 62.52577 Test: 0.55167
Epoch: 6240 Train: 61.39233 Test: 0.50911
Epoch: 6320 Train: 61.14922 Test: 0.55703
Epoch: 6400 Train: 58.59409 Test: 0.44285
Epoch: 6480 Train: 56.04401 Test: 0.50723
Epoch: 6560 Train: 57.09529 Test: 0.42968
Epoch: 6640 Train: 56.11618 Test: 0.42975
Epoch: 6720 Train: 57.60600 Test: 1.53522
Epoch: 6800 Train: 57.45743 Test: 0.42873
Epoch: 6880 Train: 57.84374 Test: 0.53146
Epoch: 6960 Train: 59.05629 Test: 0.45801
Epoch 7040: New minimal relative error: 2.58%, model saved.
Epoch: 7040 Train: 58.37967 Test: 0.45882
Epoch 7120: New minimal relative error: 2.42%, model saved.
Epoch: 7120 Train: 56.37845 Test: 0.40755
Epoch: 7200 Train: 56.44357 Test: 0.40580
Epoch: 7280 Train: 58.12230 Test: 0.46708
Epoch: 7360 Train: 59.67155 Test: 0.51869
Epoch: 7440 Train: 58.34091 Test: 0.59385
Epoch: 7520 Train: 59.00075 Test: 0.46822
Epoch: 7600 Train: 58.63531 Test: 0.48203
Epoch: 7680 Train: 59.26006 Test: 0.53207
Epoch: 7760 Train: 59.52532 Test: 0.50419
Epoch: 7840 Train: 58.82373 Test: 0.45741
Epoch: 7920 Train: 58.09374 Test: 0.51744
Epoch: 7999 Train: 55.06681 Test: 0.44044
Training Loss: tensor(55.0668)
Test Loss: tensor(0.4404)
Learned LE: [  0.88629884  -0.02919927 -14.533938  ]
True LE: [ 8.8015062e-01 -4.0864861e-03 -1.4551447e+01]
Relative Error: [1.295044   1.3110468  1.3768936  1.406683   1.2624182  1.0458668
 1.1712779  1.6459439  2.160392   2.7522883  3.2423005  3.507721
 3.5400133  3.40177    3.0241935  2.412552   1.6527855  1.1395026
 1.3102745  1.4849075  1.8099666  2.3681955  3.0893304  3.5739596
 3.789305   3.9527206  4.3656354  4.555831   4.5479193  4.635598
 4.777004   5.201453   5.648874   5.200605   4.7153378  4.1186905
 3.4872115  2.6526208  1.6485568  0.86435807 0.7529336  1.034295
 1.2220548  1.25203    1.1285605  0.775461   0.50986445 0.4928363
 0.5535118  0.68963546 0.8815557  0.99974114 0.6752123  0.40242597
 0.27086267 0.36329648 0.5457802  0.77590626 0.9782576  1.1293333
 1.204277   1.0717579  1.0026176  1.1023877  1.1759102  1.2591938
 1.1506912  1.0045358  1.0619491  0.96859664 1.5935895  2.282349
 2.8996353  3.2652545  3.4098136  3.37233    3.0921288  2.573771
 1.8395023  1.0581363  1.2305024  1.3594822  1.6789294  2.220483
 2.9719343  3.280882   3.626828   3.8635764  3.867966   3.7586975
 3.719265   3.8242605  3.9786954  4.3062625  4.749101   5.071554
 4.5582247  4.238916   3.5173655  2.8558788  1.9836628  0.99444973
 0.5517941  0.82064307 1.0477823  0.9921733  0.86380893 0.5807426
 0.49007902 0.54400814 0.6455965  0.8888849  0.91107523 0.92982715
 0.5517897  0.37503788 0.33193156 0.3947081  0.54341227 0.79840934
 1.112069   1.3523259  1.3621954  1.1150707  0.8708813  0.78609324
 0.8750043  1.0150393  1.0160563  0.87256676 1.0177373  0.8464959
 0.8487857  1.5520346  2.3034465  2.8492181  3.1564128  3.2583349
 3.0931723  2.6522174  1.94675    1.0770807  1.0203927  1.1785167
 1.4351937  2.0091987  2.5991294  2.9719064  3.460398   3.751931
 3.2130957  2.772044   2.740634   2.8258455  3.0035546  3.416817
 3.9234054  4.2865114  4.5804515  4.115741   3.6239772  2.7979238
 2.1717074  1.3952298  0.53271705 0.56281304 0.7366721  0.7266914
 0.64046776 0.4224611  0.50721073 0.6742611  0.7913972  0.8079792
 0.9072882  0.90612286 0.5297688  0.32681012 0.3481311  0.5068801
 0.66827685 0.90224874 1.0648274  1.2806383  1.3794577  1.268642
 1.0376348  0.6954961  0.729159   0.8820754  0.78039294 0.7729813
 0.8394115  0.76197076 0.66898566 0.6948617  1.5051605  2.2715795
 2.7935023  3.0787616  3.072424   2.734308   2.129444   1.2692906
 0.71619695 0.9314415  1.1300024  1.6366886  2.202779   2.5709915
 3.1315644  3.27876    2.6675696  2.0545232  1.6658746  1.6436692
 1.8530767  2.0756183  2.76029    3.4618716  3.8211093  4.1804886
 3.5928125  2.9694884  2.190157   1.6547348  0.9270592  0.3701188
 0.3980347  0.46974662 0.4485395  0.27372772 0.5124551  0.8847468
 0.79568493 0.7287699  0.8813682  0.88006556 0.5765059  0.3926062
 0.33994377 0.45297512 0.7556546  0.8225146  0.9583056  1.2289783
 1.4138794  1.4244128  1.2516414  1.0234362  0.735586   0.6663489
 0.74269766 0.66758895 0.7121725  0.6541082  0.41640767 0.49915263
 0.64010274 1.4120164  2.2052338  2.6475587  2.8038476  2.673777
 2.2408533  1.521887   0.82804507 0.49713367 0.7027379  1.1436927
 1.7184583  2.0880678  2.6193452  2.7762425  2.242425   1.57593
 0.85079616 0.66951245 0.69092923 1.0009176  1.3154552  1.9530747
 2.6130092  3.3811471  3.734003   3.028089   2.4583447  1.732658
 1.2495216  0.7957847  0.21841033 0.2258501  0.278344   0.20815851
 0.32232323 0.904648   0.7223132  0.557619   0.66057277 0.7785454
 0.5383756  0.45782784 0.46313316 0.5909959  0.633157   0.6428492
 0.83490664 1.0242782  1.3731818  1.6078006  1.4662715  1.3199713
 1.0489879  0.8475189  0.5217541  0.5656024  0.559105   0.7454281
 0.42654955 0.05802603 0.3725737  0.60381025 1.1780943  1.8469738
 2.3013155  2.4340634  2.2565684  1.7699798  1.0025535  0.64784503
 0.3156836  0.6894565  1.2116536  1.4604168  1.9828475  2.2117913
 1.8699467  1.2496138  0.7718227  0.3304134  0.33306623 0.3038831
 0.47470412 0.804988   1.2186085  1.8156158  2.6397252  3.3038645
 2.5674534  2.0537472  1.4146918  1.1160958  0.7875289  0.26465482
 0.11205582 0.21527518 0.15553404 0.526274   0.7614733  0.49537852
 0.4799401  0.7254202  0.3794446  0.36993033 0.52985793 0.7598763
 0.801343   0.6536521  0.6128593  0.86148566 1.1221962  1.5310917
 1.8407365  1.613343   1.2718011  0.9806673  0.88496304 0.61089516
 0.43425685 0.53083456 0.4862797  0.35584837 0.1257699  0.3506663
 0.41532755 0.6061849  1.3706322  1.8602722  2.018583   1.8412162
 1.3620901  0.90589494 0.5078794  0.27558604 0.85708535 1.0828421
 1.3133106  1.7397529  1.5156429  1.0495933 ]
