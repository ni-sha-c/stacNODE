time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 5000
num_train: 2000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 512
n_layers: 3
reg_param: 20.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 13.392731667 Test: 5.205388546
Epoch 0: New minimal relative error: 5.21%, model saved.
Epoch: 50 Train: 1.412747145 Test: 1.351167440
Epoch 50: New minimal relative error: 1.35%, model saved.
Epoch: 100 Train: 0.924962580 Test: 0.915413678
Epoch 100: New minimal relative error: 0.92%, model saved.
Epoch: 150 Train: 0.502894878 Test: 0.548269808
Epoch 150: New minimal relative error: 0.55%, model saved.
Epoch: 200 Train: 0.488963783 Test: 0.540104151
Epoch 200: New minimal relative error: 0.54%, model saved.
Epoch: 250 Train: 0.487957329 Test: 0.540479660
Epoch: 300 Train: 0.476260871 Test: 0.529361188
Epoch 300: New minimal relative error: 0.53%, model saved.
Epoch: 350 Train: 0.464579761 Test: 0.519596934
Epoch 350: New minimal relative error: 0.52%, model saved.
Epoch: 400 Train: 0.459768951 Test: 0.516152978
Epoch 400: New minimal relative error: 0.52%, model saved.
Epoch: 450 Train: 0.457776964 Test: 0.511123896
Epoch 450: New minimal relative error: 0.51%, model saved.
Epoch: 500 Train: 0.451906025 Test: 0.508003592
Epoch 500: New minimal relative error: 0.51%, model saved.
Epoch: 550 Train: 0.449395806 Test: 0.507805943
Epoch 550: New minimal relative error: 0.51%, model saved.
Epoch: 600 Train: 0.449006259 Test: 0.504784346
Epoch 600: New minimal relative error: 0.50%, model saved.
Epoch: 650 Train: 0.445767850 Test: 0.503828764
Epoch 650: New minimal relative error: 0.50%, model saved.
Epoch: 700 Train: 0.446617812 Test: 0.503630638
Epoch 700: New minimal relative error: 0.50%, model saved.
Epoch: 750 Train: 0.444230616 Test: 0.502435684
Epoch 750: New minimal relative error: 0.50%, model saved.
Epoch: 800 Train: 0.447070599 Test: 0.503462553
Epoch: 850 Train: 0.447911680 Test: 0.504424274
Epoch: 900 Train: 0.449131787 Test: 0.505719960
Epoch: 950 Train: 0.448978573 Test: 0.505476058
Epoch: 1000 Train: 0.448471099 Test: 0.504935622
Epoch: 1050 Train: 0.449617147 Test: 0.504766524
Epoch: 1100 Train: 0.446679115 Test: 0.503603160
Epoch: 1150 Train: 0.448096633 Test: 0.504686475
Epoch: 1200 Train: 0.446293443 Test: 0.507575750
Epoch: 1250 Train: 0.446405083 Test: 0.502330601
Epoch 1250: New minimal relative error: 0.50%, model saved.
Epoch: 1300 Train: 0.449376404 Test: 0.504337251
Epoch: 1350 Train: 0.448675185 Test: 0.506300092
Epoch: 1400 Train: 0.447414339 Test: 0.503258824
Epoch: 1450 Train: 0.447949350 Test: 0.503735065
Epoch: 1500 Train: 0.448591113 Test: 0.503005147
Epoch: 1550 Train: 0.449839294 Test: 0.508132994
Epoch: 1600 Train: 0.450259566 Test: 0.505409956
Epoch: 1650 Train: 0.448819160 Test: 0.504253983
Epoch: 1700 Train: 0.448824823 Test: 0.504483461
Epoch: 1750 Train: 0.447147489 Test: 0.502582073
Epoch: 1800 Train: 0.448112488 Test: 0.504073501
Epoch: 1850 Train: 0.448476017 Test: 0.503997803
Epoch: 1900 Train: 0.448051751 Test: 0.503891945
Epoch: 1950 Train: 0.449075043 Test: 0.505970120
Epoch: 2000 Train: 0.449522287 Test: 0.504804134
Epoch: 2050 Train: 0.449382335 Test: 0.505549133
Epoch: 2100 Train: 0.449402958 Test: 0.504750848
Epoch: 2150 Train: 0.448930115 Test: 0.506265640
Epoch: 2200 Train: 0.448846877 Test: 0.503939986
Epoch: 2250 Train: 0.449817270 Test: 0.506660104
Epoch: 2300 Train: 0.450783968 Test: 0.505164206
Epoch: 2350 Train: 0.450497776 Test: 0.506582677
Epoch: 2400 Train: 0.449544072 Test: 0.506845355
Epoch: 2450 Train: 0.449671745 Test: 0.505922914
Epoch: 2500 Train: 0.449541330 Test: 0.504911184
Epoch: 2550 Train: 0.449333847 Test: 0.505697846
Epoch: 2600 Train: 0.449670523 Test: 0.505393565
Epoch: 2650 Train: 0.449917614 Test: 0.506582439
Epoch: 2700 Train: 0.450760812 Test: 0.511028051
Epoch: 2750 Train: 0.450700253 Test: 0.507200837
Epoch: 2800 Train: 0.449254364 Test: 0.507105947
Epoch: 2850 Train: 0.449268639 Test: 0.507607460
Epoch: 2900 Train: 0.450023323 Test: 0.507022977
Epoch: 2950 Train: 0.449444413 Test: 0.506348848
Epoch: 3000 Train: 0.449807465 Test: 0.505991161
Epoch: 3050 Train: 0.450208068 Test: 0.506171227
Epoch: 3100 Train: 0.449400127 Test: 0.506397367
Epoch: 3150 Train: 0.448537320 Test: 0.506471813
Epoch: 3200 Train: 0.447857440 Test: 0.505312622
Epoch: 3250 Train: 0.448280573 Test: 0.504021823
Epoch: 3300 Train: 0.448361218 Test: 0.504450321
Epoch: 3350 Train: 0.449039817 Test: 0.504089355
Epoch: 3400 Train: 0.447872043 Test: 0.504515707
Epoch: 3450 Train: 0.448390305 Test: 0.504616141
Epoch: 3500 Train: 0.449074864 Test: 0.504608274
Epoch: 3550 Train: 0.448904425 Test: 0.503632903
Epoch: 3600 Train: 0.448898971 Test: 0.505094767
Epoch: 3650 Train: 0.449509680 Test: 0.504601240
Epoch: 3700 Train: 0.449713081 Test: 0.506282568
Epoch: 3750 Train: 0.450334966 Test: 0.507305741
Epoch: 3800 Train: 0.450087368 Test: 0.506575048
Epoch: 3850 Train: 0.450730085 Test: 0.508045435
Epoch: 3900 Train: 0.450713068 Test: 0.508022487
Epoch: 3950 Train: 0.451026380 Test: 0.507030845
Epoch: 4000 Train: 0.451272547 Test: 0.506334484
Epoch: 4050 Train: 0.451305479 Test: 0.508640528
Epoch: 4100 Train: 0.450197548 Test: 0.506956935
Epoch: 4150 Train: 0.451560706 Test: 0.508967280
Epoch: 4200 Train: 0.452109814 Test: 0.509572625
Epoch: 4250 Train: 0.451724291 Test: 0.509369195
Epoch: 4300 Train: 0.452034473 Test: 0.509272754
Epoch: 4350 Train: 0.452195793 Test: 0.508799732
Epoch: 4400 Train: 0.451975405 Test: 0.508485973
Epoch: 4450 Train: 0.451821327 Test: 0.508345127
Epoch: 4500 Train: 0.451988995 Test: 0.508505166
Epoch: 4550 Train: 0.452218026 Test: 0.508212507
Epoch: 4600 Train: 0.451979607 Test: 0.508338988
Epoch: 4650 Train: 0.452077329 Test: 0.508008838
Epoch: 4700 Train: 0.451538563 Test: 0.507332921
Epoch: 4750 Train: 0.451137990 Test: 0.507299483
Epoch: 4800 Train: 0.451380461 Test: 0.507708907
Epoch: 4850 Train: 0.451124728 Test: 0.506830573
Epoch: 4900 Train: 0.450218141 Test: 0.506017208
Epoch: 4950 Train: 0.450930089 Test: 0.506150723
Epoch: 4999 Train: 0.451530337 Test: 0.507483304
Training Loss: tensor(0.4515)
Test Loss: tensor(0.5075)
True Mean x: tensor(2.8976, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.1842, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.4858, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(1.7467, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0091)
Jacobian term Test Loss: tensor(0.0119)
Learned LE: [1.3027328 0.222629 ]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
