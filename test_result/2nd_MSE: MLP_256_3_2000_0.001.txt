time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.65%, model saved.
Epoch: 0 Train: 3821.79834 Test: 4153.49463
Epoch 100: New minimal relative error: 50.44%, model saved.
Epoch: 100 Train: 121.53690 Test: 129.39279
Epoch 200: New minimal relative error: 46.43%, model saved.
Epoch: 200 Train: 14.41217 Test: 13.61900
Epoch 300: New minimal relative error: 19.68%, model saved.
Epoch: 300 Train: 6.56389 Test: 6.22684
Epoch: 400 Train: 4.42703 Test: 4.20058
Epoch 500: New minimal relative error: 10.60%, model saved.
Epoch: 500 Train: 3.40409 Test: 3.26519
Epoch: 600 Train: 1.89388 Test: 2.63591
Epoch: 700 Train: 5.04714 Test: 6.92303
Epoch: 800 Train: 1.09898 Test: 0.94448
Epoch 900: New minimal relative error: 10.25%, model saved.
Epoch: 900 Train: 0.84679 Test: 0.72412
Epoch: 1000 Train: 0.91849 Test: 0.94273
Epoch: 1100 Train: 0.66722 Test: 0.73244
Epoch: 1200 Train: 1.13364 Test: 0.78596
Epoch: 1300 Train: 0.49251 Test: 0.40422
Epoch: 1400 Train: 0.66424 Test: 0.60007
Epoch: 1500 Train: 1.30642 Test: 1.42817
Epoch: 1600 Train: 0.46054 Test: 0.44142
Epoch: 1700 Train: 0.31109 Test: 0.30267
Epoch 1800: New minimal relative error: 7.93%, model saved.
Epoch: 1800 Train: 0.27668 Test: 0.21766
Epoch: 1900 Train: 0.35331 Test: 0.27613
Epoch: 2000 Train: 0.23798 Test: 0.18665
Epoch: 2100 Train: 0.25067 Test: 0.20253
Epoch: 2200 Train: 0.24171 Test: 0.24451
Epoch: 2300 Train: 2.00469 Test: 1.85960
Epoch: 2400 Train: 0.18559 Test: 0.16020
Epoch: 2500 Train: 0.69620 Test: 0.92129
Epoch 2600: New minimal relative error: 7.84%, model saved.
Epoch: 2600 Train: 0.18419 Test: 0.16797
Epoch: 2700 Train: 1.23839 Test: 0.64861
Epoch: 2800 Train: 0.25248 Test: 0.18968
Epoch: 2900 Train: 0.16600 Test: 0.12091
Epoch: 3000 Train: 1.01256 Test: 1.29299
Epoch: 3100 Train: 0.22331 Test: 0.22299
Epoch: 3200 Train: 0.20828 Test: 0.15602
Epoch: 3300 Train: 0.15406 Test: 0.16638
Epoch: 3400 Train: 0.13941 Test: 0.12853
Epoch: 3500 Train: 0.66632 Test: 0.67329
Epoch: 3600 Train: 0.10293 Test: 0.08271
Epoch 3700: New minimal relative error: 6.51%, model saved.
Epoch: 3700 Train: 0.10661 Test: 0.08925
Epoch: 3800 Train: 0.09947 Test: 0.09620
Epoch: 3900 Train: 0.49383 Test: 0.54126
Epoch: 4000 Train: 0.09282 Test: 0.08004
Epoch: 4100 Train: 0.09712 Test: 0.07857
Epoch: 4200 Train: 0.09181 Test: 0.07432
Epoch: 4300 Train: 0.09482 Test: 0.08177
Epoch: 4400 Train: 0.20207 Test: 0.16513
Epoch: 4500 Train: 0.10300 Test: 0.07339
Epoch: 4600 Train: 0.97174 Test: 0.88875
Epoch: 4700 Train: 0.13522 Test: 0.14663
Epoch: 4800 Train: 0.07433 Test: 0.06066
Epoch: 4900 Train: 0.08166 Test: 0.07120
Epoch: 5000 Train: 0.07642 Test: 0.06675
Epoch: 5100 Train: 0.06966 Test: 0.05703
Epoch: 5200 Train: 0.06835 Test: 0.05599
Epoch: 5300 Train: 0.07691 Test: 0.06514
Epoch: 5400 Train: 0.06640 Test: 0.05403
Epoch: 5500 Train: 0.08573 Test: 0.12615
Epoch: 5600 Train: 0.06902 Test: 0.06050
Epoch: 5700 Train: 0.06064 Test: 0.05079
Epoch: 5800 Train: 0.06204 Test: 0.05275
Epoch: 5900 Train: 0.32874 Test: 0.13765
Epoch: 6000 Train: 0.05659 Test: 0.04742
Epoch: 6100 Train: 0.07452 Test: 0.06230
Epoch: 6200 Train: 0.05426 Test: 0.04572
Epoch: 6300 Train: 0.05376 Test: 0.04514
Epoch: 6400 Train: 0.05691 Test: 0.04981
Epoch: 6500 Train: 0.05150 Test: 0.04347
Epoch 6600: New minimal relative error: 5.52%, model saved.
Epoch: 6600 Train: 0.17081 Test: 0.14048
Epoch: 6700 Train: 0.22643 Test: 0.16826
Epoch: 6800 Train: 0.05016 Test: 0.04755
Epoch: 6900 Train: 0.05646 Test: 0.05169
Epoch: 7000 Train: 0.04698 Test: 0.04014
Epoch: 7100 Train: 0.05947 Test: 0.04867
Epoch 7200: New minimal relative error: 5.25%, model saved.
Epoch: 7200 Train: 0.05835 Test: 0.04652
Epoch: 7300 Train: 0.11080 Test: 0.13398
Epoch: 7400 Train: 0.04388 Test: 0.03797
Epoch: 7500 Train: 0.04335 Test: 0.03729
Epoch: 7600 Train: 0.04260 Test: 0.03678
Epoch: 7700 Train: 0.24938 Test: 0.33485
Epoch: 7800 Train: 0.04108 Test: 0.03585
Epoch: 7900 Train: 0.38862 Test: 0.47741
Epoch: 8000 Train: 0.03985 Test: 0.03493
Epoch: 8100 Train: 0.04517 Test: 0.04212
Epoch: 8200 Train: 0.03868 Test: 0.03399
Epoch: 8300 Train: 0.03987 Test: 0.03452
Epoch: 8400 Train: 0.03771 Test: 0.03328
Epoch: 8500 Train: 0.04021 Test: 0.03751
Epoch: 8600 Train: 0.03985 Test: 0.03623
Epoch: 8700 Train: 0.03604 Test: 0.03199
Epoch: 8800 Train: 0.03620 Test: 0.03281
Epoch: 8900 Train: 0.03499 Test: 0.03117
Epoch: 9000 Train: 0.03478 Test: 0.03227
Epoch 9100: New minimal relative error: 4.13%, model saved.
Epoch: 9100 Train: 0.03410 Test: 0.03063
Epoch: 9200 Train: 0.03387 Test: 0.03052
Epoch: 9300 Train: 0.03531 Test: 0.03246
Epoch 9400: New minimal relative error: 3.92%, model saved.
Epoch: 9400 Train: 0.04757 Test: 0.04897
Epoch: 9500 Train: 0.03241 Test: 0.02917
Epoch: 9600 Train: 0.04491 Test: 0.05017
Epoch: 9700 Train: 0.03160 Test: 0.02854
Epoch: 9800 Train: 0.03129 Test: 0.02882
Epoch: 9900 Train: 0.03085 Test: 0.02800
Epoch: 9999 Train: 0.03356 Test: 0.03015
Training Loss: tensor(0.0336)
Test Loss: tensor(0.0301)
Learned LE: [ 0.89369595  0.03248049 -6.1991887 ]
True LE: [ 8.5701889e-01  1.0001856e-02 -1.4540939e+01]
Relative Error: [1.8552668  1.7958589  1.7119868  1.627327   1.4782445  1.2072473
 0.85045147 0.4794362  0.19271944 0.2510889  0.29028192 0.21962805
 0.3555251  0.62945306 0.9324685  1.2762777  1.6733967  2.0247216
 2.1219702  2.0269082  2.0321844  2.1746845  2.3724034  2.5400546
 2.5255332  2.2785923  1.907809   1.4720346  0.98241204 0.70162296
 0.8693414  1.2619946  1.6620374  1.9497958  2.0906663  2.0696826
 1.8690606  1.5587403  1.307112   1.2027092  1.1506886  1.0209934
 0.83375907 0.722916   0.75144416 0.8842567  1.0861781  1.216626
 1.0892273  0.7255645  0.47131366 0.3342172  0.41151732 0.7869328
 1.1117178  1.1941497  1.2517076  1.2474577  1.3609877  1.5206436
 1.6364299  1.6846949  1.6417807  1.5387129  1.4612274  1.4144444
 1.3263185  1.1181808  0.8073462  0.46439517 0.13834839 0.3021757
 0.40021557 0.22537935 0.16687688 0.43915644 0.7035833  0.95515436
 1.2255344  1.5381902  1.7662132  1.7607126  1.7275095  1.8300388
 1.9928211  2.1473749  2.1809638  1.9992051  1.6783265  1.2890711
 0.8452122  0.55292803 0.61860055 0.9246475  1.3642521  1.8335713
 2.1709816  2.2670097  2.1230617  1.8046064  1.4677814  1.262127
 1.154434   1.0118688  0.8212341  0.6931705  0.68268794 0.7213941
 0.7700836  0.8886191  0.95409167 0.7472197  0.49540457 0.40795448
 0.32092124 0.5808886  0.854388   0.95371467 0.9779723  1.0081037
 1.1566032  1.3244424  1.4474454  1.5171119  1.4703532  1.3320808
 1.2332016  1.202746   1.1700718  1.0355586  0.770753   0.45845872
 0.13610901 0.37117115 0.5926778  0.48557654 0.24003924 0.23963124
 0.46279007 0.67370856 0.84782344 1.0386313  1.2827501  1.441433
 1.4493375  1.5077555  1.6395986  1.7714427  1.842576   1.7413988
 1.4845428  1.148168   0.76294845 0.5067075  0.50054234 0.65358174
 1.0047442  1.5599772  2.078391   2.302728   2.2332528  1.9676772
 1.6031265  1.2977154  1.1135468  0.95197874 0.7633249  0.61899203
 0.58881223 0.63698316 0.67574286 0.66004163 0.6838748  0.6819914
 0.5195185  0.50102997 0.3957006  0.50964814 0.6759204  0.7827911
 0.7411343  0.7891428  0.9862673  1.1622689  1.2634109  1.3391322
 1.358663   1.243308   1.0702736  0.996045   0.9907468  0.93244165
 0.72968054 0.44002518 0.16068584 0.37439513 0.7171487  0.75583434
 0.5542475  0.29602757 0.18932013 0.34013557 0.51253253 0.6676941
 0.8305723  1.0390006  1.1604118  1.20027    1.2920569  1.4052126
 1.4841048  1.4648383  1.290803   1.0314459  0.7281217  0.54939336
 0.5402497  0.5530901  0.70454216 1.1657584  1.7666624  2.138215
 2.1768727  2.00273    1.6952783  1.3290464  1.0288875  0.8001311
 0.61256015 0.50322926 0.480014   0.5003571  0.5412276  0.5828273
 0.56507945 0.50724715 0.47478476 0.5027057  0.545049   0.49754274
 0.69471127 0.70918083 0.68441075 0.62527627 0.86431044 1.0773191
 1.1485344  1.1753037  1.2527803  1.2837191  1.082736   0.8536307
 0.78982383 0.7779531  0.6898483  0.44931257 0.20306215 0.27025586
 0.65009695 0.8511642  0.81068426 0.5878889  0.3166541  0.07211981
 0.16762054 0.35754883 0.50709605 0.6549221  0.8245233  0.9013191
 0.97687787 1.0849994  1.1635109  1.1745809  1.0780294  0.8799188
 0.64740413 0.5134012  0.6238421  0.63506657 0.60701025 0.8142202
 1.2993424  1.7419212  1.9318808  1.8732408  1.6581029  1.3241205
 0.9237619  0.595112   0.40512565 0.36587742 0.41322204 0.4468821
 0.4213357  0.394907   0.43072957 0.48174444 0.42612687 0.41896865
 0.5561014  0.62032646 0.66446304 0.81341535 0.73395514 0.6205549
 0.73103136 1.0164956  1.0978264  1.0711348  1.1000552  1.2687469
 1.2683699  0.9539667  0.67283034 0.58864856 0.6002145  0.5059162
 0.26979315 0.13623616 0.40489006 0.67277855 0.829926   0.7894587
 0.5558575  0.32655412 0.23411499 0.22937626 0.23034625 0.32153362
 0.46999446 0.60842127 0.6556519  0.800039   0.9364808  0.9806148
 0.9172991  0.7649297  0.5522993  0.359187   0.47464782 0.67345953
 0.6855757  0.68923694 0.9136447  1.2218544  1.4621229  1.568133
 1.4775368  1.2128484  0.849416   0.59013706 0.4804817  0.4355547
 0.42372692 0.41683054 0.41483533 0.41203865 0.3674561  0.2570666
 0.36357975 0.47648394 0.49452972 0.6242311  0.75018424 0.81689245
 0.87994915 0.72616315 0.6025603  0.842827   1.0064142  0.9774937
 0.9149041  1.0380089  1.2576458  1.2331735  0.8842068  0.58029616
 0.4617479  0.4902875  0.36902338 0.20608735 0.16840747 0.42580795
 0.5727795  0.78090763 0.7361251  0.4914396  0.3125138  0.26938432
 0.24471486 0.21803598 0.19252548 0.25533485 0.40115452 0.44266367
 0.67242587 0.8622407  0.90751845 0.785975  ]
