time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 124.21%, model saved.
Epoch: 0 Train: 170958.09375 Test: 3981.31128
Epoch: 80 Train: 45583.70703 Test: 1831.07825
Epoch 160: New minimal relative error: 119.43%, model saved.
Epoch: 160 Train: 45145.19531 Test: 1714.27002
Epoch 240: New minimal relative error: 107.77%, model saved.
Epoch: 240 Train: 42146.33203 Test: 1473.71143
Epoch 320: New minimal relative error: 95.36%, model saved.
Epoch: 320 Train: 43978.87891 Test: 1475.59717
Epoch 400: New minimal relative error: 68.84%, model saved.
Epoch: 400 Train: 43994.08203 Test: 1353.50073
Epoch: 480 Train: 41217.73828 Test: 1381.18933
Epoch: 560 Train: 45185.14453 Test: 1408.78271
Epoch: 640 Train: 41749.98047 Test: 1423.77441
Epoch 720: New minimal relative error: 57.71%, model saved.
Epoch: 720 Train: 43587.06641 Test: 1346.65881
Epoch: 800 Train: 40841.08203 Test: 1446.94800
Epoch: 880 Train: 43741.23828 Test: 1365.51245
Epoch: 960 Train: 42221.64062 Test: 1396.41125
Epoch: 1040 Train: 39142.78125 Test: 1312.32019
Epoch: 1120 Train: 45148.95703 Test: 1440.26831
Epoch: 1200 Train: 40657.86328 Test: 1449.46826
Epoch: 1280 Train: 42788.08203 Test: 1430.01172
Epoch: 1360 Train: 41917.18750 Test: 1410.90198
Epoch: 1440 Train: 44010.62109 Test: 1436.45642
Epoch: 1520 Train: 43132.77734 Test: 1522.40393
Epoch: 1600 Train: 41577.80469 Test: 1403.69580
Epoch: 1680 Train: 42141.42969 Test: 1415.50879
Epoch: 1760 Train: 41236.47656 Test: 1396.61121
Epoch: 1840 Train: 40929.16016 Test: 1323.59033
Epoch: 1920 Train: 41828.57812 Test: 1405.76196
Epoch: 2000 Train: 38967.07031 Test: 1324.11743
Epoch 2080: New minimal relative error: 56.86%, model saved.
Epoch: 2080 Train: 40511.91797 Test: 1382.29028
Epoch: 2160 Train: 39605.21875 Test: 1343.53918
Epoch: 2240 Train: 38579.79688 Test: 1305.99280
Epoch: 2320 Train: 36877.96094 Test: 1249.52368
Epoch: 2400 Train: 39199.90625 Test: 1283.84167
Epoch: 2480 Train: 38746.13672 Test: 1285.01978
Epoch: 2560 Train: 35697.44922 Test: 1256.47144
Epoch: 2640 Train: 34942.47266 Test: 1111.80493
Epoch: 2720 Train: 37904.57422 Test: 1148.79883
Epoch: 2800 Train: 34537.57031 Test: 1129.44495
Epoch: 2880 Train: 31645.14648 Test: 1037.04102
Epoch: 2960 Train: 30610.25781 Test: 900.61865
Epoch: 3040 Train: 30918.47461 Test: 896.80560
Epoch: 3120 Train: 29211.78125 Test: 811.81018
Epoch: 3200 Train: 27237.49609 Test: 733.98834
Epoch: 3280 Train: 26067.77344 Test: 669.51697
Epoch: 3360 Train: 24161.89844 Test: 567.58569
Epoch: 3440 Train: 22951.68164 Test: 501.78070
Epoch: 3520 Train: 21743.92578 Test: 453.86963
Epoch: 3600 Train: 19106.98047 Test: 355.92780
Epoch: 3680 Train: 17000.13477 Test: 263.87344
Epoch: 3760 Train: 14317.01172 Test: 191.78772
Epoch: 3840 Train: 9148.00684 Test: 107.61929
Epoch 3920: New minimal relative error: 30.48%, model saved.
Epoch: 3920 Train: 5296.61670 Test: 52.72028
Epoch 4000: New minimal relative error: 19.19%, model saved.
Epoch: 4000 Train: 3563.16650 Test: 32.87123
Epoch 4080: New minimal relative error: 17.89%, model saved.
Epoch: 4080 Train: 2718.58667 Test: 19.90999
Epoch 4160: New minimal relative error: 15.36%, model saved.
Epoch: 4160 Train: 2288.09131 Test: 14.04974
Epoch 4240: New minimal relative error: 10.84%, model saved.
Epoch: 4240 Train: 2031.59326 Test: 12.49456
Epoch 4320: New minimal relative error: 7.79%, model saved.
Epoch: 4320 Train: 1893.36047 Test: 9.65827
Epoch: 4400 Train: 1635.66443 Test: 8.53613
Epoch: 4480 Train: 1606.67432 Test: 13.18028
Epoch: 4560 Train: 1506.79810 Test: 9.18919
Epoch: 4640 Train: 1318.51099 Test: 4.71713
Epoch: 4720 Train: 1202.02405 Test: 4.46525
Epoch: 4800 Train: 1221.72229 Test: 4.95981
Epoch: 4880 Train: 1052.43933 Test: 2.54760
Epoch 4960: New minimal relative error: 6.50%, model saved.
Epoch: 4960 Train: 1042.61621 Test: 3.49602
Epoch: 5040 Train: 961.95789 Test: 2.42189
Epoch: 5120 Train: 964.64282 Test: 2.71126
Epoch 5200: New minimal relative error: 5.22%, model saved.
Epoch: 5200 Train: 909.40613 Test: 3.18903
Epoch: 5280 Train: 912.00128 Test: 2.72232
Epoch: 5360 Train: 863.36267 Test: 1.82995
Epoch: 5440 Train: 879.36578 Test: 2.09912
Epoch: 5520 Train: 836.58917 Test: 1.87965
Epoch: 5600 Train: 867.87274 Test: 2.54794
Epoch: 5680 Train: 794.94543 Test: 1.80625
Epoch: 5760 Train: 777.20026 Test: 1.77826
Epoch: 5840 Train: 771.73737 Test: 1.70477
Epoch: 5920 Train: 735.96497 Test: 1.61062
Epoch: 6000 Train: 739.04266 Test: 1.48893
Epoch: 6080 Train: 706.64709 Test: 1.43834
Epoch: 6160 Train: 693.37268 Test: 1.33321
Epoch: 6240 Train: 690.43597 Test: 1.26098
Epoch: 6320 Train: 692.98999 Test: 1.33352
Epoch 6400: New minimal relative error: 4.38%, model saved.
Epoch: 6400 Train: 664.90253 Test: 1.41703
Epoch: 6480 Train: 665.87341 Test: 1.20508
Epoch: 6560 Train: 650.76025 Test: 1.24582
Epoch: 6640 Train: 641.37439 Test: 1.16540
Epoch: 6720 Train: 631.24487 Test: 1.22868
Epoch: 6800 Train: 614.15485 Test: 1.08510
Epoch: 6880 Train: 611.74878 Test: 1.27782
Epoch: 6960 Train: 610.52930 Test: 1.19908
Epoch: 7040 Train: 601.94415 Test: 1.52602
Epoch: 7120 Train: 688.66626 Test: 2.86443
Epoch: 7200 Train: 674.65833 Test: 2.83385
Epoch: 7280 Train: 607.11243 Test: 2.68241
Epoch: 7360 Train: 557.84180 Test: 1.09498
Epoch: 7440 Train: 556.48029 Test: 1.24545
Epoch: 7520 Train: 536.56445 Test: 1.36130
Epoch: 7600 Train: 527.39203 Test: 1.73626
Epoch: 7680 Train: 563.91589 Test: 2.21081
Epoch: 7760 Train: 562.85468 Test: 2.80864
Epoch: 7840 Train: 515.63000 Test: 1.67970
Epoch: 7920 Train: 497.02713 Test: 1.39124
Epoch 7999: New minimal relative error: 3.38%, model saved.
Epoch: 7999 Train: 476.28809 Test: 1.03392
Training Loss: tensor(476.2881)
Test Loss: tensor(1.0339)
Learned LE: [ 8.5631144e-01  8.2001118e-03 -1.4504300e+01]
True LE: [ 8.5451305e-01  5.4269107e-03 -1.4531544e+01]
Relative Error: [1.8372023 2.389181  2.8643692 3.3252676 3.766095  4.20767   4.615811
 5.025747  5.48019   5.9806085 6.36911   6.713272  7.318825  7.6503077
 7.8839855 7.9076433 7.332631  6.9053254 6.862158  6.810075  6.086549
 5.6454425 5.4538627 5.479712  5.3068833 4.853922  4.6834354 4.5963907
 4.622404  4.6406584 4.900053  5.417193  5.6175966 5.6060514 5.403513
 5.073139  4.8310237 4.762336  4.986061  5.6028852 6.477268  7.52162
 7.8453035 8.155988  7.864813  7.8683167 8.12832   7.7166214 7.435876
 6.4302297 5.2919893 4.332189  3.5535634 3.0159276 2.6738307 2.4380286
 2.2776122 2.1295433 1.9181436 1.7122071 1.494264  1.4984194 1.9106328
 2.4797566 2.967275  3.4141555 3.827148  4.2266917 4.570045  4.8984494
 5.267405  5.6123695 5.95733   6.3553867 6.918644  7.189809  7.408288
 7.3775597 6.7668476 6.3758917 6.3850055 6.377081  5.7032876 5.3075933
 5.169365  5.260884  4.9152894 4.542379  4.373068  4.379804  4.4933233
 4.5260444 4.6706185 5.219522  5.4578314 5.4688926 5.1800413 4.8368535
 4.5555124 4.405678  4.546398  5.104312  5.8911967 6.9173803 7.209286
 7.7125363 7.4793935 7.449238  7.728868  7.457529  7.1819806 6.128196
 5.021942  4.139972  3.457745  2.9357476 2.5856926 2.3897502 2.2403474
 2.0930927 1.9075562 1.771847  1.6695259 1.6721146 2.0520575 2.6418228
 3.1448438 3.5808153 3.9708939 4.326375  4.604116  4.8440995 5.130944
 5.309488  5.649555  6.036073  6.550781  6.7738204 6.9385724 6.8781605
 6.261445  5.905239  5.9485493 6.0202074 5.3742857 5.0477037 4.978665
 5.118837  4.696617  4.410722  4.2862473 4.2401676 4.378386  4.461851
 4.5085487 5.0417104 5.3355517 5.338607  5.0321136 4.6879587 4.374461
 4.1384    4.174322  4.6346126 5.326818  6.3074155 6.80608   7.3313313
 7.1208386 7.0655    7.305319  7.173133  6.930727  5.8541903 4.8162165
 4.0183916 3.431146  2.931844  2.5208452 2.209887  2.0601058 1.9461697
 1.8364714 1.8235434 1.8567979 1.9207077 2.305429  2.8760288 3.399304
 3.8372684 4.202141  4.517761  4.7332125 4.8818088 5.0759497 5.099236
 5.448714  5.7706676 6.224378  6.401983  6.515242  6.4114237 5.8358407
 5.490488  5.5316615 5.736346  5.136414  4.84132   4.8033733 4.97298
 4.523743  4.304874  4.2650847 4.2765265 4.3536105 4.4003754 4.458324
 4.8766847 5.2281737 5.2188253 4.95301   4.6231956 4.291371  3.9726186
 3.8802817 4.1896477 4.784092  5.801928  6.4487853 6.949446  6.78875
 6.681388  6.8894424 6.9180264 6.6952715 5.656512  4.6783066 3.9154713
 3.3120263 2.8723247 2.5142958 2.184093  1.9695414 1.8693991 1.7997669
 1.8199354 2.0250816 2.221323  2.6512818 3.1841853 3.7346468 4.186253
 4.5322237 4.811874  4.973365  5.0340266 5.1253667 5.046959  5.327726
 5.5897565 5.941908  6.088617  6.131155  5.98468   5.496288  5.1233335
 5.1405473 5.5191865 4.960748  4.6720595 4.637488  4.7830496 4.3691225
 4.196944  4.2336764 4.2942905 4.4234242 4.433849  4.4332767 4.6817703
 5.1203675 5.1421814 4.9335456 4.6449795 4.3085055 3.923263  3.6884866
 3.8130991 4.2995954 5.3688755 6.1091533 6.5626936 6.461395  6.3200555
 6.4825325 6.704394  6.4556746 5.4575934 4.4673996 3.6995416 3.161718
 2.7901905 2.508693  2.2563334 2.0063946 1.9211907 1.9237174 2.0103383
 2.2420964 2.5936532 3.0555599 3.5737767 4.1529274 4.6271367 4.9642534
 5.2190022 5.341696  5.3278146 5.3044233 5.171507  5.3263693 5.504371
 5.730884  5.857001  5.791528  5.6149135 5.2526803 4.811298  4.787647
 5.133349  4.8335776 4.521859  4.4679046 4.6251035 4.217552  4.0653353
 4.140078  4.2684817 4.4305744 4.550108  4.4708686 4.5683565 4.9991465
 5.0939765 4.961707  4.7403064 4.423786  4.002577  3.518213  3.4288745
 3.896245  4.9335604 5.8076825 6.159308  6.177831  5.9885664 6.090189
 6.4596233 6.1179905 5.3022027 4.265809  3.508336  3.0287466 2.7471392
 2.5695364 2.3655436 2.1543922 2.0647178 2.1752183 2.3680413 2.656381
 3.0410843 3.5370805 4.0997157 4.6539745 5.1629133 5.5184426 5.749945
 5.7571893 5.6251097 5.5129642 5.3710237 5.49855   5.555012  5.6330833
 5.7481713 5.5404677 5.341379  5.1199856 4.5612288 4.4615226 4.782654
 4.8119473 4.431874  4.2903595 4.4058003 4.0702844 3.9090605 3.977944
 4.188235 ]
