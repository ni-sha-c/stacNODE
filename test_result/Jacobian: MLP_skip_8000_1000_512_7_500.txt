time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 102.03%, model saved.
Epoch: 0 Train: 31705.52734 Test: 4114.97119
Epoch: 80 Train: 8764.72266 Test: 1561.58301
Epoch: 160 Train: 8195.65137 Test: 1177.19995
Epoch: 240 Train: 7353.48730 Test: 1073.54126
Epoch: 320 Train: 7501.62793 Test: 1029.04187
Epoch: 400 Train: 6367.90479 Test: 1439.65735
Epoch 480: New minimal relative error: 83.63%, model saved.
Epoch: 480 Train: 8598.50879 Test: 1349.63049
Epoch 560: New minimal relative error: 53.92%, model saved.
Epoch: 560 Train: 6152.59375 Test: 879.27380
Epoch: 640 Train: 5464.17969 Test: 771.70422
Epoch: 720 Train: 4657.95752 Test: 502.62833
Epoch: 800 Train: 2949.82544 Test: 191.79016
Epoch: 880 Train: 1256.61621 Test: 47.71758
Epoch 960: New minimal relative error: 36.20%, model saved.
Epoch: 960 Train: 941.29840 Test: 36.80580
Epoch 1040: New minimal relative error: 5.80%, model saved.
Epoch: 1040 Train: 318.89481 Test: 4.75604
Epoch: 1120 Train: 259.73175 Test: 6.00585
Epoch: 1200 Train: 199.84633 Test: 3.31251
Epoch: 1280 Train: 186.05232 Test: 7.03199
Epoch: 1360 Train: 198.15538 Test: 8.98300
Epoch: 1440 Train: 218.65909 Test: 22.60599
Epoch: 1520 Train: 128.11200 Test: 1.74789
Epoch: 1600 Train: 138.79874 Test: 5.10236
Epoch: 1680 Train: 149.31921 Test: 6.98633
Epoch: 1760 Train: 114.73191 Test: 3.15508
Epoch: 1840 Train: 132.94769 Test: 14.16998
Epoch: 1920 Train: 110.08981 Test: 3.51838
Epoch: 2000 Train: 93.90703 Test: 1.00516
Epoch: 2080 Train: 88.07210 Test: 1.14469
Epoch: 2160 Train: 136.90361 Test: 17.39800
Epoch 2240: New minimal relative error: 4.93%, model saved.
Epoch: 2240 Train: 77.51087 Test: 0.54891
Epoch 2320: New minimal relative error: 2.95%, model saved.
Epoch: 2320 Train: 71.72078 Test: 0.53464
Epoch: 2400 Train: 71.60189 Test: 0.80179
Epoch: 2480 Train: 65.35715 Test: 0.79769
Epoch: 2560 Train: 90.59283 Test: 5.79174
Epoch: 2640 Train: 78.32718 Test: 4.97919
Epoch: 2720 Train: 61.80137 Test: 1.02724
Epoch: 2800 Train: 61.61995 Test: 5.66428
Epoch: 2880 Train: 59.40017 Test: 0.56315
Epoch: 2960 Train: 69.74302 Test: 6.55965
Epoch: 3040 Train: 50.07843 Test: 0.40773
Epoch: 3120 Train: 53.01906 Test: 3.39411
Epoch: 3200 Train: 58.27128 Test: 4.45147
Epoch: 3280 Train: 64.78092 Test: 8.52779
Epoch: 3360 Train: 48.63085 Test: 0.23776
Epoch: 3440 Train: 49.29377 Test: 0.23769
Epoch: 3520 Train: 96.09297 Test: 9.37740
Epoch: 3600 Train: 61.42902 Test: 0.45742
Epoch: 3680 Train: 53.47343 Test: 0.27401
Epoch: 3760 Train: 48.08340 Test: 1.06958
Epoch: 3840 Train: 52.25934 Test: 3.20946
Epoch: 3920 Train: 41.81680 Test: 0.25413
Epoch: 4000 Train: 53.24918 Test: 8.34706
Epoch: 4080 Train: 41.06664 Test: 0.16198
Epoch: 4160 Train: 41.56204 Test: 0.55628
Epoch: 4240 Train: 40.00980 Test: 0.13768
Epoch: 4320 Train: 39.90569 Test: 0.31306
Epoch: 4400 Train: 45.25724 Test: 2.03896
Epoch: 4480 Train: 43.08876 Test: 0.70501
Epoch: 4560 Train: 36.99808 Test: 0.10814
Epoch: 4640 Train: 41.80005 Test: 2.46455
Epoch: 4720 Train: 38.26217 Test: 0.23072
Epoch 4800: New minimal relative error: 2.42%, model saved.
Epoch: 4800 Train: 33.56501 Test: 0.09100
Epoch: 4880 Train: 36.18754 Test: 0.89037
Epoch: 4960 Train: 32.95352 Test: 1.01239
Epoch: 5040 Train: 30.96281 Test: 0.07262
Epoch: 5120 Train: 30.18813 Test: 0.12577
Epoch: 5200 Train: 29.45791 Test: 0.06890
Epoch: 5280 Train: 29.11267 Test: 0.08940
Epoch: 5360 Train: 32.39410 Test: 1.28908
Epoch: 5440 Train: 28.64588 Test: 0.46057
Epoch: 5520 Train: 27.60140 Test: 0.05794
Epoch: 5600 Train: 28.80265 Test: 0.08808
Epoch: 5680 Train: 27.60799 Test: 0.09745
Epoch: 5760 Train: 27.96351 Test: 0.06094
Epoch: 5840 Train: 29.26720 Test: 1.80285
Epoch 5920: New minimal relative error: 0.78%, model saved.
Epoch: 5920 Train: 26.28480 Test: 0.04915
Epoch: 6000 Train: 26.52741 Test: 0.23904
Epoch: 6080 Train: 27.44781 Test: 0.14391
Epoch: 6160 Train: 26.56428 Test: 0.09169
Epoch: 6240 Train: 26.33120 Test: 0.05539
Epoch: 6320 Train: 27.16049 Test: 0.66899
Epoch: 6400 Train: 25.57370 Test: 0.09390
Epoch: 6480 Train: 25.07751 Test: 0.14580
Epoch: 6560 Train: 25.07748 Test: 0.15905
Epoch: 6640 Train: 27.02148 Test: 0.08953
Epoch: 6720 Train: 24.93989 Test: 0.07294
Epoch: 6800 Train: 25.26741 Test: 0.07307
Epoch: 6880 Train: 24.19695 Test: 0.35264
Epoch: 6960 Train: 24.00512 Test: 0.21951
Epoch: 7040 Train: 24.31415 Test: 0.04755
Epoch: 7120 Train: 25.91217 Test: 0.08876
Epoch: 7200 Train: 25.50220 Test: 0.05771
Epoch: 7280 Train: 25.40217 Test: 0.06586
Epoch: 7360 Train: 28.43847 Test: 1.22188
Epoch: 7440 Train: 25.07610 Test: 0.09676
Epoch: 7520 Train: 25.59165 Test: 0.55673
Epoch: 7600 Train: 24.83029 Test: 0.35191
Epoch: 7680 Train: 23.97957 Test: 0.34462
Epoch: 7760 Train: 23.79154 Test: 0.33764
Epoch: 7840 Train: 23.72775 Test: 0.05763
Epoch: 7920 Train: 24.01481 Test: 0.07047
Epoch: 7999 Train: 23.55779 Test: 0.09199
Training Loss: tensor(23.5578)
Test Loss: tensor(0.0920)
Learned LE: [ 8.7234747e-01 -5.7070414e-03 -1.4542167e+01]
True LE: [ 8.7800872e-01 -1.4379184e-03 -1.4547398e+01]
Relative Error: [2.4191606  1.8886057  1.579651   1.3136196  0.9185556  0.42594293
 0.44422984 0.6798332  0.82310766 0.95883024 1.143249   1.4344759
 1.9796312  2.1426167  2.1977108  1.95245    2.228898   2.187878
 2.2069519  2.0704641  1.8350309  1.5823476  1.3684032  1.3848789
 1.2303236  1.1047943  1.0588803  1.1585763  1.3681005  1.7972536
 2.565811   3.3832033  3.451381   3.3696208  3.0048683  3.4434624
 4.2191615  3.9816298  3.9669714  4.0055795  4.224768   3.9127924
 3.5314045  3.180882   2.9005957  2.8712049  2.840819   2.778388
 2.7582793  2.5993383  2.321818   1.8321637  1.1897448  0.6306484
 1.2308731  2.0453107  2.6566153  3.0500958  3.1474578  3.03606
 2.7082613  2.3738568  2.0946658  1.7848009  1.427011   1.1445026
 0.948009   0.6337196  0.7895399  1.1086402  1.0800248  0.80064994
 0.61082417 0.6445824  1.2738199  1.7200481  1.9385383  1.7869893
 1.7431563  1.6010685  1.4615544  1.6307576  1.5805874  1.3498228
 1.076751   1.1199688  1.1808085  0.9848846  0.9183769  0.9715871
 1.2111479  1.6088837  2.2259781  3.0492258  3.3524222  3.2530274
 2.9389477  2.4930737  3.0922117  3.7811399  3.6096911  3.6169045
 3.6856492  3.9343183  3.9668615  3.6460385  3.2639372  3.057321
 2.7858996  2.473375   2.6660864  2.3116379  2.128762   1.7463082
 1.4312974  0.83501107 1.1578617  1.8073145  2.4491272  2.8909633
 3.1105158  3.1173718  2.785111   2.3185027  1.8504862  1.7725487
 1.4466288  1.0423201  0.7606216  0.8056136  0.9656355  1.5448304
 1.8217206  1.698667   1.1545908  0.68415564 0.4790706  1.0779076
 1.46716    1.5629076  1.5908943  1.4741168  1.0766199  1.0671352
 1.0688925  0.9927056  0.81526625 0.853384   1.133987   1.0247223
 0.8105304  0.8172325  1.0256592  1.3981204  1.9920038  2.649252
 2.959555   3.0585806  3.0016696  2.4710948  1.8562151  2.4776664
 3.144159   3.099814   3.2721813  3.4493167  3.6871243  4.18962
 3.8258486  3.4324794  3.114618   2.514606   2.269659   2.0878823
 1.6469476  1.4926338  1.367786   1.1844947  1.1211745  1.6842134
 2.2136114  2.720099   3.0008664  3.080911   2.880331   2.4357917
 1.9906074  1.7709938  1.4630091  1.1007111  0.62976974 0.53515536
 1.0483814  1.4780115  2.091058   2.4623487  2.2006888  1.4697144
 0.9497467  0.6090895  0.891453   1.3049629  1.2470181  1.3044087
 1.0044758  0.7742678  0.730808   0.6192266  0.4656198  0.6023691
 0.8467028  1.0643785  0.8738741  0.68624127 0.73454964 0.99338704
 1.4583654  2.1997845  2.8137887  2.8905337  2.754211   2.4091678
 1.8246827  1.3383933  1.7138485  2.557164   2.548408   2.7221284
 3.1384296  3.3391488  3.9918206  4.1941934  3.801314   2.9980128
 2.3010325  1.9326302  1.5514181  1.0827123  1.1077449  1.1648706
 1.1138     1.4745021  2.0669527  2.4024515  2.7578187  2.988325
 2.942003   2.6061454  2.196338   1.9300828  1.5626961  1.0657127
 0.6578825  0.4190912  0.790714   1.4958378  1.9410242  2.33693
 2.5403156  2.3306894  1.6573774  1.1945032  0.9107499  0.68554366
 1.0743461  1.0122864  1.0314242  0.8367472  0.6662306  0.53199697
 0.1752641  0.43457836 0.74621946 1.0014678  1.0579383  0.79109335
 0.61008054 0.62647027 0.8085418  1.4166818  2.1297364  2.6423612
 2.8416142  2.4968162  1.8374652  1.4143976  1.1024595  1.0572258
 2.017994   2.1323695  2.2395036  2.5526268  2.9970243  3.8005118
 4.1988893  4.0222187  3.0728307  2.2797937  1.6451776  1.102483
 0.9517354  1.156462   1.2275809  1.270602   1.60445    2.2191818
 2.5313082  2.7463193  2.8035994  2.7078373  2.4326997  2.0550437
 1.7692891  1.3437942  0.87151283 0.49771857 0.45159528 0.83000165
 1.261283   1.5639359  1.7014434  2.1265333  2.2189405  1.6921487
 1.1514221  0.80933785 0.6309323  0.8400593  0.8619518  0.93805045
 0.7633017  0.84415424 0.6693528  0.39495853 0.5190854  0.78336495
 0.95899916 1.0749894  0.98212844 0.5696421  0.6076041  0.60819316
 1.0728906  1.7687868  2.345646   2.665389   2.272854   1.4891533
 1.0783148  1.0394918  0.9071174  1.1484926  1.8472824  1.8922634
 2.0582483  2.455723   3.2579057  3.9136403  3.9872327  3.583675
 2.5556457  1.6490653  0.99190307 0.9719347  1.3013685  1.3488605
 1.2784253  1.3426344  1.9792341  2.3721008  2.464346   2.4622831
 2.2848206  2.02756    1.7312043  1.5288188  1.1932787  0.8486513
 0.3801919  0.30071232 0.5499344  0.8578496  0.9561494  1.1501436
 1.311347   1.5566331  1.5434115  1.0304385  0.71634257 0.53708524
 0.67725617 0.7651026  0.65294373 0.6647131  0.87735337 0.9171401
 0.6832483  0.5700731  0.6327672  0.82436484]
