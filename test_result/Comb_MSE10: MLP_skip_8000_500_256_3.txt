time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.92%, model saved.
Epoch: 0 Train: 3961.52295 Test: 4127.66016
Epoch: 80 Train: 382.71082 Test: 408.23984
Epoch 160: New minimal relative error: 19.17%, model saved.
Epoch: 160 Train: 56.36008 Test: 59.57489
Epoch: 240 Train: 19.00425 Test: 20.44226
Epoch 320: New minimal relative error: 15.31%, model saved.
Epoch: 320 Train: 10.14952 Test: 11.03524
Epoch: 400 Train: 7.32553 Test: 8.01767
Epoch 480: New minimal relative error: 11.36%, model saved.
Epoch: 480 Train: 6.10075 Test: 6.56258
Epoch: 560 Train: 9.77282 Test: 9.48152
Epoch: 640 Train: 4.48424 Test: 4.86923
Epoch: 720 Train: 4.13431 Test: 4.54349
Epoch: 800 Train: 4.76349 Test: 5.58748
Epoch: 880 Train: 3.37275 Test: 3.69084
Epoch: 960 Train: 3.13804 Test: 3.43306
Epoch: 1040 Train: 2.89210 Test: 3.16401
Epoch: 1120 Train: 2.70532 Test: 2.99353
Epoch: 1200 Train: 2.51213 Test: 2.74230
Epoch 1280: New minimal relative error: 10.01%, model saved.
Epoch: 1280 Train: 2.41676 Test: 2.64535
Epoch: 1360 Train: 2.28706 Test: 2.48339
Epoch: 1440 Train: 2.16847 Test: 2.37933
Epoch: 1520 Train: 2.05050 Test: 2.22589
Epoch: 1600 Train: 1.94423 Test: 2.13529
Epoch: 1680 Train: 1.83955 Test: 1.99614
Epoch: 1760 Train: 2.13644 Test: 2.21806
Epoch: 1840 Train: 1.61369 Test: 1.77726
Epoch: 1920 Train: 1.74432 Test: 1.99086
Epoch: 2000 Train: 1.44680 Test: 1.57861
Epoch: 2080 Train: 2.77616 Test: 2.74467
Epoch: 2160 Train: 1.31410 Test: 1.43263
Epoch: 2240 Train: 1.26036 Test: 1.37913
Epoch: 2320 Train: 1.21974 Test: 1.33246
Epoch: 2400 Train: 1.13442 Test: 1.24022
Epoch: 2480 Train: 1.08166 Test: 1.18329
Epoch: 2560 Train: 1.05163 Test: 1.16713
Epoch: 2640 Train: 0.99218 Test: 1.08718
Epoch: 2720 Train: 0.94691 Test: 1.03781
Epoch: 2800 Train: 1.30425 Test: 1.28987
Epoch: 2880 Train: 0.86880 Test: 0.95400
Epoch: 2960 Train: 0.83147 Test: 0.91351
Epoch: 3040 Train: 1.45259 Test: 1.72615
Epoch: 3120 Train: 0.76418 Test: 0.84220
Epoch: 3200 Train: 0.73278 Test: 0.80851
Epoch: 3280 Train: 0.70444 Test: 0.78243
Epoch: 3360 Train: 0.67336 Test: 0.74808
Epoch: 3440 Train: 0.68169 Test: 0.73287
Epoch: 3520 Train: 0.62838 Test: 0.70010
Epoch: 3600 Train: 0.61003 Test: 0.68059
Epoch: 3680 Train: 0.80020 Test: 0.95626
Epoch: 3760 Train: 0.57209 Test: 0.64178
Epoch: 3840 Train: 1.34069 Test: 1.49236
Epoch: 3920 Train: 0.54158 Test: 0.61007
Epoch: 4000 Train: 1.45389 Test: 1.43088
Epoch 4080: New minimal relative error: 8.57%, model saved.
Epoch: 4080 Train: 0.51502 Test: 0.58281
Epoch: 4160 Train: 0.82406 Test: 0.98231
Epoch: 4240 Train: 0.49142 Test: 0.55873
Epoch: 4320 Train: 0.48929 Test: 0.56016
Epoch: 4400 Train: 0.47419 Test: 0.53799
Epoch: 4480 Train: 0.45843 Test: 0.52519
Epoch: 4560 Train: 0.46026 Test: 0.53267
Epoch: 4640 Train: 0.43459 Test: 0.49973
Epoch: 4720 Train: 0.43531 Test: 0.50122
Epoch: 4800 Train: 0.41615 Test: 0.48081
Epoch: 4880 Train: 0.40623 Test: 0.46998
Epoch: 4960 Train: 0.57265 Test: 0.62900
Epoch: 5040 Train: 0.39860 Test: 0.46109
Epoch 5120: New minimal relative error: 8.27%, model saved.
Epoch: 5120 Train: 0.37779 Test: 0.43902
Epoch: 5200 Train: 0.51752 Test: 0.53539
Epoch: 5280 Train: 0.36107 Test: 0.42192
Epoch: 5360 Train: 0.35698 Test: 0.42002
Epoch: 5440 Train: 0.34663 Test: 0.40669
Epoch: 5520 Train: 0.37871 Test: 0.41770
Epoch: 5600 Train: 0.33353 Test: 0.39293
Epoch: 5680 Train: 0.35855 Test: 0.43479
Epoch: 5760 Train: 0.32155 Test: 0.38010
Epoch: 5840 Train: 0.37072 Test: 0.41192
Epoch: 5920 Train: 0.31030 Test: 0.36835
Epoch 6000: New minimal relative error: 6.84%, model saved.
Epoch: 6000 Train: 0.31560 Test: 0.39016
Epoch: 6080 Train: 0.29980 Test: 0.35702
Epoch 6160: New minimal relative error: 6.40%, model saved.
Epoch: 6160 Train: 0.33708 Test: 0.39374
Epoch: 6240 Train: 0.36062 Test: 0.41680
Epoch: 6320 Train: 0.34230 Test: 0.41154
Epoch: 6400 Train: 0.28099 Test: 0.33685
Epoch: 6480 Train: 0.28516 Test: 0.33940
Epoch: 6560 Train: 0.27230 Test: 0.32720
Epoch: 6640 Train: 0.26867 Test: 0.32308
Epoch: 6720 Train: 0.26436 Test: 0.31862
Epoch: 6800 Train: 0.26265 Test: 0.31497
Epoch: 6880 Train: 0.25686 Test: 0.31074
Epoch: 6960 Train: 0.25558 Test: 0.30942
Epoch: 7040 Train: 0.24963 Test: 0.30335
Epoch: 7120 Train: 0.24615 Test: 0.29987
Epoch: 7200 Train: 0.24276 Test: 0.29674
Epoch: 7280 Train: 0.26184 Test: 0.32190
Epoch: 7360 Train: 0.23514 Test: 0.28924
Epoch: 7440 Train: 0.23070 Test: 0.28419
Epoch: 7520 Train: 0.22716 Test: 0.28000
Epoch: 7600 Train: 0.22577 Test: 0.27842
Epoch: 7680 Train: 0.22102 Test: 0.27325
Epoch 7760: New minimal relative error: 6.31%, model saved.
Epoch: 7760 Train: 0.21851 Test: 0.27019
Epoch: 7840 Train: 0.21522 Test: 0.26694
Epoch: 7920 Train: 0.21389 Test: 0.26478
Epoch: 7999 Train: 0.20972 Test: 0.26089
Training Loss: tensor(0.2097)
Test Loss: tensor(0.2609)
Learned LE: [ 0.84042317  0.02503511 -4.4105644 ]
True LE: [ 8.7428838e-01  4.0412406e-03 -1.4548159e+01]
Relative Error: [ 5.488302    7.4649043   9.774712   12.102034   13.841679   15.014294
 14.871885   14.121925   13.132482   11.963694   11.4122     10.677268
 10.3703785   9.918406    9.4990225   8.485945    7.7996716   7.405109
  7.0007415   6.5171146   5.9865823   5.9121614   5.7547426   6.0688868
  6.2348356   4.435808    3.365542    3.8318884   5.21003     6.3421545
  6.4961433   6.444426    6.250457    5.77116     4.9850297   4.7914495
  4.6488667   4.693681    5.061896    5.564706    6.218035    6.895657
  7.349964    7.638815    6.7894273   5.767264    4.8801446   4.0065994
  3.2704313   2.7999115   2.5565624   2.2167811   1.65962     1.3123212
  1.4015927   1.3080653   0.8673979   1.7922574   2.316672    2.3931031
  2.9482071   3.7410042   4.82309     6.442271    8.81613    10.624966
 11.91406    12.652852   13.393219   12.927154   12.199645   11.2000265
 10.047073    9.427168    9.167383    8.755049    8.385956    7.86864
  7.4025207   7.269724    6.9195566   6.3021364   5.6898637   5.1872563
  5.19143     4.900007    4.8927894   4.902493    3.3238046   2.8125079
  4.1622076   5.53772     5.557715    5.7142067   5.737215    5.3973036
  4.7408547   4.4163465   4.234172    3.939303    3.8520463   4.23604
  4.975428    5.7692227   6.4657927   6.891611    7.0620937   6.1681614
  5.054796    4.0149903   2.978722    2.1317177   1.8470049   1.9917111
  1.8475565   1.6736003   1.4196461   0.9169771   0.7144088   1.7798529
  2.2370021   2.1987894   2.2988424   2.820322    3.8611584   5.1680765
  7.060672    9.48726    10.859138   11.316666   12.197092   11.465719
 11.10088    10.287985    9.202127    8.281703    8.049418    7.7253394
  7.47374     7.1883874   6.6691694   6.391023    6.570919    6.29715
  5.797639    5.1005955   4.4773335   4.2481465   4.0127964   3.8335752
  3.646547    2.9034202   2.8399873   4.5705338   4.8914676   4.9028234
  4.6601205   4.61588     4.188994    3.6277778   3.7998066   3.896559
  3.3255386   3.0160847   3.4912322   4.5558753   5.669613    6.3116355
  6.52243     6.483142    5.72994     4.4765234   3.2403271   2.1229904
  1.5591841   1.7288582   2.339864    2.8216913   3.1111693   3.233871
  3.0351875   2.5026138   2.106203    2.6180882   2.3347602   2.2762728
  2.501996    3.6997445   5.2225885   7.116127    9.135787    9.971837
 10.6585655  11.565734   10.645524    9.322952    8.464573    7.539309
  6.7988014   6.82079     6.6878653   6.4511833   5.9844775   5.37876
  5.176528    5.548656    5.5171294   5.1968427   4.7221003   3.9102507
  3.3429437   3.2691007   2.7997022   2.948689    2.9401278   2.7177382
  4.4843416   4.3258324   3.911091    3.565047    3.5644207   3.0522997
  2.8342316   3.0407093   3.1823833   2.9800682   2.2178032   2.8800993
  4.3626533   5.3534284   5.914006    6.0209966   5.650664    5.18577
  3.977459    2.7176354   1.7817192   1.3348206   1.698401    2.6022277
  3.7466977   4.8397675   5.1942897   4.4964356   3.5870988   2.577526
  2.87988     3.0061097   2.5698862   2.1596344   2.9939263   4.7221594
  6.682109    7.937802    8.980688    9.592349   10.703678    9.866063
  8.635347    7.3565354   6.10403     5.544466    5.501807    5.6991124
  5.5805583   5.108044    4.4225554   4.104193    4.02351     4.274686
  4.452621    4.123796    3.5823946   2.8679159   2.4060254   2.3034096
  2.226199    2.2192056   2.4950624   3.6659126   3.6975455   3.1688552
  2.881109    2.6526709   2.2452247   1.9667102   2.3528955   2.377518
  2.10443     1.5186845   2.2593272   3.4882967   4.530228    4.6901703
  4.542519    4.227214    3.924425    3.139805    2.1864316   1.5875039
  1.2796017   1.6347762   2.4900863   3.6013477   5.192114    6.157261
  5.82486     5.070537    4.3189254   3.3246386   3.5507667   3.3758235
  2.6522155   2.0181065   3.2826138   5.3394957   6.532094    7.6440344
  7.933117    9.109798    9.073135    8.17701     7.429426    6.1638174
  4.679829    4.537857    5.066353    5.183122    4.8662605   4.2661567
  3.8308685   3.2218628   3.3134353   3.5492978   3.636384    3.0786707
  2.6907775   2.165507    1.9223872   1.7752448   1.5698078   1.8140949
  2.3891652   3.09946     2.6724234   2.411586    2.043129    1.7202967
  1.288662    1.4732217   1.5103464   1.5520613   0.64561385  1.0985985
  2.2963476   3.1243346   3.428695    3.3199298   3.001922    2.7397542
  2.8727803   2.3292687   1.5054806   1.1247454   1.4556139   2.219467
  3.0954118   4.091785    4.4709945   4.739925    5.2171783   5.193757
  5.0016856   4.5679874   4.415396    4.010185    2.9641254   2.0393815
  2.7999747   4.133235    5.439783    6.391103    6.843101    8.194386
  7.7027264   7.3717885   6.7959547   5.5605764   4.369149    3.848774
  4.4814577   4.698039    4.239361    3.6835916   3.328459    3.0239117
  3.0977936   2.952717    2.697112    2.1625328 ]
