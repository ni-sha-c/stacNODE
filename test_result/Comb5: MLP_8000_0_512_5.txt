time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.97%, model saved.
Epoch: 0 Train: 3621.04614 Test: 4417.55273
Epoch 80: New minimal relative error: 28.85%, model saved.
Epoch: 80 Train: 61.30510 Test: 55.71562
Epoch: 160 Train: 27.33022 Test: 21.45864
Epoch: 240 Train: 36.95860 Test: 22.15872
Epoch: 320 Train: 3.20028 Test: 2.29006
Epoch: 400 Train: 24.19387 Test: 25.52815
Epoch: 480 Train: 6.11530 Test: 1.85947
Epoch 560: New minimal relative error: 9.35%, model saved.
Epoch: 560 Train: 0.97009 Test: 0.58040
Epoch: 640 Train: 9.10118 Test: 10.45672
Epoch: 720 Train: 0.66761 Test: 0.83166
Epoch: 800 Train: 0.48758 Test: 0.44770
Epoch: 880 Train: 0.47543 Test: 0.28798
Epoch: 960 Train: 0.37491 Test: 0.19809
Epoch: 1040 Train: 0.50435 Test: 0.49957
Epoch: 1120 Train: 0.65897 Test: 0.90865
Epoch 1200: New minimal relative error: 7.67%, model saved.
Epoch: 1200 Train: 0.35567 Test: 0.43829
Epoch 1280: New minimal relative error: 6.43%, model saved.
Epoch: 1280 Train: 1.22073 Test: 1.21063
Epoch 1360: New minimal relative error: 6.09%, model saved.
Epoch: 1360 Train: 0.32110 Test: 0.18799
Epoch: 1440 Train: 0.72658 Test: 0.75523
Epoch: 1520 Train: 2.63802 Test: 3.34410
Epoch: 1600 Train: 8.58579 Test: 8.28934
Epoch: 1680 Train: 2.28671 Test: 1.41596
Epoch: 1760 Train: 4.66711 Test: 6.15276
Epoch: 1840 Train: 2.49962 Test: 1.92223
Epoch: 1920 Train: 0.83193 Test: 1.00940
Epoch: 2000 Train: 1.16011 Test: 0.60429
Epoch: 2080 Train: 0.98870 Test: 1.18846
Epoch: 2160 Train: 0.65032 Test: 0.79360
Epoch: 2240 Train: 0.71443 Test: 0.69032
Epoch: 2320 Train: 0.21997 Test: 0.23487
Epoch: 2400 Train: 0.57765 Test: 0.40074
Epoch: 2480 Train: 0.33908 Test: 0.25499
Epoch: 2560 Train: 1.23951 Test: 1.84259
Epoch 2640: New minimal relative error: 5.84%, model saved.
Epoch: 2640 Train: 1.22418 Test: 0.45801
Epoch: 2720 Train: 0.31079 Test: 0.43440
Epoch: 2800 Train: 0.12104 Test: 0.13878
Epoch: 2880 Train: 0.12175 Test: 0.12279
Epoch: 2960 Train: 1.69063 Test: 1.82360
Epoch: 3040 Train: 0.42336 Test: 0.40616
Epoch 3120: New minimal relative error: 4.35%, model saved.
Epoch: 3120 Train: 0.22093 Test: 0.07697
Epoch: 3200 Train: 2.49850 Test: 1.92196
Epoch: 3280 Train: 0.80392 Test: 1.03853
Epoch: 3360 Train: 0.56008 Test: 0.36646
Epoch: 3440 Train: 0.45157 Test: 0.46042
Epoch: 3520 Train: 2.02770 Test: 2.18474
Epoch 3600: New minimal relative error: 2.10%, model saved.
Epoch: 3600 Train: 0.05808 Test: 0.04158
Epoch: 3680 Train: 0.28326 Test: 0.31170
Epoch: 3760 Train: 0.30057 Test: 0.26367
Epoch: 3840 Train: 0.20914 Test: 0.08075
Epoch: 3920 Train: 0.06028 Test: 0.07164
Epoch: 4000 Train: 0.03831 Test: 0.03406
Epoch: 4080 Train: 0.04442 Test: 0.03619
Epoch: 4160 Train: 0.09620 Test: 0.09838
Epoch: 4240 Train: 0.40750 Test: 0.48083
Epoch: 4320 Train: 0.03238 Test: 0.03214
Epoch: 4400 Train: 0.05701 Test: 0.05842
Epoch: 4480 Train: 0.37397 Test: 0.39979
Epoch: 4560 Train: 0.27381 Test: 0.25798
Epoch: 4640 Train: 0.38490 Test: 0.29733
Epoch: 4720 Train: 0.18305 Test: 0.28907
Epoch 4800: New minimal relative error: 1.62%, model saved.
Epoch: 4800 Train: 0.02784 Test: 0.02817
Epoch: 4880 Train: 0.18432 Test: 0.27199
Epoch: 4960 Train: 0.10730 Test: 0.14350
Epoch: 5040 Train: 0.02849 Test: 0.02904
Epoch: 5120 Train: 0.02956 Test: 0.03301
Epoch: 5200 Train: 0.29528 Test: 0.29269
Epoch: 5280 Train: 0.02182 Test: 0.02259
Epoch: 5360 Train: 0.03745 Test: 0.03273
Epoch: 5440 Train: 0.13874 Test: 0.19888
Epoch: 5520 Train: 0.02134 Test: 0.02189
Epoch: 5600 Train: 0.02992 Test: 0.04784
Epoch: 5680 Train: 0.27069 Test: 0.33120
Epoch: 5760 Train: 0.02054 Test: 0.02160
Epoch: 5840 Train: 0.02165 Test: 0.02295
Epoch: 5920 Train: 0.11354 Test: 0.11296
Epoch: 6000 Train: 0.03463 Test: 0.04362
Epoch: 6080 Train: 0.01852 Test: 0.01955
Epoch: 6160 Train: 0.09293 Test: 0.04568
Epoch: 6240 Train: 0.05668 Test: 0.05349
Epoch: 6320 Train: 0.15173 Test: 0.28252
Epoch: 6400 Train: 0.01653 Test: 0.01812
Epoch: 6480 Train: 0.03730 Test: 0.03698
Epoch: 6560 Train: 1.40006 Test: 1.24367
Epoch: 6640 Train: 0.01589 Test: 0.01744
Epoch: 6720 Train: 0.02568 Test: 0.02288
Epoch: 6800 Train: 1.23809 Test: 1.44619
Epoch: 6880 Train: 0.01508 Test: 0.01712
Epoch: 6960 Train: 0.82144 Test: 0.73538
Epoch: 7040 Train: 0.04740 Test: 0.06204
Epoch 7120: New minimal relative error: 1.28%, model saved.
Epoch: 7120 Train: 0.01504 Test: 0.01690
Epoch: 7200 Train: 0.11674 Test: 0.10321
Epoch: 7280 Train: 0.01337 Test: 0.01571
Epoch: 7360 Train: 0.01752 Test: 0.02491
Epoch: 7440 Train: 0.01300 Test: 0.01574
Epoch: 7520 Train: 0.01590 Test: 0.01828
Epoch: 7600 Train: 0.01491 Test: 0.01538
Epoch: 7680 Train: 0.09977 Test: 0.13628
Epoch: 7760 Train: 0.01238 Test: 0.01480
Epoch: 7840 Train: 0.04479 Test: 0.07560
Epoch: 7920 Train: 0.01175 Test: 0.01413
Epoch: 7999 Train: 0.05155 Test: 0.06279
Training Loss: tensor(0.0516)
Test Loss: tensor(0.0628)
Learned LE: [ 0.8729942  -0.03946495 -5.2322173 ]
True LE: [ 8.4631860e-01  1.2351533e-02 -1.4530384e+01]
Relative Error: [2.0846894  2.1613083  2.3817723  2.7151723  3.045046   3.3940394
 3.7619953  3.988274   3.9741848  3.731664   3.4249492  3.2649264
 3.4262066  3.9914126  4.6335077  4.5842915  4.406833   4.3099756
 4.1659317  3.950474   3.668635   3.346907   3.007126   2.683211
 2.3388348  1.8711696  1.2757168  0.6740141  0.21480674 0.5225656
 1.1249833  1.8636746  2.8637855  4.0664067  4.874492   4.852581
 4.291294   3.4499545  2.7667916  2.0223253  1.2963387  0.79290044
 1.051908   1.6890591  2.383333   3.0330262  3.5149722  3.8094301
 3.8856428  3.834988   3.7827723  3.7589893  3.7182841  3.6077895
 3.4067438  3.1225216  2.7743847  2.5001392  2.277188   2.0333698
 1.8168997  1.6408931  1.5229738  1.4828671  1.5563664  1.7958912
 2.1804464  2.5606947  2.9101744  3.2146363  3.3422737  3.268738
 3.0370486  2.7653766  2.589203   2.7274976  3.3161645  4.2159677
 4.5434375  4.4382477  4.416636   4.3288956  4.1268106  3.8158865
 3.4370513  3.0022182  2.5628357  2.199266   1.8403798  1.3658448
 0.8229588  0.37708938 0.5348992  1.0528783  1.5700525  2.3358443
 3.4506674  4.275723   4.223857   3.6558318  2.8373873  2.2716637
 1.6951177  1.1132966  0.6102172  0.7881373  1.3601769  1.9514921
 2.4710286  2.8689096  3.092714   3.075073   2.957508   2.8896704
 2.8902557  2.9001222  2.8510654  2.697018   2.4393048  2.1509645
 2.0033348  1.8413905  1.6061517  1.3807933  1.1799084  1.0326524
 0.9692127  1.0060842  1.1926057  1.5664028  1.987696   2.3299284
 2.5961018  2.6637285  2.5827975  2.4110448  2.1889312  1.9543655
 2.00585    2.585653   3.5204728  4.4207873  4.5270786  4.5658817
 4.5769534  4.4556985  4.172118   3.7861645  3.3438504  2.8386557
 2.3136296  1.8842963  1.5095483  1.0625374  0.6020345  0.46380502
 0.8888339  1.2476143  1.6554661  2.5158653  3.4756944  3.5935044
 3.1030369  2.335159   1.8278728  1.4552778  1.0236806  0.538072
 0.44652656 0.9372727  1.4029684  1.842162   2.1648173  2.2925243
 2.18544    1.994079   1.8879218  1.8939346  1.9560041  2.0092149
 1.9950492  1.8691918  1.6388806  1.5211427  1.4608309  1.2775984
 1.0889295  0.88750196 0.69984746 0.60739166 0.6325521  0.7179753
 0.9434862  1.3259646  1.6688695  1.9292371  2.0152397  1.9414964
 1.8333437  1.6865257  1.4374055  1.3068125  1.7917491  2.62035
 3.583751   4.4386325  4.5411773  4.619829   4.629444   4.4826107
 4.188431   3.810681   3.3724372  2.877099   2.3114626  1.7676297
 1.3524739  0.9579657  0.58362854 0.60138404 0.9830094  1.1309704
 1.4445424  2.2771225  2.9003115  2.5961256  2.0168855  1.4031017
 1.192783   0.9500129  0.6126707  0.2527073  0.4947583  0.85956806
 1.1979109  1.3894852  1.4456712  1.3433682  1.1206841  0.95052284
 0.916934   0.9823139  1.0837696  1.174087   1.2130322  1.169161
 1.0842935  1.1434745  1.0028105  0.85053605 0.7336168  0.5629832
 0.38671908 0.40074712 0.522312   0.59077096 0.7402538  1.0066769
 1.2176132  1.378561   1.3714916  1.29082    1.2239692  1.0738496
 0.8170951  0.93111575 1.6260022  2.3773563  3.1247647  3.9791744
 4.163067   4.1900797  4.216684   4.144811   3.947876   3.6002846
 3.166638   2.7669864  2.365824   1.8614074  1.3638622  0.9713163
 0.6488371  0.6041551  0.88053846 0.8923491  0.97791916 1.5966121
 2.1013043  1.7192165  1.2323096  0.803311   0.7537657  0.62279654
 0.40162265 0.2594711  0.4212522  0.66970193 0.7414495  0.69641894
 0.66782373 0.5439773  0.365302   0.24260652 0.26005426 0.3605397
 0.4570804  0.5340258  0.5932377  0.6196642  0.66376466 0.86377317
 0.72551364 0.5300182  0.47396064 0.3860292  0.20450048 0.2462664
 0.44185147 0.51095307 0.5735242  0.713167   0.7739144  0.8018136
 0.8149725  0.7674321  0.7462912  0.6330171  0.4247435  0.5842432
 1.1680841  1.6946087  2.0872166  2.5565228  3.1476326  3.0805562
 3.064114   3.083043   3.028966   2.8253431  2.4867413  2.1496472
 1.9475726  1.7864655  1.5034677  1.1479391  0.7812599  0.54085946
 0.62991    0.75866365 0.5500072  0.69033045 1.198694   1.1488049
 0.74331725 0.43176967 0.38016596 0.37593663 0.30243316 0.28030908
 0.29193696 0.43393523 0.33598775 0.275074   0.30429006 0.25977328
 0.15881075 0.07996311 0.04084742 0.13044101 0.21517013 0.25329074
 0.26499978 0.27575532 0.29732653 0.49793905 0.6215542  0.39625195
 0.24640158 0.21603853 0.1884105  0.05287933 0.21714121 0.35612214
 0.36586168 0.45093644 0.55400336 0.46875972 0.36036137 0.36130628
 0.37859035 0.36818793 0.275985   0.21169344 0.50052786 0.84887
 1.0112963  1.0409249  1.1983951  1.7451009 ]
