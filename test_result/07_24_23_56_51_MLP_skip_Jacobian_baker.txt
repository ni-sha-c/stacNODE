time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 1000
num_train: 1000
num_test: 1000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 5
reg_param: 50.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 83.544563293 Test: 17.557683945
Epoch 0: New minimal relative error: 17.56%, model saved.
Epoch: 10 Train: 42.942264557 Test: 5.537259579
Epoch 10: New minimal relative error: 5.54%, model saved.
Epoch: 20 Train: 16.196752548 Test: 10.552113533
Epoch: 30 Train: 11.626238823 Test: 3.340900898
Epoch 30: New minimal relative error: 3.34%, model saved.
Epoch: 40 Train: 9.302860260 Test: 5.644945621
Epoch: 50 Train: 8.354024887 Test: 4.088956356
Epoch: 60 Train: 7.927933693 Test: 4.825411320
Epoch: 70 Train: 7.872519493 Test: 4.229304314
Epoch: 80 Train: 7.657339096 Test: 4.518116474
Epoch: 90 Train: 7.564518929 Test: 4.315501690
Epoch: 100 Train: 7.601779938 Test: 4.307299614
Epoch: 110 Train: 7.524971485 Test: 4.258587837
Epoch: 120 Train: 7.517998695 Test: 4.282392979
Epoch: 130 Train: 7.377119064 Test: 4.167207718
Epoch: 140 Train: 7.232619762 Test: 4.332960606
Epoch: 150 Train: 7.224667549 Test: 4.225275517
Epoch: 160 Train: 7.518846989 Test: 4.363717556
Epoch: 170 Train: 7.753262997 Test: 4.124242306
Epoch: 180 Train: 7.913752556 Test: 4.253251553
Epoch: 190 Train: 7.732497215 Test: 4.102314472
Epoch: 200 Train: 7.862926483 Test: 4.437567234
Epoch: 210 Train: 8.193330765 Test: 4.082815647
Epoch: 220 Train: 8.312060356 Test: 3.974828243
Epoch: 230 Train: 8.021219254 Test: 4.199552536
Epoch: 240 Train: 7.669623375 Test: 4.337766647
Epoch: 250 Train: 7.718028069 Test: 4.091199875
Epoch: 260 Train: 7.905684471 Test: 3.966903925
Epoch: 270 Train: 7.966248512 Test: 3.943972111
Epoch: 280 Train: 8.121641159 Test: 3.882467270
Epoch: 290 Train: 8.267057419 Test: 3.732158184
Epoch: 300 Train: 8.546430588 Test: 3.544129610
Epoch: 310 Train: 8.878211975 Test: 3.336151600
Epoch 310: New minimal relative error: 3.34%, model saved.
Epoch: 320 Train: 7.797234535 Test: 3.962094069
Epoch: 330 Train: 7.348791599 Test: 4.179043770
Epoch: 340 Train: 7.399202347 Test: 3.891041040
Epoch: 350 Train: 7.531101704 Test: 3.730649233
Epoch: 360 Train: 7.486981392 Test: 3.873950481
Epoch: 370 Train: 7.271144867 Test: 4.066483974
Epoch: 380 Train: 7.155346394 Test: 4.142469406
Epoch: 390 Train: 7.050067902 Test: 4.145371914
Epoch: 400 Train: 6.988016129 Test: 4.126043320
Epoch: 410 Train: 6.984295845 Test: 4.073431015
Epoch: 420 Train: 6.958186150 Test: 4.057975292
Epoch: 430 Train: 6.871733665 Test: 4.114535332
Epoch: 440 Train: 6.799635887 Test: 4.152831078
Epoch: 450 Train: 6.731932640 Test: 4.160957336
Epoch: 460 Train: 6.672588348 Test: 4.163763523
Epoch: 470 Train: 6.630842686 Test: 4.157330513
Epoch: 480 Train: 6.577607155 Test: 4.143826008
Epoch: 490 Train: 6.516037941 Test: 4.132746220
Epoch: 500 Train: 6.432518959 Test: 4.137062073
Epoch: 510 Train: 6.351558685 Test: 4.140115738
Epoch: 520 Train: 6.285399914 Test: 4.143116951
Epoch: 530 Train: 6.242729187 Test: 4.153144360
Epoch: 540 Train: 6.216741562 Test: 4.174184799
Epoch: 550 Train: 6.180671215 Test: 4.184482098
Epoch: 560 Train: 6.155135155 Test: 4.176693439
Epoch: 570 Train: 6.178576469 Test: 4.171631813
Epoch: 580 Train: 6.210453510 Test: 4.162401199
Epoch: 590 Train: 6.406709194 Test: 4.079918385
Epoch: 600 Train: 6.345280170 Test: 4.086633205
Epoch: 610 Train: 6.202978134 Test: 4.383627415
Epoch: 620 Train: 6.108697414 Test: 4.184023857
Epoch: 630 Train: 6.078763485 Test: 4.253061771
Epoch: 640 Train: 6.049322128 Test: 4.322800636
Epoch: 650 Train: 6.072980404 Test: 4.250370502
Epoch: 660 Train: 6.089833736 Test: 4.267066479
Epoch: 670 Train: 6.058046818 Test: 4.203329086
Epoch: 680 Train: 6.028853893 Test: 4.293872833
Epoch: 690 Train: 6.066185951 Test: 4.354538441
Epoch: 700 Train: 6.124808311 Test: 4.312963009
Epoch: 710 Train: 6.159660816 Test: 4.255919933
Epoch: 720 Train: 6.163096905 Test: 4.230507851
Epoch: 730 Train: 6.164124489 Test: 4.231359959
Epoch: 740 Train: 6.151496410 Test: 4.227630615
Epoch: 750 Train: 6.145853996 Test: 4.226380825
Epoch: 760 Train: 6.132004738 Test: 4.229087830
Epoch: 770 Train: 6.121742725 Test: 4.230253696
Epoch: 780 Train: 6.111600399 Test: 4.233778477
Epoch: 790 Train: 6.100678921 Test: 4.236763477
Epoch: 800 Train: 6.087944508 Test: 4.242621899
Epoch: 810 Train: 6.084596157 Test: 4.248917103
Epoch: 820 Train: 6.079902172 Test: 4.250840187
Epoch: 830 Train: 6.064609528 Test: 4.253228188
Epoch: 840 Train: 6.071743965 Test: 4.254960060
Epoch: 850 Train: 6.092471600 Test: 4.249257565
Epoch: 860 Train: 6.097826958 Test: 4.242250919
Epoch: 870 Train: 6.106614113 Test: 4.252845764
Epoch: 880 Train: 6.105960369 Test: 4.263583660
Epoch: 890 Train: 6.110300064 Test: 4.267817497
Epoch: 900 Train: 6.102650642 Test: 4.267267227
Epoch: 910 Train: 6.097614288 Test: 4.266545296
Epoch: 920 Train: 6.090164661 Test: 4.267168045
Epoch: 930 Train: 6.087162971 Test: 4.270654678
Epoch: 940 Train: 6.079769135 Test: 4.274987698
Epoch: 950 Train: 6.072985649 Test: 4.276866913
Epoch: 960 Train: 6.064288139 Test: 4.278299809
Epoch: 970 Train: 6.059884548 Test: 4.281153679
Epoch: 980 Train: 6.057886124 Test: 4.285237789
Epoch: 990 Train: 6.052574158 Test: 4.285724163
Epoch: 999 Train: 6.052321911 Test: 4.282932281
Training Loss: tensor(6.0523)
Test Loss: tensor(4.2829)
True Mean x: tensor(3.4447, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(5.5674e+22, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.5065, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(inf, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0356)
Jacobian term Test Loss: tensor(0.0005)
Learned LE: [1.7253022  0.38672814]
True LE: tensor([ 0.6932, -0.7446], dtype=torch.float64)
