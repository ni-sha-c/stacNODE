time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 102.15%, model saved.
Epoch: 0 Train: 3246.94653 Test: 3951.57227
Epoch 80: New minimal relative error: 59.06%, model saved.
Epoch: 80 Train: 164.92891 Test: 139.85521
Epoch 160: New minimal relative error: 46.60%, model saved.
Epoch: 160 Train: 41.53743 Test: 72.80522
Epoch 240: New minimal relative error: 31.33%, model saved.
Epoch: 240 Train: 16.12814 Test: 25.52410
Epoch 320: New minimal relative error: 15.19%, model saved.
Epoch: 320 Train: 6.79712 Test: 7.06602
Epoch: 400 Train: 2.18694 Test: 4.63459
Epoch: 480 Train: 18.75622 Test: 28.81263
Epoch: 560 Train: 1.15919 Test: 2.17866
Epoch: 640 Train: 10.65621 Test: 6.14855
Epoch: 720 Train: 1.38191 Test: 1.68783
Epoch: 800 Train: 1.36753 Test: 3.96920
Epoch: 880 Train: 2.29371 Test: 3.78973
Epoch: 960 Train: 0.71348 Test: 1.63894
Epoch: 1040 Train: 0.88245 Test: 1.31040
Epoch: 1120 Train: 4.95510 Test: 4.12493
Epoch 1200: New minimal relative error: 12.83%, model saved.
Epoch: 1200 Train: 1.36554 Test: 2.09808
Epoch 1280: New minimal relative error: 12.25%, model saved.
Epoch: 1280 Train: 3.16836 Test: 4.25942
Epoch 1360: New minimal relative error: 9.67%, model saved.
Epoch: 1360 Train: 1.64404 Test: 2.05199
Epoch: 1440 Train: 4.22451 Test: 4.14819
Epoch: 1520 Train: 2.60318 Test: 3.63788
Epoch: 1600 Train: 2.40468 Test: 3.31659
Epoch: 1680 Train: 0.40907 Test: 0.49774
Epoch 1760: New minimal relative error: 9.26%, model saved.
Epoch: 1760 Train: 2.04982 Test: 2.41467
Epoch: 1840 Train: 5.46199 Test: 6.62285
Epoch: 1920 Train: 6.96837 Test: 7.82169
Epoch: 2000 Train: 0.17946 Test: 0.40597
Epoch: 2080 Train: 0.25946 Test: 0.59330
Epoch: 2160 Train: 0.94185 Test: 1.37070
Epoch: 2240 Train: 1.20596 Test: 1.66813
Epoch: 2320 Train: 0.25770 Test: 0.84656
Epoch: 2400 Train: 3.38876 Test: 3.04962
Epoch: 2480 Train: 1.75788 Test: 2.90872
Epoch: 2560 Train: 1.76279 Test: 2.27153
Epoch: 2640 Train: 0.61678 Test: 1.31900
Epoch: 2720 Train: 1.69976 Test: 2.40296
Epoch: 2800 Train: 3.64736 Test: 3.86400
Epoch: 2880 Train: 4.03107 Test: 3.39028
Epoch: 2960 Train: 0.08084 Test: 0.24993
Epoch: 3040 Train: 0.82918 Test: 1.10620
Epoch: 3120 Train: 0.23553 Test: 0.41802
Epoch: 3200 Train: 1.58942 Test: 2.00691
Epoch: 3280 Train: 6.13490 Test: 5.13160
Epoch 3360: New minimal relative error: 7.67%, model saved.
Epoch: 3360 Train: 0.21612 Test: 0.42609
Epoch: 3440 Train: 1.18187 Test: 1.95279
Epoch: 3520 Train: 1.45967 Test: 1.97884
Epoch: 3600 Train: 2.45842 Test: 3.11978
Epoch: 3680 Train: 0.15793 Test: 0.28097
Epoch: 3760 Train: 0.11507 Test: 0.27263
Epoch: 3840 Train: 0.10670 Test: 0.28975
Epoch: 3920 Train: 0.08442 Test: 0.24379
Epoch 4000: New minimal relative error: 4.95%, model saved.
Epoch: 4000 Train: 0.18097 Test: 0.46041
Epoch: 4080 Train: 0.18441 Test: 0.41651
Epoch: 4160 Train: 0.48779 Test: 0.94304
Epoch 4240: New minimal relative error: 4.22%, model saved.
Epoch: 4240 Train: 0.08677 Test: 0.28426
Epoch: 4320 Train: 0.07998 Test: 0.34049
Epoch: 4400 Train: 1.25935 Test: 1.72802
Epoch: 4480 Train: 1.28247 Test: 2.06899
Epoch: 4560 Train: 1.49784 Test: 1.57358
Epoch: 4640 Train: 2.07901 Test: 1.81607
Epoch: 4720 Train: 0.07410 Test: 0.19605
Epoch: 4800 Train: 0.04269 Test: 0.20277
Epoch: 4880 Train: 0.09288 Test: 0.30396
Epoch 4960: New minimal relative error: 3.96%, model saved.
Epoch: 4960 Train: 0.04587 Test: 0.20154
Epoch: 5040 Train: 0.04535 Test: 0.18695
Epoch: 5120 Train: 0.02935 Test: 0.18455
Epoch: 5200 Train: 0.21436 Test: 0.37003
Epoch: 5280 Train: 0.03604 Test: 0.19675
Epoch: 5360 Train: 0.04182 Test: 0.18947
Epoch: 5440 Train: 0.31047 Test: 0.36760
Epoch: 5520 Train: 0.31597 Test: 0.50902
Epoch: 5600 Train: 1.22713 Test: 1.66312
Epoch: 5680 Train: 0.94683 Test: 0.93023
Epoch: 5760 Train: 0.07831 Test: 0.22605
Epoch: 5840 Train: 0.06102 Test: 0.19944
Epoch: 5920 Train: 1.13054 Test: 1.50803
Epoch: 6000 Train: 0.06247 Test: 0.19563
Epoch: 6080 Train: 0.06883 Test: 0.19387
Epoch: 6160 Train: 0.06574 Test: 0.23650
Epoch: 6240 Train: 0.54755 Test: 0.71750
Epoch: 6320 Train: 0.43384 Test: 0.67162
Epoch: 6400 Train: 0.41985 Test: 0.74121
Epoch: 6480 Train: 0.18451 Test: 0.36864
Epoch: 6560 Train: 0.72090 Test: 0.98922
Epoch 6640: New minimal relative error: 1.37%, model saved.
Epoch: 6640 Train: 0.06781 Test: 0.19924
Epoch: 6720 Train: 0.04796 Test: 0.19662
Epoch: 6800 Train: 0.03216 Test: 0.17134
Epoch: 6880 Train: 0.03093 Test: 0.16718
Epoch: 6960 Train: 0.10064 Test: 0.27710
Epoch: 7040 Train: 0.04613 Test: 0.17601
Epoch: 7120 Train: 0.50038 Test: 0.70449
Epoch: 7200 Train: 0.24106 Test: 0.43269
Epoch: 7280 Train: 0.15492 Test: 0.34448
Epoch: 7360 Train: 0.42005 Test: 0.54358
Epoch: 7440 Train: 0.02043 Test: 0.14858
Epoch: 7520 Train: 0.02113 Test: 0.15401
Epoch: 7600 Train: 0.47148 Test: 0.68372
Epoch: 7680 Train: 0.04267 Test: 0.18295
Epoch: 7760 Train: 0.01791 Test: 0.14435
Epoch: 7840 Train: 0.03497 Test: 0.15751
Epoch: 7920 Train: 0.01865 Test: 0.14295
Epoch: 7999 Train: 0.02380 Test: 0.15418
Training Loss: tensor(0.0238)
Test Loss: tensor(0.1542)
Learned LE: [ 0.8988185   0.04126815 -4.3292084 ]
True LE: [ 8.67677689e-01  9.52084456e-03 -1.45613785e+01]
Relative Error: [18.836872  21.128498  24.199371  27.814144  31.8349    35.3078
 38.648396  41.64586   43.697624  45.638657  46.62216   47.57646
 47.77305   47.350235  47.099937  46.35107   45.470085  43.922108
 41.46146   38.14213   35.040863  32.394093  30.081596  28.289255
 26.412994  23.448715  20.984373  18.863085  16.983997  15.366946
 14.0348625 13.02873   12.719224  12.791495  12.87307   13.147823
 13.49864   13.956073  14.278597  14.809082  15.530348  16.304937
 16.792437  16.932669  16.838663  16.867     17.217346  17.258825
 17.210094  17.039133  16.85162   16.541128  16.37012   16.090878
 15.960733  16.116493  16.314123  16.400396  16.462189  16.063662
 15.728471  16.068653  17.29777   19.764912  22.924372  26.510975
 30.500814  33.53715   37.096176  39.667904  41.68783   43.717052
 44.962948  46.04306   46.031017  45.766922  45.35079   44.58822
 43.371994  41.563625  38.64161   35.034954  31.733994  28.801128
 26.782307  25.260015  23.285637  20.70457   18.661127  16.791235
 15.2013855 13.8161    12.558815  11.406508  11.177715  11.398259
 11.614008  12.062232  12.416739  13.048994  13.4917555 13.936586
 14.732693  15.674123  15.993115  16.147457  15.972166  15.963617
 16.140057  15.992898  15.754837  15.584475  15.37905   15.164685
 15.070372  15.002531  14.932245  14.981002  15.271134  15.504848
 15.451024  15.049347  14.585082  14.801905  16.039606  18.518654
 21.669308  25.208723  28.939602  31.893284  35.13934   37.627216
 39.701607  41.954502  43.240322  44.30554   44.34719   44.085514
 43.498672  42.79832   41.330177  39.197777  35.937412  32.49898
 28.727114  25.806253  23.755035  22.190104  20.386066  18.354952
 16.713373  15.021448  13.691682  12.416422  11.343314  10.159752
  9.805791  10.150281  10.599436  11.021993  11.678705  12.23016
 12.857121  13.252256  14.030611  14.958036  15.33533   15.42819
 15.0286255 15.02115   14.999503  14.790847  14.48484   14.239227
 13.99125   13.755883  13.669662  13.684411  13.792806  14.035648
 14.376715  14.524998  14.455234  14.117911  13.512507  13.738283
 14.958562  17.260365  20.247654  23.788977  27.160978  30.348068
 33.13876   35.58939   37.786026  40.076687  41.43355   42.697083
 42.58604   42.221375  41.87987   40.769352  39.18109   36.717026
 33.238514  29.70475   26.066261  23.171783  21.119436  19.24487
 17.802946  16.497078  15.118883  13.57175   12.440638  11.287236
 10.214961   9.122614   8.625857   9.01845    9.649346  10.267432
 11.037301  11.548671  12.129216  12.696503  13.365801  14.2632885
 14.810978  14.614275  14.163502  14.07413   13.988227  13.74342
 13.289988  12.869928  12.525314  12.356447  12.364249  12.433252
 12.530145  13.017452  13.4906025 13.53311   13.449433  13.277583
 12.663482  12.714609  13.837573  16.018003  18.863205  22.316002
 25.444815  28.5155    31.097832  33.5865    35.947803  38.087093
 39.63692   40.64336   40.568886  40.27646   39.79407   38.2979
 36.697956  33.88405   30.715767  27.128012  23.569305  20.391636
 18.175814  16.548147  15.577118  14.61764   13.52681   12.460243
 11.432064  10.301208   9.291546   8.241159   7.6137238  7.9833293
  8.669345   9.57197   10.375973  10.932999  11.431961  12.147197
 12.731394  13.700305  14.159086  14.030437  13.36654   13.204328
 13.077343  12.613465  12.140857  11.644453  11.134581  10.916062
 11.0907    11.26145   11.516958  11.895372  12.337325  12.573831
 12.522189  12.405501  11.993015  11.762475  12.654948  14.705632
 17.338644  20.773897  23.749279  26.549023  29.000956  31.631645
 34.0415    36.131405  37.518345  38.415154  38.64023   38.178204
 37.476017  35.95346   34.09889   31.034449  28.077225  24.567785
 20.76621   17.85217   15.635138  14.25563   13.619796  12.957671
 12.069149  11.142467  10.4844675  9.604879   8.516702   7.5657687
  6.765865   7.0053687  7.7429137  8.82879    9.688407  10.4121065
 10.823289  11.624399  12.230112  12.962749  13.344535  13.180131
 12.494769  12.25486   11.981185  11.54963   11.077178  10.467677
  9.835017   9.532034   9.674842  10.003522  10.438341  10.855804
 11.039158  11.45947   11.62587   11.529132  11.357026  10.932994
 11.503733  13.302933  15.836549  19.013187  21.946253  24.508791
 26.808336  29.580841  32.00806   33.94232   35.325287  36.215633
 36.575356  36.020313  35.015446  33.42898   31.51846   28.688225
 25.60928   21.969976  18.49083   15.601085  13.431846  12.023549
 11.500138  11.09432   10.418456   9.820961 ]
