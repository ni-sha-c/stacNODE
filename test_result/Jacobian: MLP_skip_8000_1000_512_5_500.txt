time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.10%, model saved.
Epoch: 0 Train: 32583.21289 Test: 4294.69092
Epoch 80: New minimal relative error: 105.05%, model saved.
Epoch: 80 Train: 9019.15332 Test: 1550.74402
Epoch: 160 Train: 7755.79395 Test: 2015.73450
Epoch: 240 Train: 6646.84082 Test: 1580.08679
Epoch: 320 Train: 6397.91162 Test: 981.47412
Epoch 400: New minimal relative error: 41.58%, model saved.
Epoch: 400 Train: 5216.33984 Test: 902.09253
Epoch: 480 Train: 4302.89062 Test: 410.02176
Epoch: 560 Train: 2608.72388 Test: 200.57620
Epoch: 640 Train: 1370.16968 Test: 273.85101
Epoch: 720 Train: 777.41803 Test: 26.76034
Epoch: 800 Train: 583.13910 Test: 98.86076
Epoch 880: New minimal relative error: 29.21%, model saved.
Epoch: 880 Train: 380.37607 Test: 11.94244
Epoch 960: New minimal relative error: 19.87%, model saved.
Epoch: 960 Train: 305.98431 Test: 6.74228
Epoch 1040: New minimal relative error: 14.83%, model saved.
Epoch: 1040 Train: 348.80139 Test: 5.31786
Epoch: 1120 Train: 265.61740 Test: 21.96464
Epoch: 1200 Train: 198.32970 Test: 19.58811
Epoch 1280: New minimal relative error: 6.69%, model saved.
Epoch: 1280 Train: 178.36456 Test: 1.95301
Epoch: 1360 Train: 190.06984 Test: 23.55925
Epoch: 1440 Train: 161.65274 Test: 1.81884
Epoch: 1520 Train: 144.72308 Test: 1.37696
Epoch: 1600 Train: 148.44093 Test: 9.47115
Epoch: 1680 Train: 123.49627 Test: 1.19169
Epoch: 1760 Train: 113.91228 Test: 1.05624
Epoch: 1840 Train: 118.55127 Test: 1.13229
Epoch: 1920 Train: 109.89004 Test: 5.40981
Epoch: 2000 Train: 99.85252 Test: 1.61920
Epoch: 2080 Train: 119.17995 Test: 26.38877
Epoch: 2160 Train: 97.49993 Test: 0.69368
Epoch: 2240 Train: 147.70825 Test: 2.04818
Epoch: 2320 Train: 85.35890 Test: 0.53570
Epoch: 2400 Train: 103.47520 Test: 2.73673
Epoch: 2480 Train: 84.41051 Test: 0.99273
Epoch 2560: New minimal relative error: 5.43%, model saved.
Epoch: 2560 Train: 74.46673 Test: 0.64957
Epoch 2640: New minimal relative error: 2.98%, model saved.
Epoch: 2640 Train: 67.24069 Test: 0.33573
Epoch: 2720 Train: 69.24084 Test: 0.31890
Epoch: 2800 Train: 65.61335 Test: 1.76464
Epoch: 2880 Train: 61.74940 Test: 0.40275
Epoch: 2960 Train: 61.11784 Test: 0.59024
Epoch: 3040 Train: 61.52993 Test: 0.31083
Epoch: 3120 Train: 69.52261 Test: 0.73294
Epoch: 3200 Train: 65.70982 Test: 0.38676
Epoch: 3280 Train: 57.74395 Test: 0.32707
Epoch: 3360 Train: 59.67700 Test: 2.35678
Epoch: 3440 Train: 60.29343 Test: 2.38337
Epoch: 3520 Train: 60.24839 Test: 3.43774
Epoch: 3600 Train: 65.67648 Test: 12.19242
Epoch: 3680 Train: 49.23506 Test: 0.25660
Epoch: 3760 Train: 56.98893 Test: 1.40567
Epoch: 3840 Train: 58.73695 Test: 1.82824
Epoch: 3920 Train: 46.86573 Test: 0.27100
Epoch: 4000 Train: 63.24883 Test: 12.16729
Epoch: 4080 Train: 47.47057 Test: 0.22006
Epoch: 4160 Train: 50.05292 Test: 2.03128
Epoch: 4240 Train: 48.26900 Test: 1.05786
Epoch: 4320 Train: 55.74630 Test: 1.47730
Epoch: 4400 Train: 45.26371 Test: 0.17342
Epoch: 4480 Train: 41.89776 Test: 0.14565
Epoch: 4560 Train: 41.54930 Test: 0.39384
Epoch: 4640 Train: 39.65296 Test: 0.28377
Epoch: 4720 Train: 40.34632 Test: 0.31421
Epoch 4800: New minimal relative error: 2.66%, model saved.
Epoch: 4800 Train: 39.13013 Test: 0.16946
Epoch: 4880 Train: 40.38911 Test: 1.00994
Epoch: 4960 Train: 38.95909 Test: 0.27858
Epoch: 5040 Train: 38.92879 Test: 0.19366
Epoch: 5120 Train: 40.88028 Test: 0.47676
Epoch: 5200 Train: 41.06111 Test: 0.51076
Epoch: 5280 Train: 41.69492 Test: 0.64740
Epoch 5360: New minimal relative error: 2.46%, model saved.
Epoch: 5360 Train: 39.77769 Test: 0.21727
Epoch: 5440 Train: 39.46534 Test: 0.22996
Epoch: 5520 Train: 37.07084 Test: 0.13811
Epoch: 5600 Train: 35.74270 Test: 0.14706
Epoch: 5680 Train: 36.39827 Test: 0.17894
Epoch: 5760 Train: 37.46018 Test: 0.70998
Epoch: 5840 Train: 36.43479 Test: 0.40097
Epoch: 5920 Train: 35.98726 Test: 0.17410
Epoch: 6000 Train: 36.67096 Test: 0.24740
Epoch: 6080 Train: 36.30072 Test: 0.28733
Epoch: 6160 Train: 37.27229 Test: 0.28399
Epoch: 6240 Train: 35.47915 Test: 0.16623
Epoch: 6320 Train: 35.17958 Test: 0.16417
Epoch: 6400 Train: 34.64286 Test: 0.12392
Epoch: 6480 Train: 35.30651 Test: 0.21985
Epoch: 6560 Train: 34.38341 Test: 0.16636
Epoch: 6640 Train: 32.53892 Test: 0.24362
Epoch: 6720 Train: 37.36604 Test: 0.64062
Epoch: 6800 Train: 33.98610 Test: 0.13330
Epoch: 6880 Train: 32.82304 Test: 0.13627
Epoch 6960: New minimal relative error: 2.41%, model saved.
Epoch: 6960 Train: 33.45998 Test: 0.18583
Epoch: 7040 Train: 32.04816 Test: 0.23602
Epoch: 7120 Train: 29.14412 Test: 0.09471
Epoch: 7200 Train: 28.47750 Test: 0.18272
Epoch 7280: New minimal relative error: 1.33%, model saved.
Epoch: 7280 Train: 27.48658 Test: 0.07615
Epoch: 7360 Train: 26.82135 Test: 0.28556
Epoch: 7440 Train: 26.03451 Test: 0.16860
Epoch: 7520 Train: 25.46446 Test: 0.08114
Epoch: 7600 Train: 26.84429 Test: 0.11650
Epoch: 7680 Train: 26.19912 Test: 0.09243
Epoch: 7760 Train: 26.40859 Test: 0.26912
Epoch: 7840 Train: 26.46372 Test: 0.07593
Epoch: 7920 Train: 26.82398 Test: 0.09058
Epoch: 7999 Train: 26.26023 Test: 0.08036
Training Loss: tensor(26.2602)
Test Loss: tensor(0.0804)
Learned LE: [  0.82474613   0.0346071  -14.518687  ]
True LE: [ 8.6601216e-01 -6.5043229e-03 -1.4541411e+01]
Relative Error: [ 6.219055   6.4445744  6.7068963  7.0031924  7.153157   7.3480844
  7.2763853  7.3753357  7.7507815  8.393684   9.039469   9.712715
 10.04371   10.020739   9.957445   9.703859   9.29291    8.731427
  8.031299   7.381378   6.786298   6.48902    6.5445995  6.514702
  6.575708   7.050167   7.5277233  7.9096065  8.301519   8.557189
  8.559636   8.6904     9.109839   9.545511  10.091395  10.648062
 10.803748  10.802565  10.672949  10.46578    9.751669   9.062867
  8.847776   8.308      7.869208   7.9546585  7.9316964  7.8333693
  7.566884   7.116972   6.535045   6.1020026  5.7021384  5.180727
  4.82243    4.794832   4.8147874  4.8329244  4.8774347  4.8936477
  4.925928   5.1645846  5.5106463  5.824459   5.981586   6.1624928
  6.2469482  6.3192053  6.1545167  6.1275697  6.535975   7.2708077
  8.043816   8.882342   9.115178   9.223177   9.199056   9.009451
  8.647906   8.10324    7.370399   6.7205415  6.137082   5.9500036
  6.0461836  5.966322   6.0472856  6.4541063  6.9438276  7.336566
  7.632772   8.030843   8.05525    8.073297   8.410983   8.795755
  9.30507    9.865814  10.122534  10.175234  10.071939   9.819213
  9.1533     8.512634   8.145157   7.7440224  7.209043   7.1353493
  7.229596   7.1632557  6.9191947  6.4374504  5.9658904  5.5460095
  5.113205   4.542445   4.1637597  4.1249123  4.1340413  4.1199164
  4.102492   4.2719975  4.475528   4.73299    5.026699   5.300281
  5.4243965  5.5983534  5.588056   5.482377   5.1527634  5.121263
  5.4657407  6.1617017  7.04087    7.863906   8.166639   8.412659
  8.453882   8.339494   7.971099   7.408967   6.6862373  6.0852933
  5.449527   5.270955   5.3585844  5.3834114  5.531061   5.822013
  6.2936945  6.6669607  6.939      7.270813   7.54498    7.4551845
  7.639334   7.9757347  8.448577   9.038209   9.383262   9.492017
  9.468843   9.207568   8.553704   7.999371   7.4676065  7.2140656
  6.6624775  6.364188   6.503977   6.4884872  6.2283173  5.8158817
  5.457385   5.0455003  4.5827127  3.9805202  3.5672438  3.4530222
  3.3554342  3.3138049  3.4516282  3.772668   4.0123663  4.3039527
  4.574674   4.764619   4.9171233  4.982198   5.055936   4.987805
  4.57304    4.17038    4.437204   5.152298   5.9996357  6.8592706
  7.285846   7.557787   7.706242   7.59413    7.291232   6.7441573
  6.053288   5.532126   4.888971   4.6454573  4.678068   4.664533
  4.8849535  5.2363586  5.5943856  5.9169426  6.207832   6.511815
  6.934982   6.848404   6.8043284  7.0797997  7.515164   8.117978
  8.545896   8.737008   8.808043   8.629066   7.9069524  7.465821
  6.826055   6.649078   6.173511   5.6393204  5.782573   5.748714
  5.555995   5.3289566  5.012524   4.609659   4.10623    3.483941
  2.965229   2.821845   2.6093194  2.5267944  2.7886405  3.1088321
  3.3685236  3.6899939  4.076137   4.338352   4.431146   4.4501233
  4.5388455  4.415317   4.1829677  3.599741   3.414962   4.003008
  4.9627995  5.728389   6.3344     6.7507358  6.945239   6.865875
  6.5784907  6.14045    5.672615   5.2108636  4.5501666  4.153238
  4.2154217  4.142978   4.153325   4.554954   4.977222   5.17707
  5.3854566  5.7277775  6.013188   6.243701   6.0331016  6.177048
  6.515044   7.0648127  7.621654   7.894696   8.067984   7.978718
  7.3633456  6.907789   6.311862   6.070715   5.7278266  5.2829895
  5.0211854  5.116932   5.021196   4.9190116  4.6667047  4.192384
  3.6352065  3.0018244  2.3669107  2.1592045  1.871262   1.955435
  2.1371202  2.3356109  2.6375167  3.0173948  3.444789   3.829793
  4.0769014  4.121683   4.090064   4.0763297  3.8177722  3.2540054
  2.7031884  2.8337448  3.6488743  4.616113   5.2118654  5.7987075
  6.1544704  6.228693   5.950585   5.700328   5.3520174  4.86961
  4.2440853  3.629975   3.7317753  3.6889882  3.6332595  3.848526
  4.1863194  4.5307403  4.5898275  4.821885   5.1373143  5.369934
  5.356703   5.2160935  5.4427176  5.86528    6.5414295  6.9427357
  7.2121     7.221345   6.780187   6.450802   5.848974   5.448605
  5.266741   5.008049   4.3718624  4.49673    4.500328   4.5104594
  4.298509   3.8636713  3.335198   2.696087   1.9383547  1.5707211
  1.4089086  1.4379208  1.4968334  1.6167974  1.8761251  2.3523703
  2.7942822  3.205644   3.5565372  3.7926183  3.7134368  3.547654
  3.437739   3.0545197  2.491408   2.0602996  2.2723384  3.2196398
  4.0094786  4.625691   5.1854467  5.4669447  5.4825006  5.356189
  4.992554   4.554917   4.013325   3.3118434  3.1811838  3.190456
  3.064412   3.264912   3.4546535  3.6707063]
