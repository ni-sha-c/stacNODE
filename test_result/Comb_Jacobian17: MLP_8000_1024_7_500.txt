time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 7
reg_param: 500
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.00%, model saved.
Epoch: 0 Train: 30975.58008 Test: 3984.24121
Epoch 80: New minimal relative error: 75.80%, model saved.
Epoch: 80 Train: 5292.35303 Test: 1106.32703
Epoch: 160 Train: 1033.70190 Test: 394.32117
Epoch 240: New minimal relative error: 17.71%, model saved.
Epoch: 240 Train: 110.37702 Test: 1.89681
Epoch 320: New minimal relative error: 8.66%, model saved.
Epoch: 320 Train: 6.37093 Test: 0.68479
Epoch: 400 Train: 2.30826 Test: 0.00863
Epoch 480: New minimal relative error: 7.90%, model saved.
Epoch: 480 Train: 12.15204 Test: 0.75194
Epoch 560: New minimal relative error: 2.01%, model saved.
Epoch: 560 Train: 18.34806 Test: 2.18652
Epoch: 640 Train: 22.06952 Test: 2.75607
Epoch: 720 Train: 38.08505 Test: 11.99228
Epoch: 800 Train: 9.58491 Test: 8.66878
Epoch: 880 Train: 16.45673 Test: 2.59401
Epoch: 960 Train: 2.76519 Test: 0.43664
Epoch: 1040 Train: 19.11455 Test: 12.54793
Epoch: 1120 Train: 1.47823 Test: 0.56244
Epoch: 1200 Train: 9.11176 Test: 5.23142
Epoch: 1280 Train: 2.30510 Test: 1.89165
Epoch: 1360 Train: 1.33834 Test: 0.68757
Epoch: 1440 Train: 5.85186 Test: 1.41930
Epoch: 1520 Train: 8.05217 Test: 4.98042
Epoch: 1600 Train: 1.58102 Test: 0.74108
Epoch: 1680 Train: 3.19152 Test: 0.86553
Epoch: 1760 Train: 2.22873 Test: 0.20646
Epoch: 1840 Train: 6.18024 Test: 1.13043
Epoch: 1920 Train: 8.39584 Test: 3.00705
Epoch: 2000 Train: 7.27044 Test: 3.30429
Epoch: 2080 Train: 8.96364 Test: 3.28791
Epoch: 2160 Train: 13.06600 Test: 3.14037
Epoch: 2240 Train: 9.10402 Test: 3.08564
Epoch: 2320 Train: 8.21215 Test: 1.69283
Epoch: 2400 Train: 0.53398 Test: 0.09804
Epoch: 2480 Train: 4.69508 Test: 1.54480
Epoch 2560: New minimal relative error: 1.42%, model saved.
Epoch: 2560 Train: 1.00399 Test: 0.16030
Epoch: 2640 Train: 7.47419 Test: 3.23719
Epoch: 2720 Train: 8.39388 Test: 3.34471
Epoch: 2800 Train: 2.39997 Test: 0.73745
Epoch: 2880 Train: 6.12155 Test: 1.78459
Epoch: 2960 Train: 2.28082 Test: 0.72261
Epoch: 3040 Train: 6.32629 Test: 1.99128
Epoch: 3120 Train: 10.54467 Test: 1.30100
Epoch: 3200 Train: 2.54119 Test: 1.03533
Epoch: 3280 Train: 0.71017 Test: 0.16039
Epoch: 3360 Train: 6.11547 Test: 2.25016
Epoch: 3440 Train: 3.30501 Test: 0.72745
Epoch: 3520 Train: 0.99770 Test: 0.33792
Epoch: 3600 Train: 3.37087 Test: 0.91397
Epoch: 3680 Train: 2.60913 Test: 1.24325
Epoch: 3760 Train: 1.11793 Test: 0.41506
Epoch: 3840 Train: 1.01401 Test: 0.27625
Epoch: 3920 Train: 2.00600 Test: 0.26690
Epoch: 4000 Train: 5.81044 Test: 2.01631
Epoch: 4080 Train: 0.72729 Test: 0.08157
Epoch: 4160 Train: 0.90034 Test: 0.37501
Epoch 4240: New minimal relative error: 1.26%, model saved.
Epoch: 4240 Train: 1.40546 Test: 0.50592
Epoch: 4320 Train: 5.12171 Test: 1.99878
Epoch: 4400 Train: 0.27190 Test: 0.04272
Epoch: 4480 Train: 3.81094 Test: 1.56329
Epoch: 4560 Train: 1.37776 Test: 0.62348
Epoch: 4640 Train: 0.47593 Test: 0.16366
Epoch: 4720 Train: 5.55732 Test: 2.75222
Epoch: 4800 Train: 1.27146 Test: 0.54462
Epoch: 4880 Train: 0.84637 Test: 0.35214
Epoch: 4960 Train: 0.17344 Test: 0.02459
Epoch: 5040 Train: 0.71768 Test: 0.30268
Epoch: 5120 Train: 0.06138 Test: 0.01354
Epoch: 5200 Train: 0.14971 Test: 0.04448
Epoch: 5280 Train: 0.43376 Test: 0.13236
Epoch: 5360 Train: 1.55917 Test: 0.50861
Epoch: 5440 Train: 3.82951 Test: 0.87303
Epoch: 5520 Train: 7.18068 Test: 1.69441
Epoch: 5600 Train: 3.61852 Test: 1.52408
Epoch 5680: New minimal relative error: 0.31%, model saved.
Epoch: 5680 Train: 0.06616 Test: 0.01251
Epoch: 5760 Train: 0.09534 Test: 0.01771
Epoch: 5840 Train: 3.99592 Test: 1.18906
Epoch: 5920 Train: 1.30390 Test: 0.49775
Epoch: 6000 Train: 1.50753 Test: 0.53488
Epoch: 6080 Train: 3.12950 Test: 1.03588
Epoch: 6160 Train: 5.23457 Test: 1.95746
Epoch: 6240 Train: 0.11368 Test: 0.02455
Epoch: 6320 Train: 0.68620 Test: 0.19347
Epoch: 6400 Train: 0.35395 Test: 0.11072
Epoch: 6480 Train: 1.24994 Test: 0.23574
Epoch: 6560 Train: 0.53098 Test: 0.22074
Epoch: 6640 Train: 0.41262 Test: 0.09873
Epoch: 6720 Train: 0.92263 Test: 0.37871
Epoch: 6800 Train: 1.58598 Test: 0.45738
Epoch: 6880 Train: 3.65289 Test: 1.47798
Epoch: 6960 Train: 0.02532 Test: 0.00233
Epoch: 7040 Train: 0.08355 Test: 0.01801
Epoch: 7120 Train: 0.09188 Test: 0.01801
Epoch: 7200 Train: 0.16137 Test: 0.03360
Epoch: 7280 Train: 0.31096 Test: 0.07356
Epoch: 7360 Train: 1.11374 Test: 0.48396
Epoch: 7440 Train: 0.11152 Test: 0.03407
Epoch: 7520 Train: 0.19601 Test: 0.02462
Epoch: 7600 Train: 4.64286 Test: 1.59875
Epoch 7680: New minimal relative error: 0.24%, model saved.
Epoch: 7680 Train: 0.02033 Test: 0.00227
Epoch: 7760 Train: 0.10360 Test: 0.02017
Epoch: 7840 Train: 0.10938 Test: 0.03386
Epoch: 7920 Train: 0.17303 Test: 0.03533
Epoch: 7999 Train: 0.58000 Test: 0.28591
Training Loss: tensor(0.5800)
Test Loss: tensor(0.2859)
Learned LE: [ 8.6664659e-01  7.3968940e-03 -1.4552492e+01]
True LE: [ 8.6735159e-01  3.7380618e-03 -1.4545534e+01]
Relative Error: [0.09860031 0.10181241 0.10034415 0.09507185 0.08760256 0.07905358
 0.06987546 0.06066633 0.05193959 0.04374788 0.03558151 0.0272191
 0.01796464 0.00829448 0.01142552 0.02219742 0.03141503 0.03782344
 0.04044252 0.03793097 0.02999929 0.01833521 0.00717358 0.00312304
 0.00436375 0.00985663 0.0166871  0.02170062 0.02761894 0.03203343
 0.03182928 0.02930945 0.02403259 0.020638   0.02500784 0.02927315
 0.0285169  0.02299926 0.01767885 0.02057003 0.03008242 0.03922441
 0.04358285 0.04226947 0.03880168 0.03666729 0.03736591 0.04073418
 0.04525933 0.04926226 0.05238337 0.05573023 0.06027596 0.06523042
 0.0693266  0.07114673 0.0705319  0.06860031 0.06717049 0.06820135
 0.07247944 0.07915992 0.08575092 0.0895047  0.08904614 0.08490609
 0.07880366 0.07192819 0.06448223 0.05635444 0.04851205 0.0418247
 0.03575302 0.0298556  0.02334224 0.01562344 0.01115494 0.01499344
 0.02080845 0.02611622 0.03058031 0.03231611 0.02944577 0.02166772
 0.01134513 0.00584546 0.00743088 0.00823339 0.01602017 0.0225195
 0.02392002 0.02716447 0.02880522 0.02641677 0.02430839 0.02242526
 0.02438142 0.02780287 0.02789603 0.02424414 0.0185813  0.01400291
 0.0209107  0.03247806 0.03921707 0.03846446 0.03487829 0.03343837
 0.03512217 0.03849209 0.04225633 0.04569804 0.0484568  0.05143084
 0.05576156 0.06110808 0.06591365 0.06864087 0.06871046 0.06666085
 0.0640161  0.06274784 0.06436863 0.06901083 0.07484191 0.07915834
 0.07987647 0.07714894 0.07263709 0.0679985  0.06309446 0.05696383
 0.04968987 0.04305168 0.03795969 0.0337464  0.02970035 0.02420739
 0.01801334 0.01446739 0.0136314  0.01440634 0.01748906 0.0219363
 0.02489245 0.02373512 0.01762138 0.00898081 0.00856717 0.010982
 0.01114144 0.01960387 0.02496728 0.02197762 0.02241551 0.02246306
 0.01962171 0.01998817 0.02182501 0.02491394 0.02558432 0.02392618
 0.02219904 0.01849853 0.01529959 0.02407913 0.0339609  0.03610416
 0.03266261 0.03079509 0.0330072  0.03679277 0.04008595 0.04268163
 0.04497714 0.04747849 0.0514102  0.05667562 0.06194672 0.0656701
 0.06690498 0.06559044 0.06270799 0.06001868 0.05941821 0.06175254
 0.06625897 0.07064363 0.07263496 0.07143813 0.06826755 0.06535927
 0.06349176 0.06095527 0.05585837 0.0488825  0.04269426 0.03833077
 0.03534024 0.03184997 0.02642202 0.02083944 0.01654528 0.01303168
 0.01063124 0.01141194 0.01651486 0.022143   0.02413705 0.02001436
 0.01053163 0.00850905 0.01251581 0.01193923 0.01945864 0.02474335
 0.01880634 0.01539713 0.01522715 0.0138698  0.01695893 0.02028145
 0.02203579 0.02034023 0.0196487  0.02146952 0.01963075 0.01850614
 0.02617087 0.03234342 0.03129336 0.02857586 0.03007122 0.03441192
 0.03795801 0.040007   0.04155934 0.04364844 0.0468593  0.05179062
 0.05717146 0.06152424 0.06399301 0.06393913 0.06181621 0.05882661
 0.05684014 0.05728901 0.06022935 0.06418589 0.06711112 0.06742156
 0.06544219 0.06325834 0.06305236 0.0643786  0.06406909 0.0594488
 0.05190828 0.04493237 0.04032347 0.03740602 0.03341776 0.02767186
 0.02252384 0.0182447  0.01490869 0.0127166  0.01196321 0.01652802
 0.02440836 0.02946964 0.02736929 0.0168389  0.00606093 0.01096301
 0.01080834 0.01647716 0.02250005 0.01843393 0.00888659 0.00856892
 0.01041772 0.01456727 0.01768296 0.01855425 0.01514796 0.01577575
 0.02063169 0.02155031 0.02172295 0.02639994 0.02926869 0.02789752
 0.02741841 0.03083398 0.0347381  0.0368827  0.03774342 0.03904127
 0.04150195 0.04595038 0.05154733 0.05642561 0.05975538 0.06097568
 0.06009321 0.05770344 0.05524794 0.05442175 0.05613003 0.05968846
 0.06319487 0.06485578 0.06414425 0.06220132 0.06144729 0.06378296
 0.06799974 0.06994016 0.0657679  0.05669906 0.04758565 0.04147057
 0.03777101 0.03293562 0.02712338 0.02281005 0.01895088 0.0150878
 0.01356762 0.01325132 0.01696492 0.02578405 0.03321956 0.03413735
 0.02557031 0.0096994  0.00721034 0.00918483 0.01356475 0.0194993
 0.01915835 0.01132589 0.0057729  0.00971724 0.01458032 0.01565501
 0.01795349 0.01369301 0.01307821 0.02046935 0.02378083 0.02416201
 0.02661675 0.02835944 0.02786104 0.02877137 0.03155496 0.0330813
 0.0336063  0.03367329 0.03499357 0.03801841 0.04361608 0.04984043
 0.05440203 0.05687673 0.0571911  0.05581453 0.05333264 0.05133424
 0.05156093 0.05465438 0.05924776 0.06294733 0.06410505 0.06289484
 0.06104074 0.06103037 0.06469194 0.07101863 0.0752575  0.07237546
 0.06198301 0.04956442 0.04055892 0.03581056 0.03091358 0.02544823
 0.02296162 0.02103548 0.01662348 0.01257352 0.01122121 0.01274745
 0.02119563 0.03053436 0.03531773 0.03187307]
