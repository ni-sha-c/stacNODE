time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.15%, model saved.
Epoch: 0 Train: 4204.33301 Test: 4005.24072
Epoch: 80 Train: 107.92004 Test: 86.35529
Epoch 160: New minimal relative error: 51.20%, model saved.
Epoch: 160 Train: 32.23008 Test: 18.16862
Epoch 240: New minimal relative error: 12.11%, model saved.
Epoch: 240 Train: 13.29943 Test: 9.28982
Epoch: 320 Train: 8.62592 Test: 9.75156
Epoch: 400 Train: 3.88368 Test: 4.79503
Epoch: 480 Train: 5.06454 Test: 6.60950
Epoch: 560 Train: 5.52807 Test: 7.74714
Epoch: 640 Train: 4.53854 Test: 3.58747
Epoch: 720 Train: 10.09005 Test: 11.44779
Epoch: 800 Train: 5.53339 Test: 4.51210
Epoch: 880 Train: 4.44285 Test: 6.23076
Epoch: 960 Train: 3.71687 Test: 4.66685
Epoch: 1040 Train: 2.50196 Test: 2.91674
Epoch: 1120 Train: 1.49732 Test: 2.46809
Epoch: 1200 Train: 6.95676 Test: 7.25966
Epoch: 1280 Train: 2.56987 Test: 3.24544
Epoch 1360: New minimal relative error: 10.96%, model saved.
Epoch: 1360 Train: 1.60175 Test: 2.01476
Epoch: 1440 Train: 0.96933 Test: 1.55436
Epoch: 1520 Train: 1.38367 Test: 1.96025
Epoch: 1600 Train: 0.92484 Test: 1.56264
Epoch 1680: New minimal relative error: 9.26%, model saved.
Epoch: 1680 Train: 2.21327 Test: 2.45610
Epoch: 1760 Train: 1.22568 Test: 1.54883
Epoch: 1840 Train: 1.48502 Test: 1.62600
Epoch: 1920 Train: 0.90201 Test: 1.42511
Epoch: 2000 Train: 2.21309 Test: 2.94991
Epoch: 2080 Train: 1.73889 Test: 2.79156
Epoch: 2160 Train: 0.64660 Test: 1.23494
Epoch: 2240 Train: 0.43368 Test: 0.87108
Epoch: 2320 Train: 0.49776 Test: 1.01492
Epoch: 2400 Train: 1.64266 Test: 2.27273
Epoch: 2480 Train: 0.41756 Test: 0.80868
Epoch: 2560 Train: 0.36107 Test: 0.79428
Epoch: 2640 Train: 0.41240 Test: 0.76694
Epoch: 2720 Train: 0.57601 Test: 1.02384
Epoch: 2800 Train: 0.59814 Test: 0.98527
Epoch: 2880 Train: 5.11412 Test: 4.52906
Epoch: 2960 Train: 0.24622 Test: 0.64009
Epoch: 3040 Train: 0.36803 Test: 0.76441
Epoch: 3120 Train: 3.56818 Test: 3.69854
Epoch: 3200 Train: 0.21660 Test: 0.59577
Epoch: 3280 Train: 0.44174 Test: 1.07839
Epoch: 3360 Train: 0.31795 Test: 0.76032
Epoch: 3440 Train: 0.31297 Test: 0.65486
Epoch: 3520 Train: 0.39964 Test: 0.87930
Epoch: 3600 Train: 0.20530 Test: 0.53435
Epoch: 3680 Train: 0.47374 Test: 0.84080
Epoch: 3760 Train: 0.17059 Test: 0.52244
Epoch: 3840 Train: 0.18357 Test: 0.55779
Epoch: 3920 Train: 0.19739 Test: 0.52566
Epoch: 4000 Train: 0.18239 Test: 0.52672
Epoch: 4080 Train: 1.07486 Test: 1.51590
Epoch: 4160 Train: 0.47179 Test: 0.88268
Epoch: 4240 Train: 0.19461 Test: 0.61750
Epoch: 4320 Train: 1.51148 Test: 1.83004
Epoch: 4400 Train: 0.51494 Test: 0.83359
Epoch: 4480 Train: 1.07012 Test: 1.28974
Epoch: 4560 Train: 0.31429 Test: 0.75089
Epoch: 4640 Train: 0.12752 Test: 0.42285
Epoch: 4720 Train: 0.12224 Test: 0.42952
Epoch: 4800 Train: 0.56904 Test: 0.81466
Epoch: 4880 Train: 0.48804 Test: 0.73442
Epoch: 4960 Train: 0.11947 Test: 0.42614
Epoch: 5040 Train: 0.11870 Test: 0.40460
Epoch: 5120 Train: 0.11831 Test: 0.39366
Epoch 5200: New minimal relative error: 8.70%, model saved.
Epoch: 5200 Train: 0.10790 Test: 0.39893
Epoch: 5280 Train: 0.51385 Test: 0.80710
Epoch: 5360 Train: 0.10669 Test: 0.37781
Epoch: 5440 Train: 0.10395 Test: 0.38461
Epoch: 5520 Train: 0.09732 Test: 0.37325
Epoch: 5600 Train: 0.09518 Test: 0.36622
Epoch: 5680 Train: 0.10621 Test: 0.38472
Epoch: 5760 Train: 1.03252 Test: 1.20395
Epoch 5840: New minimal relative error: 8.49%, model saved.
Epoch: 5840 Train: 0.09040 Test: 0.35750
Epoch: 5920 Train: 0.09382 Test: 0.35935
Epoch: 6000 Train: 0.08808 Test: 0.34826
Epoch 6080: New minimal relative error: 7.66%, model saved.
Epoch: 6080 Train: 0.30214 Test: 0.59668
Epoch: 6160 Train: 0.10765 Test: 0.38863
Epoch: 6240 Train: 0.13828 Test: 0.37524
Epoch: 6320 Train: 0.24961 Test: 0.56344
Epoch: 6400 Train: 0.22834 Test: 0.53912
Epoch: 6480 Train: 0.08603 Test: 0.32975
Epoch: 6560 Train: 0.42749 Test: 0.72187
Epoch: 6640 Train: 0.08430 Test: 0.34256
Epoch: 6720 Train: 0.07641 Test: 0.32706
Epoch: 6800 Train: 0.08215 Test: 0.36123
Epoch: 6880 Train: 0.07398 Test: 0.31592
Epoch: 6960 Train: 0.07935 Test: 0.32046
Epoch: 7040 Train: 0.71044 Test: 1.03584
Epoch: 7120 Train: 0.07110 Test: 0.31165
Epoch: 7200 Train: 0.06983 Test: 0.30456
Epoch: 7280 Train: 0.07700 Test: 0.30530
Epoch: 7360 Train: 0.06802 Test: 0.30113
Epoch: 7440 Train: 0.13667 Test: 0.38307
Epoch: 7520 Train: 0.07307 Test: 0.30970
Epoch: 7600 Train: 0.06590 Test: 0.29452
Epoch: 7680 Train: 0.07104 Test: 0.29415
Epoch: 7760 Train: 0.06390 Test: 0.28944
Epoch: 7840 Train: 0.07961 Test: 0.32241
Epoch: 7920 Train: 0.06252 Test: 0.28511
Epoch: 7999 Train: 0.06172 Test: 0.28419
Training Loss: tensor(0.0617)
Test Loss: tensor(0.2842)
Learned LE: [ 0.8497356  0.0307535 -3.9605255]
True LE: [ 8.6945093e-01  6.6825990e-03 -1.4554169e+01]
Relative Error: [ 4.7245884  4.478351   4.482348   4.91368    5.513995   6.262084
  7.4243703  8.694403  10.037695  11.350151  12.455744  13.438405
 14.314781  15.061349  15.730394  16.368847  16.873001  17.269133
 17.599642  17.855873  18.025131  18.133108  18.191156  18.276619
 18.386997  18.496162  18.373098  18.219906  18.127022  17.847252
 17.241756  16.510382  15.806844  15.0679245 14.276759  13.453924
 12.520411  11.429111  10.246793   9.1195135  8.037872   7.0060277
  6.067861   5.441568   5.025556   4.813386   4.809066   4.9551754
  5.1240416  5.3199267  5.53943    5.7594895  5.9398685  6.082631
  6.1678047  6.173721   6.145077   6.071085   5.8772926  5.572167
  5.1481194  4.784622   4.456818   4.0934787  4.09304    4.507034
  5.0702505  5.958831   7.140255   8.483823   9.912726  11.179285
 12.245407  13.181564  14.013712  14.791977  15.436954  15.99503
 16.444786  16.842846  17.142612  17.368746  17.518864  17.57706
 17.622158  17.650263  17.713638  17.749348  17.540827  17.364695
 17.254463  16.861214  16.168022  15.401901  14.659649  13.943116
 13.184272  12.397996  11.379905  10.246709   9.159757   8.104185
  7.076101   6.104089   5.3156533  4.8526063  4.594364   4.558812
  4.706706   4.8894124  5.1065216  5.3576956  5.6265836  5.8940964
  6.128993   6.3004217  6.387785   6.3766522  6.3226995  6.0704823
  5.770289   5.4440393  4.9764833  4.6468005  4.231805   3.7724805
  3.761734   4.116198   4.729001   5.758702   6.9509864  8.364466
  9.813815  11.000871  11.981004  12.931983  13.77345   14.48365
 15.080107  15.60262   16.04405   16.38654   16.649788  16.865273
 17.000366  17.05188   17.04138   17.013445  17.03014   16.966326
 16.715721  16.509903  16.386246  15.857013  15.0917425 14.329884
 13.585184  12.866937  12.160141  11.334907  10.231849   9.189932
  8.173579   7.1620593  6.1765766  5.312489   4.7178     4.4080367
  4.32729    4.4600635  4.6543255  4.8832717  5.162468   5.472033
  5.791525   6.102408   6.3732214  6.5884886  6.6748233  6.6318026
  6.400591   6.0672674  5.702439   5.3588486  4.912052   4.5605364
  4.055439   3.5362117  3.4834936  3.7274885  4.4570274  5.59876
  6.857182   8.304184   9.655397  10.781326  11.764286  12.687044
 13.509776  14.156067  14.786947  15.2956705 15.679632  15.951453
 16.13372   16.284025  16.467875  16.511225  16.444597  16.36227
 16.335325  16.172253  15.894402  15.655611  15.4716    14.840616
 14.038528  13.283999  12.55368   11.8598995 11.189183  10.257208
  9.194159   8.23305    7.264591   6.2758803  5.379655   4.6502275
  4.263228   4.1071396  4.214032   4.4270678  4.6620703  4.9507966
  5.2890882  5.6589     6.0297766  6.366094   6.6215625  6.8227067
  6.901321   6.681204   6.416565   6.0571237  5.7307405  5.3177266
  4.916158   4.4929733  3.92722    3.4083817  3.2376852  3.4169748
  4.2540326  5.479127   6.87898    8.258736   9.475223  10.570115
 11.503017  12.453361  13.25334   13.853304  14.41289   14.86179
 15.207388  15.468879  15.675463  15.7822695 15.85186   15.918779
 15.835637  15.708588  15.6369705 15.370059  15.06772   14.799192
 14.498719  13.829642  13.004734  12.23564   11.581087  10.928363
 10.22616    9.176465   8.262911   7.3566623  6.3953214  5.490505
  4.7098303  4.099949   3.9082675  3.9807682  4.21208    4.451022
  4.7597103  5.1199923  5.506056   5.9133043  6.2655687  6.593421
  6.8839006  7.097423   7.009717   6.784889   6.479457   6.060631
  5.737793   5.3505235  4.972176   4.473426   3.8497725  3.3276536
  2.9817216  3.1552975  4.09262    5.4128056  6.9185815  8.162306
  9.295917  10.2925205 11.327156  12.233805  12.839554  13.429297
 13.939791  14.343959  14.660891  14.916251  15.149431  15.288689
 15.295021  15.293052  15.229416  15.062502  14.948214  14.58815
 14.251821  13.960549  13.524835  12.803485  12.003861  11.250602
 10.668782  10.052344   9.21949    8.247418   7.4238806  6.5195575
  5.627237   4.781885   4.077958   3.734006   3.7639792  3.996088
  4.2392564  4.552415   4.9371343  5.3694057  5.796335   6.142192
  6.5114646  6.8830857  7.2040043  7.2471676  7.159176   6.938412
  6.593147   6.133613   5.783565   5.4198995  5.0527267  4.502335
  3.8390923  3.2769659  2.74941    2.9144335  3.9636784  5.4187837
  6.8706145  8.055196   9.035532  10.094904  11.092195  11.811092
 12.428353  13.01535   13.477633  13.829407  14.093275  14.339572
 14.565262  14.738347  14.7601385 14.674923  14.573746  14.396906
 14.2333555 13.835714  13.493329  13.170261 ]
