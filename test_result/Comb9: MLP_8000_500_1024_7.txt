time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.05%, model saved.
Epoch: 0 Train: 3645.89136 Test: 4107.62305
Epoch 80: New minimal relative error: 55.10%, model saved.
Epoch: 80 Train: 27.79832 Test: 46.92219
Epoch 160: New minimal relative error: 38.44%, model saved.
Epoch: 160 Train: 3.96621 Test: 6.84848
Epoch 240: New minimal relative error: 26.16%, model saved.
Epoch: 240 Train: 6.64661 Test: 12.88773
Epoch: 320 Train: 2.51960 Test: 1.21739
Epoch 400: New minimal relative error: 18.17%, model saved.
Epoch: 400 Train: 12.23160 Test: 6.44241
Epoch: 480 Train: 17.03316 Test: 17.25822
Epoch: 560 Train: 0.64368 Test: 1.89168
Epoch: 640 Train: 0.46318 Test: 1.47801
Epoch: 720 Train: 0.50245 Test: 1.00574
Epoch: 800 Train: 1.16455 Test: 2.14528
Epoch: 880 Train: 4.61148 Test: 6.69995
Epoch: 960 Train: 5.29841 Test: 4.84435
Epoch: 1040 Train: 1.36990 Test: 2.66239
Epoch: 1120 Train: 6.78627 Test: 5.84489
Epoch: 1200 Train: 1.86283 Test: 2.62852
Epoch: 1280 Train: 0.24903 Test: 0.41659
Epoch: 1360 Train: 2.16820 Test: 1.97910
Epoch: 1440 Train: 2.20068 Test: 2.96498
Epoch: 1520 Train: 0.99344 Test: 1.29173
Epoch: 1600 Train: 0.94337 Test: 1.47910
Epoch: 1680 Train: 0.74192 Test: 1.38915
Epoch: 1760 Train: 6.12416 Test: 3.54697
Epoch: 1840 Train: 0.05187 Test: 0.28862
Epoch: 1920 Train: 0.08357 Test: 0.36792
Epoch: 2000 Train: 2.21091 Test: 2.42597
Epoch 2080: New minimal relative error: 15.93%, model saved.
Epoch: 2080 Train: 1.57366 Test: 2.17580
Epoch: 2160 Train: 1.69154 Test: 1.63097
Epoch: 2240 Train: 0.88896 Test: 1.14796
Epoch: 2320 Train: 0.34124 Test: 0.66201
Epoch: 2400 Train: 0.16357 Test: 0.49234
Epoch: 2480 Train: 0.55192 Test: 0.61104
Epoch: 2560 Train: 0.43437 Test: 0.52375
Epoch: 2640 Train: 0.36140 Test: 0.72063
Epoch: 2720 Train: 0.14939 Test: 0.49721
Epoch 2800: New minimal relative error: 12.73%, model saved.
Epoch: 2800 Train: 0.23457 Test: 0.62488
Epoch: 2880 Train: 0.06816 Test: 0.38873
Epoch: 2960 Train: 0.38466 Test: 0.83557
Epoch: 3040 Train: 0.40560 Test: 0.84392
Epoch: 3120 Train: 0.31078 Test: 0.62571
Epoch: 3200 Train: 1.21223 Test: 1.30008
Epoch: 3280 Train: 0.41096 Test: 0.78895
Epoch: 3360 Train: 0.63688 Test: 1.05461
Epoch: 3440 Train: 0.42045 Test: 0.85656
Epoch: 3520 Train: 0.17694 Test: 0.52631
Epoch: 3600 Train: 0.20457 Test: 0.51576
Epoch: 3680 Train: 1.03047 Test: 1.48027
Epoch: 3760 Train: 0.86548 Test: 1.12804
Epoch: 3840 Train: 0.45447 Test: 0.85165
Epoch: 3920 Train: 0.09747 Test: 0.47395
Epoch: 4000 Train: 1.61271 Test: 2.17168
Epoch: 4080 Train: 0.98009 Test: 1.59634
Epoch: 4160 Train: 0.07548 Test: 0.42454
Epoch: 4240 Train: 1.61246 Test: 2.42011
Epoch: 4320 Train: 0.12030 Test: 0.49907
Epoch: 4400 Train: 0.09376 Test: 0.45317
Epoch: 4480 Train: 0.65130 Test: 0.95651
Epoch: 4560 Train: 0.01616 Test: 0.34487
Epoch: 4640 Train: 0.08320 Test: 0.56858
Epoch: 4720 Train: 0.04524 Test: 0.37695
Epoch: 4800 Train: 0.19476 Test: 0.57423
Epoch: 4880 Train: 0.45826 Test: 0.84459
Epoch: 4960 Train: 0.01687 Test: 0.36105
Epoch: 5040 Train: 0.40536 Test: 0.86316
Epoch: 5120 Train: 0.94375 Test: 1.12120
Epoch: 5200 Train: 0.45060 Test: 0.88047
Epoch: 5280 Train: 0.03492 Test: 0.37644
Epoch: 5360 Train: 0.04519 Test: 0.39958
Epoch: 5440 Train: 0.02533 Test: 0.37564
Epoch: 5520 Train: 0.02943 Test: 0.37907
Epoch: 5600 Train: 0.13010 Test: 0.53116
Epoch: 5680 Train: 0.26364 Test: 0.64913
Epoch: 5760 Train: 0.02128 Test: 0.36404
Epoch: 5840 Train: 0.16860 Test: 0.44997
Epoch: 5920 Train: 0.05743 Test: 0.42447
Epoch: 6000 Train: 0.05835 Test: 0.42690
Epoch: 6080 Train: 0.07022 Test: 0.44362
Epoch: 6160 Train: 0.02147 Test: 0.37706
Epoch: 6240 Train: 0.02693 Test: 0.38043
Epoch: 6320 Train: 0.00882 Test: 0.37817
Epoch: 6400 Train: 0.02009 Test: 0.37866
Epoch: 6480 Train: 0.56456 Test: 1.01012
Epoch: 6560 Train: 0.02901 Test: 0.37635
Epoch: 6640 Train: 0.02057 Test: 0.37545
Epoch: 6720 Train: 0.02356 Test: 0.37767
Epoch: 6800 Train: 0.01148 Test: 0.37112
Epoch: 6880 Train: 0.00949 Test: 0.36207
Epoch: 6960 Train: 0.01750 Test: 0.37627
Epoch: 7040 Train: 0.54947 Test: 0.85683
Epoch: 7120 Train: 0.00995 Test: 0.36429
Epoch: 7200 Train: 0.00478 Test: 0.35322
Epoch: 7280 Train: 0.01945 Test: 0.37482
Epoch: 7360 Train: 0.00745 Test: 0.35726
Epoch: 7440 Train: 0.17085 Test: 0.55399
Epoch: 7520 Train: 0.04564 Test: 0.42843
Epoch: 7600 Train: 0.01129 Test: 0.36474
Epoch 7680: New minimal relative error: 12.32%, model saved.
Epoch: 7680 Train: 0.00457 Test: 0.35517
Epoch: 7760 Train: 0.02199 Test: 0.38319
Epoch: 7840 Train: 0.01646 Test: 0.37454
Epoch: 7920 Train: 0.00569 Test: 0.35617
Epoch: 7999 Train: 0.01910 Test: 0.39215
Training Loss: tensor(0.0191)
Test Loss: tensor(0.3922)
Learned LE: [ 0.91239613 -0.01309836 -6.5463886 ]
True LE: [ 8.5766643e-01  1.3155035e-02 -1.4547918e+01]
Relative Error: [3.0219564  2.9522154  2.872414   2.7780728  2.6747828  2.586769
 2.5591342  2.634415   2.8022447  2.9765353  3.041513   2.9268985
 2.6507926  2.3025331  1.9846115  1.7493997  1.5674733  1.3772432
 1.1807415  1.1071969  1.3467432  1.8987646  2.6259542  3.4029942
 4.1398277  4.7916245  5.334229   5.7379565  5.9913054  6.113046
 6.123271   6.0425334  5.9243884  5.830912   5.776266   5.7486544
 5.7584076  5.8216515  5.9312444  6.0497932  6.100022   6.0015664
 5.7196097  5.267201   4.711437   4.1533885  3.6682196  3.303678
 3.1081092  3.0900862  3.1771574  3.2707372  3.3130772  3.2914643
 3.2167912  3.1081755  2.985301   2.8643696  2.7566414  2.6682684
 2.6004667  2.5500185  2.5102508  2.472475   2.4271662  2.3662913
 2.2874618  2.202246   2.1450207  2.1672485  2.2958071  2.482121
 2.61435    2.5919313  2.395144   2.0967886  1.8126273  1.6250366
 1.518208   1.403853   1.2300035  1.0443059  1.0148292  1.3144286
 1.8868973  2.5833466  3.2814736  3.9108377  4.4525714  4.890195
 5.1970735  5.3729053  5.4337254  5.383801   5.250049   5.1104994
 5.01391    4.940329   4.878329   4.854659   4.8848753  4.9577866
 5.021254   4.991535   4.8162026  4.4822993  4.021394   3.5289078
 3.0903974  2.7372704  2.505279   2.4339075  2.4907675  2.5818567
 2.6361547  2.632597   2.5785952  2.4913173  2.3887503  2.2857652
 2.1932418  2.118334   2.0644403  2.0307791  2.0126603  2.0023367
 1.990129   1.9657582  1.9208848  1.8546565  1.7834905  1.7496792
 1.8061515  1.9605709  2.1362667  2.2154493  2.1249628  1.8910054
 1.6221253  1.4375716  1.3715763  1.3370278  1.2354285  1.0578426
 0.8884143  0.9085986  1.2457002  1.803223   2.4370399  3.0359426
 3.5547566  3.9968503  4.3481207  4.5838747  4.70628    4.720279
 4.618927   4.4490623  4.3075757  4.212455   4.1160855  4.021512
 3.9676535  3.9682767  4.0095744  4.025071   3.9438572  3.7375436
 3.394335   2.9723253  2.5796301  2.2533906  1.996845   1.8560218
 1.8560807  1.9330829  2.000555   2.019596   1.9900839  1.9266914
 1.8460032  1.7612778  1.6819097  1.6150775  1.5663823  1.5385623
 1.5305693  1.537476   1.5512205  1.5613666  1.5564556  1.5270741
 1.4720705  1.4097447  1.3865951  1.4557649  1.6128597  1.7680718
 1.8078487  1.6822871  1.4472378  1.2299953  1.1409516  1.1566746
 1.1476777  1.0431627  0.87760454 0.747384   0.8062643  1.129455
 1.626232   2.1720326  2.6656568  3.0815837  3.4404192  3.7298567
 3.92244    4.01288    3.996096   3.8595605  3.670745   3.5381515
 3.447395   3.3313773  3.2097895  3.1282496  3.1021574  3.1170332
 3.100436   3.003745   2.8037448  2.4830194  2.1316254  1.8412806
 1.5960606  1.3986202  1.3098208  1.3402568  1.4124336  1.4577999
 1.4575288  1.4206587  1.3638117  1.2998092  1.2354937  1.1757255
 1.1264309  1.0938132  1.0815146  1.0892239  1.112781   1.1446899
 1.1743261  1.1890796  1.1773621  1.1351472  1.0776777  1.0503656
 1.1091839  1.2524339  1.3921779  1.4196607  1.292552   1.0743151
 0.89989585 0.8758429  0.9361826  0.93664545 0.84127116 0.71463144
 0.63482994 0.69064564 0.9369419  1.332559   1.779691   2.1776075
 2.5043454  2.795668   3.0472276  3.2254486  3.309006   3.2862923
 3.1397984  2.9423676  2.8186476  2.739741   2.6163301  2.4750426
 2.3686805  2.316042   2.309655   2.2796793  2.1965377  2.0304468
 1.7545316  1.4826438  1.2776285  1.0861071  0.91864896 0.84836763
 0.88447726 0.9486596  0.9833099  0.9770345  0.94465005 0.9035485
 0.8598922  0.8133553  0.7660193  0.7249297  0.6984367  0.6915435
 0.704585   0.7347557  0.77643895 0.820467   0.85426074 0.86402583
 0.8406155  0.7904318  0.74917865 0.7757617  0.889186   1.0205011
 1.0652906  0.9689699  0.7761342  0.6146415  0.61269945 0.69794697
 0.71510494 0.6455     0.57015073 0.5313087  0.54719543 0.67172027
 0.93399084 1.2792634  1.6014365  1.8548621  2.0832663  2.3061209
 2.4912746  2.5989242  2.6036284  2.4867864  2.2891304  2.1607425
 2.101741   1.994938   1.8445166  1.7195052  1.6370647  1.6116276
 1.5808274  1.5242194  1.4145756  1.1969315  0.99326754 0.8576667
 0.71172595 0.5583074  0.47691423 0.5024527  0.56257606 0.5960758
 0.5905806  0.568701   0.5480683  0.5248743  0.49180993 0.45033288
 0.4104049  0.38366532 0.37614986 0.38707203 0.41353315 0.45285407
 0.5002788  0.5464633  0.5779078  0.5810068  0.55020326 0.5017679
 0.48414436 0.5450689  0.65852076 0.73708963 0.70505244 0.56160456
 0.3904137  0.35087743 0.4463271  0.49671268 0.46098903 0.42475727
 0.4159451  0.401883   0.40688467 0.50863594]
