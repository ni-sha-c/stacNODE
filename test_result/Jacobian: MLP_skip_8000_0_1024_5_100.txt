time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.71%, model saved.
Epoch: 0 Train: 9084.31836 Test: 4104.83057
Epoch: 80 Train: 2452.91406 Test: 1197.19067
Epoch 160: New minimal relative error: 70.24%, model saved.
Epoch: 160 Train: 2096.16748 Test: 689.51001
Epoch: 240 Train: 1093.23169 Test: 383.67712
Epoch: 320 Train: 657.39215 Test: 129.51199
Epoch 400: New minimal relative error: 24.97%, model saved.
Epoch: 400 Train: 283.92178 Test: 46.78458
Epoch: 480 Train: 189.03220 Test: 129.62807
Epoch: 560 Train: 157.64584 Test: 33.35924
Epoch 640: New minimal relative error: 15.72%, model saved.
Epoch: 640 Train: 91.16418 Test: 10.73868
Epoch 720: New minimal relative error: 14.65%, model saved.
Epoch: 720 Train: 75.61249 Test: 9.76144
Epoch 800: New minimal relative error: 10.39%, model saved.
Epoch: 800 Train: 115.61841 Test: 59.40319
Epoch: 880 Train: 43.63337 Test: 11.45901
Epoch: 960 Train: 41.55074 Test: 3.47291
Epoch: 1040 Train: 30.68511 Test: 2.63596
Epoch: 1120 Train: 37.70842 Test: 6.69099
Epoch: 1200 Train: 40.22195 Test: 15.60344
Epoch 1280: New minimal relative error: 5.88%, model saved.
Epoch: 1280 Train: 23.25755 Test: 1.18092
Epoch: 1360 Train: 29.16965 Test: 3.87795
Epoch 1440: New minimal relative error: 4.11%, model saved.
Epoch: 1440 Train: 21.10502 Test: 4.17710
Epoch: 1520 Train: 18.06374 Test: 1.37258
Epoch: 1600 Train: 22.36477 Test: 2.77489
Epoch: 1680 Train: 22.22421 Test: 8.56125
Epoch: 1760 Train: 25.51261 Test: 4.44081
Epoch: 1840 Train: 17.88826 Test: 5.24178
Epoch: 1920 Train: 20.77790 Test: 4.47126
Epoch: 2000 Train: 16.39306 Test: 2.22725
Epoch: 2080 Train: 17.70945 Test: 6.51472
Epoch: 2160 Train: 16.32921 Test: 2.01739
Epoch 2240: New minimal relative error: 2.84%, model saved.
Epoch: 2240 Train: 17.55276 Test: 6.42907
Epoch: 2320 Train: 23.00629 Test: 13.62718
Epoch: 2400 Train: 11.32309 Test: 0.38156
Epoch: 2480 Train: 13.77195 Test: 2.82094
Epoch: 2560 Train: 20.08052 Test: 6.56952
Epoch: 2640 Train: 10.36193 Test: 0.26920
Epoch: 2720 Train: 12.76072 Test: 2.42179
Epoch: 2800 Train: 12.53287 Test: 2.91502
Epoch: 2880 Train: 8.98646 Test: 0.45499
Epoch: 2960 Train: 11.93848 Test: 1.76578
Epoch 3040: New minimal relative error: 2.30%, model saved.
Epoch: 3040 Train: 8.50486 Test: 0.16932
Epoch: 3120 Train: 9.93028 Test: 0.45756
Epoch: 3200 Train: 13.60022 Test: 3.23761
Epoch: 3280 Train: 16.04660 Test: 1.06412
Epoch: 3360 Train: 8.89112 Test: 0.36159
Epoch: 3440 Train: 9.29918 Test: 1.03250
Epoch: 3520 Train: 8.56428 Test: 0.95514
Epoch: 3600 Train: 14.03890 Test: 3.26971
Epoch: 3680 Train: 7.67613 Test: 0.41003
Epoch: 3760 Train: 8.82629 Test: 1.52723
Epoch: 3840 Train: 10.76418 Test: 6.41356
Epoch: 3920 Train: 7.20218 Test: 0.16012
Epoch: 4000 Train: 8.06615 Test: 0.42625
Epoch: 4080 Train: 15.27743 Test: 4.68753
Epoch: 4160 Train: 9.62796 Test: 1.14650
Epoch: 4240 Train: 7.23810 Test: 0.44244
Epoch: 4320 Train: 13.40940 Test: 1.05862
Epoch 4400: New minimal relative error: 1.99%, model saved.
Epoch: 4400 Train: 6.96956 Test: 0.14830
Epoch: 4480 Train: 8.23105 Test: 0.51274
Epoch: 4560 Train: 6.05049 Test: 0.69118
Epoch: 4640 Train: 8.28855 Test: 2.43199
Epoch: 4720 Train: 6.08152 Test: 0.64482
Epoch: 4800 Train: 5.76730 Test: 0.12828
Epoch: 4880 Train: 6.34551 Test: 0.44561
Epoch: 4960 Train: 5.41557 Test: 0.07845
Epoch: 5040 Train: 5.65854 Test: 0.16701
Epoch: 5120 Train: 6.25114 Test: 0.70468
Epoch: 5200 Train: 7.44133 Test: 0.17309
Epoch: 5280 Train: 4.70727 Test: 0.21647
Epoch: 5360 Train: 5.73970 Test: 0.30572
Epoch: 5440 Train: 11.05821 Test: 6.06227
Epoch: 5520 Train: 4.56720 Test: 0.16060
Epoch: 5600 Train: 5.30754 Test: 1.11324
Epoch: 5680 Train: 4.35054 Test: 0.41495
Epoch: 5760 Train: 4.59032 Test: 0.11488
Epoch: 5840 Train: 4.08019 Test: 0.05964
Epoch: 5920 Train: 4.01901 Test: 0.06758
Epoch 6000: New minimal relative error: 1.13%, model saved.
Epoch: 6000 Train: 4.03577 Test: 0.06394
Epoch: 6080 Train: 5.50037 Test: 0.19257
Epoch: 6160 Train: 4.29146 Test: 0.05002
Epoch: 6240 Train: 4.15658 Test: 0.03643
Epoch: 6320 Train: 4.11526 Test: 0.11770
Epoch: 6400 Train: 3.94919 Test: 0.25378
Epoch: 6480 Train: 4.18770 Test: 0.13599
Epoch: 6560 Train: 3.83041 Test: 0.08620
Epoch: 6640 Train: 3.71042 Test: 0.04056
Epoch: 6720 Train: 3.62831 Test: 0.06089
Epoch: 6800 Train: 3.73957 Test: 0.14864
Epoch: 6880 Train: 4.16816 Test: 0.61986
Epoch: 6960 Train: 3.46582 Test: 0.03663
Epoch: 7040 Train: 4.06316 Test: 0.12147
Epoch: 7120 Train: 3.52653 Test: 0.03189
Epoch 7200: New minimal relative error: 0.40%, model saved.
Epoch: 7200 Train: 3.41349 Test: 0.02371
Epoch: 7280 Train: 3.40761 Test: 0.03904
Epoch: 7360 Train: 3.45039 Test: 0.24534
Epoch: 7440 Train: 3.59196 Test: 0.12367
Epoch: 7520 Train: 4.78185 Test: 0.93015
Epoch: 7600 Train: 3.13950 Test: 0.12826
Epoch: 7680 Train: 3.15128 Test: 0.05244
Epoch: 7760 Train: 3.02259 Test: 0.03679
Epoch: 7840 Train: 3.66707 Test: 0.23635
Epoch: 7920 Train: 3.06601 Test: 0.03107
Epoch: 7999 Train: 3.05551 Test: 0.02265
Training Loss: tensor(3.0555)
Test Loss: tensor(0.0227)
Learned LE: [ 8.64852130e-01  1.07687032e-02 -1.45571575e+01]
True LE: [ 8.8119853e-01  4.5595951e-03 -1.4555761e+01]
Relative Error: [ 6.6484914   6.6224046   6.337999    6.088308    5.905861    5.8149815
  5.825288    5.8114777   5.9634814   6.243782    7.061787    8.033379
  8.81855     9.680619   10.143854   10.224224   10.056121    9.951534
  9.940921   10.172239   10.297338   10.193071   10.020627    9.829606
  9.526493    9.36044     9.398908    9.498434    9.645242   10.061468
 10.560867   10.898418   11.241468   11.319161   11.172738   10.671409
 10.249548   10.263861   10.058212    9.599365    9.342257    9.327056
  8.660459    7.826402    6.973003    6.4500294   6.330808    6.529805
  6.4970784   6.110785    5.958856    6.0157733   6.271129    6.588663
  6.9409213   7.044594    6.8620024   6.582333    6.1860957   5.9412503
  5.8985915   6.0111175   6.1366363   5.911954    5.6285033   5.2151375
  4.969836    4.8570905   4.8670964   4.8467546   4.844215    4.9516973
  5.744734    6.710421    7.6368585   8.466959    8.907171    9.168674
  9.146525    9.10995     9.0576515   9.13681     9.080252    9.106639
  8.920673    8.807807    8.574773    8.405087    8.477378    8.561383
  8.752365    9.162796    9.596854    9.97428    10.353355   10.169781
  9.786587    9.548885    9.090769    9.014585    8.842245    8.371618
  8.04903     8.019551    7.3979235   6.545203    5.733564    5.243802
  5.404558    5.9073653   5.937713    5.6057987   5.424352    5.473155
  5.6481233   5.9864597   6.3538313   6.431222    6.2974267   6.093152
  5.648083    5.399438    5.329412    5.52005     5.4140506   5.221546
  5.0135903   4.511148    4.155856    4.044559    4.0259624   4.0256605
  3.7693608   3.803201    4.5643086   5.494726    6.4560385   7.362477
  7.8284025   7.9926977   8.252125    8.3087015   8.158595    8.0401325
  7.879465    7.879372    7.911654    7.9469113   7.7506      7.5369864
  7.550135    7.6564403   7.80788     8.2451935   8.640202    9.102982
  9.522896    9.072493    8.583272    8.412294    8.106867    7.904103
  7.7495975   7.225886    6.821561    6.7218995   6.191044    5.3189335
  4.5219655   4.172912    4.5776496   5.0124016   5.354827    5.161971
  4.95119     4.988601    5.062984    5.3669343   5.7766256   5.8379965
  5.7051377   5.615941    5.1423492   4.9001703   4.8957157   4.8381624
  4.719604    4.6316223   4.4477844   3.9794154   3.555535    3.3074718
  3.2611842   3.1642582   2.9113896   2.8967812   3.4761243   4.3968096
  5.326719    6.3345      6.8490243   6.869581    7.060854    7.3899016
  7.2192163   6.927353    6.6702394   6.588062    6.7856526   7.1407137
  6.988723    6.7412863   6.6234155   6.6742697   6.7869687   7.2526226
  7.709433    8.203828    8.594814    8.152862    7.6429834   7.2264686
  7.1561418   6.831458    6.6212893   5.981898    5.60878     5.487586
  5.2079253   4.262302    3.4524064   3.2551877   3.7763138   4.1871414
  4.6690183   4.7785087   4.5594      4.5139694   4.5523953   4.763236
  5.041949    5.196817    5.1242127   5.101771    4.666265    4.4225984
  4.3724813   4.2470603   4.1735907   4.0772667   3.9620802   3.547175
  3.1849422   2.7911093   2.6718688   2.3750422   2.2291608   2.1884742
  2.5246992   3.3915465   4.3309712   5.234518    5.8792863   5.8844223
  5.923893    6.1195855   6.3279934   5.968149    5.64961     5.5091357
  5.695276    5.990709    6.0286307   5.768442    5.635742    5.6552863
  5.7620206   6.080076    6.5794854   7.2935014   7.7020555   7.280235
  6.744814    6.275169    6.0932164   5.9629574   5.548495    4.974715
  4.4628167   4.38544     4.2366548   3.378401    2.5859852   2.4622996
  2.9393175   3.3951514   3.8421354   4.3526      4.198165    4.1213717
  4.0586767   4.182975    4.3133693   4.538861    4.563636    4.5387673
  4.2822504   4.0448084   3.8116972   3.645893    3.677617    3.6768389
  3.5222518   3.1934352   2.9234948   2.5139875   2.14416     1.7317725
  1.4599949   1.4080116   1.7184343   2.5080433   3.4033728   4.15108
  4.937258    5.0016294   4.9068937   4.976775    5.1884217   5.218954
  4.842266    4.5564966   4.6310425   4.7348337   4.9370933   4.809709
  4.681469    4.7327695   4.821745    5.0668597   5.3946633   6.07133
  6.6066356   6.555671    5.9237657   5.366641    4.9968853   4.9612646
  4.533781    4.0856504   3.544836    3.4226139   3.3312335   2.7079732
  1.8246018   1.7284247   2.1670098   2.6412032   3.020442    3.6254022
  3.892743    3.737057    3.5846174   3.5930886   3.6532278   3.86872
  4.012705    3.9796724   3.9125714   3.6789482   3.3089905   3.1281867
  3.1944904   3.2658887   3.2146745   2.9769633   2.6660314   2.2557418
  1.7353177   1.2836516   0.86111546  0.6003141   0.88130665  1.5542573
  2.340399    3.0709088   3.8905933   4.269236    4.1264133   4.06522
  4.228114    4.264527    4.2594714   3.8089938   3.6458282   3.6911356
  3.7386694   3.8448524   3.8185363   3.851743  ]
