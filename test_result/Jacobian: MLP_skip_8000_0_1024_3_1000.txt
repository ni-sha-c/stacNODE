time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 97.39%, model saved.
Epoch: 0 Train: 60031.92578 Test: 3733.95898
Epoch: 80 Train: 13010.25098 Test: 1321.13965
Epoch: 160 Train: 11367.24707 Test: 887.77319
Epoch: 240 Train: 12159.50293 Test: 1154.37988
Epoch: 320 Train: 11885.76172 Test: 1686.33362
Epoch 400: New minimal relative error: 46.13%, model saved.
Epoch: 400 Train: 10946.69727 Test: 906.04034
Epoch: 480 Train: 11775.07422 Test: 920.39484
Epoch: 560 Train: 9748.39551 Test: 785.61847
Epoch: 640 Train: 7903.95312 Test: 580.49786
Epoch: 720 Train: 7568.64307 Test: 538.58704
Epoch: 800 Train: 6458.20996 Test: 354.41827
Epoch: 880 Train: 4656.96191 Test: 196.12660
Epoch: 960 Train: 3142.24829 Test: 110.60584
Epoch 1040: New minimal relative error: 30.77%, model saved.
Epoch: 1040 Train: 2213.41479 Test: 96.30022
Epoch: 1120 Train: 1412.03503 Test: 41.00603
Epoch 1200: New minimal relative error: 14.39%, model saved.
Epoch: 1200 Train: 1076.12256 Test: 24.41227
Epoch: 1280 Train: 912.97186 Test: 14.13387
Epoch: 1360 Train: 940.47217 Test: 24.55794
Epoch: 1440 Train: 788.82562 Test: 27.18705
Epoch: 1520 Train: 728.17694 Test: 29.27208
Epoch: 1600 Train: 681.90784 Test: 47.41825
Epoch 1680: New minimal relative error: 12.73%, model saved.
Epoch: 1680 Train: 572.93860 Test: 13.96302
Epoch 1760: New minimal relative error: 9.66%, model saved.
Epoch: 1760 Train: 514.03064 Test: 5.89346
Epoch: 1840 Train: 570.17023 Test: 18.18280
Epoch: 1920 Train: 533.47705 Test: 9.63148
Epoch: 2000 Train: 562.82556 Test: 8.51918
Epoch: 2080 Train: 514.77911 Test: 13.48235
Epoch: 2160 Train: 475.98029 Test: 11.03612
Epoch: 2240 Train: 473.02509 Test: 5.91473
Epoch 2320: New minimal relative error: 6.35%, model saved.
Epoch: 2320 Train: 415.48996 Test: 6.53648
Epoch: 2400 Train: 418.51147 Test: 5.27344
Epoch: 2480 Train: 373.09741 Test: 6.76076
Epoch: 2560 Train: 405.44431 Test: 6.37921
Epoch: 2640 Train: 392.91730 Test: 3.89002
Epoch: 2720 Train: 367.59027 Test: 3.80910
Epoch 2800: New minimal relative error: 4.91%, model saved.
Epoch: 2800 Train: 350.72104 Test: 3.99743
Epoch: 2880 Train: 381.28085 Test: 5.70661
Epoch: 2960 Train: 341.85345 Test: 2.93553
Epoch: 3040 Train: 331.19238 Test: 5.75668
Epoch: 3120 Train: 360.78363 Test: 17.77124
Epoch: 3200 Train: 329.58853 Test: 2.64179
Epoch: 3280 Train: 327.86295 Test: 6.74113
Epoch: 3360 Train: 293.66516 Test: 6.35812
Epoch: 3440 Train: 284.37720 Test: 2.22445
Epoch: 3520 Train: 272.38965 Test: 2.52636
Epoch: 3600 Train: 356.49524 Test: 8.55504
Epoch: 3680 Train: 364.33118 Test: 4.71431
Epoch 3760: New minimal relative error: 4.44%, model saved.
Epoch: 3760 Train: 329.92276 Test: 3.19735
Epoch: 3840 Train: 338.25095 Test: 18.98347
Epoch: 3920 Train: 269.27063 Test: 2.01723
Epoch: 4000 Train: 273.83215 Test: 2.11486
Epoch: 4080 Train: 281.25928 Test: 7.29105
Epoch 4160: New minimal relative error: 4.27%, model saved.
Epoch: 4160 Train: 265.15897 Test: 2.21500
Epoch: 4240 Train: 311.76797 Test: 6.75201
Epoch: 4320 Train: 265.94095 Test: 2.72397
Epoch: 4400 Train: 223.18048 Test: 1.38412
Epoch: 4480 Train: 249.07687 Test: 4.81686
Epoch: 4560 Train: 253.53493 Test: 3.22106
Epoch: 4640 Train: 267.82437 Test: 2.80730
Epoch: 4720 Train: 236.73613 Test: 2.45871
Epoch: 4800 Train: 290.89981 Test: 3.42222
Epoch 4880: New minimal relative error: 3.03%, model saved.
Epoch: 4880 Train: 221.31366 Test: 1.86152
Epoch: 4960 Train: 224.25964 Test: 1.39330
Epoch: 5040 Train: 235.20694 Test: 3.07959
Epoch: 5120 Train: 228.57355 Test: 2.33235
Epoch: 5200 Train: 196.41730 Test: 1.80599
Epoch: 5280 Train: 192.80757 Test: 2.26519
Epoch: 5360 Train: 217.59726 Test: 2.09187
Epoch: 5440 Train: 254.82883 Test: 2.52064
Epoch: 5520 Train: 213.70120 Test: 2.04302
Epoch: 5600 Train: 218.53168 Test: 1.83683
Epoch: 5680 Train: 260.74542 Test: 7.03664
Epoch: 5760 Train: 192.75827 Test: 1.23915
Epoch: 5840 Train: 199.32204 Test: 1.12302
Epoch: 5920 Train: 209.19676 Test: 1.53672
Epoch: 6000 Train: 194.00644 Test: 1.66395
Epoch: 6080 Train: 193.84933 Test: 1.70074
Epoch: 6160 Train: 209.60275 Test: 2.02572
Epoch: 6240 Train: 197.57446 Test: 2.03674
Epoch: 6320 Train: 183.55347 Test: 2.12559
Epoch: 6400 Train: 182.80791 Test: 1.58773
Epoch: 6480 Train: 187.72169 Test: 1.29408
Epoch: 6560 Train: 175.27052 Test: 1.15308
Epoch: 6640 Train: 190.02620 Test: 3.48038
Epoch: 6720 Train: 158.69560 Test: 1.44568
Epoch: 6800 Train: 190.33749 Test: 1.30917
Epoch: 6880 Train: 206.69003 Test: 10.85651
Epoch: 6960 Train: 181.08305 Test: 1.66725
Epoch: 7040 Train: 186.03258 Test: 2.50025
Epoch: 7120 Train: 172.40950 Test: 1.74129
Epoch: 7200 Train: 142.08533 Test: 0.79422
Epoch: 7280 Train: 157.73495 Test: 1.07395
Epoch: 7360 Train: 156.93413 Test: 1.98712
Epoch: 7440 Train: 170.00562 Test: 1.97383
Epoch: 7520 Train: 136.31154 Test: 0.88050
Epoch: 7600 Train: 177.02521 Test: 19.31335
Epoch: 7680 Train: 160.44870 Test: 1.11558
Epoch: 7760 Train: 168.82333 Test: 1.27760
Epoch: 7840 Train: 154.90234 Test: 0.86220
Epoch: 7920 Train: 158.81087 Test: 1.37016
Epoch: 7999 Train: 166.07390 Test: 2.14689
Training Loss: tensor(166.0739)
Test Loss: tensor(2.1469)
Learned LE: [ 7.8397644e-01  1.0293002e-02 -1.4526947e+01]
True LE: [ 8.6866802e-01  1.0186301e-02 -1.4557983e+01]
Relative Error: [12.317659  12.205548  12.483157  12.638572  13.057284  12.871186
 12.491953  12.535103  12.762446  13.162468  13.495869  13.9285
 14.540016  15.125711  15.024495  14.505598  13.761325  13.041663
 12.327527  11.742841  11.191802  10.835722  10.694189  10.721201
 10.888304  11.234369  11.7680435 12.547604  13.427337  14.06211
 15.006288  16.21778   17.153145  15.970844  15.034019  14.360712
 13.931492  13.487331  13.440013  14.008842  14.577971  15.130446
 15.304506  15.59965   15.645836  15.546338  15.424292  15.3489
 15.456735  15.751161  16.157911  16.659653  17.055273  15.91888
 14.763032  13.803394  13.080462  12.503576  11.996734  11.631762
 11.473003  11.543292  11.3808365 11.315429  11.680074  12.108798
 11.708844  11.236135  10.782063  10.824641  11.053863  11.450072
 11.804201  12.216487  12.75633   13.390547  14.214706  14.037999
 13.172542  12.230981  11.4565735 10.751943  10.116563   9.764663
  9.59692    9.575033   9.721807  10.029739  10.52783   11.300897
 12.237937  12.892673  13.800563  15.044793  15.517531  14.508626
 13.602705  12.959038  12.579309  12.282709  12.19476   12.792802
 13.387555  13.854019  14.102539  14.27031   14.286874  14.199898
 14.074328  13.999304  14.145788  14.385934  14.699815  15.290077
 16.188507  15.157886  14.080106  13.153082  12.455101  11.885106
 11.437762  11.051633  10.822355  10.746688  10.542358  10.594072
 10.883637  10.623411  10.077873   9.72124    9.2819     9.273612
  9.496099   9.880708  10.202352  10.647349  11.164598  11.798454
 12.570104  13.15551   12.609325  11.58854   10.733906   9.844313
  9.183089   8.752438   8.4931965  8.458371   8.584946   8.844585
  9.299721  10.02995   11.0293045 11.706188  12.634191  13.8428
 13.92226   13.198853  12.317842  11.698156  11.352233  11.175765
 11.060163  11.660286  12.15969   12.653486  12.953811  13.005296
 13.05369   12.968821  12.837821  12.858744  12.941626  13.04242
 13.310938  13.973334  14.864215  14.583625  13.537295  12.62758
 11.932728  11.358801  10.915427  10.5611    10.349599  10.272664
  9.859861   9.908494   9.627102   8.96839    8.572127   8.280071
  7.987732   7.8725123  8.080944   8.457033   8.75062    9.196206
  9.716511  10.355141  11.058602  11.548079  11.805975  11.15046
 10.167275   9.057254   8.264134   7.6678014  7.341142   7.215203
  7.3009067  7.6121764  8.136279   8.817277   9.731329  10.503295
 11.429349  12.730191  12.537872  12.007223  11.372367  10.688129
 10.239654  10.148775  10.102563  10.50387   10.987604  11.540916
 11.726074  11.844877  11.9452    11.855226  11.78758   11.813743
 11.814358  11.792406  12.057725  12.6826    13.601949  14.133075
 13.075611  12.149076  11.430432  10.863532  10.472081  10.195945
  9.974475   9.883449   9.544003   9.340307   8.221763   7.4926586
  7.1712666  6.9171996  6.835437   6.6207876  6.819345   7.1396046
  7.452724   7.885704   8.39832    9.032724   9.686917  10.062463
 10.235699  10.67222    9.803208   8.537169   7.527191   6.73962
  6.2277837  6.048261   6.0968547  6.333086   6.780513   7.491397
  8.426314   9.345073  10.314455  11.653012  11.228814  10.880591
 10.539724   9.921956   9.46727    9.252305   9.214052   9.384878
  9.865257  10.476359  10.576367  10.7614155 10.913916  10.864494
 10.86656   10.869335  10.76618   10.689447  10.922321  11.487888
 12.399536  13.511862  12.77096   11.840519  11.087756  10.493929
 10.062785   9.820162   9.74684    9.715422   9.355183   8.528149
  7.2270036  6.2618876  5.86439    5.6913424  5.667873   5.6274457
  5.730267   6.0048485  6.353897   6.645889   7.2345505  7.82014
  8.470304   8.833701   8.878938   9.202252   9.621081   8.339466
  7.0667095  6.0571747  5.404818   5.011772   4.9089146  5.0565643
  5.4097304  6.0219946  6.9056535  8.197553   9.20003   10.506001
 10.024514   9.797844   9.533246   9.237982   8.798644   8.57096
  8.526977   8.306638   8.767055   9.3303585  9.475329   9.733109
  9.96051    9.966942  10.039486  10.003665   9.843285   9.730961
  9.925732  10.486511  11.247849  12.31575   12.568048  11.586827
 10.789427  10.272462   9.871037   9.614944   9.5372     9.647577
  9.306531   7.9412313  6.584284   5.4534035  4.6647897  4.6396074
  4.5702972  4.7810893  4.82749    5.073523   5.4217725  5.65853
  6.2384825  6.8418956  7.4838147  7.8138223  7.7273026  7.8711586
  8.275448   8.481682   6.9503675  5.7158318  4.7219853  4.137933
  3.9066606  3.9040272  4.096367   4.5765805]
