time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 101.83%, model saved.
Epoch: 0 Train: 3792.26440 Test: 4074.31812
Epoch 100: New minimal relative error: 99.88%, model saved.
Epoch: 100 Train: 180.79424 Test: 176.15439
Epoch 200: New minimal relative error: 56.07%, model saved.
Epoch: 200 Train: 20.98672 Test: 20.55493
Epoch 300: New minimal relative error: 13.26%, model saved.
Epoch: 300 Train: 9.36881 Test: 8.91440
Epoch: 400 Train: 8.96545 Test: 9.04306
Epoch: 500 Train: 5.82274 Test: 5.66001
Epoch: 600 Train: 3.97763 Test: 3.76384
Epoch: 700 Train: 3.32437 Test: 3.15954
Epoch: 800 Train: 2.81387 Test: 2.62699
Epoch: 900 Train: 2.40317 Test: 2.22989
Epoch: 1000 Train: 2.11046 Test: 1.94224
Epoch: 1100 Train: 2.21836 Test: 2.06729
Epoch: 1200 Train: 2.43252 Test: 2.24033
Epoch: 1300 Train: 2.60362 Test: 2.91343
Epoch 1400: New minimal relative error: 9.07%, model saved.
Epoch: 1400 Train: 1.22408 Test: 1.07179
Epoch: 1500 Train: 1.11244 Test: 0.96074
Epoch: 1600 Train: 5.34940 Test: 6.49720
Epoch: 1700 Train: 0.91818 Test: 0.77879
Epoch: 1800 Train: 0.87065 Test: 0.73814
Epoch: 1900 Train: 1.08288 Test: 0.99343
Epoch: 2000 Train: 1.61365 Test: 1.69967
Epoch: 2100 Train: 0.68541 Test: 0.57784
Epoch: 2200 Train: 0.70507 Test: 0.58235
Epoch: 2300 Train: 0.85352 Test: 0.79273
Epoch: 2400 Train: 0.68086 Test: 0.60792
Epoch: 2500 Train: 0.90251 Test: 0.72545
Epoch: 2600 Train: 0.73256 Test: 0.66052
Epoch: 2700 Train: 0.51961 Test: 0.48016
Epoch 2800: New minimal relative error: 9.00%, model saved.
Epoch: 2800 Train: 0.51691 Test: 0.45830
Epoch: 2900 Train: 1.06688 Test: 1.00864
Epoch: 3000 Train: 0.42996 Test: 0.35434
Epoch: 3100 Train: 0.53886 Test: 0.46283
Epoch: 3200 Train: 0.39985 Test: 0.33725
Epoch: 3300 Train: 0.67716 Test: 0.57998
Epoch: 3400 Train: 1.05760 Test: 1.31625
Epoch: 3500 Train: 0.34011 Test: 0.28420
Epoch: 3600 Train: 0.35515 Test: 0.30311
Epoch: 3700 Train: 0.32400 Test: 0.27494
Epoch: 3800 Train: 0.32680 Test: 0.27681
Epoch: 3900 Train: 0.34368 Test: 0.31905
Epoch: 4000 Train: 0.30228 Test: 0.26013
Epoch 4100: New minimal relative error: 8.51%, model saved.
Epoch: 4100 Train: 0.27733 Test: 0.23415
Epoch: 4200 Train: 0.29715 Test: 0.24797
Epoch: 4300 Train: 0.52540 Test: 0.44053
Epoch: 4400 Train: 0.29986 Test: 0.27319
Epoch: 4500 Train: 0.34916 Test: 0.31372
Epoch 4600: New minimal relative error: 7.60%, model saved.
Epoch: 4600 Train: 0.24009 Test: 0.20427
Epoch: 4700 Train: 0.32409 Test: 0.26733
Epoch: 4800 Train: 0.24188 Test: 0.19793
Epoch 4900: New minimal relative error: 7.48%, model saved.
Epoch: 4900 Train: 0.28069 Test: 0.24465
Epoch: 5000 Train: 0.21766 Test: 0.18717
Epoch 5100: New minimal relative error: 4.07%, model saved.
Epoch: 5100 Train: 0.21099 Test: 0.18151
Epoch: 5200 Train: 0.20527 Test: 0.17672
Epoch: 5300 Train: 0.19964 Test: 0.17189
Epoch: 5400 Train: 0.20403 Test: 0.17945
Epoch: 5500 Train: 0.22205 Test: 0.19379
Epoch: 5600 Train: 0.18702 Test: 0.16224
Epoch: 5700 Train: 0.19300 Test: 0.16973
Epoch: 5800 Train: 0.23867 Test: 0.23350
Epoch: 5900 Train: 0.17598 Test: 0.15312
Epoch: 6000 Train: 0.17204 Test: 0.15072
Epoch: 6100 Train: 0.17362 Test: 0.15252
Epoch: 6200 Train: 0.52740 Test: 0.37510
Epoch: 6300 Train: 0.16289 Test: 0.14360
Epoch: 6400 Train: 0.16035 Test: 0.14138
Epoch: 6500 Train: 0.16424 Test: 0.14662
Epoch: 6600 Train: 0.16445 Test: 0.14335
Epoch: 6700 Train: 0.26333 Test: 0.21305
Epoch: 6800 Train: 0.15009 Test: 0.13412
Epoch: 6900 Train: 0.14872 Test: 0.13326
Epoch: 7000 Train: 0.15345 Test: 0.14400
Epoch: 7100 Train: 0.14416 Test: 0.12948
Epoch: 7200 Train: 0.15842 Test: 0.15243
Epoch: 7300 Train: 0.13984 Test: 0.12647
Epoch: 7400 Train: 0.15075 Test: 0.14144
Epoch: 7500 Train: 0.18531 Test: 0.18420
Epoch: 7600 Train: 0.13588 Test: 0.12283
Epoch: 7700 Train: 0.16823 Test: 0.17383
Epoch: 7800 Train: 0.13087 Test: 0.12005
Epoch: 7900 Train: 0.13327 Test: 0.14411
Epoch: 8000 Train: 0.12770 Test: 0.11779
Epoch: 8100 Train: 0.13085 Test: 0.12034
Epoch: 8200 Train: 0.20591 Test: 0.18891
Epoch: 8300 Train: 0.13876 Test: 0.13618
Epoch: 8400 Train: 0.13585 Test: 0.13667
Epoch: 8500 Train: 0.13188 Test: 0.12655
Epoch: 8600 Train: 0.11908 Test: 0.11139
Epoch: 8700 Train: 0.11797 Test: 0.11046
Epoch: 8800 Train: 0.11748 Test: 0.11098
Epoch: 8900 Train: 0.13721 Test: 0.13980
Epoch: 9000 Train: 0.11431 Test: 0.10829
Epoch: 9100 Train: 0.11282 Test: 0.10686
Epoch: 9200 Train: 0.12015 Test: 0.11055
Epoch: 9300 Train: 0.13775 Test: 0.12576
Epoch: 9400 Train: 0.10939 Test: 0.10513
Epoch: 9500 Train: 0.10822 Test: 0.10354
Epoch: 9600 Train: 0.10823 Test: 0.10319
Epoch: 9700 Train: 0.13607 Test: 0.13214
Epoch: 9800 Train: 0.10557 Test: 0.10165
Epoch: 9900 Train: 0.10405 Test: 0.10045
Epoch: 9999 Train: 0.10481 Test: 0.10150
Training Loss: tensor(0.1048)
Test Loss: tensor(0.1015)
Learned LE: [ 0.8029646   0.02513311 -3.9937232 ]
True LE: [ 8.8931173e-01  3.9755683e-03 -1.4564885e+01]
Relative Error: [19.271315  17.138596  15.072044  13.2774515 11.718748  10.342265
  9.11361    8.127977   7.385736   6.8607635  6.535653   6.400334
  6.4626775  6.6095886  6.757759   7.0309615  7.38049    7.6239166
  7.948428   8.423038   8.730478   9.115941   9.737569  10.270404
 10.955247  11.681845  12.770496  13.721921  14.300481  15.1260805
 16.178871  16.077623  15.923733  15.411258  15.051283  14.140159
 13.46651   12.747262  11.846415  11.31335   11.187023  11.012372
 11.082211  11.198011  11.329836  11.681238  12.214308  12.876903
 13.26521   13.740057  14.592761  15.550569  16.113003  16.73799
 16.987465  17.22437   17.530151  17.90728   18.373854  19.324999
 19.823357  19.50182   17.948936  15.748471  13.6806965 12.187532
 10.683391   9.25058    8.06534    7.15672    6.4907684  6.035579
  5.7634773  5.6861987  5.8171496  5.9554133  6.1802173  6.2765813
  6.521295   6.7956595  7.151889   7.462363   7.716366   8.017497
  8.580696   9.288831  10.022004  10.688565  11.713659  12.580991
 13.096828  13.845344  14.682363  14.484622  14.370893  14.1323
 13.655552  12.815304  12.188595  11.120348  10.336356   9.965343
  9.890111   9.767094   9.958116  10.092308  10.314033  10.756984
 11.385387  11.99217   12.241257  12.604657  13.268845  13.69492
 14.216221  14.841083  15.367012  15.566807  15.823203  16.1431
 16.586905  17.497387  18.419619  18.190714  16.528984  14.458079
 12.560678  11.25561    9.646903   8.217025   7.099442   6.267346
  5.6753035  5.271909   5.051681   5.025641   5.1673875  5.33184
  5.334389   5.504693   5.742722   6.0328665  6.358283   6.4837813
  6.809329   7.12318    7.568616   8.197601   9.1175785  9.779574
 10.726525  11.474951  11.954845  12.621863  13.213197  12.977426
 12.836003  12.922083  12.341722  11.566051  10.7507     9.637794
  8.975301   8.719526   8.629284   8.620569   8.86186    8.942275
  9.232114   9.766211  10.461454  10.911782  11.284892  11.590725
 11.394797  11.7752905 12.332208  13.041065  13.842124  14.052982
 14.255166  14.512833  14.94081   15.779124  16.801352  16.758898
 15.211513  13.289229  11.644025  10.390082   8.656929   7.2660775
  6.216063   5.4623103  4.932848   4.546938   4.316035   4.3179502
  4.5104637  4.4511538  4.520617   4.7589993  5.051405   5.3370485
  5.4445667  5.62165    5.9829025  6.405493   6.6982603  7.227246
  8.053219   8.943301   9.803455  10.442879  10.81235   11.452513
 11.8263035 11.553762  11.375321  11.525213  11.107136  10.376143
  9.281766   8.2792635  7.7158966  7.565388   7.4641066  7.415835
  7.723906   7.8712144  8.272458   8.894701   9.41637    9.785132
 10.064502   9.911323   9.792215   9.852445  10.411592  11.126046
 11.990984  12.674849  12.8273325 13.016738  13.407704  14.166705
 15.136421  15.359278  13.818889  12.200367  10.837828   9.490362
  7.750681   6.3983946  5.4181123  4.74215    4.167531   3.7720575
  3.5962894  3.5852346  3.5297961  3.5016007  3.6517906  3.92313
  4.2683268  4.530111   4.646606   4.8653746  5.2521214  5.7284684
  5.893714   6.274925   7.031022   7.9510846  8.942038   9.487486
  9.734571  10.308388  10.524138  10.216129   9.988197  10.18593
 10.118037   9.4972725  8.10778    7.108231   6.553372   6.4770026
  6.2364364  6.3403144  6.7389755  6.974722   7.476911   8.017017
  8.395733   8.6713085  8.561392   8.315467   8.230868   8.27952
  8.62422    9.347189  10.190395  11.1337    11.4923725 11.657383
 11.986498  12.65866   13.558645  14.014519  12.657876  11.154703
 10.033103   8.672615   6.93662    5.615656   4.710361   3.9709828
  3.4327102  3.1130462  2.9699032  2.890612   2.7870004  2.7634563
  2.881424   3.1818972  3.5974815  3.67079    3.8211215  4.177733
  4.4539704  4.9243     5.158672   5.3234315  5.9028387  6.6993976
  8.034069   8.6136     8.726435   9.179758   9.312667   8.969315
  8.88691    9.117323   9.162879   8.436785   7.0981817  6.1289334
  5.6096196  5.4002633  5.181716   5.363225   5.8161917  6.1372285
  6.7002187  6.9620557  7.39694    7.5707965  7.1969805  6.892755
  6.823418   6.869495   6.984022   7.6896296  8.528772   9.443242
 10.036717  10.398546  10.680223  11.256707  12.06879   12.802145
 11.514219  10.212491   9.241152   7.954592   6.219117   4.9209704
  3.975014   3.2753778  2.7817495  2.5185418  2.530639   2.413932
  2.3924716  2.3825195  2.4330134  2.6257272  2.9783866  2.9613187
  3.0407605  3.2639449  3.708309   4.207122   4.718735   4.613092
  4.9611464  5.5709314  6.694109   7.643331 ]
