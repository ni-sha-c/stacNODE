time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 100.46%, model saved.
Epoch: 0 Train: 4152.49561 Test: 4245.77246
Epoch 100: New minimal relative error: 86.44%, model saved.
Epoch: 100 Train: 248.42719 Test: 493.68930
Epoch: 200 Train: 191.51718 Test: 134.85860
Epoch 300: New minimal relative error: 31.86%, model saved.
Epoch: 300 Train: 10.03456 Test: 8.84337
Epoch 400: New minimal relative error: 14.19%, model saved.
Epoch: 400 Train: 40.31822 Test: 14.40003
Epoch: 500 Train: 5.67453 Test: 4.85595
Epoch: 600 Train: 8.91852 Test: 9.15207
Epoch: 700 Train: 8.26198 Test: 9.92253
Epoch: 800 Train: 4.55606 Test: 3.77542
Epoch: 900 Train: 1.38762 Test: 1.42533
Epoch: 1000 Train: 5.73895 Test: 16.09838
Epoch: 1100 Train: 2.66591 Test: 3.51088
Epoch: 1200 Train: 3.48455 Test: 2.58711
Epoch: 1300 Train: 2.86455 Test: 4.99253
Epoch: 1400 Train: 9.16188 Test: 8.81868
Epoch: 1500 Train: 3.19364 Test: 2.86622
Epoch: 1600 Train: 16.87281 Test: 21.86628
Epoch: 1700 Train: 0.87338 Test: 1.09301
Epoch 1800: New minimal relative error: 12.93%, model saved.
Epoch: 1800 Train: 4.36940 Test: 6.67611
Epoch: 1900 Train: 3.21016 Test: 8.22261
Epoch: 2000 Train: 0.51881 Test: 0.72054
Epoch: 2100 Train: 11.99695 Test: 10.49228
Epoch 2200: New minimal relative error: 7.42%, model saved.
Epoch: 2200 Train: 0.45407 Test: 0.65675
Epoch: 2300 Train: 0.47085 Test: 0.35526
Epoch: 2400 Train: 0.32505 Test: 0.36675
Epoch: 2500 Train: 2.91546 Test: 4.13022
Epoch: 2600 Train: 0.26232 Test: 0.44766
Epoch: 2700 Train: 6.93601 Test: 8.26963
Epoch: 2800 Train: 0.75218 Test: 0.51959
Epoch: 2900 Train: 4.62654 Test: 3.72319
Epoch: 3000 Train: 1.50012 Test: 2.17882
Epoch: 3100 Train: 0.28570 Test: 0.44763
Epoch: 3200 Train: 0.23533 Test: 0.25355
Epoch: 3300 Train: 0.24552 Test: 0.29381
Epoch: 3400 Train: 6.08084 Test: 8.75869
Epoch: 3500 Train: 0.60824 Test: 0.82336
Epoch: 3600 Train: 0.26678 Test: 0.26029
Epoch: 3700 Train: 0.40623 Test: 0.55266
Epoch: 3800 Train: 0.56798 Test: 0.98507
Epoch: 3900 Train: 0.35917 Test: 0.45094
Epoch 4000: New minimal relative error: 6.91%, model saved.
Epoch: 4000 Train: 0.35247 Test: 0.53977
Epoch: 4100 Train: 5.14445 Test: 6.07797
Epoch: 4200 Train: 3.36620 Test: 4.21280
Epoch: 4300 Train: 0.34859 Test: 0.54043
Epoch: 4400 Train: 0.40719 Test: 0.29115
Epoch: 4500 Train: 0.12865 Test: 0.18191
Epoch: 4600 Train: 0.34342 Test: 0.47799
Epoch: 4700 Train: 3.96489 Test: 5.34841
Epoch 4800: New minimal relative error: 5.88%, model saved.
Epoch: 4800 Train: 0.09724 Test: 0.14467
Epoch: 4900 Train: 0.60364 Test: 0.84865
Epoch: 5000 Train: 0.27817 Test: 0.26798
Epoch: 5100 Train: 0.14389 Test: 0.19847
Epoch: 5200 Train: 0.38457 Test: 0.58947
Epoch: 5300 Train: 0.73093 Test: 1.10223
Epoch: 5400 Train: 0.29230 Test: 0.38561
Epoch: 5500 Train: 1.30212 Test: 1.63927
Epoch: 5600 Train: 0.18990 Test: 0.28529
Epoch: 5700 Train: 1.13057 Test: 1.38441
Epoch: 5800 Train: 1.80831 Test: 2.42022
Epoch: 5900 Train: 1.33331 Test: 0.63405
Epoch: 6000 Train: 0.19785 Test: 0.24785
Epoch: 6100 Train: 0.53381 Test: 0.38842
Epoch: 6200 Train: 0.14783 Test: 0.26628
Epoch: 6300 Train: 0.06990 Test: 0.11115
Epoch: 6400 Train: 0.10908 Test: 0.12112
Epoch: 6500 Train: 0.10501 Test: 0.14806
Epoch: 6600 Train: 0.05831 Test: 0.08848
Epoch: 6700 Train: 0.06191 Test: 0.09682
Epoch: 6800 Train: 0.11711 Test: 0.33693
Epoch: 6900 Train: 0.13753 Test: 0.15090
Epoch: 7000 Train: 0.15783 Test: 0.13195
Epoch: 7100 Train: 0.27301 Test: 0.36466
Epoch: 7200 Train: 0.13164 Test: 0.11959
Epoch: 7300 Train: 0.28106 Test: 0.26086
Epoch: 7400 Train: 0.07571 Test: 0.11756
Epoch: 7500 Train: 0.82522 Test: 1.07745
Epoch: 7600 Train: 0.10887 Test: 0.15448
Epoch: 7700 Train: 0.27814 Test: 0.39967
Epoch: 7800 Train: 0.35626 Test: 0.38910
Epoch: 7900 Train: 0.04471 Test: 0.07109
Epoch 8000: New minimal relative error: 5.73%, model saved.
Epoch: 8000 Train: 0.04266 Test: 0.06962
Epoch: 8100 Train: 0.04632 Test: 0.07318
Epoch: 8200 Train: 0.04391 Test: 0.07206
Epoch: 8300 Train: 0.90025 Test: 0.62873
Epoch: 8400 Train: 0.03977 Test: 0.06576
Epoch: 8500 Train: 0.04038 Test: 0.06564
Epoch: 8600 Train: 0.04342 Test: 0.06985
Epoch: 8700 Train: 0.03780 Test: 0.06304
Epoch: 8800 Train: 0.05237 Test: 0.06982
Epoch: 8900 Train: 0.13395 Test: 0.18392
Epoch: 9000 Train: 0.05154 Test: 0.07924
Epoch: 9100 Train: 0.03602 Test: 0.06129
Epoch: 9200 Train: 0.21845 Test: 0.38869
Epoch: 9300 Train: 0.21638 Test: 0.10204
Epoch: 9400 Train: 0.08241 Test: 0.12389
Epoch: 9500 Train: 0.03403 Test: 0.05793
Epoch: 9600 Train: 0.04756 Test: 0.06171
Epoch: 9700 Train: 0.20397 Test: 0.17778
Epoch: 9800 Train: 0.03261 Test: 0.05584
Epoch: 9900 Train: 0.04055 Test: 0.06624
Epoch: 9999 Train: 0.06407 Test: 0.07340
Training Loss: tensor(0.0641)
Test Loss: tensor(0.0734)
Learned LE: [ 0.8359977  -0.02128245 -3.3067293 ]
True LE: [ 8.7682319e-01  3.7415139e-03 -1.4551999e+01]
Relative Error: [5.4070463  5.5032573  5.3646383  4.9140463  4.0746465  3.478549
 2.9054866  2.658882   2.6482387  3.1697729  3.6245368  3.8000379
 3.7228558  3.3287385  2.063839   1.2108237  1.4506496  2.2729883
 2.7098656  2.7227721  2.5685525  2.305112   2.2635295  2.1570601
 2.2360344  2.4303463  2.2789903  1.9803239  1.5595315  1.121127
 0.8570972  0.8532585  0.99533004 1.1731755  1.1587555  1.6865207
 2.3829532  2.9652972  3.0685775  3.0501595  2.58201    2.3945982
 2.1739266  1.9882768  1.7739196  1.748647   1.7875787  1.6881326
 1.4172264  1.3325306  1.2388949  1.4818661  1.8948764  2.3221588
 2.3843052  2.1887646  2.1175528  2.5057454  2.973551   3.4952466
 3.9681334  4.4678926  4.7753787  4.6674743  4.5144906  4.2732887
 3.5408423  3.008241   2.3734014  2.0824287  2.284543   2.709713
 3.3004444  3.673597   3.7575274  3.0553305  1.7989244  1.0182383
 1.801976   3.524306   4.3599586  4.7195272  4.2952023  3.542724
 2.8271909  2.3663962  2.0945754  2.0815177  2.0245285  1.8319659
 1.602715   1.2529243  0.83214355 0.7594226  1.0161432  1.2792985
 1.3844674  1.6410509  1.9443672  2.5291169  2.5712197  2.5751607
 2.263804   2.0629866  1.8964572  1.6935958  1.5030668  1.4298131
 1.3508272  1.3480719  1.2828612  1.063121   1.020927   1.0426294
 1.3875548  1.9590142  2.0972023  1.9717588  1.8898404  1.868602
 2.4180317  2.9378116  3.3203118  3.7979035  3.9455261  3.9274201
 3.748361   3.5742116  2.975092   2.4609878  2.074099   1.9273663
 1.9005237  2.183699   2.3665354  2.9206367  3.420475   2.713866
 1.6439776  1.0660558  2.380466   4.023537   5.394234   6.2841907
 6.475067   6.4135046  4.7443037  2.955616   2.3667562  2.21886
 1.9460804  1.6720418  1.4441683  1.2676418  0.93335426 0.6623763
 0.96096015 1.2105796  1.421396   1.6131961  1.5553557  1.9397637
 2.1468737  2.1550684  1.9570513  1.7029245  1.5850842  1.4384104
 1.3342443  1.1692439  0.9928036  0.865375   0.9864413  1.0192528
 0.9210892  0.7918408  0.90198547 1.4408443  1.7589436  1.7732097
 1.7549598  1.5431232  1.7076559  2.2838929  2.6788707  3.0104535
 3.0400407  3.1055725  3.0500066  2.9887493  2.533988   1.9317234
 1.7731538  1.9145478  1.6695046  1.6389042  1.5748593  1.9791445
 2.4933712  2.299267   1.5783896  1.0004566  2.447857   4.109787
 5.747844   6.8797536  7.5559483  7.5311275  6.8564177  5.5359826
 3.192316   2.5346382  2.3582692  2.0738752  1.5472735  1.1830404
 1.1071489  0.8930973  0.9284844  0.92311054 1.3009584  1.6105645
 1.3510537  1.4522192  1.6375953  1.745614   1.5938327  1.5043246
 1.1887116  1.1335298  1.1814489  1.0888988  0.8424131  0.56731516
 0.51383203 0.76137674 0.91135067 0.82942647 0.8244794  0.9367332
 1.2851124  1.4240742  1.3908972  1.305275   1.2530277  1.621552
 2.0838733  2.376984   2.3591428  2.2971888  2.3432257  2.2361825
 2.165485   1.7018296  1.1730409  1.8313768  1.8286493  1.5295291
 1.2105054  1.2995616  1.5690767  1.6698129  1.5526804  1.260826
 1.9647244  3.3322353  4.600803   5.691311   5.878537   6.1148057
 6.4005265  6.1409     5.013979   3.905297   3.231056   2.8782294
 2.351279   1.6418176  1.2076231  1.0633059  1.0945607  1.1672584
 0.9678567  1.3213373  1.4303489  1.132544   1.1671054  1.4082797
 1.4248716  1.3006306  1.0182117  0.841754   0.96908164 0.95154905
 0.72550637 0.4615908  0.21817626 0.32256815 0.69729257 0.64834106
 0.7643151  0.6794655  0.73361665 0.83092153 0.9418501  0.9132267
 0.9457285  1.0412499  1.3045962  1.7061664  1.9843949  1.7966787
 1.6996667  1.708395   1.6061584  1.534362   1.1110559  1.2244464
 1.8313421  1.970141   1.5471716  1.293931   1.2666293  1.2889004
 1.3484508  1.3000998  0.9502285  1.9771954  3.272889   4.275631
 4.3631716  4.5250897  5.0038314  5.320227   5.056751   4.1539154
 3.85298    3.811498   3.1489902  2.4165874  1.8476613  1.2481599
 0.9566257  1.0488765  1.1674458  1.3103337  1.2222105  1.054463
 0.82287955 0.82164097 1.0804101  1.0790113  0.96861005 0.68633825
 0.63315296 0.6866767  0.5985585  0.49702385 0.28103903 0.31476358
 0.36901963 0.48705927 0.47753438 0.5286587  0.60815454 0.4862905
 0.3766893  0.4924699  0.5862312  0.78429115 0.8808583  0.9728773
 1.3409789  1.4662502  1.3086871  1.2169855  1.1432058  1.0185524
 0.8757896  0.82737327 1.1673012  1.7667552  2.2089953  1.6513729
 1.4538214  1.5038575  1.3057673  1.2446407  1.1536801  0.8190889
 1.841266   2.9703221  3.0943913  3.191138   3.8076785  3.2808325
 3.0007908  2.7367196  3.0118756  3.0910208 ]
