time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 105.38%, model saved.
Epoch: 0 Train: 9443.49121 Test: 4157.92627
Epoch: 100 Train: 2401.20508 Test: 1076.56384
Epoch: 200 Train: 1904.30627 Test: 641.89612
Epoch: 300 Train: 837.40137 Test: 207.40839
Epoch: 400 Train: 474.09619 Test: 90.01088
Epoch 500: New minimal relative error: 71.00%, model saved.
Epoch: 500 Train: 195.59885 Test: 41.91643
Epoch 600: New minimal relative error: 11.63%, model saved.
Epoch: 600 Train: 94.42512 Test: 13.61956
Epoch 700: New minimal relative error: 3.87%, model saved.
Epoch: 700 Train: 55.45748 Test: 5.21083
Epoch: 800 Train: 45.65044 Test: 18.33629
Epoch: 900 Train: 43.03251 Test: 6.55577
Epoch: 1000 Train: 37.66635 Test: 2.48041
Epoch: 1100 Train: 60.69036 Test: 17.33056
Epoch: 1200 Train: 42.88132 Test: 12.64575
Epoch: 1300 Train: 51.92479 Test: 14.35235
Epoch: 1400 Train: 31.61527 Test: 2.82644
Epoch: 1500 Train: 29.96005 Test: 4.50667
Epoch: 1600 Train: 26.50443 Test: 3.53749
Epoch: 1700 Train: 40.56695 Test: 9.14610
Epoch: 1800 Train: 16.54218 Test: 1.16080
Epoch: 1900 Train: 20.89427 Test: 3.52641
Epoch: 2000 Train: 23.35846 Test: 7.01955
Epoch: 2100 Train: 22.58639 Test: 5.47713
Epoch: 2200 Train: 15.95613 Test: 1.47468
Epoch: 2300 Train: 29.09643 Test: 6.09236
Epoch: 2400 Train: 15.02840 Test: 2.91226
Epoch: 2500 Train: 12.48350 Test: 0.74454
Epoch: 2600 Train: 15.74224 Test: 3.74827
Epoch: 2700 Train: 18.54767 Test: 6.52721
Epoch: 2800 Train: 21.67696 Test: 8.69259
Epoch: 2900 Train: 10.26881 Test: 0.26808
Epoch: 3000 Train: 11.07290 Test: 0.52679
Epoch: 3100 Train: 12.36069 Test: 2.62794
Epoch: 3200 Train: 10.12911 Test: 1.01279
Epoch: 3300 Train: 13.92738 Test: 2.83333
Epoch: 3400 Train: 15.57490 Test: 3.95664
Epoch: 3500 Train: 17.30445 Test: 8.20694
Epoch: 3600 Train: 14.13842 Test: 4.56973
Epoch: 3700 Train: 11.57914 Test: 0.52535
Epoch: 3800 Train: 11.17430 Test: 3.07026
Epoch: 3900 Train: 7.55125 Test: 0.14132
Epoch: 4000 Train: 11.49778 Test: 3.19113
Epoch: 4100 Train: 10.62633 Test: 1.33623
Epoch: 4200 Train: 8.28001 Test: 1.09373
Epoch: 4300 Train: 12.51381 Test: 1.01781
Epoch: 4400 Train: 7.24702 Test: 0.63545
Epoch: 4500 Train: 6.11363 Test: 0.17287
Epoch: 4600 Train: 6.26821 Test: 0.47021
Epoch 4700: New minimal relative error: 3.07%, model saved.
Epoch: 4700 Train: 5.45355 Test: 0.45057
Epoch: 4800 Train: 5.57634 Test: 0.25649
Epoch: 4900 Train: 12.04842 Test: 5.32448
Epoch: 5000 Train: 5.41457 Test: 0.51644
Epoch: 5100 Train: 6.36215 Test: 1.39297
Epoch: 5200 Train: 9.18227 Test: 0.72899
Epoch: 5300 Train: 4.90761 Test: 0.43230
Epoch: 5400 Train: 4.98184 Test: 0.46533
Epoch: 5500 Train: 7.01328 Test: 3.24848
Epoch: 5600 Train: 7.64723 Test: 2.63009
Epoch: 5700 Train: 4.39159 Test: 0.45311
Epoch 5800: New minimal relative error: 2.77%, model saved.
Epoch: 5800 Train: 4.67351 Test: 0.22439
Epoch: 5900 Train: 5.63944 Test: 1.14179
Epoch: 6000 Train: 4.99643 Test: 0.73436
Epoch: 6100 Train: 4.62119 Test: 0.30055
Epoch: 6200 Train: 3.85677 Test: 0.11689
Epoch: 6300 Train: 4.01506 Test: 0.09513
Epoch 6400: New minimal relative error: 2.55%, model saved.
Epoch: 6400 Train: 3.90405 Test: 0.10752
Epoch: 6500 Train: 4.90462 Test: 0.48114
Epoch: 6600 Train: 5.12582 Test: 0.90232
Epoch: 6700 Train: 3.72327 Test: 0.06841
Epoch: 6800 Train: 5.01579 Test: 0.76474
Epoch: 6900 Train: 4.83128 Test: 0.72574
Epoch: 7000 Train: 5.43726 Test: 1.80674
Epoch: 7100 Train: 4.09072 Test: 0.45010
Epoch: 7200 Train: 3.47585 Test: 0.10119
Epoch: 7300 Train: 3.78444 Test: 0.30216
Epoch: 7400 Train: 3.84158 Test: 0.39974
Epoch 7500: New minimal relative error: 2.28%, model saved.
Epoch: 7500 Train: 3.36223 Test: 0.06353
Epoch: 7600 Train: 5.33284 Test: 2.15324
Epoch: 7700 Train: 3.37453 Test: 0.26083
Epoch: 7800 Train: 3.74652 Test: 0.90543
Epoch 7900: New minimal relative error: 1.96%, model saved.
Epoch: 7900 Train: 3.26802 Test: 0.29013
Epoch: 8000 Train: 3.88972 Test: 0.35519
Epoch: 8100 Train: 3.06079 Test: 0.07514
Epoch: 8200 Train: 5.00642 Test: 0.56376
Epoch: 8300 Train: 3.25756 Test: 0.35228
Epoch: 8400 Train: 3.08266 Test: 0.06519
Epoch: 8500 Train: 2.99710 Test: 0.06881
Epoch 8600: New minimal relative error: 1.92%, model saved.
Epoch: 8600 Train: 3.03488 Test: 0.04458
Epoch: 8700 Train: 3.97357 Test: 0.54121
Epoch: 8800 Train: 3.82404 Test: 1.42399
Epoch: 8900 Train: 4.90356 Test: 1.19493
Epoch 9000: New minimal relative error: 1.22%, model saved.
Epoch: 9000 Train: 2.79420 Test: 0.07645
Epoch: 9100 Train: 3.43942 Test: 0.17119
Epoch: 9200 Train: 3.61828 Test: 0.27301
Epoch: 9300 Train: 2.75085 Test: 0.10066
Epoch: 9400 Train: 2.76717 Test: 0.12997
Epoch: 9500 Train: 4.12455 Test: 1.25249
Epoch: 9600 Train: 4.38503 Test: 1.21287
Epoch: 9700 Train: 3.25971 Test: 0.46173
Epoch: 9800 Train: 2.60645 Test: 0.03801
Epoch: 9900 Train: 2.59140 Test: 0.05969
Epoch: 9999 Train: 2.95329 Test: 0.32069
Training Loss: tensor(2.9533)
Test Loss: tensor(0.3207)
Learned LE: [  0.9271278   -0.05326467 -14.545016  ]
True LE: [ 8.8119853e-01  4.5595951e-03 -1.4555761e+01]
Relative Error: [3.4848247  3.9048533  4.3450656  4.7984896  5.0590663  4.9479923
 4.565478   4.104913   3.678176   3.3010669  3.1426058  3.4215138
 3.743371   3.8357723  3.6446066  3.1874113  2.8068588  2.8713548
 3.0385962  3.1982794  3.245293   3.1616137  3.0222476  2.7830844
 2.4183822  2.0284164  1.5480354  1.3057299  1.2866175  1.1547025
 1.1445899  1.1594495  1.2002448  1.3634503  1.2127151  1.2439288
 1.5804698  1.9583504  2.4705863  2.9805567  3.2046254  3.0831268
 2.4368663  1.8418945  1.1395962  0.51135004 1.3499115  2.3909676
 3.1345818  3.6091833  3.5662546  3.3832495  3.033852   2.6753135
 2.5128999  2.3187022  2.2307906  2.289211   2.460391   2.7056272
 2.867223   3.0357502  3.3681154  3.7162476  4.0392246  4.3474393
 4.6278787  4.529409   4.195269   3.7029161  3.2519355  2.999975
 2.9205837  3.051106   3.401956   3.5383735  3.3948045  2.9391243
 2.6853554  2.5640662  2.7586153  2.9718945  3.0387423  2.9354022
 2.7785091  2.53196    2.2017581  1.82533    1.3642365  1.1746706
 1.1507616  1.0577265  1.0384951  1.0251864  1.1237241  1.2566632
 1.1237781  1.1315147  1.48931    1.8615878  2.4248562  2.9646006
 3.3851202  3.332929   2.7840185  2.2515085  1.2572764  0.42097634
 1.0470753  2.25909    3.3078392  3.6885042  3.7608807  3.4340065
 3.056299   2.7097235  2.469669   2.1995404  2.1147075  2.1341066
 2.220434   2.2918007  2.5018458  2.7990491  3.1804805  3.4792418
 3.6970205  3.8995504  4.1526604  4.0869026  3.6934502  3.2463882
 2.8887331  2.6477952  2.6427453  2.790751   3.0182352  3.1917236
 3.119081   2.717572   2.4984884  2.2410178  2.4944274  2.7264342
 2.8290837  2.7677472  2.450142   2.1715612  2.0235572  1.752978
 1.3373985  1.1153355  0.9755312  0.86062115 0.89066243 0.8572852
 1.0161568  1.1212097  1.0256943  1.0110705  1.2972531  1.7081187
 2.2778473  2.985758   3.4881532  3.5304222  3.0735877  2.6321437
 1.5952874  0.51372594 0.64604074 1.8928202  3.0420175  3.7335086
 3.9119675  3.540657   3.0162532  2.5797694  2.2724783  1.9925584
 1.9548347  1.9850545  2.0528512  2.0877953  2.1042461  2.4743485
 2.8766582  3.1714506  3.3305678  3.4280617  3.543738   3.512283
 3.148038   2.7902555  2.4321513  2.2513452  2.318986   2.5369139
 2.7162347  2.8032448  2.7915456  2.49191    2.289883   2.0106943
 2.1861205  2.4733448  2.6072917  2.545017   2.2655976  1.9287643
 1.8580234  1.6481731  1.2617034  1.0753902  0.9287573  0.75697035
 0.7055241  0.69040895 0.84907913 0.9617423  0.9665145  0.8726996
 1.0560212  1.5464576  2.1122959  2.8771124  3.4852545  3.633892
 3.2752435  2.9643939  1.9639711  0.7093641  0.39960602 1.3397187
 2.543787   3.564989   3.9235332  3.5981946  2.971263   2.4677367
 2.066524   1.7513486  1.7209496  1.8195994  1.8612864  1.8779656
 1.9073334  2.0288548  2.44393    2.747453   2.9314437  2.970637
 2.9385247  2.9015057  2.5694456  2.1943142  2.0492046  1.925692
 2.099688   2.3105268  2.5489914  2.5523608  2.5548923  2.3207865
 2.058279   1.8550845  1.7997622  2.1667688  2.3826778  2.308222
 2.1857982  1.8233471  1.7151673  1.5608252  1.1901648  0.98060566
 0.83913916 0.7350239  0.7022938  0.66665095 0.7320137  0.8023062
 0.9383808  0.79600066 0.8040221  1.1823623  1.7719129  2.599029
 3.3589032  3.6373813  3.370616   3.205471   2.3322875  1.0352622
 0.30744773 0.88771147 2.0261245  3.1585138  3.6959007  3.6629844
 3.0479813  2.4367683  1.8935366  1.5552077  1.5247266  1.6665511
 1.7291868  1.728188   1.7418543  1.8481424  2.0084128  2.2691603
 2.4560874  2.5227752  2.4102838  2.24216    1.9778807  1.8448244
 1.6506923  1.4792943  1.7336555  2.0047812  2.2022579  2.373578
 2.2905426  2.1387284  1.9810385  1.7463641  1.4752517  1.7909864
 2.0407798  2.1230123  2.0851488  1.8382642  1.6215043  1.475563
 1.1598599  0.8858159  0.8883667  0.7403128  0.6760727  0.57422453
 0.6763836  0.66617113 0.77926165 0.8026246  0.63888687 0.8088885
 1.4934852  2.1339042  2.9752455  3.4752123  3.3949404  3.3110878
 2.6693332  1.5150253  0.5637418  0.5625479  1.503326   2.370581
 3.1603005  3.4770389  3.1906817  2.474999   1.7625356  1.4068086
 1.3392806  1.4783132  1.5724385  1.621508   1.5995909  1.6558889
 1.7809147  1.9447625  2.0688853  2.1286545  2.0557704  1.8282139
 1.6417395  1.5436497  1.2341542  0.97530687 1.1451873  1.5138401
 1.7616285  1.993122   2.1251173  1.9204007  1.8175218  1.6852388
 1.528376   1.3292438  1.580243   1.870549   1.912231   1.8164841
 1.6961176  1.5121257  1.2581024  1.0364372 ]
