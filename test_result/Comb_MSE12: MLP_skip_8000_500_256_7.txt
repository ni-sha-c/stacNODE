time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 102.73%, model saved.
Epoch: 0 Train: 3692.12866 Test: 4276.28809
Epoch: 80 Train: 541.12408 Test: 774.80347
Epoch 160: New minimal relative error: 85.49%, model saved.
Epoch: 160 Train: 81.24441 Test: 100.02271
Epoch 240: New minimal relative error: 28.80%, model saved.
Epoch: 240 Train: 32.80258 Test: 43.97202
Epoch: 320 Train: 13.89437 Test: 18.24279
Epoch: 400 Train: 68.46103 Test: 105.47745
Epoch: 480 Train: 14.01434 Test: 14.46702
Epoch 560: New minimal relative error: 20.76%, model saved.
Epoch: 560 Train: 18.39949 Test: 27.00874
Epoch 640: New minimal relative error: 14.38%, model saved.
Epoch: 640 Train: 5.62388 Test: 8.48710
Epoch: 720 Train: 7.09973 Test: 7.83205
Epoch 800: New minimal relative error: 14.16%, model saved.
Epoch: 800 Train: 5.20582 Test: 7.45654
Epoch: 880 Train: 4.34513 Test: 7.05164
Epoch: 960 Train: 13.73651 Test: 19.14067
Epoch: 1040 Train: 6.15345 Test: 8.29393
Epoch: 1120 Train: 3.33648 Test: 5.91177
Epoch: 1200 Train: 11.93626 Test: 14.85566
Epoch: 1280 Train: 5.74652 Test: 9.96151
Epoch: 1360 Train: 16.82021 Test: 21.21830
Epoch: 1440 Train: 2.35853 Test: 4.00044
Epoch: 1520 Train: 4.50300 Test: 7.97887
Epoch: 1600 Train: 4.04032 Test: 6.24703
Epoch: 1680 Train: 1.21147 Test: 3.26536
Epoch 1760: New minimal relative error: 12.87%, model saved.
Epoch: 1760 Train: 1.06207 Test: 2.85765
Epoch: 1840 Train: 1.25774 Test: 3.04692
Epoch: 1920 Train: 1.13307 Test: 2.79139
Epoch: 2000 Train: 10.54087 Test: 12.09875
Epoch 2080: New minimal relative error: 11.12%, model saved.
Epoch: 2080 Train: 3.05494 Test: 5.43786
Epoch: 2160 Train: 1.76864 Test: 3.56122
Epoch: 2240 Train: 3.49500 Test: 4.95974
Epoch 2320: New minimal relative error: 6.87%, model saved.
Epoch: 2320 Train: 0.95830 Test: 2.65821
Epoch: 2400 Train: 0.75359 Test: 2.30902
Epoch: 2480 Train: 0.62356 Test: 2.07080
Epoch: 2560 Train: 0.88992 Test: 2.46594
Epoch: 2640 Train: 4.12348 Test: 6.12374
Epoch: 2720 Train: 0.67224 Test: 2.15160
Epoch: 2800 Train: 0.54068 Test: 1.93364
Epoch: 2880 Train: 0.53488 Test: 1.93254
Epoch: 2960 Train: 0.52823 Test: 2.01659
Epoch: 3040 Train: 1.74521 Test: 3.62287
Epoch: 3120 Train: 0.76859 Test: 2.15542
Epoch: 3200 Train: 1.34962 Test: 2.72393
Epoch: 3280 Train: 6.63959 Test: 9.09366
Epoch 3360: New minimal relative error: 5.94%, model saved.
Epoch: 3360 Train: 0.40854 Test: 1.69673
Epoch: 3440 Train: 1.36347 Test: 3.05615
Epoch: 3520 Train: 0.38276 Test: 1.64916
Epoch: 3600 Train: 0.51282 Test: 1.87433
Epoch: 3680 Train: 0.36454 Test: 1.57294
Epoch: 3760 Train: 0.35996 Test: 1.66572
Epoch: 3840 Train: 0.72886 Test: 2.11944
Epoch: 3920 Train: 3.24981 Test: 4.45277
Epoch: 4000 Train: 0.78704 Test: 1.81908
Epoch: 4080 Train: 5.62076 Test: 8.39803
Epoch: 4160 Train: 0.30854 Test: 1.49452
Epoch: 4240 Train: 0.65996 Test: 1.80266
Epoch: 4320 Train: 0.36894 Test: 1.89461
Epoch: 4400 Train: 0.29082 Test: 1.43043
Epoch: 4480 Train: 0.28556 Test: 1.47819
Epoch: 4560 Train: 0.39092 Test: 1.52423
Epoch: 4640 Train: 0.31655 Test: 1.50068
Epoch: 4720 Train: 0.36661 Test: 1.47278
Epoch 4800: New minimal relative error: 5.68%, model saved.
Epoch: 4800 Train: 0.31109 Test: 1.50008
Epoch: 4880 Train: 0.66991 Test: 1.93883
Epoch: 4960 Train: 0.31236 Test: 1.41499
Epoch: 5040 Train: 0.24623 Test: 1.35435
Epoch: 5120 Train: 0.64212 Test: 1.68082
Epoch: 5200 Train: 0.84566 Test: 2.32425
Epoch: 5280 Train: 0.42649 Test: 1.54272
Epoch: 5360 Train: 0.49224 Test: 1.68194
Epoch: 5440 Train: 0.26109 Test: 1.30465
Epoch: 5520 Train: 0.21822 Test: 1.25715
Epoch: 5600 Train: 0.22635 Test: 1.31786
Epoch: 5680 Train: 0.24036 Test: 1.26641
Epoch: 5760 Train: 0.20795 Test: 1.24567
Epoch: 5840 Train: 0.24736 Test: 1.24227
Epoch: 5920 Train: 0.20398 Test: 1.23346
Epoch: 6000 Train: 3.45706 Test: 4.10211
Epoch: 6080 Train: 0.19116 Test: 1.20275
Epoch: 6160 Train: 0.51751 Test: 1.61165
Epoch 6240: New minimal relative error: 5.00%, model saved.
Epoch: 6240 Train: 0.18521 Test: 1.19618
Epoch: 6320 Train: 0.72723 Test: 1.74977
Epoch: 6400 Train: 0.18152 Test: 1.15209
Epoch: 6480 Train: 0.18313 Test: 1.16299
Epoch: 6560 Train: 0.25620 Test: 1.13765
Epoch: 6640 Train: 0.21640 Test: 1.20962
Epoch: 6720 Train: 0.20231 Test: 1.15086
Epoch: 6800 Train: 0.16771 Test: 1.12463
Epoch: 6880 Train: 0.43995 Test: 1.38182
Epoch: 6960 Train: 0.21086 Test: 1.20403
Epoch: 7040 Train: 0.17985 Test: 1.17209
Epoch: 7120 Train: 0.64786 Test: 1.46341
Epoch: 7200 Train: 0.18365 Test: 1.08965
Epoch: 7280 Train: 0.17592 Test: 1.10892
Epoch: 7360 Train: 0.15424 Test: 1.08657
Epoch: 7440 Train: 0.15679 Test: 1.08193
Epoch: 7520 Train: 0.26411 Test: 1.15524
Epoch: 7600 Train: 0.20165 Test: 1.14803
Epoch: 7680 Train: 0.14678 Test: 1.06161
Epoch 7760: New minimal relative error: 4.42%, model saved.
Epoch: 7760 Train: 0.46840 Test: 1.41663
Epoch: 7840 Train: 0.14326 Test: 1.04751
Epoch: 7920 Train: 0.14300 Test: 1.07024
Epoch: 7999 Train: 0.14032 Test: 1.03715
Training Loss: tensor(0.1403)
Test Loss: tensor(1.0371)
Learned LE: [ 0.8335307  -0.05930697 -2.6617522 ]
True LE: [ 8.7579572e-01  1.9328307e-03 -1.4549075e+01]
Relative Error: [5.3552237  5.756427   6.1507144  6.6041136  7.2138853  7.867644
 8.634015   8.9881735  8.402612   7.6993003  7.4007025  7.570085
 8.197449   7.756895   6.7527237  6.255547   6.1004467  6.0702252
 6.052701   6.0267015  6.0736113  5.880152   5.552883   5.281859
 5.2899647  5.7144065  5.5871615  5.462015   4.6944633  3.8963351
 3.4896293  3.2488596  3.3671298  4.00969    5.7628007  5.8870735
 5.525448   5.6791058  5.659302   5.8879747  6.346896   6.6276536
 6.545214   6.0842876  5.0795884  4.123366   3.4769423  3.2165747
 3.380141   3.5684004  3.6701884  3.8118918  4.0301924  4.204979
 3.839254   3.5269394  3.572874   3.7343442  3.8965027  4.158067
 4.3460345  4.1085854  3.9904573  4.1165037  4.4732976  4.9150815
 5.288864   5.8781247  6.5930367  7.430666   7.9857564  7.386735
 6.721425   6.718755   7.372608   8.083736   6.969656   6.2219214
 5.5815864  5.3221006  5.4562054  5.4952884  5.592455   5.597151
 5.3927345  5.09392    4.86755    4.8495436  5.3093853  5.5810204
 5.3213315  4.679963   3.7268353  3.3495426  3.1750276  3.444669
 4.2266874  5.83332    5.4782314  5.049316   5.158058   5.128065
 5.197252   5.5128074  5.663455   5.544482   5.088711   4.0310707
 3.2047958  2.8371105  2.7478426  2.9068937  3.0508997  3.1378903
 3.19022    3.3528652  3.3323462  3.135191   3.0177329  3.0665953
 3.1620495  3.3772414  3.6990829  3.8990211  3.3402104  3.0046258
 2.9658017  3.209899   3.6656518  3.9814658  4.5333314  5.2081423
 6.0175543  6.628119   6.393937   5.9609923  6.235254   7.2262435
 7.6357594  6.643662   5.967118   5.3782434  4.989392   5.009439
 5.1469626  5.317663   5.2640696  5.135677   4.793883   4.589678
 4.403877   4.8557153  5.2722087  5.0274744  4.6702247  3.903879
 3.5489333  3.3781831  3.3718207  4.164498   5.326768   5.1584077
 4.786678   4.697625   4.3368225  4.370426   4.702608   4.9835653
 4.774923   4.18841    3.306353   2.8290021  2.4642336  2.425385
 2.579897   2.7341175  2.7639334  2.7453392  2.7685702  2.7873592
 2.7099993  2.8822558  2.7603579  2.7469935  2.8726618  3.2077777
 3.433096   2.7801743  2.3426075  2.0788598  2.1783714  2.5036972
 2.786722   3.095201   3.7774415  4.5335526  5.160764   5.2915754
 5.3228745  5.9964175  6.3690996  7.186596   6.5989366  6.1008716
 5.5614157  5.334114   5.0346437  4.897103   4.9995503  5.1533904
 5.1709538  4.647807   4.1509786  3.8880136  4.118021   4.4237695
 4.939493   4.621716   4.2814384  3.7555819  3.6335092  3.4611542
 3.672654   4.4203997  4.7414794  4.5457373  4.34545    3.7280853
 3.5914204  3.8713567  4.376682   4.294355   3.361664   3.073907
 2.7736657  2.5331032  2.3529353  2.6055093  2.6156197  2.5805008
 2.5472484  2.7066355  2.7288747  2.581191   2.6431642  2.8245807
 2.6726837  2.5945566  2.609855   2.758614   2.4508924  1.9289496
 1.5468684  1.2895982  1.5161707  1.8160012  1.9338956  2.2682512
 2.9696684  3.6416483  4.034338   4.383912   5.213958   5.3700185
 6.239201   7.053109   6.482988   6.132748   5.8816524  5.202083
 4.985892   4.6073256  4.717715   4.799943   4.4944897  4.055231
 3.8748999  3.6188846  3.573462   3.8555474  4.4180226  4.483467
 4.3162255  3.5128176  3.5715606  3.5814443  3.8823838  3.9205737
 4.071656   3.829678   3.3538985  2.8222258  3.0195765  3.7010972
 3.9399028  3.3033855  2.8211558  2.8139915  2.5232449  2.1895175
 2.6179564  2.5680249  2.3515666  2.2951076  2.4893537  2.5118847
 2.5815794  2.5730379  2.61871    2.8400774  2.4718294  2.3616552
 2.4018555  2.471729   1.847473   1.4000335  1.0975045  0.6855732
 0.8391339  1.1348749  1.1712668  1.404894   2.05569    2.6528678
 3.273738   4.522288   4.5450287  5.1144915  6.340043   7.167848
 6.7890086  6.325709   6.0414014  5.634985   5.231725   4.582778
 4.379308   4.264597   3.8032393  3.4614286  3.3992064  3.3280525
 3.1895955  3.201824   3.5728369  4.118322   4.2626576  3.2111187
 3.2743685  3.46605    3.3205032  3.1020472  3.1912842  2.8794818
 2.5557094  2.129397   2.9277124  3.5473876  3.2910373  2.822522
 2.3552907  2.380308   2.2122078  2.1408682  2.4410641  2.3659215
 2.2209167  2.353161   2.2775764  2.24437    2.4564207  2.4722962
 2.475415   2.5582175  2.522424   1.9741768  2.0302029  2.3037684
 1.8682665  1.3570143  1.2234038  0.8360858  0.67743534 0.58766246
 0.5756937  0.6370713  1.0375077  1.656917   2.802786   4.3817267
 4.2569604  4.5884924  6.0947037  7.545724   6.5569344  5.7117257
 5.3483753  4.801463   4.139056   3.614915  ]
