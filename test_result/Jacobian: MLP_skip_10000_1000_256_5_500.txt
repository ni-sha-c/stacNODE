time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.90%, model saved.
Epoch: 0 Train: 31945.15234 Test: 3798.29248
Epoch 100: New minimal relative error: 90.34%, model saved.
Epoch: 100 Train: 9078.16016 Test: 1394.80579
Epoch 200: New minimal relative error: 66.12%, model saved.
Epoch: 200 Train: 7993.51465 Test: 1167.57642
Epoch: 300 Train: 7986.09766 Test: 1116.16113
Epoch: 400 Train: 8102.55371 Test: 1106.87451
Epoch 500: New minimal relative error: 62.68%, model saved.
Epoch: 500 Train: 6859.33350 Test: 954.95239
Epoch: 600 Train: 4538.22705 Test: 449.68710
Epoch: 700 Train: 2420.03613 Test: 243.81297
Epoch 800: New minimal relative error: 55.60%, model saved.
Epoch: 800 Train: 1009.50714 Test: 36.52180
Epoch: 900 Train: 705.45239 Test: 21.62881
Epoch 1000: New minimal relative error: 18.54%, model saved.
Epoch: 1000 Train: 621.28668 Test: 17.58881
Epoch 1100: New minimal relative error: 11.23%, model saved.
Epoch: 1100 Train: 572.82507 Test: 20.20448
Epoch: 1200 Train: 551.63245 Test: 15.75782
Epoch: 1300 Train: 421.33563 Test: 10.47644
Epoch: 1400 Train: 412.87946 Test: 67.21034
Epoch: 1500 Train: 355.87283 Test: 7.42450
Epoch: 1600 Train: 380.09006 Test: 10.17908
Epoch: 1700 Train: 288.79483 Test: 5.32827
Epoch: 1800 Train: 272.03818 Test: 8.05027
Epoch: 1900 Train: 274.74533 Test: 10.25311
Epoch: 2000 Train: 253.08136 Test: 13.00889
Epoch: 2100 Train: 235.33018 Test: 4.84822
Epoch: 2200 Train: 208.44205 Test: 3.05578
Epoch 2300: New minimal relative error: 10.62%, model saved.
Epoch: 2300 Train: 191.93883 Test: 2.60603
Epoch: 2400 Train: 194.03070 Test: 6.87011
Epoch: 2500 Train: 180.51494 Test: 2.44592
Epoch 2600: New minimal relative error: 5.17%, model saved.
Epoch: 2600 Train: 173.24545 Test: 2.37469
Epoch 2700: New minimal relative error: 5.04%, model saved.
Epoch: 2700 Train: 170.15086 Test: 2.41468
Epoch: 2800 Train: 173.14598 Test: 2.45073
Epoch: 2900 Train: 159.12465 Test: 2.19850
Epoch: 3000 Train: 143.39955 Test: 2.88093
Epoch: 3100 Train: 154.43547 Test: 9.12829
Epoch: 3200 Train: 130.81755 Test: 1.41880
Epoch: 3300 Train: 129.49806 Test: 1.67714
Epoch: 3400 Train: 154.92593 Test: 2.52204
Epoch: 3500 Train: 125.37315 Test: 1.25141
Epoch: 3600 Train: 124.72476 Test: 1.30124
Epoch: 3700 Train: 121.64019 Test: 1.19757
Epoch: 3800 Train: 123.54677 Test: 1.32250
Epoch: 3900 Train: 124.99221 Test: 2.18988
Epoch: 4000 Train: 131.38226 Test: 3.10473
Epoch: 4100 Train: 128.66641 Test: 1.48112
Epoch: 4200 Train: 117.64535 Test: 1.16637
Epoch: 4300 Train: 120.27710 Test: 1.53406
Epoch: 4400 Train: 116.18797 Test: 2.34500
Epoch: 4500 Train: 114.69865 Test: 1.01293
Epoch: 4600 Train: 119.07280 Test: 1.91717
Epoch: 4700 Train: 101.67238 Test: 0.94768
Epoch: 4800 Train: 112.94762 Test: 2.46511
Epoch: 4900 Train: 102.34214 Test: 1.03712
Epoch 5000: New minimal relative error: 4.52%, model saved.
Epoch: 5000 Train: 101.29713 Test: 1.05232
Epoch: 5100 Train: 97.09039 Test: 1.73843
Epoch: 5200 Train: 98.42950 Test: 0.97245
Epoch: 5300 Train: 92.30223 Test: 0.81494
Epoch 5400: New minimal relative error: 3.90%, model saved.
Epoch: 5400 Train: 96.28207 Test: 1.03494
Epoch: 5500 Train: 94.66399 Test: 1.01150
Epoch: 5600 Train: 84.84686 Test: 0.75984
Epoch: 5700 Train: 90.72324 Test: 0.82564
Epoch: 5800 Train: 82.39416 Test: 0.70251
Epoch: 5900 Train: 78.23035 Test: 0.74561
Epoch: 6000 Train: 79.10751 Test: 0.78921
Epoch: 6100 Train: 76.82856 Test: 0.67834
Epoch: 6200 Train: 74.87292 Test: 0.58239
Epoch: 6300 Train: 78.12616 Test: 5.28616
Epoch: 6400 Train: 69.82181 Test: 0.58292
Epoch: 6500 Train: 70.24226 Test: 0.74031
Epoch: 6600 Train: 69.82446 Test: 0.62670
Epoch: 6700 Train: 69.43358 Test: 0.63289
Epoch: 6800 Train: 69.30587 Test: 1.52332
Epoch: 6900 Train: 73.26307 Test: 0.67105
Epoch: 7000 Train: 73.41281 Test: 0.71256
Epoch 7100: New minimal relative error: 3.81%, model saved.
Epoch: 7100 Train: 72.19059 Test: 0.66938
Epoch: 7200 Train: 73.92746 Test: 1.57872
Epoch: 7300 Train: 74.74868 Test: 2.40777
Epoch: 7400 Train: 71.31621 Test: 0.60926
Epoch: 7500 Train: 70.06600 Test: 0.61570
Epoch: 7600 Train: 67.69181 Test: 0.88290
Epoch: 7700 Train: 66.25535 Test: 0.81198
Epoch 7800: New minimal relative error: 3.17%, model saved.
Epoch: 7800 Train: 62.84124 Test: 0.55434
Epoch: 7900 Train: 62.97284 Test: 1.10214
Epoch: 8000 Train: 61.42816 Test: 0.52379
Epoch: 8100 Train: 61.21774 Test: 0.49450
Epoch: 8200 Train: 61.31880 Test: 0.50997
Epoch: 8300 Train: 60.41470 Test: 0.47249
Epoch: 8400 Train: 60.00939 Test: 0.48186
Epoch: 8500 Train: 58.25550 Test: 0.54321
Epoch: 8600 Train: 58.11179 Test: 0.77626
Epoch: 8700 Train: 54.69103 Test: 0.42339
Epoch 8800: New minimal relative error: 2.86%, model saved.
Epoch: 8800 Train: 53.52366 Test: 0.37654
Epoch: 8900 Train: 53.67517 Test: 0.53946
Epoch: 9000 Train: 53.60758 Test: 0.36830
Epoch: 9100 Train: 53.99995 Test: 0.43487
Epoch: 9200 Train: 55.82581 Test: 0.45625
Epoch: 9300 Train: 57.69503 Test: 0.50010
Epoch: 9400 Train: 59.24409 Test: 0.57350
Epoch: 9500 Train: 59.24439 Test: 0.54742
Epoch: 9600 Train: 59.26291 Test: 0.53946
Epoch: 9700 Train: 58.34376 Test: 0.50842
Epoch: 9800 Train: 59.92863 Test: 0.55716
Epoch: 9900 Train: 57.61320 Test: 0.51634
Epoch: 9999 Train: 60.02950 Test: 0.60373
Training Loss: tensor(60.0295)
Test Loss: tensor(0.6037)
Learned LE: [  0.83418816   0.02409671 -14.538886  ]
True LE: [ 8.8015002e-01 -4.0863231e-03 -1.4551449e+01]
Relative Error: [1.1124866  1.4016852  1.3585398  1.2451847  1.3232994  1.3915076
 1.3416687  1.3152893  1.493212   1.4226124  1.2284023  1.2719607
 1.2969801  1.1908947  0.93054056 0.7110727  1.0065513  1.8255985
 2.720599   3.4542844  4.4338627  4.2209353  3.9578352  3.963974
 3.926109   3.770815   3.5930097  3.6414683  3.5358806  3.462582
 3.580322   3.4542625  3.4753208  3.6696508  3.543567   2.6927495
 2.1137948  1.784278   1.6091896  1.7647207  2.3371687  2.0820746
 1.4003736  0.80471617 0.33385152 0.16943741 0.22557977 0.27441293
 0.48114982 0.79893684 1.0786713  1.0921018  1.408793   1.7733045
 2.042094   2.2749586  2.4686427  2.3979619  2.1755903  1.8686335
 1.4990064  1.1979536  1.158987   1.246739   1.3874805  1.029624
 1.0415329  1.1174462  1.1257445  1.1550646  1.360302   1.341927
 1.0238178  1.0014546  1.0797716  1.0956278  0.963496   0.80044615
 0.9955169  1.7289048  2.508228   3.3304121  4.244701   3.9367304
 3.6422267  3.6457245  3.765466   3.7263896  3.3877432  3.2285345
 3.2211423  3.0134416  2.8352487  2.83663    3.1096115  3.2161806
 3.38279    2.806122   2.053463   1.5679445  1.3038919  1.5443879
 2.0273035  2.4013398  1.5826216  0.94894004 0.44179302 0.08721507
 0.2220833  0.38011542 0.48118028 0.8725984  0.92578346 0.8174383
 1.0171437  1.3807708  1.6780314  1.8775351  2.1094172  2.4056647
 2.5186212  2.1727166  1.7109541  1.3088968  1.1685902  1.1849874
 1.3392885  1.2533854  0.950054   0.86912    0.8657012  0.953596
 1.1819775  1.2859375  1.1297699  0.7877414  0.83714354 0.99636114
 1.0016116  0.9017971  0.94859254 1.5257274  2.210454   2.9908183
 3.9382923  3.6837215  3.402684   3.3588972  3.5295117  3.300804
 3.128383   3.066938   3.1435006  2.9651582  2.494324   2.2851982
 2.0810056  2.4670439  2.943982   3.1379313  2.1928294  1.511615
 1.1237372  1.372225   1.7308067  2.2530086  1.9072341  1.1747758
 0.617147   0.28302896 0.37869707 0.37969804 0.6189032  1.0729238
 0.99923384 0.7387507  0.75917137 0.9846991  1.2606031  1.4852906
 1.6632749  1.868943   2.182015   2.3031921  2.133505   1.6073612
 1.2647291  1.1234518  1.1174241  1.211695   1.2508433  0.95505047
 0.76421005 0.81338954 1.0379192  1.2127333  1.4129728  0.92583287
 0.57290393 0.82678914 0.97006506 0.9522264  0.92913955 1.2573042
 1.8417882  2.5292664  3.4751606  3.4659092  3.1441648  3.0997505
 3.1041045  2.9096982  2.8762338  2.9936094  3.100895   3.0785747
 2.5974863  2.1924713  1.7874923  1.4572957  1.9280242  2.72223
 2.5978398  1.6893538  1.1445817  1.2693684  1.4883324  1.8297458
 2.386406   1.5314759  0.87190557 0.51860154 0.3889363  0.4422087
 0.72224265 1.2163664  1.1662341  0.80339545 0.62663496 0.7101822
 0.8937273  1.1599028  1.3440071  1.4142042  1.560984   1.7723147
 1.8977791  2.067435   1.6505364  1.2870578  1.0767744  1.1111742
 1.0603932  1.1261708  1.0728545  0.9674215  0.9876423  1.2126768
 1.244918   1.2431488  0.56855434 0.53415173 0.87746155 1.0037271
 0.9378586  1.0345861  1.4885674  2.015224   2.814628   3.3796146
 2.890354   2.7803512  2.6900535  2.6053417  2.536391   2.782754
 3.0915756  3.157828   2.8808389  2.3682165  2.0705235  1.4565262
 1.0619503  1.4216472  2.4153633  2.1937282  1.3094721  1.1879536
 1.254998   1.4237098  1.8470887  2.0373366  1.249612   0.8152597
 0.46590367 0.43825567 0.6926787  1.1943427  1.2769465  0.9243204
 0.74807066 0.7132729  0.67494416 0.87518674 1.1147821  1.1940898
 1.1328968  1.1767569  1.3032304  1.4051728  1.7677135  1.8169073
 1.3862348  1.1369075  0.98873097 0.9098056  1.0307455  1.0516651
 1.2696588  1.1888566  1.2087437  1.2919437  0.9002262  0.41557992
 0.6137808  0.97018915 1.0208371  0.9026372  1.0732423  1.5312346
 2.1200926  3.037351   2.835373   2.4485402  2.269167   2.1849108
 2.3547995  2.3630235  2.7853608  3.1054788  2.9896955  2.7379906
 2.3991818  2.0752113  1.3827066  0.8938185  0.88166755 1.8595957
 1.8487943  1.304093   1.1263227  1.0798402  1.2244827  1.7472551
 1.7759936  1.1225559  0.58515847 0.40881425 0.5939604  1.0209571
 1.4483643  1.0722497  0.8682316  0.8775806  0.61836743 0.7454339
 0.9290071  0.97145885 0.91880774 0.76001185 0.733517   0.8565459
 1.0393255  1.4454099  1.8535535  1.7420504  1.3276628  0.99225354
 0.81679916 0.96242297 1.057923   1.2082446  1.4222049  1.2649465
 1.1719595  0.59801054 0.3733332  0.6940663  1.0213469  1.003574
 0.77231085 0.91777825 1.3766513  2.0700037  2.911658   2.4306924
 2.1193988  1.583006   1.8208332  2.1534226 ]
