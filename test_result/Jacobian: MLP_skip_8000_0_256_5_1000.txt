time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.79%, model saved.
Epoch: 0 Train: 58948.31250 Test: 3945.31641
Epoch: 80 Train: 15501.04297 Test: 1593.10913
Epoch 160: New minimal relative error: 95.91%, model saved.
Epoch: 160 Train: 13762.47852 Test: 1428.38428
Epoch: 240 Train: 13251.42969 Test: 1156.86304
Epoch: 320 Train: 13344.36816 Test: 1384.09839
Epoch 400: New minimal relative error: 59.27%, model saved.
Epoch: 400 Train: 13097.20215 Test: 1241.29553
Epoch: 480 Train: 13514.05859 Test: 1300.14111
Epoch: 560 Train: 12618.35742 Test: 1251.14929
Epoch: 640 Train: 11907.25977 Test: 1246.74426
Epoch 720: New minimal relative error: 55.49%, model saved.
Epoch: 720 Train: 12650.99707 Test: 1090.31824
Epoch: 800 Train: 10971.48730 Test: 947.93634
Epoch: 880 Train: 9604.29004 Test: 750.98486
Epoch: 960 Train: 8593.66895 Test: 584.15051
Epoch: 1040 Train: 6308.19775 Test: 330.17133
Epoch: 1120 Train: 4829.83203 Test: 196.86740
Epoch: 1200 Train: 3211.11011 Test: 111.05453
Epoch: 1280 Train: 2470.46362 Test: 113.63433
Epoch: 1360 Train: 1885.92224 Test: 43.23466
Epoch 1440: New minimal relative error: 23.54%, model saved.
Epoch: 1440 Train: 1728.47607 Test: 47.39274
Epoch: 1520 Train: 1751.86511 Test: 49.27949
Epoch 1600: New minimal relative error: 21.93%, model saved.
Epoch: 1600 Train: 1600.18518 Test: 42.88015
Epoch: 1680 Train: 1380.80701 Test: 28.98468
Epoch: 1760 Train: 1158.00952 Test: 27.71861
Epoch 1840: New minimal relative error: 12.47%, model saved.
Epoch: 1840 Train: 932.20300 Test: 16.70130
Epoch: 1920 Train: 817.84943 Test: 12.42898
Epoch: 2000 Train: 736.73969 Test: 11.30123
Epoch: 2080 Train: 691.38940 Test: 10.01768
Epoch: 2160 Train: 658.97870 Test: 12.50311
Epoch: 2240 Train: 632.84583 Test: 19.71807
Epoch: 2320 Train: 619.09570 Test: 23.39729
Epoch 2400: New minimal relative error: 5.11%, model saved.
Epoch: 2400 Train: 506.24945 Test: 5.05598
Epoch: 2480 Train: 473.16049 Test: 7.55733
Epoch: 2560 Train: 509.87933 Test: 10.30338
Epoch 2640: New minimal relative error: 4.41%, model saved.
Epoch: 2640 Train: 415.78671 Test: 5.07036
Epoch: 2720 Train: 402.70700 Test: 5.56905
Epoch: 2800 Train: 362.14587 Test: 4.64618
Epoch: 2880 Train: 359.51794 Test: 10.41173
Epoch: 2960 Train: 327.48663 Test: 3.08223
Epoch: 3040 Train: 287.43683 Test: 2.52818
Epoch: 3120 Train: 292.61765 Test: 11.27207
Epoch: 3200 Train: 256.45569 Test: 1.81401
Epoch: 3280 Train: 245.54767 Test: 1.86775
Epoch: 3360 Train: 261.52939 Test: 3.12700
Epoch: 3440 Train: 268.30963 Test: 4.02613
Epoch: 3520 Train: 231.92813 Test: 1.63545
Epoch: 3600 Train: 221.12875 Test: 1.82526
Epoch: 3680 Train: 205.71588 Test: 1.46132
Epoch: 3760 Train: 201.68599 Test: 1.85612
Epoch: 3840 Train: 207.64519 Test: 1.80472
Epoch 3920: New minimal relative error: 3.88%, model saved.
Epoch: 3920 Train: 211.37067 Test: 1.67813
Epoch: 4000 Train: 204.64804 Test: 1.86043
Epoch: 4080 Train: 213.84021 Test: 1.87301
Epoch: 4160 Train: 200.85100 Test: 1.26877
Epoch: 4240 Train: 187.99620 Test: 1.44810
Epoch: 4320 Train: 186.56561 Test: 1.15983
Epoch: 4400 Train: 194.36923 Test: 6.92406
Epoch 4480: New minimal relative error: 3.01%, model saved.
Epoch: 4480 Train: 184.83862 Test: 1.10152
Epoch: 4560 Train: 178.30785 Test: 1.04171
Epoch: 4640 Train: 175.52849 Test: 0.94410
Epoch: 4720 Train: 172.87651 Test: 0.96507
Epoch 4800: New minimal relative error: 1.25%, model saved.
Epoch: 4800 Train: 166.31209 Test: 0.96658
Epoch: 4880 Train: 158.82237 Test: 0.79805
Epoch: 4960 Train: 154.50340 Test: 1.70440
Epoch: 5040 Train: 151.51961 Test: 2.56802
Epoch: 5120 Train: 149.15157 Test: 1.20680
Epoch: 5200 Train: 150.56155 Test: 0.88221
Epoch: 5280 Train: 166.16994 Test: 4.79777
Epoch: 5360 Train: 193.96806 Test: 20.34725
Epoch: 5440 Train: 180.24350 Test: 1.45079
Epoch: 5520 Train: 178.34967 Test: 1.52239
Epoch: 5600 Train: 171.04788 Test: 1.72708
Epoch: 5680 Train: 167.85144 Test: 1.73086
Epoch: 5760 Train: 177.36116 Test: 1.88096
Epoch: 5840 Train: 207.90369 Test: 3.72920
Epoch: 5920 Train: 172.62018 Test: 1.24554
Epoch: 6000 Train: 160.24889 Test: 1.04709
Epoch: 6080 Train: 161.28757 Test: 0.92022
Epoch: 6160 Train: 158.82654 Test: 0.98646
Epoch: 6240 Train: 151.89897 Test: 0.81439
Epoch: 6320 Train: 144.21185 Test: 1.06271
Epoch: 6400 Train: 144.66635 Test: 0.82986
Epoch: 6480 Train: 144.04263 Test: 0.99773
Epoch: 6560 Train: 141.33687 Test: 1.15275
Epoch: 6640 Train: 138.76814 Test: 0.79064
Epoch: 6720 Train: 148.55414 Test: 1.55778
Epoch: 6800 Train: 139.58170 Test: 0.79783
Epoch: 6880 Train: 126.05280 Test: 0.67103
Epoch: 6960 Train: 119.09414 Test: 0.64067
Epoch: 7040 Train: 117.74966 Test: 0.57434
Epoch: 7120 Train: 121.86914 Test: 0.58380
Epoch: 7200 Train: 121.28065 Test: 0.56637
Epoch: 7280 Train: 116.13018 Test: 0.92970
Epoch: 7360 Train: 116.34928 Test: 0.54250
Epoch: 7440 Train: 115.05660 Test: 0.56342
Epoch: 7520 Train: 112.33430 Test: 0.55903
Epoch: 7600 Train: 114.48343 Test: 0.81788
Epoch: 7680 Train: 113.21152 Test: 0.67336
Epoch: 7760 Train: 124.76299 Test: 0.62509
Epoch: 7840 Train: 120.71410 Test: 0.91491
Epoch: 7920 Train: 112.24203 Test: 0.60705
Epoch: 7999 Train: 113.91612 Test: 0.51453
Training Loss: tensor(113.9161)
Test Loss: tensor(0.5145)
Learned LE: [  0.8872174   -0.01785861 -14.517541  ]
True LE: [ 8.7128180e-01 -6.1653648e-03 -1.4541613e+01]
Relative Error: [0.79261696 0.7735015  0.8138551  0.5438423  0.509029   0.5778472
 0.7897715  1.1275554  1.0168842  0.9055151  0.86938536 0.73442924
 0.50463367 0.42141834 0.2819426  0.5275621  0.6057193  0.6855975
 0.705399   0.6824901  0.6145357  0.6770881  0.85450506 1.0379841
 1.1023238  1.289632   1.3913971  1.4758849  1.5851598  1.3185308
 1.3394799  1.0906826  0.6476602  0.799563   1.4052504  1.5782157
 1.1822706  0.4831789  0.27587074 0.6935987  0.955057   0.97525567
 0.7545835  0.3153427  0.37843168 1.1899035  1.291107   1.4268615
 1.5337632  1.4647831  1.4863182  1.5474695  1.4156201  1.2894611
 0.97403854 0.45722887 0.18675968 0.22853659 0.36488703 0.53223383
 0.6441283  0.591682   0.74795145 0.6225728  0.79052216 0.5943036
 0.56748843 0.6843221  0.8882812  1.0116131  0.8121848  0.8404874
 0.8739897  0.7466564  0.54908097 0.3763707  0.27255443 0.40667894
 0.65006864 0.8085718  0.81830144 0.75116146 0.6432694  0.58405864
 0.570695   0.82337356 0.91932666 0.99002916 1.1488432  1.3508112
 1.5200062  1.3650522  1.4785681  1.3006046  0.9673029  0.77271247
 1.0273349  1.4555023  1.0585811  0.5087212  0.47134236 0.8486355
 1.0577012  1.0650779  0.84253263 0.40471596 0.30939063 1.1059239
 1.2317448  1.4784317  1.7297887  1.6651783  1.5103091  1.4017019
 1.1960292  1.0325823  0.8186112  0.50898355 0.52596134 0.37561107
 0.34989727 0.4631921  0.5841832  0.5672771  0.69344985 0.5633858
 0.6504129  0.73109263 0.69988513 0.86142653 0.92849016 0.99644446
 0.7403707  0.8436228  0.92129755 0.7778679  0.618421   0.40003982
 0.3314417  0.36244953 0.6397972  0.86824733 1.0061772  0.9555447
 0.8618327  0.7370239  0.51383406 0.695364   0.7851147  0.8864984
 1.1582575  1.3658811  1.5469108  1.3686618  1.4865047  1.4484142
 1.3266255  1.0469092  0.94090915 1.0354156  0.8146468  0.62473786
 0.49320495 0.89872426 1.1003746  1.1347485  0.92742574 0.5117233
 0.20692474 0.99864405 1.141187   1.4693893  1.849407   1.9538448
 1.3854489  1.1541464  1.107697   0.9060395  0.7109262  0.3893071
 0.49233752 0.7225801  0.64134055 0.48367232 0.523683   0.5079545
 0.65695024 0.5901872  0.5933285  0.8079153  0.8226342  0.9033804
 0.90326613 1.0012903  0.7872376  0.8592138  0.9791348  0.8535983
 0.7076348  0.47994992 0.41538462 0.502398   0.6853059  0.9235087
 1.1276423  1.2854367  1.2601523  1.1840905  1.0017945  0.7639797
 0.8148927  0.90254164 1.1535786  1.3286555  1.6439146  1.6901896
 1.7216612  1.6356643  1.5377895  1.3506963  1.070096   0.78516686
 0.77314204 0.3388407  0.4438635  0.77450895 1.0231854  1.1014384
 0.976312   0.5982247  0.07537796 0.87430066 1.0401099  1.3941908
 1.8766487  1.8368682  1.6098207  1.1061232  1.1051679  0.9236738
 0.6820412  0.43176612 0.48483115 0.6957441  0.7196887  0.7722062
 0.5933116  0.49799234 0.61693585 0.61420035 0.58346295 0.6882921
 0.90919805 0.92072684 0.8800626  0.9823717  0.78349245 0.82975113
 0.9016808  0.7734483  0.6357146  0.47923875 0.5436026  0.5993945
 0.7695499  1.0627737  1.3027878  1.5442486  1.7725146  1.8254675
 1.5228888  1.1422237  0.9225022  0.9003651  0.96857005 1.155888
 1.4187716  1.7866645  1.8259175  1.9720279  1.9106336  1.8065991
 1.2872835  0.99851936 0.65728855 0.47451723 0.2667318  0.6291334
 0.93386555 1.0297334  0.98072577 0.71559817 0.23248263 0.6614404
 1.0057956  1.287041   1.6595318  1.6700479  1.6582202  1.455038
 1.06429    1.1021125  0.8274696  0.58033353 0.591424   0.65292066
 0.5801803  0.6503911  0.8830565  0.7117665  0.68920225 0.7369371
 0.62913495 0.765053   1.0197784  1.1324506  1.0316858  0.9836975
 0.76627374 0.75435627 0.8606365  0.8330102  0.68711877 0.5800423
 0.6277326  0.7265599  0.81827205 0.9752936  1.3383654  1.7447537
 2.1212497  2.4325063  2.194808   1.7497569  1.4659599  1.1320865
 1.0221266  0.95078635 1.0182595  1.375241   1.6583126  2.0278172
 2.0766048  2.305911   1.9427898  1.3106371  0.92317694 0.626698
 0.44169044 0.38826147 0.6991999  0.92563003 0.931033   0.8000508
 0.48439103 0.50465846 0.987846   1.2230381  1.3608893  1.4163809
 1.4530814  1.6499228  1.4480728  1.2040844  1.1498455  0.8312159
 0.5263525  0.42788422 0.47036564 0.3219657  0.5241853  0.77725464
 0.8960671  0.9767832  0.8136547  0.88004255 1.051931   1.3631614
 1.2576724  1.177855   1.1178893  0.74742264 0.805883   0.85012966
 0.67834526 0.5538092  0.42392886 0.5232137  0.7472855  1.0101787
 1.4022343  1.8419998  2.377273   2.6673486  2.7726178  2.6325145
 2.3500938  1.9949152  1.482844   1.1606712 ]
