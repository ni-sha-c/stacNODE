time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 102.41%, model saved.
Epoch: 0 Train: 3846.30151 Test: 3986.75610
Epoch 100: New minimal relative error: 90.27%, model saved.
Epoch: 100 Train: 224.55728 Test: 331.37308
Epoch 200: New minimal relative error: 47.07%, model saved.
Epoch: 200 Train: 50.30206 Test: 93.49678
Epoch 300: New minimal relative error: 19.94%, model saved.
Epoch: 300 Train: 7.10524 Test: 9.89546
Epoch: 400 Train: 14.26534 Test: 21.04538
Epoch 500: New minimal relative error: 17.24%, model saved.
Epoch: 500 Train: 5.24502 Test: 10.60779
Epoch: 600 Train: 8.56836 Test: 7.40212
Epoch: 700 Train: 3.66234 Test: 3.70647
Epoch 800: New minimal relative error: 13.27%, model saved.
Epoch: 800 Train: 3.23779 Test: 3.81432
Epoch: 900 Train: 3.33159 Test: 2.61881
Epoch: 1000 Train: 7.95564 Test: 9.59458
Epoch: 1100 Train: 1.34137 Test: 2.29089
Epoch 1200: New minimal relative error: 7.74%, model saved.
Epoch: 1200 Train: 0.89113 Test: 0.96892
Epoch: 1300 Train: 6.17042 Test: 8.61625
Epoch: 1400 Train: 1.05617 Test: 2.93847
Epoch: 1500 Train: 4.56167 Test: 4.28590
Epoch: 1600 Train: 2.35213 Test: 3.13967
Epoch: 1700 Train: 2.90719 Test: 2.79809
Epoch: 1800 Train: 4.76288 Test: 4.82369
Epoch: 1900 Train: 0.87172 Test: 1.75812
Epoch: 2000 Train: 0.93794 Test: 0.69544
Epoch: 2100 Train: 0.76737 Test: 0.82069
Epoch: 2200 Train: 1.45572 Test: 2.11593
Epoch: 2300 Train: 6.29979 Test: 6.58309
Epoch: 2400 Train: 1.51038 Test: 1.49365
Epoch: 2500 Train: 3.95610 Test: 4.81409
Epoch: 2600 Train: 1.64701 Test: 0.93207
Epoch 2700: New minimal relative error: 7.45%, model saved.
Epoch: 2700 Train: 0.52880 Test: 0.69753
Epoch: 2800 Train: 0.60784 Test: 0.29252
Epoch: 2900 Train: 2.05342 Test: 2.54954
Epoch: 3000 Train: 1.85535 Test: 2.27084
Epoch: 3100 Train: 0.97559 Test: 1.15360
Epoch: 3200 Train: 0.74159 Test: 0.54075
Epoch: 3300 Train: 1.01631 Test: 0.89938
Epoch: 3400 Train: 0.23947 Test: 0.27945
Epoch: 3500 Train: 0.57338 Test: 0.69453
Epoch: 3600 Train: 2.11126 Test: 2.44693
Epoch: 3700 Train: 1.09358 Test: 1.10929
Epoch: 3800 Train: 0.50661 Test: 0.42800
Epoch: 3900 Train: 0.23070 Test: 0.26710
Epoch: 4000 Train: 0.79671 Test: 1.09995
Epoch 4100: New minimal relative error: 6.76%, model saved.
Epoch: 4100 Train: 0.60767 Test: 0.84759
Epoch: 4200 Train: 0.24553 Test: 0.31351
Epoch: 4300 Train: 0.13803 Test: 0.20272
Epoch: 4400 Train: 0.42263 Test: 0.43521
Epoch: 4500 Train: 0.54868 Test: 0.60544
Epoch: 4600 Train: 0.23273 Test: 0.52443
Epoch: 4700 Train: 1.26691 Test: 0.63433
Epoch: 4800 Train: 0.17600 Test: 0.32017
Epoch: 4900 Train: 0.71130 Test: 0.73302
Epoch: 5000 Train: 2.78912 Test: 2.53403
Epoch: 5100 Train: 0.47752 Test: 0.59933
Epoch: 5200 Train: 0.37348 Test: 0.46448
Epoch: 5300 Train: 0.10378 Test: 0.17086
Epoch: 5400 Train: 0.54061 Test: 0.61701
Epoch: 5500 Train: 0.33175 Test: 0.36663
Epoch 5600: New minimal relative error: 3.97%, model saved.
Epoch: 5600 Train: 0.33929 Test: 0.45540
Epoch: 5700 Train: 0.14665 Test: 0.21451
Epoch: 5800 Train: 0.64372 Test: 0.70571
Epoch: 5900 Train: 0.13622 Test: 0.20272
Epoch: 6000 Train: 0.81929 Test: 0.99380
Epoch: 6100 Train: 0.10623 Test: 0.20131
Epoch: 6200 Train: 0.12552 Test: 0.21372
Epoch: 6300 Train: 0.18155 Test: 0.23028
Epoch: 6400 Train: 0.21823 Test: 0.23495
Epoch: 6500 Train: 0.09280 Test: 0.12346
Epoch 6600: New minimal relative error: 3.67%, model saved.
Epoch: 6600 Train: 0.11081 Test: 0.13798
Epoch: 6700 Train: 0.80632 Test: 0.72549
Epoch: 6800 Train: 0.23694 Test: 0.33410
Epoch: 6900 Train: 0.29305 Test: 0.35561
Epoch: 7000 Train: 0.69900 Test: 0.74809
Epoch: 7100 Train: 0.08865 Test: 0.10992
Epoch 7200: New minimal relative error: 3.30%, model saved.
Epoch: 7200 Train: 0.20577 Test: 0.15598
Epoch: 7300 Train: 0.11756 Test: 0.14415
Epoch: 7400 Train: 0.36900 Test: 0.25657
Epoch: 7500 Train: 0.25072 Test: 0.25580
Epoch: 7600 Train: 0.03828 Test: 0.05359
Epoch: 7700 Train: 0.74437 Test: 0.71482
Epoch: 7800 Train: 0.19997 Test: 0.20609
Epoch: 7900 Train: 0.03024 Test: 0.05204
Epoch: 8000 Train: 0.09673 Test: 0.12149
Epoch: 8100 Train: 2.01568 Test: 1.63912
Epoch: 8200 Train: 0.02570 Test: 0.05028
Epoch: 8300 Train: 0.05175 Test: 0.06790
Epoch: 8400 Train: 0.05916 Test: 0.06969
Epoch: 8500 Train: 0.03744 Test: 0.05216
Epoch: 8600 Train: 0.30190 Test: 0.53967
Epoch: 8700 Train: 0.05184 Test: 0.05383
Epoch: 8800 Train: 0.01993 Test: 0.03900
Epoch: 8900 Train: 0.02363 Test: 0.05092
Epoch: 9000 Train: 0.03097 Test: 0.05241
Epoch: 9100 Train: 0.02439 Test: 0.04207
Epoch: 9200 Train: 0.11229 Test: 0.14098
Epoch: 9300 Train: 0.04926 Test: 0.06052
Epoch: 9400 Train: 0.02220 Test: 0.04043
Epoch 9500: New minimal relative error: 2.60%, model saved.
Epoch: 9500 Train: 0.04397 Test: 0.09033
Epoch: 9600 Train: 0.02694 Test: 0.05146
Epoch: 9700 Train: 0.07246 Test: 0.08189
Epoch: 9800 Train: 0.03333 Test: 0.05450
Epoch: 9900 Train: 0.19126 Test: 0.17597
Epoch: 9999 Train: 0.50210 Test: 0.47574
Training Loss: tensor(0.5021)
Test Loss: tensor(0.4757)
Learned LE: [ 0.7705805   0.04914342 -3.6586258 ]
True LE: [ 8.6842531e-01 -4.1172798e-03 -1.4539361e+01]
Relative Error: [3.1991024  2.938754   2.7909565  2.7459388  2.536961   2.370258
 2.1436388  1.9578732  1.8667831  1.906296   2.0437536  2.2793686
 2.4743552  2.5906737  2.6731105  2.6946754  2.8863373  3.292534
 3.6832075  3.8549554  3.8274724  3.9845288  4.33781    4.408069
 4.4933634  4.575841   4.307085   4.103012   3.6673381  3.2895894
 2.9698853  2.4800417  2.1235273  1.8174149  1.6048359  1.3788995
 1.1210614  1.0660635  1.009953   1.1078459  1.2230177  1.3252171
 1.1878211  1.1189018  1.0147521  0.9162045  0.83815783 0.7287938
 0.672932   0.71841985 1.0257607  1.0468136  1.2857475  1.431071
 1.5615443  1.8112596  1.8300377  1.7082042  1.8490368  2.3486507
 2.8744433  2.9468846  2.7130072  2.4108713  2.3042095  2.3452973
 2.1663148  1.9520005  1.7432773  1.5632864  1.528981   1.5311838
 1.7185775  1.9529178  2.144696   2.293375   2.34158    2.3076963
 2.3908381  2.8117485  3.2198765  3.378135   3.2985072  3.3563633
 3.589968   3.905087   4.063734   4.036771   3.7136064  3.4775023
 2.9295702  2.5967722  2.2159364  1.7691461  1.5141329  1.2324078
 1.1134567  0.879056   0.65309393 0.61784315 0.65454    0.7376969
 1.0461005  1.0877172  0.99170256 0.87496436 0.72034055 0.7546048
 0.8157899  0.74396193 0.71949005 0.7936336  1.0476373  1.0522654
 1.2561017  1.357268   1.400043   1.6567746  1.6426772  1.5112802
 1.6066396  2.1670747  2.5381408  2.346523   2.174296   2.01467
 1.9394145  1.9904811  1.8743845  1.5779915  1.3354518  1.1961846
 1.1379118  1.1820625  1.3663794  1.5473124  1.7877095  1.9373255
 1.9560292  2.011429   1.9790467  2.2586339  2.680585   2.8530374
 2.6567519  2.6293726  2.7022433  3.0442505  3.3454962  3.427258
 3.1423514  2.7119865  2.2179966  1.9084374  1.5184964  1.1649877
 0.9723857  0.7749294  0.76192766 0.584153   0.48043296 0.432259
 0.391116   0.3979485  0.7133245  0.88321084 0.7966454  0.6716225
 0.48366487 0.5506377  0.6434976  0.6678079  0.7684006  0.8295061
 0.9280708  1.0880234  1.2883694  1.3554419  1.4128264  1.6539528
 1.6692888  1.4619726  1.5321621  2.0178802  2.1721857  1.9540653
 1.8068857  1.6706593  1.6734917  1.7105434  1.652416   1.4028586
 1.0949192  1.0278414  0.98110867 0.9498689  1.0359042  1.1762489
 1.3028373  1.5864037  1.7006971  1.6990807  1.6679479  1.6553519
 1.8937458  2.1430292  1.9595459  1.9156927  1.8697673  2.0384579
 2.3717318  2.646451   2.6001372  2.076557   1.4914176  1.2049235
 0.948294   0.6953731  0.5832964  0.497829   0.4407165  0.34085932
 0.40697846 0.3165285  0.29025298 0.24876687 0.41087973 0.55088353
 0.5653464  0.4973333  0.39533237 0.5398762  0.5916628  0.620063
 0.73689616 0.79449826 0.77115923 0.9127993  1.2310704  1.3159294
 1.4374919  1.6840669  1.6141001  1.3763834  1.4185306  1.752158
 1.7610494  1.6370984  1.5220339  1.4383062  1.3344886  1.4469303
 1.4269944  1.3154969  1.030664   0.92239165 0.9203735  0.879876
 0.868309   0.92643255 0.8934129  1.1200163  1.3212363  1.5073372
 1.3801535  1.2541739  1.2285718  1.471571   1.4038405  1.2290117
 1.0546861  1.179408   1.2729783  1.6970288  1.9500262  1.4402939
 0.7985708  0.57805926 0.43054697 0.32203043 0.26785442 0.28878877
 0.16895309 0.21612258 0.34352717 0.32363358 0.41087    0.31503478
 0.09917948 0.13953425 0.2392153  0.30394575 0.42500174 0.6046923
 0.5434912  0.48180315 0.579602   0.7291216  0.7166375  0.7489586
 1.0878347  1.2339727  1.3732076  1.5399745  1.5026649  1.2985674
 1.3495637  1.4039106  1.3303903  1.3386298  1.2541728  1.2865605
 1.1289499  1.2256473  1.2197409  1.168607   1.1011232  0.9956782
 0.91639364 0.89486337 0.8223873  0.79923373 0.8020348  0.8341135
 0.9964656  1.1404141  1.2594076  1.0625545  0.8625295  0.8888563
 0.906602   0.7524894  0.65231586 0.50715643 0.61982286 0.7912273
 1.1938     0.860274   0.4045952  0.26097992 0.1697648  0.25272262
 0.21026011 0.28520152 0.22698803 0.30725354 0.4217768  0.46747154
 0.62092793 0.5237384  0.32606152 0.18859108 0.18198529 0.18789218
 0.40254587 0.6303071  0.49293545 0.37344027 0.4724076  0.6590083
 0.6945922  0.61344355 0.8483423  1.0256201  1.151631   1.3177744
 1.2679206  1.1068674  1.1922607  1.1589202  1.0555853  0.996332
 1.004218   1.0222396  1.0261204  0.917816   1.0346763  1.1053845
 1.0033698  0.97023034 0.89298224 0.8070977  0.79166836 0.79296875
 0.77048784 0.7986877  0.81994414 0.8665654  1.1235116  1.0906757
 1.0411264  0.6551469  0.60936594 0.6751571  0.7354754  0.64742917
 0.75636137 0.7298557  0.583199   0.6525104 ]
