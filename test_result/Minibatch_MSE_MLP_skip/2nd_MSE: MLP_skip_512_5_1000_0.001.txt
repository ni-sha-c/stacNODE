time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.89%, model saved.
Epoch: 0 Train: 3744.88037 Test: 4057.37256
Epoch 100: New minimal relative error: 74.66%, model saved.
Epoch: 100 Train: 65.17273 Test: 64.88152
Epoch 200: New minimal relative error: 30.90%, model saved.
Epoch: 200 Train: 16.68719 Test: 15.27849
Epoch: 300 Train: 46.40665 Test: 37.19275
Epoch 400: New minimal relative error: 21.85%, model saved.
Epoch: 400 Train: 5.64164 Test: 5.18582
Epoch 500: New minimal relative error: 14.77%, model saved.
Epoch: 500 Train: 4.65029 Test: 7.98216
Epoch: 600 Train: 7.94835 Test: 10.65567
Epoch: 700 Train: 4.71060 Test: 5.34725
Epoch: 800 Train: 10.30906 Test: 17.26377
Epoch 900: New minimal relative error: 11.39%, model saved.
Epoch: 900 Train: 1.89574 Test: 1.65611
Epoch: 1000 Train: 11.80140 Test: 9.84593
Epoch: 1100 Train: 1.77978 Test: 1.77360
Epoch: 1200 Train: 6.62715 Test: 6.50885
Epoch: 1300 Train: 9.76537 Test: 10.48072
Epoch: 1400 Train: 0.89796 Test: 0.81858
Epoch: 1500 Train: 1.12227 Test: 1.05087
Epoch: 1600 Train: 1.72986 Test: 2.27917
Epoch 1700: New minimal relative error: 10.21%, model saved.
Epoch: 1700 Train: 3.29326 Test: 2.16847
Epoch: 1800 Train: 0.85339 Test: 1.03908
Epoch: 1900 Train: 10.09341 Test: 15.28057
Epoch: 2000 Train: 0.40447 Test: 0.59373
Epoch: 2100 Train: 0.99243 Test: 0.56337
Epoch: 2200 Train: 2.31223 Test: 3.16834
Epoch: 2300 Train: 0.47910 Test: 0.46600
Epoch: 2400 Train: 3.24936 Test: 1.78389
Epoch: 2500 Train: 0.89845 Test: 1.60828
Epoch: 2600 Train: 0.56793 Test: 0.68500
Epoch: 2700 Train: 0.80869 Test: 0.68623
Epoch: 2800 Train: 0.26201 Test: 0.27532
Epoch: 2900 Train: 0.51170 Test: 0.99616
Epoch: 3000 Train: 0.83909 Test: 0.81897
Epoch: 3100 Train: 0.81167 Test: 0.73727
Epoch: 3200 Train: 0.67780 Test: 0.83261
Epoch: 3300 Train: 1.93739 Test: 2.09010
Epoch 3400: New minimal relative error: 8.31%, model saved.
Epoch: 3400 Train: 0.38503 Test: 0.47187
Epoch: 3500 Train: 2.32929 Test: 2.45031
Epoch: 3600 Train: 0.32315 Test: 0.26673
Epoch: 3700 Train: 1.70340 Test: 2.50837
Epoch: 3800 Train: 0.29642 Test: 0.47289
Epoch: 3900 Train: 0.33495 Test: 0.28980
Epoch: 4000 Train: 0.51208 Test: 0.60343
Epoch 4100: New minimal relative error: 7.16%, model saved.
Epoch: 4100 Train: 1.44090 Test: 1.24751
Epoch: 4200 Train: 0.15690 Test: 0.15051
Epoch: 4300 Train: 0.68795 Test: 0.65713
Epoch: 4400 Train: 0.37001 Test: 0.38498
Epoch: 4500 Train: 1.09683 Test: 1.15110
Epoch: 4600 Train: 0.67254 Test: 0.49454
Epoch: 4700 Train: 4.80391 Test: 4.11565
Epoch: 4800 Train: 1.82626 Test: 1.80535
Epoch: 4900 Train: 0.14781 Test: 0.18358
Epoch: 5000 Train: 0.82011 Test: 1.33513
Epoch: 5100 Train: 0.44280 Test: 0.59208
Epoch: 5200 Train: 0.10790 Test: 0.10736
Epoch 5300: New minimal relative error: 7.14%, model saved.
Epoch: 5300 Train: 0.15498 Test: 0.18651
Epoch: 5400 Train: 0.69452 Test: 0.51891
Epoch: 5500 Train: 0.19153 Test: 0.20894
Epoch: 5600 Train: 0.13517 Test: 0.13369
Epoch: 5700 Train: 0.12829 Test: 0.11873
Epoch: 5800 Train: 1.31354 Test: 1.53866
Epoch: 5900 Train: 0.54494 Test: 0.60290
Epoch: 6000 Train: 0.23415 Test: 0.26989
Epoch: 6100 Train: 0.33785 Test: 0.33399
Epoch: 6200 Train: 0.30805 Test: 0.33985
Epoch: 6300 Train: 0.16145 Test: 0.16110
Epoch: 6400 Train: 0.10224 Test: 0.10449
Epoch: 6500 Train: 0.58822 Test: 0.37778
Epoch: 6600 Train: 0.20452 Test: 0.16049
Epoch: 6700 Train: 0.12906 Test: 0.12289
Epoch 6800: New minimal relative error: 5.61%, model saved.
Epoch: 6800 Train: 0.88383 Test: 0.86945
Epoch: 6900 Train: 0.20048 Test: 0.22163
Epoch: 7000 Train: 0.77201 Test: 0.95587
Epoch: 7100 Train: 0.13519 Test: 0.12794
Epoch: 7200 Train: 0.10378 Test: 0.12149
Epoch: 7300 Train: 0.18873 Test: 0.20631
Epoch: 7400 Train: 1.10026 Test: 1.47404
Epoch: 7500 Train: 0.27497 Test: 0.36674
Epoch: 7600 Train: 0.14681 Test: 0.13147
Epoch 7700: New minimal relative error: 4.87%, model saved.
Epoch: 7700 Train: 0.14095 Test: 0.18805
Epoch: 7800 Train: 0.08745 Test: 0.08120
Epoch: 7900 Train: 0.06309 Test: 0.06655
Epoch: 8000 Train: 0.33231 Test: 0.25741
Epoch: 8100 Train: 0.06796 Test: 0.07820
Epoch: 8200 Train: 0.18117 Test: 0.19718
Epoch: 8300 Train: 0.06766 Test: 0.07375
Epoch: 8400 Train: 0.06105 Test: 0.07243
Epoch: 8500 Train: 0.13969 Test: 0.17412
Epoch: 8600 Train: 0.05829 Test: 0.05943
Epoch: 8700 Train: 0.05422 Test: 0.06034
Epoch: 8800 Train: 0.17438 Test: 0.22351
Epoch: 8900 Train: 0.08113 Test: 0.08281
Epoch: 9000 Train: 0.05078 Test: 0.05797
Epoch: 9100 Train: 0.79448 Test: 0.93095
Epoch: 9200 Train: 0.05138 Test: 0.05884
Epoch: 9300 Train: 0.04620 Test: 0.05230
Epoch: 9400 Train: 0.04563 Test: 0.05153
Epoch: 9500 Train: 0.04501 Test: 0.05108
Epoch: 9600 Train: 0.04397 Test: 0.05011
Epoch: 9700 Train: 0.04578 Test: 0.05142
Epoch: 9800 Train: 0.04315 Test: 0.04881
Epoch: 9900 Train: 0.16135 Test: 0.19268
Epoch: 9999 Train: 0.33014 Test: 0.28232
Training Loss: tensor(0.3301)
Test Loss: tensor(0.2823)
Learned LE: [ 8.3000129e-01  6.0735916e-04 -2.9671812e+00]
True LE: [ 8.7286568e-01  3.0190118e-03 -1.4548296e+01]
Relative Error: [6.709817   5.9473515  5.4539638  5.347474   5.416124   5.420909
 5.2140102  4.850157   4.502245   4.0992937  3.9032474  4.0406632
 4.206908   4.236703   3.966187   3.4524436  3.2737985  3.311913
 2.9448392  2.4442625  2.5438766  2.8367505  3.3561442  3.9699566
 4.2942653  4.346365   4.4928274  4.5583563  4.99722    5.179141
 5.105231   4.7785583  4.4745603  4.326243   4.3226027  4.64035
 4.9282403  4.585758   4.0423794  3.7003942  3.6403928  3.4866962
 3.1847363  3.0564935  2.8768117  2.2198706  1.596933   2.3283541
 3.2391992  4.545432   5.1198716  5.9567614  6.8359385  6.838
 6.2514834  5.791096   5.761088   5.864488   6.187795   6.533811
 6.861397   6.9067636  6.3497944  5.460224   4.7637997  4.4024305
 4.478185   4.701531   4.5870333  4.4142547  4.1274357  3.8792365
 3.6188378  3.6026537  3.8855784  3.955859   3.864982   3.4049592
 3.012655   2.9177814  2.9645941  2.444314   2.3461757  2.6027415
 2.7360141  3.3807032  3.8292177  3.9591897  4.337759   4.463152
 4.8945436  5.2470794  5.2146754  4.9368215  4.6388483  4.4255924
 4.100518   4.3115697  4.7048817  5.1215243  4.6369033  3.8414617
 3.4813085  3.368456   3.1873856  2.8438597  2.7674332  2.5364769
 1.7907394  1.7588     2.6630073  3.573764   4.289861   5.206974
 6.1729693  6.126927   5.4870234  4.989676   4.761405   4.887935
 5.232657   5.4689884  5.648374   5.9012337  6.0178194  5.108106
 4.2234054  3.7175138  3.594937   3.7538     4.003369   3.933763
 3.600889   3.524997   3.3803914  3.2829058  3.3079126  3.5488143
 3.6570504  3.2646782  2.9400837  2.606364   2.585156   2.5250618
 2.1898775  2.4057882  2.2391841  2.6375885  3.1282022  3.403876
 4.080713   4.64815    5.052329   5.3024116  5.4729586  5.29406
 5.1944394  4.8521194  4.470128   4.3147435  4.3297358  4.7283983
 5.0253487  4.7488527  3.860275   3.200147   3.0246086  2.8274617
 2.4314857  2.486678   2.235531   1.5376225  2.0268195  2.607759
 3.2848148  4.2206063  5.3487153  5.487047   4.874947   4.278797
 3.9053123  3.9801836  4.2895036  4.5208077  4.5950127  4.637205
 4.766755   4.785157   3.8490708  3.0797324  2.7822635  2.826424
 3.0468783  3.3544943  3.4025095  2.9460254  2.8866456  2.8239367
 2.6007     2.560874   2.8089578  2.7354298  2.4480906  2.3512454
 2.1689565  2.1122289  2.103703   2.0482247  2.020071   1.892923
 2.3679805  2.8084648  3.3508465  4.578905   5.293688   5.502331
 5.7058554  5.687723   5.7664213  5.532966   4.8584557  4.457376
 4.5283318  4.408869   4.5893636  4.675488   4.8691134  3.874063
 3.0338118  2.7219932  2.4633946  2.0713286  2.2558327  1.9497849
 1.4497651  1.9926925  2.0997894  3.026206   4.0878115  4.818908
 4.198379   3.7645075  3.2693458  3.1355824  3.351444   3.5868433
 3.6655464  3.5988624  3.485898   3.5990677  3.5698528  3.0335739
 2.2979882  2.0008926  2.114911   2.5184267  2.713154   2.86857
 2.5627809  2.0976453  1.9422352  1.7873757  1.8346568  2.1976972
 2.008      1.657939   1.5260217  1.6443752  1.6590195  1.7527852
 1.8758546  1.7087162  1.5882736  1.9084858  2.413469   3.6707501
 4.956598   5.363558   5.183325   5.0999665  5.108651   5.1662827
 5.0176563  4.626546   4.4258537  4.481847   4.6059837  4.4591703
 4.343788   4.376408   3.9476585  3.0606344  2.5904276  1.99519
 1.8442489  2.0722966  1.9287897  1.6678582  1.7274892  1.7531393
 2.720231   3.7818964  3.5950058  3.0615215  2.8629003  2.5453248
 2.6415114  2.7178419  2.7551336  2.5760493  2.3855152  2.4700625
 2.8192687  2.7612517  2.5266297  1.7293088  1.2724181  1.4542184
 1.8083304  2.0442352  2.1456046  2.2433832  1.7665589  1.2740737
 1.1714315  1.1903106  1.429244   1.5154841  1.1734211  0.79141444
 0.977394   1.2200143  1.4447758  1.6847879  1.468148   1.3891412
 1.4130098  1.9962069  3.6749594  4.4904017  4.6793666  4.708641
 4.651988   4.7360525  4.8365703  4.713745   4.2274146  3.7958984
 3.859088   4.37579    4.5444155  4.0147586  3.7843351  3.8206592
 3.1758678  2.5853431  2.1354203  1.7329409  1.815479   2.0070853
 1.7827066  1.8615627  1.5914     2.4469573  3.5063756  2.6093006
 2.0770366  2.2119753  2.0689068  1.9416202  2.0799181  1.9870222
 1.5873089  1.381743   1.5273608  1.8842742  1.9031852  1.8188854
 1.6882356  0.98283386 0.65867764 1.0395412  1.4360193  1.5347817
 1.3750349  1.4778247  1.1167127  0.7924184  0.8560735  0.866284
 0.9940519  0.87842184 0.48994306 0.41698223 0.7959043  1.1066648
 1.3594992  1.3101273  1.2817636  1.152657  ]
