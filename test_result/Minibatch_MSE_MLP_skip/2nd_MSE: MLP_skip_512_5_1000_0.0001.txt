time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 103.86%, model saved.
Epoch: 0 Train: 4089.36353 Test: 4070.43579
Epoch 100: New minimal relative error: 48.14%, model saved.
Epoch: 100 Train: 61.55964 Test: 49.55941
Epoch 200: New minimal relative error: 21.43%, model saved.
Epoch: 200 Train: 9.95843 Test: 10.51066
Epoch 300: New minimal relative error: 20.69%, model saved.
Epoch: 300 Train: 10.47028 Test: 13.95238
Epoch: 400 Train: 29.70956 Test: 22.16078
Epoch 500: New minimal relative error: 16.86%, model saved.
Epoch: 500 Train: 5.10258 Test: 6.30541
Epoch: 600 Train: 16.71388 Test: 11.03075
Epoch 700: New minimal relative error: 13.86%, model saved.
Epoch: 700 Train: 4.47454 Test: 6.65665
Epoch: 800 Train: 3.76633 Test: 3.41154
Epoch: 900 Train: 5.36049 Test: 3.17863
Epoch: 1000 Train: 2.10802 Test: 2.39972
Epoch: 1100 Train: 2.54300 Test: 3.62678
Epoch: 1200 Train: 4.64160 Test: 6.70146
Epoch: 1300 Train: 2.02887 Test: 2.46805
Epoch 1400: New minimal relative error: 13.67%, model saved.
Epoch: 1400 Train: 2.75547 Test: 3.39318
Epoch: 1500 Train: 3.92992 Test: 3.66207
Epoch: 1600 Train: 18.39564 Test: 15.36647
Epoch 1700: New minimal relative error: 10.87%, model saved.
Epoch: 1700 Train: 2.02547 Test: 1.87349
Epoch: 1800 Train: 0.97166 Test: 1.15158
Epoch: 1900 Train: 8.28880 Test: 8.29907
Epoch: 2000 Train: 0.97691 Test: 1.28962
Epoch: 2100 Train: 1.86550 Test: 2.22757
Epoch: 2200 Train: 0.65831 Test: 0.74140
Epoch: 2300 Train: 7.02249 Test: 4.60328
Epoch: 2400 Train: 0.69279 Test: 0.79454
Epoch: 2500 Train: 0.53090 Test: 0.73587
Epoch 2600: New minimal relative error: 8.98%, model saved.
Epoch: 2600 Train: 0.48250 Test: 0.58812
Epoch: 2700 Train: 0.53641 Test: 0.46370
Epoch: 2800 Train: 0.41794 Test: 0.54143
Epoch: 2900 Train: 1.90628 Test: 2.52851
Epoch: 3000 Train: 0.70723 Test: 0.78246
Epoch: 3100 Train: 0.25346 Test: 0.42041
Epoch: 3200 Train: 0.46184 Test: 0.58172
Epoch: 3300 Train: 1.54049 Test: 1.33583
Epoch: 3400 Train: 0.75518 Test: 0.82519
Epoch 3500: New minimal relative error: 8.16%, model saved.
Epoch: 3500 Train: 0.54100 Test: 0.64214
Epoch: 3600 Train: 0.74185 Test: 1.00029
Epoch: 3700 Train: 3.64883 Test: 4.10380
Epoch: 3800 Train: 1.09668 Test: 1.22398
Epoch: 3900 Train: 0.29697 Test: 0.39653
Epoch: 4000 Train: 0.96090 Test: 0.96231
Epoch: 4100 Train: 0.79092 Test: 1.02610
Epoch: 4200 Train: 1.15133 Test: 1.26918
Epoch: 4300 Train: 0.16538 Test: 0.21299
Epoch: 4400 Train: 0.26748 Test: 0.41321
Epoch: 4500 Train: 0.46936 Test: 0.70016
Epoch: 4600 Train: 0.71654 Test: 0.78915
Epoch: 4700 Train: 0.47393 Test: 0.58066
Epoch: 4800 Train: 0.11402 Test: 0.16812
Epoch: 4900 Train: 0.11512 Test: 0.14495
Epoch 5000: New minimal relative error: 7.17%, model saved.
Epoch: 5000 Train: 0.12785 Test: 0.17657
Epoch: 5100 Train: 1.11824 Test: 0.96604
Epoch: 5200 Train: 0.09133 Test: 0.14843
Epoch: 5300 Train: 0.36927 Test: 0.42883
Epoch: 5400 Train: 0.58326 Test: 0.49279
Epoch: 5500 Train: 0.15671 Test: 0.18861
Epoch: 5600 Train: 0.11614 Test: 0.18714
Epoch: 5700 Train: 0.24987 Test: 0.24273
Epoch: 5800 Train: 0.22973 Test: 0.27988
Epoch: 5900 Train: 0.18266 Test: 0.20609
Epoch: 6000 Train: 0.32594 Test: 0.43134
Epoch: 6100 Train: 0.18018 Test: 0.22282
Epoch: 6200 Train: 0.39642 Test: 0.46008
Epoch: 6300 Train: 0.13909 Test: 0.15024
Epoch: 6400 Train: 0.09933 Test: 0.12901
Epoch 6500: New minimal relative error: 6.57%, model saved.
Epoch: 6500 Train: 0.12465 Test: 0.14080
Epoch: 6600 Train: 0.14598 Test: 0.22445
Epoch: 6700 Train: 0.27286 Test: 0.31462
Epoch: 6800 Train: 0.10311 Test: 0.13631
Epoch: 6900 Train: 0.14619 Test: 0.10808
Epoch: 7000 Train: 1.46374 Test: 0.91611
Epoch: 7100 Train: 0.06991 Test: 0.12342
Epoch: 7200 Train: 0.06243 Test: 0.09549
Epoch: 7300 Train: 0.11242 Test: 0.13013
Epoch: 7400 Train: 0.10282 Test: 0.15751
Epoch: 7500 Train: 0.30870 Test: 0.43102
Epoch: 7600 Train: 0.06734 Test: 0.11343
Epoch: 7700 Train: 0.06017 Test: 0.10675
Epoch: 7800 Train: 0.17072 Test: 0.20351
Epoch: 7900 Train: 0.09503 Test: 0.13530
Epoch: 8000 Train: 0.11796 Test: 0.17086
Epoch: 8100 Train: 0.21978 Test: 0.21663
Epoch: 8200 Train: 0.61338 Test: 0.65840
Epoch: 8300 Train: 0.07133 Test: 0.11440
Epoch: 8400 Train: 0.04620 Test: 0.07719
Epoch: 8500 Train: 0.08462 Test: 0.12176
Epoch: 8600 Train: 0.04350 Test: 0.07635
Epoch: 8700 Train: 0.44765 Test: 0.54212
Epoch: 8800 Train: 0.26096 Test: 0.28437
Epoch: 8900 Train: 0.06045 Test: 0.08822
Epoch: 9000 Train: 0.12662 Test: 0.13394
Epoch: 9100 Train: 0.09219 Test: 0.10103
Epoch: 9200 Train: 0.24831 Test: 0.29076
Epoch: 9300 Train: 0.15675 Test: 0.18822
Epoch: 9400 Train: 0.07467 Test: 0.09288
Epoch 9500: New minimal relative error: 5.77%, model saved.
Epoch: 9500 Train: 0.08347 Test: 0.10685
Epoch 9600: New minimal relative error: 4.74%, model saved.
Epoch: 9600 Train: 0.05165 Test: 0.07833
Epoch: 9700 Train: 0.06124 Test: 0.06981
Epoch: 9800 Train: 0.12072 Test: 0.16388
Epoch: 9900 Train: 0.07399 Test: 0.12560
Epoch: 9999 Train: 0.10080 Test: 0.10051
Training Loss: tensor(0.1008)
Test Loss: tensor(0.1005)
Learned LE: [ 0.8446278   0.01891499 -4.0853114 ]
True LE: [ 8.7675911e-01  7.2344532e-03 -1.4563449e+01]
Relative Error: [3.9049551  3.6519527  3.508026   3.4063895  3.1182485  3.351946
 3.6428652  3.662388   3.2909532  3.1710477  3.1356912  3.248393
 2.3573463  2.33269    4.0889416  5.724896   6.80769    7.4550667
 7.5487957  7.625699   7.730303   7.7346997  6.9941745  5.6640058
 4.8037715  4.3217854  3.9464765  3.9982512  4.12999    4.0652065
 4.138884   4.875386   5.836227   5.322185   4.931809   4.8883533
 4.449509   4.510092   5.0142655  5.130732   4.1844425  3.880904
 3.600063   3.540749   2.8879702  1.9493761  1.1327991  0.7921707
 0.75051254 0.85288703 1.0319465  1.0103112  1.0851713  1.1291542
 0.96545887 0.9067716  0.9029923  1.1957793  1.8253571  2.5575323
 3.065367   3.3353596  3.2965627  3.3945675  3.1826296  3.0222375
 2.9946868  2.8130336  2.8524985  3.0673325  3.4837537  3.0289533
 2.9309878  3.0313568  3.3068478  2.1180224  1.9843532  3.451273
 5.079167   6.2262063  6.5803895  6.6742377  6.633133   6.888355
 7.106536   7.0047574  6.1318097  4.7527337  4.141007   3.3686366
 3.4063902  3.40681    3.577971   3.5905666  3.9706867  4.90258
 4.5700183  3.931658   3.8309433  3.4611146  3.5941985  4.526897
 4.1256037  3.2328753  2.833607   2.8066058  2.7955868  1.9814779
 1.4258921  1.0226318  0.9263837  0.79002595 0.8019691  0.7775432
 0.8050925  0.8081229  0.7400739  0.73332906 0.71377385 0.7858796
 0.9745737  1.5037596  1.863929   2.4343193  2.5771868  2.6678371
 2.6457655  2.7676487  2.4152079  2.363677   2.3483944  2.0326312
 2.11188    2.7884486  2.5124226  2.4529276  2.5006752  2.7748458
 1.8216428  1.5437583  2.6887693  4.1454663  5.3399873  5.632677
 5.982346   5.9675326  6.095843   6.376138   6.5822096  6.517922
 5.2452726  4.356594   3.1239285  3.064361   3.0351517  2.9617915
 3.0517042  3.235707   3.6382143  3.95418    3.2443027  2.8336248
 2.413801   2.4893708  3.4636784  3.294576   2.5494277  2.0903106
 2.0809147  1.9573519  1.5830686  1.2870253  0.989799   0.89170706
 0.76113546 0.6627228  0.63480353 0.6507471  0.5080809  0.4672466
 0.55107313 0.59313565 0.5999623  0.6342995  0.7539527  1.0221773
 1.5290188  1.8854173  2.0171518  2.0210345  1.840102   1.8306205
 1.6862067  1.7940545  1.4588928  1.2801602  1.7932849  1.8010375
 1.864445   1.866488   2.0849597  1.3544049  0.92075354 2.1308303
 3.137136   4.039531   4.7092924  5.1746817  5.2160373  5.2542553
 5.579406   5.7492714  5.971172   5.9293246  4.7500553  3.5719824
 2.8633244  2.71966    2.647224   2.6538095  2.438598   2.4766796
 2.9508069  2.7220893  2.1506526  1.5522738  1.2901908  1.9332601
 2.5205798  1.9802336  1.576962   1.3492328  1.344282   1.4176095
 1.1978879  0.8010041  0.6550288  0.5945905  0.5870695  0.53511786
 0.5095383  0.2970017  0.3572422  0.5670034  0.51327413 0.63269347
 0.53572154 0.39443862 0.26594117 0.438414   0.9472403  1.3636587
 1.4068632  1.1488571  0.9616739  0.9280516  0.99975646 1.0650305
 0.7130149  0.63360536 1.2811289  1.1263957  1.2742586  1.2235681
 1.3217194  0.3594863  1.2489393  2.399141   3.3877637  3.5470605
 4.113013   4.4057746  4.4114294  4.145381   4.1106305  4.1670775
 4.2549896  4.311903   4.0307803  2.7646015  2.5783427  2.1626432
 2.0186865  2.161251   1.7881761  1.4986718  1.7153373  1.5726
 1.2388427  0.5389302  0.5486147  1.0505441  1.3276973  1.1819408
 0.96819913 0.776128   1.1227568  1.0829836  0.63713074 0.45678127
 0.46148756 0.3684588  0.3204287  0.341694   0.24252355 0.22071183
 0.4715413  0.5044174  0.49805295 0.50766826 0.44308272 0.43256566
 0.33567542 0.18733093 0.5639595  0.6539299  0.72046137 0.6453017
 0.3061577  0.24362703 0.52950734 0.6695749  0.39696875 0.33834442
 0.58986163 0.60461617 0.790871   0.635897   0.33462337 0.45368862
 1.2641708  2.8785517  3.3694289  2.9827666  3.1025815  3.1058006
 2.6835175  2.3477743  2.3418949  2.3380172  2.2481532  2.3699956
 2.593925   2.0570154  1.9104447  2.0361345  1.421267   1.358566
 1.2465972  0.60084856 0.62785614 0.7362895  0.8653242  0.6003361
 0.41687548 0.6997986  0.664438   0.67774284 0.50916207 0.663694
 0.7321762  0.4206953  0.39187738 0.35035944 0.34476736 0.27465445
 0.16764788 0.20575541 0.25376204 0.42037228 0.5180934  0.4327942
 0.3447373  0.37514478 0.440364   0.16394871 0.13032174 0.38249812
 0.5119375  0.4086332  0.34991935 0.24749695 0.212692   0.28610718
 0.44526315 0.40058827 0.36608902 0.80123043 0.4647794  0.36279118
 0.5427629  0.6166487  0.8939112  0.7643676  1.037981   2.6091807
 3.1159456  2.4950714  2.1061368  1.7162528 ]
