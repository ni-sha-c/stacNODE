time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 101.25%, model saved.
Epoch: 0 Train: 3749.89160 Test: 3973.39282
Epoch 100: New minimal relative error: 57.75%, model saved.
Epoch: 100 Train: 69.40822 Test: 74.40916
Epoch 200: New minimal relative error: 14.64%, model saved.
Epoch: 200 Train: 12.90178 Test: 16.08938
Epoch: 300 Train: 7.77568 Test: 8.44285
Epoch 400: New minimal relative error: 12.22%, model saved.
Epoch: 400 Train: 4.90528 Test: 6.79837
Epoch: 500 Train: 5.25169 Test: 7.02413
Epoch: 600 Train: 3.36933 Test: 4.40671
Epoch: 700 Train: 8.73528 Test: 12.13292
Epoch 800: New minimal relative error: 11.43%, model saved.
Epoch: 800 Train: 3.19712 Test: 3.62414
Epoch: 900 Train: 2.46591 Test: 3.79390
Epoch 1000: New minimal relative error: 8.77%, model saved.
Epoch: 1000 Train: 2.17098 Test: 2.77835
Epoch: 1100 Train: 6.63022 Test: 5.84797
Epoch: 1200 Train: 2.38652 Test: 3.07353
Epoch: 1300 Train: 2.13164 Test: 2.42009
Epoch: 1400 Train: 4.06450 Test: 3.64417
Epoch: 1500 Train: 8.61451 Test: 5.18792
Epoch: 1600 Train: 10.49177 Test: 11.78232
Epoch: 1700 Train: 2.44907 Test: 3.38293
Epoch: 1800 Train: 1.20069 Test: 1.62124
Epoch: 1900 Train: 1.46881 Test: 2.10629
Epoch: 2000 Train: 1.37053 Test: 1.46841
Epoch: 2100 Train: 0.76449 Test: 1.12672
Epoch: 2200 Train: 0.82654 Test: 1.04655
Epoch: 2300 Train: 2.22150 Test: 2.92838
Epoch: 2400 Train: 0.65936 Test: 1.23345
Epoch: 2500 Train: 0.91433 Test: 0.96255
Epoch: 2600 Train: 0.86594 Test: 1.04582
Epoch: 2700 Train: 0.55810 Test: 0.77327
Epoch: 2800 Train: 0.47525 Test: 0.72102
Epoch: 2900 Train: 0.79871 Test: 1.03917
Epoch: 3000 Train: 0.48114 Test: 0.67870
Epoch: 3100 Train: 0.36642 Test: 0.57734
Epoch: 3200 Train: 0.38375 Test: 0.57526
Epoch: 3300 Train: 0.37640 Test: 0.79186
Epoch: 3400 Train: 0.34876 Test: 0.54872
Epoch 3500: New minimal relative error: 6.88%, model saved.
Epoch: 3500 Train: 0.29289 Test: 0.47434
Epoch 3600: New minimal relative error: 6.25%, model saved.
Epoch: 3600 Train: 0.28040 Test: 0.45882
Epoch: 3700 Train: 0.26719 Test: 0.44506
Epoch: 3800 Train: 0.61022 Test: 0.85856
Epoch: 3900 Train: 2.46464 Test: 2.96305
Epoch: 4000 Train: 0.40238 Test: 0.56473
Epoch: 4100 Train: 0.24806 Test: 0.41157
Epoch: 4200 Train: 0.22951 Test: 0.38066
Epoch: 4300 Train: 0.47853 Test: 0.70632
Epoch: 4400 Train: 0.30411 Test: 0.47157
Epoch: 4500 Train: 0.21301 Test: 0.35690
Epoch: 4600 Train: 0.25268 Test: 0.41037
Epoch: 4700 Train: 0.69996 Test: 0.82536
Epoch: 4800 Train: 0.37616 Test: 0.51069
Epoch: 4900 Train: 0.44969 Test: 0.64922
Epoch: 5000 Train: 0.16709 Test: 0.29672
Epoch: 5100 Train: 0.22052 Test: 0.37702
Epoch: 5200 Train: 0.22045 Test: 0.34711
Epoch: 5300 Train: 0.23754 Test: 0.36402
Epoch: 5400 Train: 0.19828 Test: 0.33066
Epoch: 5500 Train: 0.14242 Test: 0.26008
Epoch: 5600 Train: 0.14962 Test: 0.27929
Epoch: 5700 Train: 0.17461 Test: 0.29372
Epoch: 5800 Train: 0.15168 Test: 0.26866
Epoch: 5900 Train: 0.42074 Test: 0.53252
Epoch 6000: New minimal relative error: 5.23%, model saved.
Epoch: 6000 Train: 0.43005 Test: 0.59275
Epoch: 6100 Train: 0.16456 Test: 0.30062
Epoch: 6200 Train: 0.14602 Test: 0.23950
Epoch: 6300 Train: 0.12258 Test: 0.22603
Epoch: 6400 Train: 0.27662 Test: 0.41261
Epoch: 6500 Train: 0.17223 Test: 0.27721
Epoch: 6600 Train: 0.20340 Test: 0.27080
Epoch: 6700 Train: 0.11444 Test: 0.21099
Epoch: 6800 Train: 0.10839 Test: 0.20869
Epoch: 6900 Train: 0.17484 Test: 0.25400
Epoch: 7000 Train: 0.11462 Test: 0.21086
Epoch: 7100 Train: 0.15994 Test: 0.26550
Epoch: 7200 Train: 0.10096 Test: 0.19587
Epoch: 7300 Train: 0.32462 Test: 0.46570
Epoch: 7400 Train: 0.14477 Test: 0.20948
Epoch: 7500 Train: 0.15615 Test: 0.25930
Epoch: 7600 Train: 0.10689 Test: 0.19693
Epoch: 7700 Train: 0.12694 Test: 0.21909
Epoch: 7800 Train: 0.15836 Test: 0.18957
Epoch: 7900 Train: 0.09791 Test: 0.18303
Epoch 8000: New minimal relative error: 4.54%, model saved.
Epoch: 8000 Train: 0.12210 Test: 0.21153
Epoch: 8100 Train: 0.08666 Test: 0.17162
Epoch: 8200 Train: 0.08599 Test: 0.16591
Epoch: 8300 Train: 0.09801 Test: 0.18705
Epoch: 8400 Train: 0.08588 Test: 0.16967
Epoch: 8500 Train: 0.09409 Test: 0.17969
Epoch: 8600 Train: 0.07660 Test: 0.15578
Epoch: 8700 Train: 0.07621 Test: 0.15676
Epoch: 8800 Train: 0.09387 Test: 0.17520
Epoch: 8900 Train: 0.07413 Test: 0.15132
Epoch: 9000 Train: 0.07266 Test: 0.14845
Epoch: 9100 Train: 0.07138 Test: 0.14653
Epoch 9200: New minimal relative error: 4.47%, model saved.
Epoch: 9200 Train: 0.07273 Test: 0.14747
Epoch: 9300 Train: 0.07163 Test: 0.14452
Epoch: 9400 Train: 0.07041 Test: 0.14393
Epoch: 9500 Train: 0.07657 Test: 0.14769
Epoch: 9600 Train: 0.08411 Test: 0.14848
Epoch: 9700 Train: 0.06720 Test: 0.13771
Epoch: 9800 Train: 0.06619 Test: 0.13552
Epoch: 9900 Train: 0.12135 Test: 0.20072
Epoch: 9999 Train: 0.07814 Test: 0.15101
Training Loss: tensor(0.0781)
Test Loss: tensor(0.1510)
Learned LE: [ 0.83082765  0.02302949 -4.139734  ]
True LE: [ 8.6010712e-01  4.1422783e-03 -1.4544641e+01]
Relative Error: [2.3408766  2.5644655  2.566028   2.785955   2.9890685  2.6627786
 2.382611   2.2610393  2.6631775  3.0606244  3.2107353  2.8040612
 2.6650345  2.7701287  2.4889457  2.2047892  1.6069422  1.033724
 0.8229265  0.7545506  0.5915049  0.46872604 0.40244517 0.46629757
 0.6114313  0.8782061  1.2326908  1.6261486  2.4128585  2.1478744
 2.3207715  2.3636966  2.0792975  2.1314375  2.6315815  3.5436566
 3.344154   3.1436772  3.0695732  2.070866   1.2002023  0.9401642
 0.6768574  0.54210454 0.2716885  0.19721252 0.3045481  0.31869447
 0.12269974 0.252133   0.28759825 0.37831128 0.7402626  1.210416
 1.1978447  1.0556717  0.84598136 0.8076257  0.70240736 1.0240531
 1.2558614  1.5993651  1.9722697  2.278181   2.3903775  2.3669493
 2.622896   2.6182234  2.278434   2.0146935  2.4268556  2.848706
 3.096178   2.7641823  2.6353426  2.6352205  2.4170048  2.1002617
 1.7686572  1.0313729  0.9437084  1.0241195  0.9350244  0.8520866
 0.85081446 0.81334525 0.8675348  0.73547095 0.81461734 1.1498907
 1.7655548  2.4012485  2.1410758  2.2571757  2.1879714  1.9893696
 2.212686   3.0428312  3.1372433  2.9634972  2.7707253  2.1386478
 1.3382605  0.8782831  0.743229   0.52016413 0.33533782 0.1668114
 0.16429906 0.145475   0.05164155 0.34132716 0.48946416 0.47963625
 0.72785497 1.1475357  1.3280911  1.3165292  1.293248   1.2360144
 1.0709438  0.93186915 0.9617809  1.1315255  1.6360382  1.9143767
 2.1118603  2.1117887  2.091818   2.2603533  2.2563846  1.9766588
 2.187634   2.6434584  2.9093418  2.7837105  2.640476   2.658164
 2.34809    1.9120144  1.4165974  1.0555564  1.5882913  1.8282036
 1.6596575  1.407887   1.4407122  1.3913716  1.3450986  1.2009534
 0.886483   0.6949249  1.0431646  1.6676087  2.2619388  2.1463542
 2.1314478  1.9802624  1.8523799  2.2823088  2.8132553  2.7367537
 2.6529427  2.201003   1.472424   0.9774599  0.73073566 0.44354698
 0.29440615 0.24856305 0.30711576 0.07878169 0.06930308 0.3342232
 0.68967396 0.51744866 0.617713   0.9089394  1.2290934  1.3515996
 1.3891444  1.3969072  1.2250564  1.1432363  1.0746953  1.100234
 1.1817474  1.6657219  1.8535647  1.9745473  1.8659177  1.8130891
 1.8520567  1.8288695  1.6697638  2.2975407  2.58628    2.700345
 2.5532708  2.57787    2.4566288  1.8326432  0.9871914  1.5645571
 2.2815602  2.9077628  3.2097318  3.0152724  2.4826646  1.8498365
 1.5515251  1.6285511  1.3062859  0.837007   0.7048384  0.9208035
 1.5502725  2.2435763  2.0530076  1.8732535  1.6814278  1.6299238
 2.175832   2.3373199  2.3529575  2.251448   1.6660349  1.0098243
 0.80953044 0.45634168 0.1558328  0.36834437 0.1335528  0.4056167
 0.11776298 0.3404375  0.4995217  0.580941   0.5705675  0.7261271
 0.9245013  1.2919341  1.3530993  1.3019508  1.1902101  1.1672903
 1.1838498  1.1107721  1.2040755  1.2291033  1.7629617  1.9338388
 1.926798   1.6325715  1.4975493  1.4972196  1.4711962  1.4502572
 2.038736   2.4679492  2.4600635  2.386386   2.3541934  1.6439574
 0.99603796 1.8276632  2.766373   3.5030496  4.16619    4.037274
 3.867516   3.6560066  3.3734899  2.4349434  1.6540151  1.4073263
 1.0645788  0.85715455 0.8877203  1.313888   2.162444   1.9344839
 1.6118941  1.3451893  1.264746   1.7438203  1.8527627  1.8507733
 1.7571092  1.2458946  0.84625006 0.5784416  0.12576567 0.17723864
 0.37918654 0.22230375 0.52386165 0.3342594  0.3432998  0.54183203
 0.57623315 0.6945594  0.8287049  0.9668898  1.2546718  1.0061574
 0.85320735 0.874194   0.9203758  0.88236743 0.90902674 1.057574
 1.205188   1.5988022  1.9067873  1.9332913  1.6334763  1.326217
 1.1951473  1.1581775  1.2507011  1.6543323  2.0188258  2.3865275
 2.2828658  1.8481376  1.0197653  1.5990409  2.6761613  3.6638994
 3.356513   3.3841197  3.3352602  3.152628   2.9640021  2.7988834
 2.614644   2.580113   2.0119033  1.6020172  1.1086543  0.76317006
 0.9536486  1.5800483  1.7836733  1.3901478  1.1314782  0.7574148
 1.083036   1.2297001  1.2110955  1.2839282  1.0389854  0.6708269
 0.4331351  0.02846906 0.27994847 0.447159   0.40413785 0.6065716
 0.31040907 0.2351955  0.44075406 0.50890803 0.67879766 0.8008252
 0.9536756  0.8786483  0.51428294 0.4009127  0.53054607 0.547077
 0.6170573  0.7226138  0.8279425  0.89448756 1.1973456  1.3728536
 1.5338355  1.3228482  0.9558713  0.9050023  0.940208   0.949277
 1.1896952  1.5601414  1.9559351  2.1323934  1.3589436  1.3113146
 2.217927   3.041331   3.1502514  2.8975039  2.54711    2.4774551
 2.4251056  2.2109737  2.0260732  1.8763648 ]
