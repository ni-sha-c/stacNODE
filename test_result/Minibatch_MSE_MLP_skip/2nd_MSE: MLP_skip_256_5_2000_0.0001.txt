time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 107.69%, model saved.
Epoch: 0 Train: 4115.27148 Test: 4322.78809
Epoch: 100 Train: 172.21507 Test: 152.79260
Epoch 200: New minimal relative error: 27.13%, model saved.
Epoch: 200 Train: 34.10241 Test: 28.11536
Epoch: 300 Train: 12.76670 Test: 12.31512
Epoch 400: New minimal relative error: 20.11%, model saved.
Epoch: 400 Train: 7.00597 Test: 9.28530
Epoch: 500 Train: 6.29730 Test: 6.35232
Epoch: 600 Train: 4.59511 Test: 4.74655
Epoch 700: New minimal relative error: 19.71%, model saved.
Epoch: 700 Train: 6.76578 Test: 7.40674
Epoch: 800 Train: 5.07948 Test: 5.95913
Epoch 900: New minimal relative error: 14.49%, model saved.
Epoch: 900 Train: 4.24591 Test: 4.98797
Epoch: 1000 Train: 5.25533 Test: 4.09531
Epoch: 1100 Train: 3.16262 Test: 2.84063
Epoch 1200: New minimal relative error: 13.96%, model saved.
Epoch: 1200 Train: 5.21642 Test: 2.50000
Epoch: 1300 Train: 2.57529 Test: 2.75754
Epoch: 1400 Train: 4.57072 Test: 5.47107
Epoch: 1500 Train: 9.48242 Test: 9.89805
Epoch: 1600 Train: 2.58292 Test: 2.30542
Epoch: 1700 Train: 1.42390 Test: 1.52626
Epoch: 1800 Train: 1.54459 Test: 1.73124
Epoch 1900: New minimal relative error: 13.21%, model saved.
Epoch: 1900 Train: 0.88900 Test: 0.88954
Epoch 2000: New minimal relative error: 7.54%, model saved.
Epoch: 2000 Train: 1.10375 Test: 0.95876
Epoch: 2100 Train: 1.48174 Test: 1.60436
Epoch: 2200 Train: 0.92435 Test: 0.89867
Epoch: 2300 Train: 1.47374 Test: 1.75193
Epoch: 2400 Train: 1.23686 Test: 1.24445
Epoch: 2500 Train: 2.00589 Test: 2.27251
Epoch: 2600 Train: 3.35399 Test: 2.51096
Epoch: 2700 Train: 0.74137 Test: 0.61866
Epoch: 2800 Train: 1.26064 Test: 1.47255
Epoch: 2900 Train: 2.12929 Test: 1.92559
Epoch: 3000 Train: 2.13672 Test: 1.88905
Epoch: 3100 Train: 1.08951 Test: 1.07051
Epoch: 3200 Train: 0.46415 Test: 0.44593
Epoch: 3300 Train: 1.42384 Test: 1.59088
Epoch: 3400 Train: 1.12133 Test: 1.22749
Epoch: 3500 Train: 0.94057 Test: 0.96461
Epoch: 3600 Train: 0.98508 Test: 1.05972
Epoch: 3700 Train: 0.75772 Test: 0.70209
Epoch: 3800 Train: 0.38043 Test: 0.36446
Epoch: 3900 Train: 0.37785 Test: 0.36461
Epoch: 4000 Train: 1.28226 Test: 1.53713
Epoch: 4100 Train: 2.01213 Test: 1.61876
Epoch: 4200 Train: 1.30987 Test: 0.60983
Epoch: 4300 Train: 0.37489 Test: 0.37455
Epoch: 4400 Train: 0.70026 Test: 0.83594
Epoch: 4500 Train: 1.43700 Test: 1.75671
Epoch: 4600 Train: 0.96486 Test: 0.96165
Epoch: 4700 Train: 0.33741 Test: 0.32041
Epoch: 4800 Train: 0.36742 Test: 0.37743
Epoch: 4900 Train: 0.33484 Test: 0.44942
Epoch 5000: New minimal relative error: 7.04%, model saved.
Epoch: 5000 Train: 0.74718 Test: 0.48667
Epoch: 5100 Train: 0.77244 Test: 0.74737
Epoch: 5200 Train: 0.99811 Test: 0.96058
Epoch: 5300 Train: 0.66898 Test: 0.75391
Epoch: 5400 Train: 0.46382 Test: 0.47979
Epoch 5500: New minimal relative error: 6.37%, model saved.
Epoch: 5500 Train: 0.42394 Test: 0.43811
Epoch: 5600 Train: 0.35119 Test: 0.32950
Epoch: 5700 Train: 0.52916 Test: 0.58682
Epoch: 5800 Train: 0.57576 Test: 0.57909
Epoch: 5900 Train: 0.29583 Test: 0.29758
Epoch: 6000 Train: 0.30432 Test: 0.31620
Epoch: 6100 Train: 0.24585 Test: 0.24440
Epoch: 6200 Train: 1.27212 Test: 1.08892
Epoch: 6300 Train: 0.38880 Test: 0.34287
Epoch: 6400 Train: 0.32171 Test: 0.31492
Epoch: 6500 Train: 0.32689 Test: 0.42821
Epoch: 6600 Train: 1.16352 Test: 1.42762
Epoch: 6700 Train: 0.55156 Test: 0.64763
Epoch: 6800 Train: 0.21368 Test: 0.22147
Epoch: 6900 Train: 0.24000 Test: 0.23964
Epoch: 7000 Train: 0.21807 Test: 0.21670
Epoch: 7100 Train: 0.23999 Test: 0.25845
Epoch: 7200 Train: 0.23643 Test: 0.22500
Epoch: 7300 Train: 0.24863 Test: 0.22318
Epoch: 7400 Train: 0.21763 Test: 0.21462
Epoch: 7500 Train: 0.23397 Test: 0.24580
Epoch: 7600 Train: 0.61944 Test: 0.74797
Epoch: 7700 Train: 1.60968 Test: 1.65207
Epoch: 7800 Train: 0.18448 Test: 0.18676
Epoch: 7900 Train: 0.18662 Test: 0.19024
Epoch: 8000 Train: 0.27532 Test: 0.31247
Epoch: 8100 Train: 0.39051 Test: 0.44825
Epoch: 8200 Train: 0.64771 Test: 0.74035
Epoch: 8300 Train: 0.57258 Test: 0.65171
Epoch: 8400 Train: 0.53053 Test: 0.62472
Epoch: 8500 Train: 0.29313 Test: 0.22364
Epoch: 8600 Train: 0.74677 Test: 0.77039
Epoch: 8700 Train: 0.28177 Test: 0.32830
Epoch: 8800 Train: 0.30807 Test: 0.26517
Epoch: 8900 Train: 0.23695 Test: 0.21394
Epoch: 9000 Train: 0.79766 Test: 0.60563
Epoch: 9100 Train: 0.15954 Test: 0.16373
Epoch: 9200 Train: 0.16346 Test: 0.16645
Epoch: 9300 Train: 0.15683 Test: 0.16165
Epoch: 9400 Train: 0.15538 Test: 0.15946
Epoch: 9500 Train: 0.45192 Test: 0.28874
Epoch: 9600 Train: 0.15205 Test: 0.15649
Epoch: 9700 Train: 0.15867 Test: 0.16304
Epoch: 9800 Train: 0.15215 Test: 0.15538
Epoch: 9900 Train: 0.14863 Test: 0.15406
Epoch: 9999 Train: 0.16899 Test: 0.15683
Training Loss: tensor(0.1690)
Test Loss: tensor(0.1568)
Learned LE: [ 0.70008963 -0.07887921 -2.9538321 ]
True LE: [ 8.5706162e-01 -3.1100390e-03 -1.4524492e+01]
Relative Error: [ 9.179907  10.17972   10.814836  10.633267  10.484674  10.209303
  9.539      8.535833   7.803514   7.0662603  6.1324806  5.6028204
  5.3586044  5.1593013  4.8917317  4.814136   4.9022303  4.7117734
  4.6384397  4.6781197  4.888888   5.4061418  5.9496317  6.199175
  6.213364   5.9884944  5.411431   4.637814   3.9097183  3.832015
  3.9006476  3.7880983  3.2981906  3.0106266  3.0328336  3.7243896
  4.620316   5.498447   5.6410713  5.625046   5.3977003  5.5292954
  5.5438437  5.5602016  5.4549274  5.2783775  5.064445   4.571278
  4.0582147  3.7637842  3.7697368  3.9108675  4.098319   4.2828283
  4.4534917  4.634882   4.8918924  4.899546   5.1855845  5.810069
  6.595396   7.3791466  8.179853   9.008636   9.654462   9.372492
  9.227697   8.932247   8.657247   7.743273   6.978395   6.2099547
  5.494911   4.9382625  4.811973   4.7570033  4.5899005  4.503542
  4.5573373  4.215696   4.0742555  4.1711473  4.4472313  4.874161
  5.511152   5.9319277  6.018278   5.8737216  5.3355656  4.553671
  3.6752455  3.7904475  3.7206829  3.6680267  3.0216434  2.6021364
  2.5655053  3.097785   3.9074807  4.7417593  5.0287776  5.033494
  4.847957   5.001205   5.042406   4.946102   4.8175263  4.6458373
  4.4715834  3.8874059  3.4664795  3.1746943  3.071392   3.3006277
  3.5735826  3.8475535  4.077524   4.2616997  4.4360895  4.6441298
  4.794053   5.2311263  5.993122   6.6336365  7.2789245  7.9600563
  8.600744   8.314715   8.050531   7.847809   7.696507   7.0903034
  6.1633587  5.4427123  4.959012   4.4601355  4.224645   4.157288
  4.044445   4.155329   4.290274   4.014223   3.8003027  3.7153385
  4.006647   4.507712   5.094161   5.7342906  5.927817   5.8686333
  5.4205084  4.662361   3.5925481  3.6112995  3.5916486  3.6986244
  2.8700392  2.3015885  2.1517408  2.504132   3.1985037  3.9597359
  4.3894706  4.4377556  4.2824464  4.431997   4.5305457  4.258827
  4.1718273  4.0453715  3.7939355  3.3001788  2.9768465  2.7813745
  2.6857865  2.7633123  3.1319537  3.5005238  3.8148522  4.07259
  4.2004967  4.2303796  4.43841    4.78883    5.5150867  6.0348983
  6.5167027  7.020069   7.555494   7.5195236  7.042627   6.8870544
  6.6363664  6.3578463  5.456779   4.6514735  4.2685905  3.9741833
  3.6352806  3.5313542  3.369159   3.4211357  3.7083774  3.7686567
  3.6116738  3.608287   3.7050772  4.022373   4.5798016  5.1963243
  5.655985   5.8087273  5.660432   4.963791   3.9410033  3.537692
  3.4941814  3.6989763  2.8981428  2.1314943  1.818242   1.9892614
  2.5249152  3.1644382  3.6039238  3.838944   3.7148907  3.8150568
  3.8758018  3.5851996  3.5203702  3.4355907  3.1902096  2.7840483
  2.5483603  2.444286   2.4430354  2.5297139  2.7266908  3.1945267
  3.6281164  3.984601   4.0149055  4.1846805  4.122924   4.3696012
  4.952289   5.631518   5.948007   6.280857   6.7030177  6.887637
  6.299667   5.8773766  5.706605   5.5166864  4.8921742  3.922191
  3.566475   3.4086723  3.0910761  2.9109015  2.7815163  2.7194088
  2.8538353  3.0882626  3.1732156  3.3704813  3.5177317  3.6402602
  3.836488   4.419865   5.0072746  5.328163   5.424092   5.1451597
  4.507998   3.6967094  3.3980563  3.6400259  3.198337   2.133824
  1.5789157  1.5858027  1.9433237  2.4281576  2.7459235  3.227758
  3.1245053  3.157346   3.2409258  2.9369454  2.8661146  2.8061504
  2.5775976  2.3211265  2.1680338  2.1484787  2.2341912  2.4096851
  2.6511068  2.9085045  3.4519236  3.720244   3.7399762  3.753918
  3.7493885  3.72212    4.1130824  5.073075   5.6458693  5.8322473
  6.0523353  6.242194   5.9462357  5.1697783  4.7899985  4.517799
  4.0595436  3.4356003  2.9097507  2.7172294  2.879625   2.6922634
  2.5631118  2.3493588  2.3287354  2.490002   2.4106402  2.649551
  2.920338   3.0700843  3.2311764  3.3884451  4.1614695  4.7986183
  5.090419   5.026459   4.541944   3.8111348  3.6984642  3.2955353
  3.6739767  2.4926872  1.5106027  1.281822   1.5172211  1.8152012
  1.9255946  2.2229075  2.5110557  2.4911788  2.680426   2.4785268
  2.2536604  2.2627978  2.0271168  1.9281881  1.8468344  1.894781
  2.0480216  2.3046489  2.6284983  2.9050686  2.6276875  2.798039
  3.0217116  2.7515259  2.9633965  3.0135245  3.1436098  3.6718535
  4.779698   5.5744276  5.7378163  5.72839    5.8149953  5.018334
  4.1306367  3.6859584  3.2587483  2.8523233  2.4121459  2.41388
  2.4246788  2.5880804  2.4188387  2.327595   2.177259   2.267552
  2.455988   2.2793741  2.0791557  2.1705666  2.396549   2.677454
  3.0015078  3.659884   4.293441   4.706099 ]
