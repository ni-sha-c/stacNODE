time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 102.12%, model saved.
Epoch: 0 Train: 3729.90576 Test: 4084.66187
Epoch 100: New minimal relative error: 52.17%, model saved.
Epoch: 100 Train: 106.68467 Test: 99.70421
Epoch 200: New minimal relative error: 19.35%, model saved.
Epoch: 200 Train: 22.47898 Test: 19.33770
Epoch: 300 Train: 7.64198 Test: 9.02096
Epoch 400: New minimal relative error: 17.69%, model saved.
Epoch: 400 Train: 4.90525 Test: 4.16569
Epoch 500: New minimal relative error: 17.10%, model saved.
Epoch: 500 Train: 12.17945 Test: 6.62282
Epoch: 600 Train: 4.03326 Test: 4.18535
Epoch: 700 Train: 20.94673 Test: 22.47745
Epoch: 800 Train: 3.92037 Test: 4.70639
Epoch: 900 Train: 22.12061 Test: 14.64813
Epoch: 1000 Train: 2.08571 Test: 1.75227
Epoch 1100: New minimal relative error: 17.03%, model saved.
Epoch: 1100 Train: 6.51038 Test: 4.38384
Epoch: 1200 Train: 4.16153 Test: 4.97239
Epoch 1300: New minimal relative error: 11.92%, model saved.
Epoch: 1300 Train: 2.18671 Test: 1.13025
Epoch: 1400 Train: 5.25825 Test: 6.76315
Epoch: 1500 Train: 15.26389 Test: 13.11548
Epoch: 1600 Train: 0.76177 Test: 0.87117
Epoch: 1700 Train: 3.53812 Test: 4.71361
Epoch 1800: New minimal relative error: 8.55%, model saved.
Epoch: 1800 Train: 1.25139 Test: 1.08061
Epoch: 1900 Train: 8.50492 Test: 12.51008
Epoch: 2000 Train: 1.50520 Test: 0.88214
Epoch: 2100 Train: 0.92105 Test: 0.72188
Epoch: 2200 Train: 0.49869 Test: 0.51099
Epoch: 2300 Train: 1.08797 Test: 1.38806
Epoch: 2400 Train: 2.69194 Test: 3.65074
Epoch: 2500 Train: 1.02750 Test: 1.18874
Epoch: 2600 Train: 0.58734 Test: 0.40649
Epoch 2700: New minimal relative error: 5.55%, model saved.
Epoch: 2700 Train: 0.79563 Test: 1.03991
Epoch: 2800 Train: 4.38887 Test: 3.93695
Epoch: 2900 Train: 1.32299 Test: 1.43875
Epoch: 3000 Train: 2.53411 Test: 3.03630
Epoch: 3100 Train: 0.82188 Test: 0.63015
Epoch: 3200 Train: 0.49705 Test: 0.65676
Epoch: 3300 Train: 2.53243 Test: 2.01337
Epoch: 3400 Train: 1.00544 Test: 0.52366
Epoch: 3500 Train: 0.19412 Test: 0.19692
Epoch: 3600 Train: 0.18827 Test: 0.21435
Epoch: 3700 Train: 1.46278 Test: 1.62517
Epoch: 3800 Train: 1.58877 Test: 1.83520
Epoch 3900: New minimal relative error: 4.64%, model saved.
Epoch: 3900 Train: 0.17317 Test: 0.17834
Epoch: 4000 Train: 1.93640 Test: 2.72957
Epoch: 4100 Train: 1.27094 Test: 0.46769
Epoch: 4200 Train: 0.71964 Test: 0.48644
Epoch: 4300 Train: 0.11874 Test: 0.16198
Epoch: 4400 Train: 0.36456 Test: 0.41709
Epoch: 4500 Train: 0.13292 Test: 0.14758
Epoch: 4600 Train: 0.69524 Test: 0.80570
Epoch: 4700 Train: 2.28906 Test: 1.86544
Epoch: 4800 Train: 0.25841 Test: 0.18538
Epoch: 4900 Train: 4.22226 Test: 4.80910
Epoch: 5000 Train: 0.97361 Test: 1.03163
Epoch: 5100 Train: 1.36284 Test: 2.08982
Epoch: 5200 Train: 0.33275 Test: 0.62711
Epoch: 5300 Train: 0.34482 Test: 0.26408
Epoch: 5400 Train: 0.08666 Test: 0.10960
Epoch: 5500 Train: 0.60750 Test: 0.70046
Epoch: 5600 Train: 0.18223 Test: 0.19292
Epoch: 5700 Train: 0.55820 Test: 0.61790
Epoch: 5800 Train: 0.35788 Test: 0.17940
Epoch: 5900 Train: 0.35513 Test: 0.27138
Epoch: 6000 Train: 0.07219 Test: 0.08535
Epoch: 6100 Train: 0.07605 Test: 0.08685
Epoch: 6200 Train: 0.12867 Test: 0.13690
Epoch: 6300 Train: 0.36026 Test: 0.27681
Epoch: 6400 Train: 0.13094 Test: 0.14048
Epoch: 6500 Train: 0.10158 Test: 0.11404
Epoch: 6600 Train: 0.10405 Test: 0.09310
Epoch: 6700 Train: 0.19374 Test: 0.14797
Epoch: 6800 Train: 0.30265 Test: 0.28806
Epoch: 6900 Train: 0.06488 Test: 0.07797
Epoch: 7000 Train: 1.87582 Test: 1.83084
Epoch: 7100 Train: 0.13132 Test: 0.12666
Epoch: 7200 Train: 0.25544 Test: 0.29243
Epoch: 7300 Train: 0.05372 Test: 0.06469
Epoch: 7400 Train: 0.08448 Test: 0.09505
Epoch: 7500 Train: 0.15093 Test: 0.10297
Epoch: 7600 Train: 0.08499 Test: 0.09027
Epoch: 7700 Train: 0.57606 Test: 0.61817
Epoch: 7800 Train: 0.08151 Test: 0.07874
Epoch: 7900 Train: 0.06791 Test: 0.08809
Epoch: 8000 Train: 0.27670 Test: 0.30348
Epoch: 8100 Train: 0.19915 Test: 0.23645
Epoch: 8200 Train: 0.32319 Test: 0.29194
Epoch: 8300 Train: 0.05146 Test: 0.06523
Epoch: 8400 Train: 0.10328 Test: 0.11669
Epoch: 8500 Train: 0.07296 Test: 0.08814
Epoch: 8600 Train: 0.04550 Test: 0.05397
Epoch: 8700 Train: 0.09972 Test: 0.09799
Epoch: 8800 Train: 0.27386 Test: 0.28092
Epoch: 8900 Train: 0.04882 Test: 0.05264
Epoch: 9000 Train: 0.07365 Test: 0.09130
Epoch: 9100 Train: 0.38486 Test: 0.48856
Epoch: 9200 Train: 0.06454 Test: 0.06626
Epoch: 9300 Train: 0.47911 Test: 0.43223
Epoch: 9400 Train: 0.86798 Test: 0.63957
Epoch: 9500 Train: 0.10595 Test: 0.11409
Epoch: 9600 Train: 0.14772 Test: 0.17178
Epoch: 9700 Train: 0.05373 Test: 0.05513
Epoch: 9800 Train: 0.03477 Test: 0.04793
Epoch: 9900 Train: 0.03900 Test: 0.04787
Epoch: 9999 Train: 0.14251 Test: 0.18570
Training Loss: tensor(0.1425)
Test Loss: tensor(0.1857)
Learned LE: [ 0.7173109  0.0313522 -2.9541337]
True LE: [ 8.8327461e-01  3.3899439e-03 -1.4561515e+01]
Relative Error: [2.5381365  3.1465032  3.83927    4.553414   5.1199255  5.9652267
 6.956791   7.868531   8.467353   8.871175   9.151248   9.314681
 8.960163   8.190986   6.4611197  4.35622    2.7244806  2.5296516
 2.9421542  4.069902   5.009454   5.947077   6.488348   6.748144
 6.676649   6.3298516  5.706547   4.890366   4.072953   3.3683174
 2.7646778  2.4248402  2.2404845  2.2428858  2.6179123  2.8031337
 2.977148   3.4670684  3.9755118  3.696029   3.62079    3.31324
 3.0030863  3.247542   3.8265438  4.3971734  4.9593797  5.000652
 4.929762   4.362054   3.8672261  3.6097949  3.2891288  2.9041066
 2.492059   2.1250606  2.1049032  2.1062505  2.0517619  1.9216843
 1.804731   1.8439081  2.0394633  2.5538077  3.11215    3.8400264
 4.378653   5.0592165  5.90814    6.954655   7.669049   8.075272
 8.163428   8.441561   8.267706   7.6827645  6.2616086  4.1276903
 2.515537   2.77227    3.5981967  4.653521   5.733592   6.6783166
 7.1148753  7.4549103  7.3324733  6.943659   6.226883   5.3473153
 4.414519   3.6923344  3.1050408  2.5696588  2.1966114  2.1260273
 2.3872304  2.5790496  2.5293243  3.0698948  3.5934138  3.3857546
 3.3764906  3.0405447  2.5774715  2.7170568  3.1594424  3.602798
 4.150188   4.270987   4.3231974  3.9856057  3.5468385  3.3071377
 3.0364583  2.690816   2.2310302  1.8755456  1.8142031  1.9492241
 1.9379079  1.8156612  1.6931038  1.6043265  1.6408874  1.8973175
 2.3501008  2.977966   3.6029153  4.204767   4.8788013  5.8889556
 6.7154818  7.1228013  7.273103   7.4560018  7.245172   6.9809303
 5.8386397  4.0510807  2.6291256  3.0893083  4.124002   5.116285
 6.264735   7.1206875  7.6050086  7.8743415  8.037532   7.6326027
 6.7705884  5.9046183  4.849004   4.0810757  3.4217377  2.9674745
 2.4175918  2.1556263  2.1453853  2.386854   2.5018775  2.7604628
 3.3467188  3.122538   3.097583   2.8553848  2.2307196  2.164672
 2.3994622  2.9004347  3.274313   3.6690469  3.6929674  3.5675056
 3.2756195  2.9873674  2.77323    2.4960423  2.0808299  1.7606782
 1.5987165  1.682917   1.8025937  1.8066328  1.7347703  1.4926672
 1.3953342  1.3685479  1.6526148  2.1065423  2.7739682  3.2454197
 3.8798962  4.5812902  5.472145   5.9846306  6.292294   6.4928226
 6.3246503  6.122515   5.545362   4.0048904  2.6818619  3.4375086
 4.5276065  5.45252    6.3142047  6.946381   7.3426805  7.654381
 8.10316    8.226667   7.420777   6.4965615  5.4735994  4.5596867
 3.8217132  3.3150177  2.739585   2.3809361  2.1112456  2.2206693
 2.5261781  2.7109523  3.1145706  3.055256   2.9497135  2.4606726
 1.8161081  1.5854597  1.6738418  2.1336548  2.6855474  3.2403166
 3.1621833  2.9286585  2.7495406  2.4934762  2.4819384  2.2451115
 1.87767    1.6337451  1.480967   1.3775573  1.5504175  1.7054505
 1.7013557  1.5052067  1.2291567  1.1111023  1.1852012  1.3866179
 1.8437204  2.3598897  2.8268871  3.278482   4.012595   4.6398306
 5.153241   5.286594   5.3028717  5.365323   5.1313486  3.9882383
 2.6412623  3.6327407  4.704522   5.8121076  6.6094174  6.9175916
 7.1165934  7.4038377  7.7174335  7.8552375  7.5417542  6.731958
 5.6989164  4.9742007  4.1224065  3.4858341  2.9752364  2.5788867
 2.3325708  2.3010497  2.2812798  2.5418274  2.966218   3.041012
 2.5349123  2.2323167  1.5225776  1.0472428  0.9291101  1.2698822
 2.1220357  2.5933173  2.6522317  2.4313874  2.2686827  1.9054707
 1.8654963  1.8422451  1.6650608  1.445306   1.3261335  1.265492
 1.2558655  1.4965185  1.489886   1.3551657  1.0944263  0.94511575
 0.93951523 0.99714315 1.0962735  1.3861315  1.8410046  2.2685924
 2.6929257  3.3619697  3.8182042  4.0388575  4.1501307  4.369263
 4.384935   3.7079234  2.744589   2.9652123  4.6460586  5.903536
 6.760021   6.8903418  6.855522   7.0455017  7.2048097  7.198307
 6.824782   6.403185   5.572436   4.9022355  4.3889947  3.7199936
 3.0132535  2.6636882  2.5234914  2.2930317  2.2859228  2.128773
 2.624263   2.9292994  2.274903   2.004116   1.5970061  0.8799659
 0.5132712  0.5989252  1.2589723  1.9102196  2.043544   1.9776977
 1.848301   1.5302995  1.2835479  1.2942652  1.3047112  1.2003982
 1.1377188  1.1188946  0.9966583  0.9794579  1.1659735  1.0005147
 0.8509758  0.678819   0.72153974 0.8668526  1.0146906  0.6790953
 0.8627423  1.2805033  1.744226   2.1032457  2.6774337  2.8370461
 3.007832   3.3782575  3.459708   3.0612955  2.695637   2.2653234
 3.786572   4.947699   5.6394477  5.926763   5.997742   6.0842404
 6.23792    6.093091   5.9499116  5.6527042 ]
