time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.70%, model saved.
Epoch: 0 Train: 4405.97217 Test: 4061.48022
Epoch 100: New minimal relative error: 73.48%, model saved.
Epoch: 100 Train: 88.77604 Test: 73.41972
Epoch 200: New minimal relative error: 50.86%, model saved.
Epoch: 200 Train: 23.86035 Test: 45.48893
Epoch: 300 Train: 69.30595 Test: 77.82365
Epoch 400: New minimal relative error: 42.64%, model saved.
Epoch: 400 Train: 5.46119 Test: 20.52023
Epoch 500: New minimal relative error: 29.35%, model saved.
Epoch: 500 Train: 8.23539 Test: 9.03916
Epoch 600: New minimal relative error: 22.80%, model saved.
Epoch: 600 Train: 2.78125 Test: 6.26209
Epoch: 700 Train: 3.27920 Test: 6.48357
Epoch: 800 Train: 23.99875 Test: 25.68806
Epoch: 900 Train: 1.32147 Test: 3.95089
Epoch: 1000 Train: 17.30840 Test: 27.11281
Epoch 1100: New minimal relative error: 19.75%, model saved.
Epoch: 1100 Train: 2.88926 Test: 4.98508
Epoch: 1200 Train: 1.57513 Test: 3.59917
Epoch: 1300 Train: 1.97525 Test: 3.86580
Epoch: 1400 Train: 4.03638 Test: 6.98684
Epoch: 1500 Train: 4.20597 Test: 6.02232
Epoch: 1600 Train: 4.25754 Test: 3.81571
Epoch: 1700 Train: 2.14766 Test: 3.64446
Epoch 1800: New minimal relative error: 17.80%, model saved.
Epoch: 1800 Train: 1.53042 Test: 3.23316
Epoch: 1900 Train: 13.68322 Test: 10.50127
Epoch: 2000 Train: 3.75823 Test: 4.11146
Epoch 2100: New minimal relative error: 17.11%, model saved.
Epoch: 2100 Train: 1.57516 Test: 3.35992
Epoch 2200: New minimal relative error: 14.59%, model saved.
Epoch: 2200 Train: 0.81189 Test: 2.17379
Epoch: 2300 Train: 1.56704 Test: 2.36022
Epoch: 2400 Train: 0.86750 Test: 1.69073
Epoch: 2500 Train: 0.87908 Test: 2.42988
Epoch: 2600 Train: 0.83485 Test: 1.97606
Epoch: 2700 Train: 2.56181 Test: 2.58596
Epoch: 2800 Train: 0.48435 Test: 1.53194
Epoch: 2900 Train: 3.88394 Test: 5.28500
Epoch: 3000 Train: 0.93563 Test: 2.05042
Epoch: 3100 Train: 0.63251 Test: 1.77139
Epoch: 3200 Train: 0.97257 Test: 1.62028
Epoch: 3300 Train: 0.29468 Test: 1.46179
Epoch 3400: New minimal relative error: 12.77%, model saved.
Epoch: 3400 Train: 2.20538 Test: 3.32723
Epoch: 3500 Train: 2.78007 Test: 4.13048
Epoch: 3600 Train: 1.37254 Test: 2.08347
Epoch: 3700 Train: 1.88658 Test: 2.38394
Epoch: 3800 Train: 0.20956 Test: 1.00278
Epoch: 3900 Train: 0.49342 Test: 1.46803
Epoch: 4000 Train: 0.42445 Test: 1.28013
Epoch: 4100 Train: 0.81869 Test: 1.54531
Epoch: 4200 Train: 0.12068 Test: 0.88680
Epoch: 4300 Train: 0.09727 Test: 0.83608
Epoch: 4400 Train: 0.22438 Test: 1.04224
Epoch: 4500 Train: 0.35424 Test: 1.23733
Epoch: 4600 Train: 1.29956 Test: 2.43128
Epoch: 4700 Train: 0.16721 Test: 0.86096
Epoch: 4800 Train: 0.91128 Test: 1.89133
Epoch 4900: New minimal relative error: 11.38%, model saved.
Epoch: 4900 Train: 0.49518 Test: 1.25741
Epoch: 5000 Train: 0.21056 Test: 0.95151
Epoch: 5100 Train: 0.36696 Test: 1.03957
Epoch: 5200 Train: 2.01216 Test: 2.35420
Epoch: 5300 Train: 0.36079 Test: 1.18781
Epoch: 5400 Train: 0.17808 Test: 0.82060
Epoch 5500: New minimal relative error: 10.93%, model saved.
Epoch: 5500 Train: 0.06725 Test: 0.70350
Epoch: 5600 Train: 0.29645 Test: 0.92039
Epoch 5700: New minimal relative error: 7.98%, model saved.
Epoch: 5700 Train: 1.34886 Test: 1.74967
Epoch: 5800 Train: 0.18628 Test: 0.79851
Epoch: 5900 Train: 0.05395 Test: 0.64912
Epoch: 6000 Train: 0.05948 Test: 0.65130
Epoch: 6100 Train: 0.44057 Test: 1.13792
Epoch: 6200 Train: 2.36159 Test: 3.47614
Epoch: 6300 Train: 0.12058 Test: 0.69862
Epoch: 6400 Train: 0.09082 Test: 0.65824
Epoch: 6500 Train: 0.29451 Test: 0.81059
Epoch: 6600 Train: 0.08686 Test: 0.66354
Epoch: 6700 Train: 0.82759 Test: 1.63332
Epoch: 6800 Train: 1.25144 Test: 1.64657
Epoch: 6900 Train: 0.07362 Test: 0.63278
Epoch: 7000 Train: 0.07059 Test: 0.62798
Epoch: 7100 Train: 0.97530 Test: 1.61890
Epoch: 7200 Train: 0.08525 Test: 0.59828
Epoch: 7300 Train: 0.04907 Test: 0.56758
Epoch: 7400 Train: 0.04683 Test: 0.57112
Epoch: 7500 Train: 0.24709 Test: 0.78629
Epoch: 7600 Train: 0.14241 Test: 0.64499
Epoch: 7700 Train: 0.19453 Test: 0.69747
Epoch: 7800 Train: 0.08884 Test: 0.59850
Epoch: 7900 Train: 0.38239 Test: 0.81783
Epoch: 8000 Train: 0.17282 Test: 0.70002
Epoch: 8100 Train: 0.98007 Test: 1.78742
Epoch 8200: New minimal relative error: 7.19%, model saved.
Epoch: 8200 Train: 0.09283 Test: 0.57277
Epoch: 8300 Train: 0.59723 Test: 1.10576
Epoch: 8400 Train: 0.46694 Test: 0.87937
Epoch: 8500 Train: 0.08554 Test: 0.55242
Epoch: 8600 Train: 0.03675 Test: 0.53080
Epoch: 8700 Train: 0.10591 Test: 0.64744
Epoch: 8800 Train: 0.06566 Test: 0.53205
Epoch: 8900 Train: 0.18063 Test: 0.68363
Epoch: 9000 Train: 0.05915 Test: 0.50921
Epoch: 9100 Train: 0.03171 Test: 0.49080
Epoch: 9200 Train: 0.03066 Test: 0.50079
Epoch: 9300 Train: 0.03760 Test: 0.49056
Epoch: 9400 Train: 0.03075 Test: 0.48691
Epoch: 9500 Train: 0.04142 Test: 0.50085
Epoch: 9600 Train: 0.06746 Test: 0.50992
Epoch: 9700 Train: 0.03983 Test: 0.48275
Epoch: 9800 Train: 0.06561 Test: 0.49794
Epoch: 9900 Train: 0.07635 Test: 0.53733
Epoch: 9999 Train: 0.02952 Test: 0.47286
Training Loss: tensor(0.0295)
Test Loss: tensor(0.4729)
Learned LE: [ 0.8082825  -0.05143046 -3.8857036 ]
True LE: [ 8.71826530e-01 -5.82248438e-03 -1.45402355e+01]
Relative Error: [20.014992  21.43615   22.16702   22.52414   22.615877  22.795738
 23.155674  22.710388  21.565437  19.876612  18.80067   18.08022
 16.965067  16.242733  15.187647  13.903041  12.885636  11.854782
 11.053664  10.424517   9.908735   9.53386    9.254136   8.93963
  8.790064   8.785972   8.856778   9.020763   9.238175   9.408297
  9.528607   9.30181    8.85009    8.176677   7.0267715  6.268529
  5.573655   5.2073784  4.9598227  4.468624   3.8102632  3.638453
  3.5466413  4.471841   5.880147   6.1519156  6.121543   5.9190683
  5.8951964  5.8448496  5.771052   5.3872     5.0252767  5.1276503
  6.2305384  7.9113894  9.557475  11.237875  12.7482195 13.921388
 15.12861   16.880375  18.3526    19.621975  20.574444  20.870213
 21.046635  21.281599  21.663465  21.509724  20.604033  18.699549
 17.830484  17.039976  15.980336  15.1198    14.049588  12.862732
 11.714747  10.580545   9.697476   9.07342    8.759937   8.445688
  8.268304   8.074154   7.758893   7.616291   7.682694   7.801049
  8.162462   8.341249   8.535482   8.562393   8.262264   7.8712883
  6.7983665  6.094256   5.209072   4.899241   4.6502914  4.37124
  3.7711813  3.5096452  3.4325118  4.148751   5.938947   6.5532
  6.3744283  6.1897526  6.3144484  6.384496   6.3838773  6.1041756
  5.4330134  4.670829   4.976742   6.554022   8.296866  10.046474
 11.456421  12.431915  13.706949  14.893196  16.717102  17.994148
 18.86755   19.19432   19.426352  19.739141  20.167158  20.17746
 19.70687   17.768478  16.794088  15.991685  15.1675415 13.944322
 13.033112  11.830321  10.486339   9.306703   8.449202   7.8864136
  7.5384574  7.2978964  7.1190453  7.0955358  7.0685444  6.9149466
  6.7746124  6.816533   6.9584527  7.280411   7.4634104  7.590767
  7.557032   7.316103   6.807538   5.950154   5.161152   4.548016
  4.3479447  4.230709   3.80212    3.3884737  3.3165972  3.8598115
  5.6100383  6.606082   6.722294   6.7851624  6.7991314  6.7775226
  6.880008   6.6616764  6.005092   4.882308   4.187532   4.960598
  6.784529   8.438744  10.151045  11.014361  12.0084    12.947817
 14.317984  16.10919   17.042307  17.594637  17.751778  17.850391
 18.449167  18.883028  18.5238    17.14527   15.729245  14.915508
 14.246021  13.119449  12.156707  10.858562   9.374313   8.082258
  7.281121   6.717306   6.391909   6.138497   6.083251   6.190166
  6.316884   6.3383536  6.281002   6.119087   6.0473366  6.171877
  6.4440117  6.5928473  6.6938667  6.6366115  6.461651   5.9890313
  5.222656   4.383963   4.1361346  4.04457    3.8284116  3.3388238
  3.1993647  3.5012     4.829578   6.3416758  6.912821   7.2428865
  7.40535    7.109217   7.11191    6.8182254  6.4266415  5.536736
  4.090963   3.5552497  4.718274   6.573215   8.378083   9.473747
 10.265372  11.082758  11.934953  13.374784  14.9461    15.738291
 16.136747  16.195154  16.179565  17.022293  17.30497   16.303267
 14.771317  13.9485035 13.019705  12.25494   11.467721  10.085418
  8.422022   6.9981756  6.2704735  5.7645493  5.4553785  5.207851
  5.148587   5.256032   5.4785743  5.731467   5.7920203  5.7737274
  5.561269   5.398354   5.5328226  5.743019   5.7365184  5.894667
  5.802086   5.6306744  5.2990446  4.6033936  3.7802339  3.7206702
  3.6990304  3.3916495  3.020986   3.1815019  4.0396166  5.4859204
  6.660999   7.3570275  7.8370185  7.6369414  7.397363   7.020107
  6.5058165  6.068808   4.841858   3.2014623  2.9459572  4.195015
  6.058333   7.5228086  8.512428   9.328122   9.848593  10.755301
 12.17122   13.438663  14.218026  14.558855  14.663246  14.648925
 15.409042  15.379399  14.232109  12.972909  12.084878  11.225972
 10.631503   9.72345    7.9809217  6.3336573  5.6091537  5.3263645
  5.0229645  4.663675   4.565248   4.731077   4.906864   5.070884
  5.2154737  5.2819715  5.161363   4.94272    4.764063   4.8585405
  4.994228   5.0606     5.0714755  5.0319204  4.8571935  4.5835385
  4.0403013  3.3680186  3.3260908  3.3033018  3.1214795  2.8654938
  3.3038416  4.3606424  5.5465536  6.8033843  7.657259   8.037853
  7.6268964  7.243154   6.6248507  6.155051   5.5806236  4.1620955
  2.5834668  2.4410162  3.692972   5.363595   6.4970484  7.236829
  8.200597   8.771047   9.491594  10.577921  11.709869  12.567626
 13.044621  12.937139  13.232467  13.842203  13.499053  12.59248
 11.416465  10.517965   9.664101   9.36326    8.299948   6.2673063
  5.1946306  4.9657454  4.854779   4.4233875  4.2204285  4.2844224
  4.352702   4.502682   4.643177   4.748665 ]
