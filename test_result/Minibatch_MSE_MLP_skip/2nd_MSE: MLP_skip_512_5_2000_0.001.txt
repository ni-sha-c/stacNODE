time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 4000.80029 Test: 3944.27490
Epoch 100: New minimal relative error: 48.34%, model saved.
Epoch: 100 Train: 43.93063 Test: 50.96455
Epoch 200: New minimal relative error: 21.28%, model saved.
Epoch: 200 Train: 9.87621 Test: 15.56414
Epoch 300: New minimal relative error: 18.40%, model saved.
Epoch: 300 Train: 6.13482 Test: 6.90358
Epoch 400: New minimal relative error: 15.77%, model saved.
Epoch: 400 Train: 10.02245 Test: 13.41756
Epoch: 500 Train: 8.18027 Test: 5.47853
Epoch: 600 Train: 17.26238 Test: 15.47212
Epoch: 700 Train: 7.25562 Test: 9.45491
Epoch: 800 Train: 14.86100 Test: 18.65912
Epoch 900: New minimal relative error: 13.83%, model saved.
Epoch: 900 Train: 1.77558 Test: 2.07249
Epoch 1000: New minimal relative error: 10.48%, model saved.
Epoch: 1000 Train: 0.90314 Test: 1.25480
Epoch: 1100 Train: 5.01560 Test: 6.18407
Epoch 1200: New minimal relative error: 8.34%, model saved.
Epoch: 1200 Train: 2.50348 Test: 2.47886
Epoch: 1300 Train: 2.28294 Test: 3.27482
Epoch: 1400 Train: 2.72426 Test: 2.27322
Epoch: 1500 Train: 3.40041 Test: 1.25816
Epoch: 1600 Train: 1.42077 Test: 1.32942
Epoch: 1700 Train: 8.76995 Test: 6.33430
Epoch: 1800 Train: 0.99274 Test: 1.18720
Epoch: 1900 Train: 5.86176 Test: 5.34034
Epoch: 2000 Train: 0.96436 Test: 1.05604
Epoch: 2100 Train: 0.73349 Test: 1.01652
Epoch: 2200 Train: 1.48579 Test: 1.36368
Epoch: 2300 Train: 0.93373 Test: 0.96435
Epoch: 2400 Train: 0.55805 Test: 0.71995
Epoch: 2500 Train: 7.31294 Test: 8.11161
Epoch 2600: New minimal relative error: 7.13%, model saved.
Epoch: 2600 Train: 0.31410 Test: 0.48249
Epoch: 2700 Train: 0.55995 Test: 0.80770
Epoch: 2800 Train: 0.71494 Test: 0.94604
Epoch: 2900 Train: 2.57949 Test: 2.95518
Epoch: 3000 Train: 2.35172 Test: 1.72307
Epoch: 3100 Train: 0.29519 Test: 0.44720
Epoch 3200: New minimal relative error: 6.94%, model saved.
Epoch: 3200 Train: 0.30640 Test: 0.47921
Epoch: 3300 Train: 0.34114 Test: 0.46658
Epoch: 3400 Train: 1.31109 Test: 1.20678
Epoch: 3500 Train: 0.86956 Test: 0.84554
Epoch: 3600 Train: 0.20077 Test: 0.30265
Epoch: 3700 Train: 2.53707 Test: 2.86686
Epoch: 3800 Train: 2.05285 Test: 1.95436
Epoch: 3900 Train: 0.59014 Test: 0.77577
Epoch: 4000 Train: 0.21942 Test: 0.28628
Epoch: 4100 Train: 3.14827 Test: 3.07023
Epoch: 4200 Train: 0.55839 Test: 0.73713
Epoch: 4300 Train: 1.19215 Test: 1.06026
Epoch: 4400 Train: 0.15271 Test: 0.33586
Epoch: 4500 Train: 1.68303 Test: 1.61403
Epoch: 4600 Train: 0.40330 Test: 0.51882
Epoch: 4700 Train: 0.77677 Test: 0.76341
Epoch: 4800 Train: 0.52122 Test: 0.53703
Epoch: 4900 Train: 0.39457 Test: 0.52453
Epoch: 5000 Train: 0.10252 Test: 0.17171
Epoch: 5100 Train: 0.12456 Test: 0.21099
Epoch: 5200 Train: 1.69870 Test: 1.35044
Epoch: 5300 Train: 0.08680 Test: 0.16029
Epoch: 5400 Train: 0.48010 Test: 0.59031
Epoch: 5500 Train: 0.29292 Test: 0.31898
Epoch: 5600 Train: 0.24248 Test: 0.32074
Epoch: 5700 Train: 0.13445 Test: 0.21832
Epoch: 5800 Train: 2.32725 Test: 2.59494
Epoch: 5900 Train: 0.71883 Test: 0.45747
Epoch 6000: New minimal relative error: 6.34%, model saved.
Epoch: 6000 Train: 0.26419 Test: 0.32309
Epoch 6100: New minimal relative error: 5.97%, model saved.
Epoch: 6100 Train: 0.39735 Test: 0.42677
Epoch: 6200 Train: 0.12004 Test: 0.17413
Epoch: 6300 Train: 0.06947 Test: 0.12697
Epoch: 6400 Train: 0.14445 Test: 0.21211
Epoch: 6500 Train: 0.72410 Test: 0.97374
Epoch: 6600 Train: 0.24047 Test: 0.30214
Epoch: 6700 Train: 0.10702 Test: 0.29905
Epoch: 6800 Train: 0.18933 Test: 0.40029
Epoch: 6900 Train: 0.07305 Test: 0.13190
Epoch: 7000 Train: 0.76286 Test: 0.86304
Epoch: 7100 Train: 0.18505 Test: 0.24633
Epoch: 7200 Train: 0.06262 Test: 0.11140
Epoch: 7300 Train: 0.05475 Test: 0.10665
Epoch: 7400 Train: 0.05801 Test: 0.10773
Epoch: 7500 Train: 0.07951 Test: 0.14085
Epoch: 7600 Train: 0.05167 Test: 0.10073
Epoch: 7700 Train: 0.06059 Test: 0.10913
Epoch: 7800 Train: 0.05376 Test: 0.10126
Epoch: 7900 Train: 0.06479 Test: 0.11302
Epoch: 8000 Train: 0.29678 Test: 0.39049
Epoch: 8100 Train: 0.35088 Test: 0.24750
Epoch: 8200 Train: 0.07069 Test: 0.11060
Epoch: 8300 Train: 0.04591 Test: 0.09080
Epoch: 8400 Train: 0.04613 Test: 0.09159
Epoch: 8500 Train: 0.16657 Test: 0.29773
Epoch: 8600 Train: 0.94026 Test: 0.93838
Epoch: 8700 Train: 0.05204 Test: 0.09678
Epoch: 8800 Train: 0.04482 Test: 0.08684
Epoch: 8900 Train: 0.13166 Test: 0.20961
Epoch: 9000 Train: 1.31726 Test: 1.45740
Epoch: 9100 Train: 0.04111 Test: 0.08290
Epoch: 9200 Train: 0.04807 Test: 0.08880
Epoch: 9300 Train: 0.04846 Test: 0.08804
Epoch: 9400 Train: 0.12280 Test: 0.18581
Epoch: 9500 Train: 0.29232 Test: 0.40645
Epoch: 9600 Train: 0.39401 Test: 0.40150
Epoch: 9700 Train: 0.05871 Test: 0.11721
Epoch: 9800 Train: 0.06643 Test: 0.10000
Epoch: 9900 Train: 0.03904 Test: 0.07716
Epoch: 9999 Train: 0.04906 Test: 0.09337
Training Loss: tensor(0.0491)
Test Loss: tensor(0.0934)
Learned LE: [ 0.6015885  -0.12047301 -2.7165263 ]
True LE: [ 8.6848962e-01  1.8764746e-03 -1.4541328e+01]
Relative Error: [2.3114576  2.664116   3.1334393  3.6012006  4.1666737  4.4119363
 4.626716   4.192217   4.1904454  4.3103433  4.6503773  5.130254
 5.2394466  5.017974   4.836736   4.23508    3.9508176  3.4167821
 2.976435   2.5116293  1.9920081  1.7533786  1.5755048  1.3926111
 1.0762392  0.7745611  0.6686214  0.67400295 0.44161296 0.19013256
 0.345123   0.37519613 0.37892115 0.6294814  1.3007632  1.815782
 1.2972337  0.74400395 0.5419754  0.8694973  1.8215603  3.0360208
 3.3458478  2.9766266  2.5132797  1.8621001  1.4418727  1.1536044
 0.9078064  0.81399715 0.79511255 0.75990164 0.7827853  0.87951756
 0.9864127  0.93039685 0.96468973 1.0961312  1.1083045  1.198087
 1.34905    1.5454354  1.7576126  2.1180267  2.4513154  2.8967035
 3.3444715  3.8054972  3.9889793  4.0263934  3.7772648  3.7518137
 3.926871   4.575577   5.1986113  5.0092673  4.6143584  3.9467494
 3.4562263  2.8235583  2.242268   1.8303139  1.4193383  1.1493869
 1.1285834  1.1273751  1.0086827  0.780061   0.5330896  0.5187902
 0.51333517 0.3327573  0.09419012 0.348737   0.3064584  0.25266498
 0.7414447  1.4048773  1.4989843  0.84154254 0.4687657  0.4388765
 1.269316   2.4614582  2.9902208  2.804921   2.352211   1.7764214
 1.4506525  1.2698032  0.84271914 0.69483143 0.65207845 0.6848792
 0.78942925 0.90527767 1.073074   1.0508544  0.91451705 0.7524803
 0.89807725 1.0256968  0.9579087  1.0630733  1.3012615  1.5027257
 1.9117646  2.2842317  2.6233416  3.1410322  3.4559722  3.6178277
 3.6034818  3.4418154  3.506481   3.9872692  4.6932836  4.635835
 4.25199    3.6560233  2.957977   2.0592737  1.5202426  1.3593663
 1.0879695  0.7447078  0.70562416 0.86259913 0.9555375  0.7550162
 0.5448184  0.4010663  0.44522923 0.323429   0.14407627 0.14022116
 0.428193   0.39704567 0.44529667 0.79446363 1.46398    1.0623173
 0.3898796  0.20322031 0.7794981  1.7394396  2.7835119  2.6386437
 2.2923505  1.8466758  1.4791539  1.2666781  0.9448636  0.72309554
 0.53962886 0.50109947 0.72851074 0.88554585 1.073364   1.1428003
 1.0000184  0.85961586 0.7259108  0.7046413  0.8304679  0.80091375
 0.8544957  1.115535   1.3867325  1.7764956  2.111257   2.4781752
 2.8534544  3.1770744  3.240415   3.3765328  3.4620616  3.3320818
 3.7356467  4.2848897  3.868236   3.4375203  2.6583486  1.8817309
 1.3121576  0.9762399  0.8138532  0.6985183  0.5049693  0.6288011
 0.74425864 0.8554399  0.73175955 0.49944785 0.51448303 0.48066562
 0.30352813 0.23530486 0.29238984 0.5118929  0.5783257  0.641791
 0.5984237  1.0777997  0.5000196  0.14820085 0.41838866 1.1712418
 2.0526848  2.5081475  2.1259775  1.7527027  1.6249322  1.3410561
 1.2019577  0.868724   0.6154902  0.40079364 0.45571673 0.79265124
 1.104461   1.093668   1.0108148  0.82242525 0.755873   0.71589416
 0.63403535 0.58309895 0.73328984 0.90353674 1.1025724  1.3316045
 1.6844012  1.9527674  2.3086038  2.567805   2.775491   3.1100647
 3.3271377  3.1734962  3.0934763  3.494272   3.6035678  3.135043
 2.587854   2.0145538  1.2815006  0.88639826 0.817858   0.7399975
 0.67177534 0.6056143  0.8201717  0.85843384 0.70876545 0.7271315
 0.53718823 0.5106511  0.54511976 0.37562224 0.4764686  0.53212136
 0.51059556 0.7683397  0.85313505 0.25024384 0.59325033 0.22852972
 0.47831964 0.8593316  1.3811     1.9911462  2.1566463  1.7147841
 1.6431527  1.4976385  1.5176679  1.2248615  0.91040796 0.5680217
 0.44327337 0.5924022  0.88494986 0.9592593  0.97780013 0.95687526
 0.84814847 0.71409255 0.75884694 0.68449837 0.46362934 0.6106938
 0.9467734  1.2240329  1.3346444  1.7440236  2.011234   2.3057115
 2.432856   2.6533906  2.8233302  2.9790394  2.8524573  2.8788865
 3.1282082  2.9906106  2.3844383  2.1215482  1.366998   0.80046606
 0.77913755 0.81273896 0.74796504 0.64995414 0.65779436 0.7241101
 0.90931964 0.797485   0.40297842 0.27933872 0.3197498  0.42381057
 0.37490368 0.55546224 0.9041798  0.7824848  1.009658   1.2336011
 0.547719   0.34615737 0.42995223 1.0458137  1.3152153  1.4905387
 1.845339   1.893704   1.5827897  1.5073159  1.4573903  1.5568199
 1.2224103  1.034095   0.7397476  0.6782242  0.6553384  0.7599033
 0.7890935  0.9188106  0.9863778  0.9666724  0.9821292  0.84866464
 0.7310732  0.4873592  0.33936244 0.5784571  0.90093005 1.1746877
 1.5308837  2.0147889  2.1870744  2.5194323  2.8024483  2.5965939
 2.84156    2.7067442  2.5048614  2.4637218  2.0412846  1.5563754
 1.5826039  0.9305427  0.63132954 0.6301077  0.88874644 0.7158178
 0.5574024  0.48874864 0.62526995 0.8404576 ]
