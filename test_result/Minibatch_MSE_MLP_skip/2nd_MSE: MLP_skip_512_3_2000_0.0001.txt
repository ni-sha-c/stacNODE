time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 104.89%, model saved.
Epoch: 0 Train: 4145.48438 Test: 3904.12866
Epoch 100: New minimal relative error: 76.38%, model saved.
Epoch: 100 Train: 68.45247 Test: 68.65379
Epoch 200: New minimal relative error: 27.41%, model saved.
Epoch: 200 Train: 14.92168 Test: 16.96613
Epoch 300: New minimal relative error: 10.62%, model saved.
Epoch: 300 Train: 7.08202 Test: 9.28743
Epoch: 400 Train: 12.76030 Test: 14.38630
Epoch: 500 Train: 3.75922 Test: 5.61229
Epoch: 600 Train: 3.13250 Test: 4.74544
Epoch: 700 Train: 2.82305 Test: 4.31692
Epoch: 800 Train: 2.96073 Test: 4.75518
Epoch: 900 Train: 1.98756 Test: 3.24291
Epoch: 1000 Train: 2.00307 Test: 2.99686
Epoch: 1100 Train: 3.01646 Test: 3.42568
Epoch: 1200 Train: 3.82994 Test: 5.09066
Epoch: 1300 Train: 1.24270 Test: 2.13070
Epoch: 1400 Train: 1.52506 Test: 2.30684
Epoch: 1500 Train: 6.98650 Test: 9.67643
Epoch: 1600 Train: 0.92403 Test: 1.87160
Epoch: 1700 Train: 0.99091 Test: 2.77579
Epoch: 1800 Train: 2.23802 Test: 2.20472
Epoch: 1900 Train: 1.31249 Test: 1.99038
Epoch: 2000 Train: 0.58082 Test: 1.30654
Epoch: 2100 Train: 0.62279 Test: 1.25605
Epoch: 2200 Train: 4.44076 Test: 6.06084
Epoch: 2300 Train: 2.96232 Test: 5.11667
Epoch: 2400 Train: 0.43712 Test: 1.07459
Epoch: 2500 Train: 0.41891 Test: 1.07845
Epoch: 2600 Train: 0.99536 Test: 1.73280
Epoch: 2700 Train: 1.49350 Test: 2.39332
Epoch: 2800 Train: 0.51016 Test: 1.02626
Epoch: 2900 Train: 2.03802 Test: 2.63403
Epoch: 3000 Train: 0.38171 Test: 0.96886
Epoch 3100: New minimal relative error: 9.93%, model saved.
Epoch: 3100 Train: 0.43129 Test: 0.95470
Epoch: 3200 Train: 0.28442 Test: 0.83861
Epoch: 3300 Train: 0.27981 Test: 0.83686
Epoch 3400: New minimal relative error: 8.07%, model saved.
Epoch: 3400 Train: 0.27422 Test: 0.81506
Epoch: 3500 Train: 2.38530 Test: 3.24912
Epoch: 3600 Train: 0.26509 Test: 0.75326
Epoch: 3700 Train: 1.47838 Test: 1.69108
Epoch: 3800 Train: 0.29882 Test: 0.75205
Epoch: 3900 Train: 0.22499 Test: 0.72008
Epoch: 4000 Train: 0.35738 Test: 0.80285
Epoch: 4100 Train: 0.20730 Test: 0.67684
Epoch: 4200 Train: 1.16228 Test: 1.87656
Epoch: 4300 Train: 0.41401 Test: 0.80183
Epoch: 4400 Train: 0.34778 Test: 0.69727
Epoch 4500: New minimal relative error: 6.84%, model saved.
Epoch: 4500 Train: 0.47359 Test: 0.83614
Epoch: 4600 Train: 0.27801 Test: 0.74818
Epoch: 4700 Train: 0.25194 Test: 0.67523
Epoch: 4800 Train: 1.45931 Test: 1.66586
Epoch: 4900 Train: 0.28926 Test: 0.72107
Epoch: 5000 Train: 0.25323 Test: 0.74108
Epoch: 5100 Train: 0.82885 Test: 1.10441
Epoch: 5200 Train: 0.26728 Test: 0.60499
Epoch: 5300 Train: 1.56644 Test: 2.29647
Epoch: 5400 Train: 0.23376 Test: 0.61103
Epoch: 5500 Train: 0.33896 Test: 0.73232
Epoch: 5600 Train: 0.14215 Test: 0.53055
Epoch: 5700 Train: 0.13753 Test: 0.51216
Epoch: 5800 Train: 0.14574 Test: 0.52618
Epoch: 5900 Train: 0.51581 Test: 1.02932
Epoch: 6000 Train: 0.13382 Test: 0.51844
Epoch: 6100 Train: 0.92713 Test: 1.28499
Epoch: 6200 Train: 0.30406 Test: 0.60543
Epoch: 6300 Train: 0.12645 Test: 0.47310
Epoch: 6400 Train: 0.13317 Test: 0.47133
Epoch: 6500 Train: 0.27798 Test: 0.71268
Epoch: 6600 Train: 0.31135 Test: 0.61413
Epoch: 6700 Train: 0.11209 Test: 0.45065
Epoch: 6800 Train: 0.11154 Test: 0.44670
Epoch: 6900 Train: 0.13397 Test: 0.44590
Epoch: 7000 Train: 0.14159 Test: 0.50290
Epoch: 7100 Train: 0.22167 Test: 0.53467
Epoch: 7200 Train: 0.57082 Test: 0.93346
Epoch: 7300 Train: 0.10643 Test: 0.44171
Epoch: 7400 Train: 0.09967 Test: 0.42169
Epoch: 7500 Train: 0.10022 Test: 0.42402
Epoch: 7600 Train: 0.09859 Test: 0.40538
Epoch: 7700 Train: 0.31287 Test: 0.61250
Epoch: 7800 Train: 0.41860 Test: 0.81601
Epoch: 7900 Train: 0.17120 Test: 0.47053
Epoch: 8000 Train: 0.09146 Test: 0.39197
Epoch: 8100 Train: 0.09716 Test: 0.40214
Epoch: 8200 Train: 0.18839 Test: 0.42784
Epoch: 8300 Train: 0.09362 Test: 0.39798
Epoch: 8400 Train: 0.08699 Test: 0.38141
Epoch: 8500 Train: 0.08780 Test: 0.38582
Epoch: 8600 Train: 0.16029 Test: 0.46730
Epoch: 8700 Train: 0.08593 Test: 0.37392
Epoch: 8800 Train: 0.08390 Test: 0.37444
Epoch: 8900 Train: 0.19668 Test: 0.43917
Epoch: 9000 Train: 0.24157 Test: 0.48635
Epoch: 9100 Train: 0.16623 Test: 0.40168
Epoch: 9200 Train: 0.08024 Test: 0.35956
Epoch: 9300 Train: 0.07780 Test: 0.35234
Epoch: 9400 Train: 0.09302 Test: 0.35403
Epoch: 9500 Train: 0.07701 Test: 0.34362
Epoch: 9600 Train: 0.08370 Test: 0.34502
Epoch: 9700 Train: 0.07452 Test: 0.33926
Epoch: 9800 Train: 0.07358 Test: 0.33872
Epoch: 9900 Train: 0.08146 Test: 0.34634
Epoch: 9999 Train: 0.07191 Test: 0.33189
Training Loss: tensor(0.0719)
Test Loss: tensor(0.3319)
Learned LE: [ 0.710662    0.06552779 -3.485232  ]
True LE: [ 8.63531590e-01 -3.38078564e-04 -1.45366745e+01]
Relative Error: [1.0659287  1.1484915  1.4208528  1.7283287  2.0811343  2.5870085
 3.403649   4.121294   4.868833   5.710447   6.695842   7.520396
 7.9668574  8.29386    8.482805   8.499989   8.42042    7.644684
 6.598336   6.1396046  5.8413634  5.3097916  4.6091027  4.032563
 3.586502   3.255878   3.0721676  2.9744976  3.0120785  3.1199698
 3.2788434  3.4350178  3.7221391  3.8495688  3.795229   3.820993
 3.6493673  3.4928532  3.296707   2.9172063  2.2716901  1.8718753
 1.3816733  1.453643   1.3927344  1.2026895  0.44047272 0.47520465
 1.2073689  1.5969591  2.1027832  2.7800908  2.6392753  2.3455808
 2.0484104  2.1509295  2.3111308  2.2526822  2.24711    1.8993189
 1.3335114  0.9124156  0.7276528  0.7458468  1.0225508  1.3360934
 1.6341584  1.9374192  2.5518196  3.267924   4.075293   4.8290305
 5.6901155  6.5414853  7.237573   7.49843    7.7501936  7.80039
 7.647544   6.9918528  5.998522   5.583414   5.2620544  4.647137
 3.9453638  3.4815402  3.2154982  2.9998298  2.886682   2.797741
 2.8191395  2.8694856  2.9144733  2.9915137  3.1937678  3.397088
 3.5442278  3.527624   3.4882967  3.2201064  3.0563111  2.7846963
 2.2154555  1.7918679  1.452932   1.6105258  1.6404107  1.4407225
 0.7541884  0.2869915  0.9669565  1.2916619  1.6784773  2.369751
 2.313226   2.0340533  1.8117132  1.8926833  2.0505023  1.916961
 1.799305   1.463978   0.9621023  0.618673   0.4828971  0.45910197
 0.6604889  1.0024929  1.2561535  1.4291651  1.7772235  2.4677086
 3.1593208  3.8948243  4.7083783  5.4620004  6.2198987  6.735486
 6.878171   6.902007   6.8683624  6.4957128  5.504087   4.9889793
 4.6964297  4.052831   3.4456708  3.1844292  2.9429467  2.7687871
 2.7296975  2.7133095  2.723152   2.8434844  2.9201577  2.867771
 2.9363735  2.9715328  3.1001337  3.2641716  3.251601   3.0712678
 2.7787733  2.5289123  2.1347065  1.6702589  1.5071579  1.6546448
 1.7690301  1.5903281  1.108916   0.14175096 0.60718703 1.0233815
 1.2759115  1.882734   1.8828942  1.7922239  1.6049471  1.594675
 1.718343   1.4683783  1.2550958  1.0950965  0.70107377 0.44816184
 0.41849306 0.4580896  0.3110534  0.62117034 0.9405141  1.1656427
 1.3065605  1.6920854  2.2344196  2.9088826  3.639357   4.5194407
 5.1589975  5.772444   6.127824   6.1857386  5.990987   5.6699905
 5.0709014  4.4717197  4.1400585  3.5439894  3.104131   2.8839295
 2.7023063  2.5568862  2.5693643  2.6372938  2.6754692  2.9294777
 3.1139483  3.0821238  3.1620073  3.0677445  2.961933   2.8468077
 3.0535471  2.946798   2.6730793  2.3271255  1.9903859  1.6626642
 1.4460583  1.6717606  2.2106755  1.9265449  1.5681452  0.76195323
 0.13640036 0.61482394 0.92412204 1.3618518  1.4721236  1.5081693
 1.413617   1.236471   1.288753   1.0551144  0.8182531  0.72337013
 0.6117991  0.5159695  0.56442606 0.54747    0.39979973 0.19085722
 0.47919714 0.90146065 1.1910338  1.2860371  1.4748605  1.8801769
 2.517403   3.2055578  3.9758549  4.592957   5.0494876  5.377501
 5.3388333  4.9501886  4.431913   4.0448594  3.5762231  3.0910695
 2.8039346  2.5815387  2.545439   2.3817327  2.2930243  2.34341
 2.3383982  2.4226336  2.6472712  2.8834057  3.2574437  3.3448114
 3.2138743  2.9140468  2.756043   2.871338   2.6878536  2.3303113
 2.0412405  1.6659598  1.50781    1.753463   2.4250138  2.3968377
 2.043592   1.2841831  0.6130147  0.2977271  0.46575814 0.76350933
 1.1802635  0.9922759  1.0927275  0.9149268  0.837433   0.7349846
 0.45920908 0.6279146  0.7146656  0.8071434  0.8391724  0.77124536
 0.61459345 0.55477947 0.4339357  0.6355066  0.95069903 1.2217948
 1.344419   1.2715724  1.509397   2.0200083  2.6656168  3.35546
 3.9443731  4.224126   4.514952   4.3827662  3.861365   3.494044
 3.05122    2.741874   2.475861   2.319498   2.2530775  2.2259736
 2.0351338  1.8777013  1.9982215  2.0280797  2.032698   2.2859285
 2.5269647  2.8465705  3.0933712  3.0816174  2.9603486  2.922963
 2.831361   2.64305    2.3240733  1.935257   1.5088607  1.6938381
 2.2503695  2.8750458  2.6328733  2.1343985  1.2248639  0.6508645
 0.49429637 0.51342535 0.52761024 0.6614146  0.5122026  0.5678665
 0.40381876 0.38364893 0.5514981  0.8999679  1.0856931  1.1565721
 1.2469381  1.129143   0.8873011  0.91799295 0.8082724  0.6811533
 0.7726289  0.99515635 1.2062696  1.3571101  1.2224104  1.2408607
 1.5727654  2.1015952  2.6435354  3.2420213  3.4132833  3.6206272
 3.4731128  2.840497   2.6421025  2.318398   2.077381   2.037974
 1.8204924  2.0052564  1.8188151  1.5370526 ]
