time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 110.70%, model saved.
Epoch: 0 Train: 4064.50366 Test: 3629.73877
Epoch 100: New minimal relative error: 25.67%, model saved.
Epoch: 100 Train: 57.19443 Test: 75.94622
Epoch: 200 Train: 18.05654 Test: 27.78728
Epoch: 300 Train: 6.59777 Test: 10.56648
Epoch 400: New minimal relative error: 22.87%, model saved.
Epoch: 400 Train: 4.75926 Test: 7.86201
Epoch 500: New minimal relative error: 11.31%, model saved.
Epoch: 500 Train: 4.03453 Test: 6.63245
Epoch: 600 Train: 13.04670 Test: 19.07253
Epoch: 700 Train: 2.80178 Test: 4.81729
Epoch: 800 Train: 2.51914 Test: 4.38349
Epoch 900: New minimal relative error: 10.01%, model saved.
Epoch: 900 Train: 4.06975 Test: 4.64533
Epoch: 1000 Train: 14.77575 Test: 16.46628
Epoch: 1100 Train: 2.08518 Test: 4.27935
Epoch: 1200 Train: 6.06951 Test: 8.85539
Epoch: 1300 Train: 1.68908 Test: 2.96808
Epoch: 1400 Train: 3.76411 Test: 5.88527
Epoch: 1500 Train: 6.13225 Test: 8.16041
Epoch: 1600 Train: 1.91434 Test: 3.12690
Epoch: 1700 Train: 1.35943 Test: 2.41753
Epoch: 1800 Train: 1.13485 Test: 2.13276
Epoch: 1900 Train: 1.04047 Test: 2.03860
Epoch: 2000 Train: 5.61901 Test: 5.25833
Epoch: 2100 Train: 2.73275 Test: 4.27066
Epoch: 2200 Train: 0.96747 Test: 1.75058
Epoch: 2300 Train: 0.72269 Test: 1.62836
Epoch: 2400 Train: 0.72864 Test: 1.60821
Epoch: 2500 Train: 0.82194 Test: 1.75312
Epoch: 2600 Train: 1.23349 Test: 2.07505
Epoch: 2700 Train: 0.82122 Test: 1.59062
Epoch: 2800 Train: 0.69009 Test: 1.42684
Epoch: 2900 Train: 0.85142 Test: 2.19293
Epoch: 3000 Train: 0.80983 Test: 1.66800
Epoch: 3100 Train: 0.49216 Test: 1.26120
Epoch: 3200 Train: 0.42137 Test: 1.19528
Epoch: 3300 Train: 0.59561 Test: 1.46297
Epoch: 3400 Train: 0.38379 Test: 1.12934
Epoch 3500: New minimal relative error: 8.85%, model saved.
Epoch: 3500 Train: 0.37249 Test: 1.10616
Epoch: 3600 Train: 1.05786 Test: 1.71319
Epoch 3700: New minimal relative error: 8.50%, model saved.
Epoch: 3700 Train: 0.40826 Test: 1.10840
Epoch 3800: New minimal relative error: 8.40%, model saved.
Epoch: 3800 Train: 0.33364 Test: 1.03614
Epoch: 3900 Train: 0.42239 Test: 1.15824
Epoch: 4000 Train: 1.87403 Test: 2.39779
Epoch: 4100 Train: 0.76461 Test: 1.42502
Epoch: 4200 Train: 0.42301 Test: 1.15149
Epoch: 4300 Train: 1.09765 Test: 1.91316
Epoch: 4400 Train: 0.42266 Test: 1.00848
Epoch: 4500 Train: 0.29112 Test: 0.94197
Epoch: 4600 Train: 1.11541 Test: 2.00952
Epoch 4700: New minimal relative error: 7.84%, model saved.
Epoch: 4700 Train: 0.60244 Test: 1.23595
Epoch: 4800 Train: 1.77819 Test: 2.07106
Epoch: 4900 Train: 1.62329 Test: 2.15074
Epoch: 5000 Train: 1.02303 Test: 1.72253
Epoch: 5100 Train: 0.23338 Test: 0.80281
Epoch: 5200 Train: 0.21504 Test: 0.76411
Epoch 5300: New minimal relative error: 7.72%, model saved.
Epoch: 5300 Train: 0.20841 Test: 0.75011
Epoch: 5400 Train: 0.30739 Test: 0.80646
Epoch: 5500 Train: 0.22427 Test: 0.79567
Epoch: 5600 Train: 0.19219 Test: 0.71622
Epoch: 5700 Train: 0.27738 Test: 0.70334
Epoch: 5800 Train: 0.18576 Test: 0.67781
Epoch: 5900 Train: 0.17376 Test: 0.66632
Epoch: 6000 Train: 0.32065 Test: 0.87515
Epoch: 6100 Train: 0.18161 Test: 0.67479
Epoch: 6200 Train: 0.16083 Test: 0.63641
Epoch: 6300 Train: 0.16051 Test: 0.63308
Epoch: 6400 Train: 0.79547 Test: 1.10351
Epoch: 6500 Train: 0.15475 Test: 0.62159
Epoch: 6600 Train: 0.14544 Test: 0.59759
Epoch: 6700 Train: 0.14309 Test: 0.59263
Epoch: 6800 Train: 0.14732 Test: 0.59647
Epoch: 6900 Train: 0.13585 Test: 0.57227
Epoch: 7000 Train: 0.13861 Test: 0.57146
Epoch: 7100 Train: 0.19288 Test: 0.67602
Epoch: 7200 Train: 0.12702 Test: 0.54904
Epoch: 7300 Train: 0.12490 Test: 0.54347
Epoch: 7400 Train: 0.19568 Test: 0.63360
Epoch: 7500 Train: 0.13027 Test: 0.53106
Epoch: 7600 Train: 0.13963 Test: 0.53344
Epoch: 7700 Train: 0.11454 Test: 0.51391
Epoch: 7800 Train: 0.13593 Test: 0.59059
Epoch: 7900 Train: 0.11025 Test: 0.50180
Epoch: 8000 Train: 0.10850 Test: 0.49733
Epoch: 8100 Train: 0.10625 Test: 0.48962
Epoch: 8200 Train: 0.16462 Test: 0.51933
Epoch: 8300 Train: 0.10216 Test: 0.47639
Epoch 8400: New minimal relative error: 6.95%, model saved.
Epoch: 8400 Train: 0.10056 Test: 0.47302
Epoch: 8500 Train: 0.11549 Test: 0.49342
Epoch: 8600 Train: 0.12118 Test: 0.48570
Epoch: 8700 Train: 0.09520 Test: 0.45494
Epoch: 8800 Train: 0.09406 Test: 0.45399
Epoch 8900: New minimal relative error: 5.67%, model saved.
Epoch: 8900 Train: 0.09236 Test: 0.44606
Epoch: 9000 Train: 0.09248 Test: 0.44140
Epoch: 9100 Train: 0.12441 Test: 0.50092
Epoch: 9200 Train: 0.08831 Test: 0.43280
Epoch: 9300 Train: 0.17027 Test: 0.55664
Epoch: 9400 Train: 0.08591 Test: 0.42508
Epoch: 9500 Train: 0.08615 Test: 0.42035
Epoch: 9600 Train: 0.08410 Test: 0.41851
Epoch: 9700 Train: 0.08631 Test: 0.41563
Epoch 9800: New minimal relative error: 2.86%, model saved.
Epoch: 9800 Train: 0.08135 Test: 0.40928
Epoch: 9900 Train: 0.08042 Test: 0.40753
Epoch: 9999 Train: 0.08043 Test: 0.40817
Training Loss: tensor(0.0804)
Test Loss: tensor(0.4082)
Learned LE: [ 0.94320804  0.01336032 -4.4108024 ]
True LE: [ 8.8236403e-01  1.7170341e-03 -1.4553563e+01]
Relative Error: [2.5672295  2.3967273  2.0813906  1.877252   1.8373823  1.6386712
 1.5546796  1.5779786  1.699459   1.819645   1.8446356  1.8524805
 1.7276162  1.7381452  1.9146254  2.222741   2.5394309  2.5702221
 2.7086003  3.2580197  3.5677626  3.5089068  3.0920963  2.7851624
 2.7162578  2.4264216  2.1574435  1.9845737  2.0760596  2.4441009
 2.770358   2.9407349  2.7763538  2.6435707  2.5109546  2.4049528
 2.385722   2.1974611  1.7056249  1.2718612  1.2650744  1.2271262
 1.0618744  0.8855045  1.0663754  1.6434995  2.096331   2.667353
 2.527376   1.8111659  1.1514372  1.0221848  1.2133468  1.431058
 1.209764   1.3054872  1.5527176  1.7505116  1.986139   2.0980835
 2.2571237  2.2383425  2.0972962  1.8195848  1.4734485  1.3441828
 1.168055   1.0379038  1.0228875  1.1082573  1.3458077  1.4843559
 1.5815936  1.4015598  1.229613   1.0440279  1.1185781  1.3374041
 1.8082548  2.1754515  2.1914616  2.5181391  3.1583261  3.1386178
 3.1392212  2.675702   2.433065   2.1900644  1.901257   1.5943524
 1.6382946  1.9089441  2.6866171  2.5870833  2.4651847  2.4002185
 2.1814363  1.9824946  1.7980156  1.6287886  1.4065273  1.3838898
 1.1606256  0.75994146 0.7759321  0.6430595  0.58725613 0.8598296
 1.4145613  1.9022915  2.5157855  2.1572797  1.224268   0.87638617
 0.9056636  1.2234797  1.1052679  1.1351053  1.315793   1.533865
 1.7737949  1.7841277  1.838132   1.8086576  1.6887857  1.3621149
 1.0560168  0.9045131  0.7838248  0.803031   0.82607734 0.79425925
 0.884903   1.1135757  1.1372879  1.0466063  0.9544542  0.80038726
 0.7256062  0.7762403  1.0359361  1.5051162  1.8370645  1.8721995
 2.3030465  2.6537085  2.737179   2.808882   2.3757675  2.013855
 1.6211249  1.4156849  1.3073374  1.5492363  2.057707   2.197403
 2.1216023  2.0972009  1.8659184  1.6162908  1.4859337  1.2276287
 0.9695844  0.77246594 0.85807043 0.6204321  0.5414332  0.6066919
 0.36704314 0.34944913 0.71517473 1.1069323  1.5354669  2.1252475
 1.6570337  0.9323698  0.74753475 0.86826646 0.9537925  0.9606952
 1.0332091  1.1718723  1.3511238  1.5452102  1.5387329  1.3725436
 1.3402731  1.1049924  1.0127337  1.0032715  0.9772273  1.18154
 1.2019584  0.9860729  0.7160545  0.67611384 0.6970342  0.63292366
 0.6133802  0.5885317  0.5503829  0.6066448  0.5727447  0.7062369
 1.0873508  1.3661019  1.4866993  1.9814136  2.0330162  2.243818
 2.5118558  2.1476693  1.59827    1.1121976  1.2932116  1.3397355
 1.4754062  1.7510583  1.6361203  1.6453402  1.7075241  1.399045
 1.2873528  0.9665456  0.9224824  0.75016797 0.6069619  0.28828144
 0.46448386 0.7688064  0.7125336  0.35473356 0.2549782  0.5731626
 0.8332977  1.1150118  1.5640177  1.1564052  0.7200507  0.70705926
 0.7644993  0.7240932  0.81331843 0.92479086 0.9820079  1.1037006
 1.2900211  1.212371   1.115644   0.8346621  1.0819154  1.371782
 1.1602745  1.2326509  1.2662137  1.1183614  0.7879994  0.5872598
 0.55423766 0.46046478 0.33487415 0.32551777 0.3923723  0.45921412
 0.59357953 0.5255224  0.45631537 0.71503687 1.0646169  1.2372553
 1.680893   1.5570093  1.728299   2.0278354  1.9797513  1.2740157
 0.96767956 1.2367495  1.1529261  1.2653673  1.1502279  1.1736636
 1.4968785  1.1449288  0.88921887 0.73958534 0.6329984  0.69331616
 0.5567364  0.4322292  0.29160374 0.52021235 0.87306136 0.90195835
 0.4259382  0.205893   0.49894288 0.51769304 0.6899633  1.0124694
 0.863994   0.55022436 0.6212914  0.7261884  0.61921567 0.6645293
 0.75990516 0.82081014 0.8322929  0.9738141  0.8909356  0.68357277
 0.680636   1.1644222  1.3434308  1.14131    1.119889   0.94268423
 0.7547346  0.5045151  0.37421286 0.40402046 0.5081666  0.48703757
 0.29695714 0.36589363 0.5113207  0.51212174 0.55719346 0.3830953
 0.44621328 0.75243914 1.0987837  1.46486    1.2393576  1.3118923
 1.6536295  1.726525   1.2002164  1.0282207  0.8343851  0.8757311
 0.75116235 0.6259896  0.80999917 0.8619424  0.43172133 0.58390325
 0.61186236 0.4581736  0.458228   0.5122632  0.5275893  0.48665825
 0.40954924 0.6613376  1.0109746  0.7025846  0.39983383 0.63033026
 0.3594515  0.37797952 0.59770745 0.65890753 0.41343927 0.58071274
 0.6295088  0.5339534  0.4631448  0.5840701  0.6443085  0.6019834
 0.6758231  0.54643935 0.4377246  0.6106303  0.9054672  0.97315824
 1.0533919  0.96339    0.692969   0.5772726  0.43045086 0.23077536
 0.2160985  0.20230892 0.4641586  0.8380483  0.78618574 0.5981165
 0.41862717 0.48202607 0.41919672 0.22064349 0.57822883 0.90827614
 1.4244102  1.13155    0.9068095  0.996886  ]
