time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 101.01%, model saved.
Epoch: 0 Train: 3818.49976 Test: 4351.40576
Epoch: 100 Train: 125.29402 Test: 112.55067
Epoch 200: New minimal relative error: 47.74%, model saved.
Epoch: 200 Train: 24.64292 Test: 20.28819
Epoch: 300 Train: 9.11687 Test: 9.26370
Epoch 400: New minimal relative error: 25.83%, model saved.
Epoch: 400 Train: 6.87721 Test: 6.88075
Epoch: 500 Train: 5.75092 Test: 5.75490
Epoch 600: New minimal relative error: 21.55%, model saved.
Epoch: 600 Train: 17.92784 Test: 16.54033
Epoch: 700 Train: 14.19135 Test: 7.64115
Epoch: 800 Train: 10.35811 Test: 17.37643
Epoch: 900 Train: 4.72588 Test: 4.52714
Epoch: 1000 Train: 19.54855 Test: 20.31270
Epoch 1100: New minimal relative error: 11.08%, model saved.
Epoch: 1100 Train: 3.70255 Test: 3.97522
Epoch: 1200 Train: 3.77802 Test: 3.70097
Epoch: 1300 Train: 4.11521 Test: 4.29105
Epoch: 1400 Train: 4.09030 Test: 3.87852
Epoch: 1500 Train: 3.10210 Test: 2.86340
Epoch 1600: New minimal relative error: 7.84%, model saved.
Epoch: 1600 Train: 2.47353 Test: 2.23928
Epoch: 1700 Train: 2.47385 Test: 2.11879
Epoch: 1800 Train: 11.01306 Test: 12.05068
Epoch: 1900 Train: 2.73297 Test: 1.87686
Epoch: 2000 Train: 2.13737 Test: 1.96406
Epoch: 2100 Train: 2.69099 Test: 1.81825
Epoch: 2200 Train: 1.75062 Test: 1.26662
Epoch 2300: New minimal relative error: 6.71%, model saved.
Epoch: 2300 Train: 1.69612 Test: 1.21300
Epoch: 2400 Train: 1.64753 Test: 1.21587
Epoch: 2500 Train: 2.12509 Test: 1.89741
Epoch: 2600 Train: 1.46949 Test: 1.03183
Epoch: 2700 Train: 2.38883 Test: 2.60386
Epoch: 2800 Train: 3.97779 Test: 3.33545
Epoch: 2900 Train: 2.80684 Test: 2.34238
Epoch: 3000 Train: 2.74116 Test: 3.34038
Epoch: 3100 Train: 1.18429 Test: 0.89336
Epoch: 3200 Train: 0.97334 Test: 0.70104
Epoch: 3300 Train: 0.90638 Test: 0.66891
Epoch: 3400 Train: 0.91032 Test: 0.68398
Epoch: 3500 Train: 1.52058 Test: 1.25146
Epoch: 3600 Train: 0.92597 Test: 0.94835
Epoch: 3700 Train: 0.91772 Test: 0.77672
Epoch: 3800 Train: 0.80514 Test: 0.70994
Epoch: 3900 Train: 0.70744 Test: 0.65816
Epoch: 4000 Train: 0.56954 Test: 0.46940
Epoch: 4100 Train: 0.52986 Test: 0.45860
Epoch: 4200 Train: 0.51064 Test: 0.44484
Epoch: 4300 Train: 0.46097 Test: 0.42337
Epoch: 4400 Train: 0.56116 Test: 0.53944
Epoch: 4500 Train: 0.47989 Test: 0.48345
Epoch: 4600 Train: 0.41543 Test: 0.42943
Epoch: 4700 Train: 0.85333 Test: 0.50587
Epoch: 4800 Train: 0.37513 Test: 0.39098
Epoch: 4900 Train: 0.32963 Test: 0.35556
Epoch: 5000 Train: 0.32629 Test: 0.35984
Epoch: 5100 Train: 0.31676 Test: 0.34566
Epoch: 5200 Train: 0.42302 Test: 0.57347
Epoch: 5300 Train: 1.75846 Test: 2.19104
Epoch: 5400 Train: 0.65425 Test: 0.81822
Epoch: 5500 Train: 0.25707 Test: 0.30589
Epoch: 5600 Train: 0.30154 Test: 0.37732
Epoch: 5700 Train: 0.24244 Test: 0.29843
Epoch: 5800 Train: 0.23515 Test: 0.29088
Epoch: 5900 Train: 0.22485 Test: 0.27205
Epoch: 6000 Train: 0.22179 Test: 0.27688
Epoch: 6100 Train: 0.21533 Test: 0.26373
Epoch: 6200 Train: 0.26744 Test: 0.30889
Epoch: 6300 Train: 1.04108 Test: 0.88460
Epoch: 6400 Train: 2.42268 Test: 3.13386
Epoch: 6500 Train: 0.19389 Test: 0.24986
Epoch: 6600 Train: 0.23615 Test: 0.35574
Epoch: 6700 Train: 0.19116 Test: 0.24420
Epoch: 6800 Train: 1.12555 Test: 1.11249
Epoch: 6900 Train: 1.35157 Test: 1.22757
Epoch: 7000 Train: 0.24470 Test: 0.35869
Epoch: 7100 Train: 0.17144 Test: 0.23013
Epoch: 7200 Train: 0.18863 Test: 0.27481
Epoch: 7300 Train: 0.28677 Test: 0.24979
Epoch: 7400 Train: 0.16270 Test: 0.21971
Epoch: 7500 Train: 0.16021 Test: 0.22212
Epoch: 7600 Train: 0.88090 Test: 0.91487
Epoch: 7700 Train: 0.19739 Test: 0.29329
Epoch: 7800 Train: 0.15267 Test: 0.20943
Epoch: 7900 Train: 0.15072 Test: 0.20865
Epoch: 8000 Train: 0.25094 Test: 0.32990
Epoch: 8100 Train: 0.14382 Test: 0.20334
Epoch: 8200 Train: 0.67059 Test: 0.72978
Epoch: 8300 Train: 0.14839 Test: 0.22225
Epoch: 8400 Train: 0.13762 Test: 0.20030
Epoch: 8500 Train: 0.17373 Test: 0.21800
Epoch 8600: New minimal relative error: 6.58%, model saved.
Epoch: 8600 Train: 0.13320 Test: 0.19359
Epoch: 8700 Train: 0.15784 Test: 0.24906
Epoch: 8800 Train: 0.14676 Test: 0.21923
Epoch 8900: New minimal relative error: 3.39%, model saved.
Epoch: 8900 Train: 0.12782 Test: 0.18783
Epoch: 9000 Train: 0.12664 Test: 0.19129
Epoch: 9100 Train: 0.26326 Test: 0.38958
Epoch: 9200 Train: 0.12533 Test: 0.18034
Epoch: 9300 Train: 0.17482 Test: 0.25898
Epoch: 9400 Train: 0.12022 Test: 0.17872
Epoch: 9500 Train: 0.27023 Test: 0.40721
Epoch: 9600 Train: 0.11754 Test: 0.17625
Epoch: 9700 Train: 0.12050 Test: 0.17702
Epoch: 9800 Train: 0.11798 Test: 0.18594
Epoch: 9900 Train: 0.11456 Test: 0.17557
Epoch: 9999 Train: 0.11261 Test: 0.16939
Training Loss: tensor(0.1126)
Test Loss: tensor(0.1694)
Learned LE: [ 0.8345755  -0.01323636 -4.030275  ]
True LE: [ 8.5831898e-01 -3.3306230e-03 -1.4527181e+01]
Relative Error: [16.93642   18.175776  18.812696  19.010668  18.925613  18.826195
 18.581575  18.15667   17.42911   16.645592  15.833608  14.933908
 13.993661  13.096828  12.298703  11.487142  10.712727  10.030189
  9.289815   8.768616   8.237476   7.552532   7.576888   8.448322
  9.795127  11.457355  13.424894  15.52723   16.82983   16.757336
 16.373552  16.410164  15.9053    15.355227  14.546593  13.462841
 12.266519  11.256898  10.317085   9.359092   8.335535   7.3073306
  6.375639   5.725143   5.542941   5.6638575  6.0122037  6.4327173
  6.7195296  7.000798   7.014471   6.751661   6.695801   6.910842
  7.428594   8.058812   8.722494   9.863488  11.234753  12.593537
 13.398181  14.330143  15.423817  16.57233   17.50835   17.46978
 17.451097  17.501297  17.392342  16.9789    16.24826   15.614144
 14.911607  14.09972   13.201391  12.414639  11.526661  10.572461
  9.6259775  8.837444   8.2150955  7.657629   7.4149904  6.8675303
  6.7156105  7.272711   8.579774   9.924561  11.853583  14.014639
 15.494132  15.503344  15.307517  15.280416  14.768447  14.309281
 13.578547  12.534896  11.33189   10.361459   9.436068   8.427304
  7.3383636  6.2405715  5.2525253  4.791902   4.8283553  5.0782747
  5.5447783  6.059453   6.401822   6.56073    6.6204777  6.4258924
  6.223005   6.239809   6.5596013  7.1601644  7.60015    8.308062
  9.604068  10.833173  11.564989  12.658326  13.9109955 15.113192
 16.177624  16.201326  16.03856   16.228148  16.271124  15.793907
 15.285316  14.715147  14.273813  13.562654  12.764872  11.9561405
 11.009579   9.973263   8.966785   7.992954   7.1864552  6.7481422
  6.570059   6.28776    5.981845   6.3538423  7.46085    8.853587
 10.456363  12.568421  14.278968  14.23715   14.232297  14.089547
 13.778855  13.416503  12.787286  11.692873  10.544741   9.682655
  8.719062   7.66461    6.5159163  5.3559723  4.3410854  4.186688
  4.2951946  4.6305904  5.0560665  5.526845   6.0486093  6.235423
  6.315573   6.374368   6.057975   5.8772964  5.9506707  6.384287
  6.8620696  7.2055264  7.944896   9.022072   9.5868635 10.797744
 12.187915  13.541367  14.79005   15.040116  14.872195  14.941156
 15.128652  14.777342  14.42132   14.044419  13.714192  13.168779
 12.4622965 11.656663  10.685108   9.625927   8.575288   7.480491
  6.5989785  5.9165597  5.757434   5.7176275  5.229693   5.4744306
  6.4429083  7.9036355  9.223988  11.066777  13.123804  13.198135
 13.135711  12.829588  12.869782  12.655842  12.163043  11.046215
  9.982022   9.178313   8.18527    7.1035175  5.9086967  4.6699586
  3.7672858  3.6938639  3.8650765  4.2024794  4.538412   4.968893
  5.509273   5.919995   6.01253    6.1452923  6.0185146  5.701599
  5.509209   5.674072   6.314844   6.7731857  7.0370836  7.649952
  7.6377416  8.674535  10.15938   11.813207  13.136068  13.99445
 13.88016   13.762246  13.921194  13.805541  13.684567  13.485516
 13.258132  12.833963  12.326178  11.548081  10.565465   9.531295
  8.426299   7.275863   6.293238   5.5376825  4.9398174  4.7622366
  4.6664248  4.7215314  5.4433784  6.689864   8.163808   9.645874
 11.499006  11.934631  11.548544  11.002286  11.085991  11.055888
 11.144177  10.597063   9.609059   8.856438   7.851495   6.760022
  5.5224733  4.1397038  3.3653731  3.275783   3.492507   3.7355742
  4.009988   4.3825006  4.8197947  5.2708073  5.6434445  5.844118
  6.0330935  5.6224856  5.2348685  5.0975904  5.3985515  6.058535
  6.7305727  6.9325776  6.707646   6.4087334  7.8171477  9.678453
 11.338014  12.692635  12.919371  12.90701   12.779096  12.783451
 12.958063  12.883563  12.541665  12.1219635 11.746691  11.281125
 10.564158   9.5854645  8.496011   7.3599176  6.2752433  5.3969545
  4.6061797  3.9319315  3.9047441  4.2235584  4.588758   5.5497484
  6.897476   8.334663   9.866819  10.4003725 10.171168   9.590346
  9.176015   9.427168   9.436898   9.245659   9.031364   8.713246
  7.7735863  6.6428146  5.3500977  3.894219   3.0745301  2.9590654
  3.1373053  3.2964733  3.5231807  3.7403808  4.0716596  4.491999
  4.8446198  5.316504   5.5771527  5.7010064  5.1519165  4.74457
  4.6484494  5.012907   5.684233   6.4621406  6.7917747  5.7609224
  5.2946978  6.9065237  9.002867  10.737328  11.827022  12.014003
 12.019459  11.768987  11.812113  11.777964  11.441175  11.125338
 10.772529  10.45982    9.921542   9.254646   8.567395   7.5510316
  6.4980745  5.585834   4.604916   3.7477221  3.1623006  3.4566886
  3.917845   4.392392   5.522469   7.0144696]
