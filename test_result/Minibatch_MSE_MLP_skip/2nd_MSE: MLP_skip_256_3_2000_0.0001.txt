time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.03%, model saved.
Epoch: 0 Train: 3875.83325 Test: 4228.82178
Epoch 100: New minimal relative error: 87.38%, model saved.
Epoch: 100 Train: 205.23172 Test: 198.83224
Epoch 200: New minimal relative error: 18.28%, model saved.
Epoch: 200 Train: 24.00023 Test: 25.33558
Epoch 300: New minimal relative error: 14.11%, model saved.
Epoch: 300 Train: 11.65689 Test: 10.94130
Epoch 400: New minimal relative error: 10.69%, model saved.
Epoch: 400 Train: 7.82838 Test: 7.45125
Epoch: 500 Train: 6.01849 Test: 5.86676
Epoch: 600 Train: 4.97892 Test: 5.09364
Epoch: 700 Train: 4.06329 Test: 4.04943
Epoch: 800 Train: 15.16146 Test: 13.40405
Epoch: 900 Train: 3.01579 Test: 3.06800
Epoch 1000: New minimal relative error: 9.00%, model saved.
Epoch: 1000 Train: 2.64856 Test: 2.73888
Epoch: 1100 Train: 2.34713 Test: 2.43408
Epoch: 1200 Train: 2.10456 Test: 2.22565
Epoch: 1300 Train: 1.91740 Test: 2.09755
Epoch: 1400 Train: 1.85762 Test: 1.97709
Epoch: 1500 Train: 1.53496 Test: 1.63054
Epoch 1600: New minimal relative error: 7.24%, model saved.
Epoch: 1600 Train: 1.39339 Test: 1.47976
Epoch: 1700 Train: 1.27018 Test: 1.35149
Epoch: 1800 Train: 1.22869 Test: 1.31705
Epoch: 1900 Train: 3.98780 Test: 2.78454
Epoch: 2000 Train: 0.98202 Test: 1.06316
Epoch: 2100 Train: 0.91179 Test: 0.98096
Epoch: 2200 Train: 0.84268 Test: 0.91209
Epoch: 2300 Train: 0.78627 Test: 0.85709
Epoch: 2400 Train: 2.19868 Test: 1.61542
Epoch: 2500 Train: 0.68330 Test: 0.75075
Epoch: 2600 Train: 0.64193 Test: 0.70585
Epoch: 2700 Train: 0.92526 Test: 1.12805
Epoch: 2800 Train: 0.56514 Test: 0.62648
Epoch 2900: New minimal relative error: 6.91%, model saved.
Epoch: 2900 Train: 0.53562 Test: 0.59765
Epoch: 3000 Train: 0.51013 Test: 0.56258
Epoch 3100: New minimal relative error: 6.27%, model saved.
Epoch: 3100 Train: 0.48748 Test: 0.53650
Epoch: 3200 Train: 0.50056 Test: 0.56545
Epoch: 3300 Train: 0.55148 Test: 0.64539
Epoch: 3400 Train: 0.71005 Test: 0.78240
Epoch: 3500 Train: 0.53988 Test: 0.56994
Epoch: 3600 Train: 0.66943 Test: 0.61113
Epoch: 3700 Train: 0.46007 Test: 0.52136
Epoch: 3800 Train: 0.35095 Test: 0.39554
Epoch: 3900 Train: 0.34043 Test: 0.38410
Epoch: 4000 Train: 0.61339 Test: 0.52854
Epoch: 4100 Train: 0.86416 Test: 1.02939
Epoch: 4200 Train: 0.30722 Test: 0.34832
Epoch: 4300 Train: 0.29774 Test: 0.34305
Epoch: 4400 Train: 0.30872 Test: 0.35351
Epoch: 4500 Train: 0.28007 Test: 0.32773
Epoch: 4600 Train: 0.27012 Test: 0.31474
Epoch: 4700 Train: 0.26172 Test: 0.30900
Epoch: 4800 Train: 0.82814 Test: 0.86545
Epoch: 4900 Train: 0.25234 Test: 0.29642
Epoch: 5000 Train: 0.24459 Test: 0.28971
Epoch: 5100 Train: 0.23434 Test: 0.28042
Epoch: 5200 Train: 0.67317 Test: 0.74541
Epoch: 5300 Train: 0.22539 Test: 0.27221
Epoch: 5400 Train: 0.22152 Test: 0.26477
Epoch: 5500 Train: 0.22127 Test: 0.39342
Epoch: 5600 Train: 0.21255 Test: 0.25586
Epoch: 5700 Train: 0.29334 Test: 0.37556
Epoch: 5800 Train: 0.20975 Test: 0.24990
Epoch: 5900 Train: 0.20476 Test: 0.24911
Epoch: 6000 Train: 0.19888 Test: 0.24078
Epoch: 6100 Train: 0.20290 Test: 0.24064
Epoch: 6200 Train: 0.20676 Test: 0.24546
Epoch: 6300 Train: 0.19057 Test: 0.22937
Epoch: 6400 Train: 0.24037 Test: 0.27088
Epoch: 6500 Train: 0.18423 Test: 0.22337
Epoch: 6600 Train: 0.18333 Test: 0.22023
Epoch: 6700 Train: 0.18634 Test: 0.22692
Epoch: 6800 Train: 0.17703 Test: 0.21574
Epoch: 6900 Train: 0.17477 Test: 0.21233
Epoch: 7000 Train: 0.17322 Test: 0.21225
Epoch: 7100 Train: 0.17057 Test: 0.20850
Epoch: 7200 Train: 0.21924 Test: 0.23674
Epoch: 7300 Train: 0.16631 Test: 0.20329
Epoch: 7400 Train: 0.17536 Test: 0.20615
Epoch: 7500 Train: 0.16249 Test: 0.19915
Epoch: 7600 Train: 0.16274 Test: 0.19733
Epoch: 7700 Train: 0.17737 Test: 0.24165
Epoch: 7800 Train: 0.15739 Test: 0.19303
Epoch: 7900 Train: 0.15572 Test: 0.19114
Epoch: 8000 Train: 0.15612 Test: 0.20270
Epoch: 8100 Train: 0.16249 Test: 0.20066
Epoch: 8200 Train: 0.40933 Test: 0.50963
Epoch: 8300 Train: 0.14915 Test: 0.18434
Epoch: 8400 Train: 0.24461 Test: 0.27221
Epoch: 8500 Train: 0.14626 Test: 0.18130
Epoch: 8600 Train: 0.15226 Test: 0.18397
Epoch: 8700 Train: 0.14355 Test: 0.17829
Epoch: 8800 Train: 0.14223 Test: 0.17685
Epoch: 8900 Train: 0.14142 Test: 0.17560
Epoch 9000: New minimal relative error: 5.41%, model saved.
Epoch: 9000 Train: 0.13978 Test: 0.17394
Epoch: 9100 Train: 0.13870 Test: 0.17336
Epoch: 9200 Train: 0.13749 Test: 0.17203
Epoch: 9300 Train: 0.13629 Test: 0.17044
Epoch: 9400 Train: 0.15033 Test: 0.17133
Epoch: 9500 Train: 0.13987 Test: 0.17723
Epoch: 9600 Train: 0.15081 Test: 0.18623
Epoch: 9700 Train: 0.13180 Test: 0.16519
Epoch: 9800 Train: 0.14662 Test: 0.17151
Epoch: 9900 Train: 0.12976 Test: 0.16289
Epoch: 9999 Train: 0.13354 Test: 0.16976
Training Loss: tensor(0.1335)
Test Loss: tensor(0.1698)
Learned LE: [ 0.766207    0.09846755 -4.677575  ]
True LE: [ 8.8093120e-01 -4.8967963e-03 -1.4551498e+01]
Relative Error: [5.6406107  4.3690286  3.1598144  2.4639683  2.7192824  3.276959
 3.5016205  3.68767    3.5881624  3.6206372  3.818753   3.9531052
 4.44734    4.695956   5.009431   5.3977327  4.6486397  3.937288
 2.9142513  1.833809   1.2035121  1.0086784  1.0889125  0.96240956
 0.9653063  0.9856032  0.9855504  1.123773   1.0626752  1.3004011
 2.5838747  4.2323265  3.5841103  2.5283797  1.5595219  1.4074993
 1.2095004  1.2035136  0.8261879  0.6359137  0.4294556  0.8505036
 0.8452328  0.8940364  1.3351692  1.9886864  2.1817195  2.2610235
 2.6749854  3.3097084  3.0392618  3.1032643  3.4763176  4.3817554
 5.583231   5.907647   5.902394   5.997754   5.7408686  5.6899323
 5.785989   6.0634146  5.4278655  4.527535   3.341812   1.9738135
 1.3694868  1.5990493  2.1924944  2.5825894  2.6588237  2.6794214
 2.6843839  2.8100145  2.9317503  3.2243955  3.374267   3.8141391
 4.808671   4.3387923  3.3266604  2.1196718  1.1345614  0.6939119
 0.62996763 0.7816229  0.9825999  1.0755476  1.1690558  1.0930834
 0.8710179  0.60914147 0.37081546 1.5654525  3.0859687  2.6145277
 1.7813927  1.0744432  1.2387594  1.0159644  0.86380786 0.57577294
 0.48780116 0.4090396  0.8838652  0.8620519  0.78213793 1.2142307
 1.5638511  1.6908314  1.634353   1.983366   2.1875846  1.8402094
 1.8140804  2.1586637  2.9326174  3.7306628  3.8371968  4.015988
 4.0930815  3.993925   4.0913606  4.385772   4.826791   4.3025155
 3.589599   2.7185187  1.4110391  0.79905224 0.9194537  1.2013
 1.8526846  1.8183241  2.01285    1.8794006  1.7572554  1.8628135
 2.0798852  2.1477501  2.7020411  3.60693    3.9087393  2.486858
 1.4876394  0.72256684 0.54322886 0.47987267 0.43838072 0.7713549
 0.99056244 1.1631941  1.2861505  1.0933247  0.60052073 0.1476466
 0.84854263 2.3710918  2.1732118  1.3607074  0.6441431  1.0190893
 0.91269475 0.6255475  0.37117645 0.37795144 0.45861715 0.818896
 0.8410333  0.58931315 0.88365376 1.0590144  0.96636426 0.9468287
 1.2805059  1.2848235  0.85438174 0.5676801  0.7345481  1.6565671
 1.9773064  1.8325107  2.1758227  2.4068701  2.4049826  2.5708375
 2.9683995  3.5589423  3.455715   2.8461435  2.2096941  1.3192784
 0.26255298 1.3345133  1.1709092  0.9828743  1.288632   1.3877662
 1.3041445  0.8341132  0.9810294  1.176693   1.1287245  1.4555911
 2.0570421  2.7620912  1.999742   1.190775   1.0134137  0.82896996
 0.85848284 1.0569253  0.87691826 0.44495296 0.88711363 1.4543337
 1.4019742  1.0319858  0.43101433 0.3142971  1.2975116  1.4931709
 1.1541303  0.6142462  0.63971776 0.86447513 0.6436079  0.26109117
 0.29441684 0.5284713  0.6633997  0.67919314 0.41662577 0.4541219
 0.46243832 0.57390535 0.9094462  1.1441553  1.2006941  1.1644983
 0.9891198  0.3902245  0.53221154 0.8068956  0.66347134 0.74680907
 0.85971457 0.9574539  1.248079   1.6108532  2.1473587  2.8355284
 2.3147826  1.8745681  1.3971636  0.5378837  0.85119426 1.8090863
 1.1778749  0.779158   0.8695634  1.0444541  0.43797684 0.46909872
 0.60956484 0.49285161 0.3024412  0.5097429  1.1030083  1.761771
 1.8014845  1.2496387  0.9668726  0.62658817 0.7157663  0.69577706
 0.4755993  0.43680972 0.5845722  1.1050389  1.605913   1.2302965
 0.3554641  0.37948686 1.4489721  0.8682238  0.57901645 0.35673812
 0.790101   0.9182269  0.71827346 0.38638195 0.45564765 0.4279822
 0.5779835  0.34747806 0.2714177  0.48041037 0.522913   0.85547364
 1.1308309  1.190113   1.0387872  0.9143096  0.5521894  0.31512973
 0.7040638  0.5663766  0.5271093  0.7105865  1.0156593  0.9209787
 0.53931147 0.6599861  1.2533709  1.8867373  1.404688   1.2246888
 0.99079674 0.52861845 0.9241611  2.118341   1.4087507  0.8325295
 0.7362302  0.74715084 0.4993403  0.78082454 0.85351753 0.6072067
 0.72314304 1.0626743  0.8148848  1.6519988  1.7327217  1.3989893
 1.0719211  1.0120801  1.0658312  0.8028991  0.3007757  0.3628096
 0.49641472 0.6489168  1.3290263  1.4687883  0.87570155 0.42554587
 1.0463881  0.6153942  0.32024753 0.29964405 0.69454247 0.6693874
 1.3337257  0.7979022  0.4500533  0.2675108  0.47250098 0.60406566
 0.5465361  0.46369442 0.47582462 0.91679066 0.8170353  0.69505924
 0.58683795 0.42675906 0.43719777 0.8749715  0.79681593 0.47981912
 0.44480047 0.41678572 0.7822429  1.2840707  1.4595519  1.3351053
 0.561113   1.0732644  0.72893035 0.43539324 0.8206899  0.74068236
 0.6254412  1.8344885  2.0661917  1.1009259  0.48101643 0.9144418
 0.76379645 0.95050305 1.1601522  1.056042   1.5647788  1.5670272
 0.510871   0.82282174 1.3722476  1.1987604 ]
