time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 114.67%, model saved.
Epoch: 0 Train: 3916.33179 Test: 3687.63843
Epoch 100: New minimal relative error: 96.10%, model saved.
Epoch: 100 Train: 217.10503 Test: 223.08850
Epoch 200: New minimal relative error: 39.97%, model saved.
Epoch: 200 Train: 27.63819 Test: 32.15697
Epoch 300: New minimal relative error: 19.15%, model saved.
Epoch: 300 Train: 14.88912 Test: 16.48235
Epoch: 400 Train: 13.88332 Test: 16.19355
Epoch 500: New minimal relative error: 14.61%, model saved.
Epoch: 500 Train: 6.21857 Test: 7.78023
Epoch: 600 Train: 5.13915 Test: 6.50347
Epoch: 700 Train: 5.46678 Test: 6.69853
Epoch: 800 Train: 3.67325 Test: 4.67121
Epoch 900: New minimal relative error: 13.09%, model saved.
Epoch: 900 Train: 3.73770 Test: 4.24334
Epoch: 1000 Train: 2.86487 Test: 3.72272
Epoch 1100: New minimal relative error: 12.94%, model saved.
Epoch: 1100 Train: 2.53090 Test: 3.32056
Epoch: 1200 Train: 2.44794 Test: 3.04532
Epoch 1300: New minimal relative error: 8.33%, model saved.
Epoch: 1300 Train: 1.95538 Test: 2.65618
Epoch: 1400 Train: 1.81435 Test: 2.43118
Epoch: 1500 Train: 1.55213 Test: 2.33302
Epoch: 1600 Train: 1.33443 Test: 1.92361
Epoch: 1700 Train: 1.20473 Test: 1.75426
Epoch: 1800 Train: 1.21022 Test: 1.75093
Epoch: 1900 Train: 1.06664 Test: 1.54042
Epoch: 2000 Train: 1.22567 Test: 1.85430
Epoch: 2100 Train: 1.06186 Test: 1.51881
Epoch: 2200 Train: 0.79028 Test: 1.22255
Epoch: 2300 Train: 1.54526 Test: 1.81999
Epoch: 2400 Train: 0.71781 Test: 1.12905
Epoch: 2500 Train: 0.89223 Test: 1.07888
Epoch: 2600 Train: 5.37520 Test: 5.39687
Epoch: 2700 Train: 0.54892 Test: 0.86706
Epoch: 2800 Train: 0.53207 Test: 0.85130
Epoch: 2900 Train: 0.49818 Test: 0.78517
Epoch: 3000 Train: 0.46936 Test: 0.75228
Epoch: 3100 Train: 0.44798 Test: 0.72623
Epoch: 3200 Train: 0.47194 Test: 0.70438
Epoch: 3300 Train: 0.40745 Test: 0.67263
Epoch: 3400 Train: 0.39293 Test: 0.65025
Epoch: 3500 Train: 0.37815 Test: 0.63147
Epoch 3600: New minimal relative error: 7.72%, model saved.
Epoch: 3600 Train: 1.42957 Test: 1.96298
Epoch: 3700 Train: 0.35006 Test: 0.59573
Epoch: 3800 Train: 2.17230 Test: 2.50171
Epoch: 3900 Train: 0.32841 Test: 0.56204
Epoch: 4000 Train: 0.32309 Test: 0.54902
Epoch: 4100 Train: 0.30789 Test: 0.53915
Epoch: 4200 Train: 0.29907 Test: 0.52693
Epoch: 4300 Train: 0.29596 Test: 0.52710
Epoch 4400: New minimal relative error: 6.97%, model saved.
Epoch: 4400 Train: 0.30912 Test: 0.54368
Epoch: 4500 Train: 0.31816 Test: 0.52248
Epoch: 4600 Train: 0.26967 Test: 0.48334
Epoch: 4700 Train: 0.27493 Test: 0.48767
Epoch: 4800 Train: 0.26363 Test: 0.46594
Epoch: 4900 Train: 0.99484 Test: 1.47456
Epoch: 5000 Train: 0.24314 Test: 0.44748
Epoch: 5100 Train: 1.10664 Test: 1.25862
Epoch: 5200 Train: 0.23218 Test: 0.43079
Epoch: 5300 Train: 0.22926 Test: 0.42581
Epoch: 5400 Train: 0.47117 Test: 0.77212
Epoch: 5500 Train: 0.30344 Test: 0.51741
Epoch: 5600 Train: 0.21380 Test: 0.40466
Epoch: 5700 Train: 0.21036 Test: 0.39655
Epoch 5800: New minimal relative error: 6.64%, model saved.
Epoch: 5800 Train: 0.24071 Test: 0.42983
Epoch: 5900 Train: 0.22465 Test: 0.40159
Epoch: 6000 Train: 0.19796 Test: 0.37981
Epoch: 6100 Train: 0.19753 Test: 0.38648
Epoch: 6200 Train: 0.30259 Test: 0.48608
Epoch: 6300 Train: 0.21261 Test: 0.40003
Epoch: 6400 Train: 0.31662 Test: 0.48384
Epoch: 6500 Train: 0.18112 Test: 0.35396
Epoch: 6600 Train: 0.17893 Test: 0.35059
Epoch: 6700 Train: 0.18715 Test: 0.35615
Epoch: 6800 Train: 0.17251 Test: 0.34156
Epoch: 6900 Train: 0.17121 Test: 0.34086
Epoch: 7000 Train: 0.17644 Test: 0.33569
Epoch: 7100 Train: 0.16503 Test: 0.32881
Epoch: 7200 Train: 0.16231 Test: 0.32540
Epoch: 7300 Train: 0.16809 Test: 0.33037
Epoch: 7400 Train: 0.16595 Test: 0.32726
Epoch: 7500 Train: 0.23584 Test: 0.40681
Epoch: 7600 Train: 0.15357 Test: 0.31018
Epoch: 7700 Train: 0.15136 Test: 0.30920
Epoch: 7800 Train: 0.14970 Test: 0.30699
Epoch: 7900 Train: 0.15477 Test: 0.31997
Epoch: 8000 Train: 0.14656 Test: 0.29948
Epoch: 8100 Train: 0.17054 Test: 0.32138
Epoch: 8200 Train: 0.14296 Test: 0.29214
Epoch: 8300 Train: 0.15972 Test: 0.28954
Epoch: 8400 Train: 0.13856 Test: 0.28741
Epoch: 8500 Train: 0.13719 Test: 0.28457
Epoch: 8600 Train: 0.13700 Test: 0.28530
Epoch: 8700 Train: 0.13397 Test: 0.27909
Epoch: 8800 Train: 0.13283 Test: 0.27628
Epoch: 8900 Train: 0.13389 Test: 0.27642
Epoch: 9000 Train: 0.13042 Test: 0.27118
Epoch: 9100 Train: 0.13656 Test: 0.28723
Epoch: 9200 Train: 0.12716 Test: 0.26769
Epoch: 9300 Train: 0.12648 Test: 0.26555
Epoch: 9400 Train: 0.12486 Test: 0.26249
Epoch: 9500 Train: 0.12349 Test: 0.26087
Epoch: 9600 Train: 0.12240 Test: 0.26028
Epoch: 9700 Train: 0.13024 Test: 0.27183
Epoch: 9800 Train: 0.12075 Test: 0.25385
Epoch: 9900 Train: 0.11903 Test: 0.25284
Epoch: 9999 Train: 0.16869 Test: 0.29871
Training Loss: tensor(0.1687)
Test Loss: tensor(0.2987)
Learned LE: [ 0.8138496   0.04677711 -4.337172  ]
True LE: [ 8.8372260e-01 -4.9999333e-03 -1.4559698e+01]
Relative Error: [3.4916666  2.9960713  2.8575313  3.207024   3.718526   3.6166456
 3.4933128  2.5850694  1.72165    1.3382404  1.549297   1.5431445
 1.1129127  0.8948369  1.1030575  1.4412193  1.9895952  2.6627917
 2.7217035  2.8830338  2.819943   2.2850065  2.1047206  2.0798264
 2.054452   2.1022143  1.7694348  1.4384229  0.53842914 0.5726673
 1.4908538  2.0990515  2.7394173  1.9432313  1.2704118  1.5505228
 1.1419834  1.0398859  1.1868973  0.8836024  1.4341441  2.2432067
 3.4544804  4.092679   3.8544967  3.8548663  3.933465   3.7361443
 3.218757   3.0511365  2.7999914  2.7431693  2.347019   1.8981991
 1.832374   1.808659   1.9509919  2.191258   2.2943907  2.257104
 2.4377437  2.6503384  3.1693764  3.7176077  3.0855865  2.7087824
 2.8892922  3.0317636  2.8430817  2.8073442  1.9347563  1.2605802
 1.4042891  1.816694   1.2572064  0.78575283 1.0755037  1.5558685
 1.6600689  1.6702186  2.1619852  1.808495   1.5896386  1.6002575
 2.1578002  2.3954515  2.2873273  2.0982525  2.2890086  1.9701335
 1.3511479  0.9104843  0.97572654 1.4212947  2.1900792  2.7147436
 2.175089   1.1729277  1.3950864  0.9985091  0.91511595 1.0043168
 0.9387412  1.2346104  2.4276793  3.6402636  3.409037   2.9251075
 2.9748833  3.1532314  3.1092281  2.6895356  2.4319224  2.3403828
 2.2315226  1.9600074  1.4664612  1.4408952  1.5030633  1.600938
 1.7614836  1.6513534  1.6972463  2.0245917  2.4066324  2.8236814
 3.4961793  3.1843245  2.6524763  2.58924    2.528822   2.2958739
 2.1720302  1.6545236  1.1526797  1.5391681  1.4300439  1.0391388
 0.7792211  1.5484499  1.7865624  1.604452   1.4793856  1.3292826
 1.2561443  1.9388106  1.9783775  2.326602   2.2762856  2.0508616
 1.9267004  2.2457159  2.3585417  1.9465479  1.5700867  1.3884411
 1.471492   1.8193443  2.4170551  2.587507   1.1793236  0.98799103
 0.83259445 0.7771378  0.9239382  0.81640023 1.159451   2.567912
 3.2086034  2.587026   2.046724   2.1515553  2.2113903  2.3659437
 2.2358012  1.8787795  1.9452796  1.8820854  1.4628667  1.1804117
 1.2436962  1.2740395  1.269723   1.3985381  1.1358567  1.2135422
 1.5616293  2.0130298  2.4077384  2.9086282  3.3482974  2.5851457
 2.2985508  2.1405692  1.9341688  1.7953849  1.70711    1.169642
 1.6913036  1.2394484  0.82663774 0.8275431  1.4675807  1.6068954
 1.9589223  2.0085742  2.614197   1.8151548  2.0142665  2.0132418
 2.3527951  2.4028962  2.1153374  1.8336196  1.6011572  1.6907139
 2.124192   2.2805815  1.8217169  1.5316635  1.50994    1.7899891
 2.3169832  1.3855788  0.7275541  0.76695967 0.68329173 0.76084125
 0.6960354  1.0139593  2.380077   2.5207853  2.0007787  1.3542386
 1.4790047  1.4049319  1.7441837  1.5239414  1.5857713  1.7651393
 1.613205   1.1332542  0.85525167 0.9762271  0.98940134 0.89735687
 1.1372142  0.81687814 0.73944485 0.86386895 1.1322124  1.5947033
 2.0479054  2.5773778  2.7296681  2.0759857  1.9029245  1.6099592
 1.5614529  1.5181504  1.350021   1.388587   1.6442677  0.83916056
 0.7877739  1.4556289  2.1950226  2.6104813  2.675674   3.3829272
 2.4016113  2.1763315  2.460165   2.3983455  2.4870782  2.174985
 1.9903896  1.7566085  1.3705975  1.3208332  1.8438971  2.2822714
 1.826487   1.3472655  1.1860919  1.3747575  1.8641088  0.83832276
 0.5040325  0.837536   0.5875218  0.67855453 0.7475178  1.648497
 2.0499625  1.5688617  1.1076345  0.8963666  0.9627847  0.83891964
 0.8375775  1.0086985  1.6386211  1.5422792  1.2265862  0.82106805
 0.8054968  0.74141735 0.601729   0.717266   0.7278504  0.7127311
 0.6835747  0.6506851  0.69673795 0.88099223 1.1222105  1.6214516
 2.0373821  1.4868749  1.4286959  1.2849303  1.3568503  1.4531069
 1.3438826  1.4487158  1.0852104  1.0459995  1.2432177  2.575503
 3.4793382  3.7451088  3.569505   2.6582358  3.0254567  2.7283814
 2.603928   2.3419874  2.0299845  2.0501635  1.969705   1.8319825
 1.2524245  1.0944738  1.3937471  1.8001968  2.1128292  1.1698521
 0.76504284 0.81492996 1.7032063  1.309916   0.7713397  0.780566
 0.6438815  1.0425218  1.1225368  1.631665   1.3389236  1.0776825
 0.61967856 0.70238304 0.8653841  0.34321424 0.24976307 1.1205416
 1.5916772  1.4223139  1.1654518  0.56608814 0.60692173 0.79736644
 0.76013243 0.86059755 0.8428223  0.79974777 0.6245702  0.41210636
 0.28163958 0.4871549  0.87432396 1.3610513  2.279488   1.9463893
 1.4649152  1.0913409  0.98187274 0.94548595 1.0482875  1.3025769
 1.032893   0.81348914 1.8617617  3.131473   4.319268   4.8568645
 3.4887378  2.826831   3.087881   3.13101   ]
