time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 4181.99951 Test: 3840.42603
Epoch: 100 Train: 180.17632 Test: 222.76129
Epoch 200: New minimal relative error: 31.60%, model saved.
Epoch: 200 Train: 24.00199 Test: 32.23283
Epoch 300: New minimal relative error: 21.71%, model saved.
Epoch: 300 Train: 9.60877 Test: 15.88602
Epoch: 400 Train: 12.59140 Test: 14.78337
Epoch: 500 Train: 5.00149 Test: 9.62529
Epoch 600: New minimal relative error: 16.55%, model saved.
Epoch: 600 Train: 3.95307 Test: 7.69608
Epoch: 700 Train: 3.30680 Test: 6.76773
Epoch: 800 Train: 2.84835 Test: 5.97356
Epoch: 900 Train: 2.50219 Test: 5.18032
Epoch 1000: New minimal relative error: 16.15%, model saved.
Epoch: 1000 Train: 4.41368 Test: 5.56874
Epoch: 1100 Train: 2.07991 Test: 4.28723
Epoch 1200: New minimal relative error: 15.86%, model saved.
Epoch: 1200 Train: 2.09769 Test: 3.96079
Epoch: 1300 Train: 2.38585 Test: 4.59329
Epoch 1400: New minimal relative error: 11.30%, model saved.
Epoch: 1400 Train: 1.37582 Test: 3.23280
Epoch: 1500 Train: 1.18975 Test: 2.88569
time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 4181.99951 Test: 3840.42603
Epoch: 100 Train: 180.17632 Test: 222.76129
time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 4181.99951 Test: 3840.42603
time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 4181.99951 Test: 3840.42603
time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 4181.99951 Test: 3912.07983
Epoch: 100 Train: 180.17632 Test: 211.94633
time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 100.45%, model saved.
Epoch: 0 Train: 4181.99951 Test: 3912.07983
Epoch: 100 Train: 180.17632 Test: 211.94633
Epoch 200: New minimal relative error: 31.60%, model saved.
Epoch: 200 Train: 24.00199 Test: 30.67616
Epoch 300: New minimal relative error: 21.71%, model saved.
Epoch: 300 Train: 9.60877 Test: 14.54924
Epoch: 400 Train: 12.59140 Test: 14.11101
Epoch: 500 Train: 5.00149 Test: 8.55413
Epoch 600: New minimal relative error: 16.55%, model saved.
Epoch: 600 Train: 3.95307 Test: 6.85615
Epoch: 700 Train: 3.30680 Test: 5.98711
Epoch: 800 Train: 2.84835 Test: 5.26619
Epoch: 900 Train: 2.50219 Test: 4.57880
Epoch 1000: New minimal relative error: 16.15%, model saved.
Epoch: 1000 Train: 4.41368 Test: 5.16485
Epoch: 1100 Train: 2.07991 Test: 3.79040
Epoch 1200: New minimal relative error: 15.86%, model saved.
Epoch: 1200 Train: 2.09769 Test: 3.56033
Epoch: 1300 Train: 2.38585 Test: 4.09051
Epoch 1400: New minimal relative error: 11.30%, model saved.
Epoch: 1400 Train: 1.37582 Test: 2.79483
Epoch: 1500 Train: 1.18975 Test: 2.49197
Epoch: 1600 Train: 1.07233 Test: 2.33126
Epoch: 1700 Train: 1.04515 Test: 2.41365
Epoch: 1800 Train: 1.16864 Test: 2.14826
Epoch: 1900 Train: 1.39431 Test: 2.89539
Epoch: 2000 Train: 0.70320 Test: 1.70182
Epoch: 2100 Train: 0.65550 Test: 1.62504
Epoch 2200: New minimal relative error: 10.45%, model saved.
Epoch: 2200 Train: 0.61376 Test: 1.56177
Epoch: 2300 Train: 0.56019 Test: 1.44243
Epoch: 2400 Train: 0.59180 Test: 1.40349
Epoch: 2500 Train: 0.50748 Test: 1.32530
Epoch: 2600 Train: 1.09625 Test: 1.90297
Epoch: 2700 Train: 1.62959 Test: 2.63820
Epoch: 2800 Train: 0.61021 Test: 1.46228
Epoch: 2900 Train: 0.40852 Test: 1.15497
Epoch: 3000 Train: 0.38847 Test: 1.10293
Epoch: 3100 Train: 0.38070 Test: 1.07138
Epoch: 3200 Train: 0.36819 Test: 1.03195
Epoch: 3300 Train: 0.39256 Test: 1.02889
Epoch: 3400 Train: 0.41261 Test: 1.02414
Epoch: 3500 Train: 0.59102 Test: 1.14232
Epoch: 3600 Train: 0.31995 Test: 0.92669
Epoch: 3700 Train: 0.45390 Test: 1.13329
Epoch: 3800 Train: 0.38326 Test: 0.94223
Epoch: 3900 Train: 0.53519 Test: 1.00180
Epoch: 4000 Train: 0.55128 Test: 0.94863
Epoch: 4100 Train: 0.28318 Test: 0.83571
Epoch: 4200 Train: 0.35303 Test: 0.99245
Epoch: 4300 Train: 0.26533 Test: 0.78966
Epoch: 4400 Train: 0.25678 Test: 0.81719
Epoch: 4500 Train: 0.28032 Test: 0.78893
Epoch: 4600 Train: 0.59025 Test: 1.24198
Epoch: 4700 Train: 0.30480 Test: 0.86219
Epoch: 4800 Train: 0.24665 Test: 0.77970
Epoch: 4900 Train: 0.51393 Test: 1.14593
Epoch: 5000 Train: 0.79531 Test: 1.18569
Epoch: 5100 Train: 0.50634 Test: 0.96391
Epoch: 5200 Train: 0.63710 Test: 0.93470
Epoch: 5300 Train: 0.25545 Test: 0.73389
Epoch: 5400 Train: 0.30439 Test: 0.79755
Epoch: 5500 Train: 0.34398 Test: 0.75858
Epoch: 5600 Train: 0.51246 Test: 1.07971
Epoch: 5700 Train: 0.51648 Test: 0.84663
Epoch: 5800 Train: 0.19209 Test: 0.63353
Epoch: 5900 Train: 0.18332 Test: 0.64011
Epoch: 6000 Train: 0.18250 Test: 0.61969
Epoch: 6100 Train: 0.17575 Test: 0.61912
Epoch: 6200 Train: 0.17215 Test: 0.61112
Epoch: 6300 Train: 0.16945 Test: 0.60551
Epoch: 6400 Train: 0.19096 Test: 0.59700
Epoch: 6500 Train: 0.16418 Test: 0.59070
Epoch: 6600 Train: 0.16166 Test: 0.57670
Epoch: 6700 Train: 0.16459 Test: 0.58711
Epoch: 6800 Train: 0.16355 Test: 0.59229
Epoch: 6900 Train: 0.15401 Test: 0.56016
Epoch: 7000 Train: 0.15965 Test: 0.56171
Epoch: 7100 Train: 0.14984 Test: 0.54802
Epoch: 7200 Train: 0.15149 Test: 0.54198
Epoch: 7300 Train: 0.33619 Test: 0.59722
Epoch: 7400 Train: 0.14390 Test: 0.53579
Epoch: 7500 Train: 0.14244 Test: 0.53385
Epoch: 7600 Train: 0.14034 Test: 0.52695
Epoch: 7700 Train: 0.13846 Test: 0.51943
Epoch: 7800 Train: 0.13691 Test: 0.51973
Epoch: 7900 Train: 0.16064 Test: 0.52168
Epoch 8000: New minimal relative error: 10.15%, model saved.
Epoch: 8000 Train: 0.13330 Test: 0.50951
Epoch: 8100 Train: 0.13248 Test: 0.50524
Epoch: 8200 Train: 0.13023 Test: 0.49869
Epoch: 8300 Train: 0.47989 Test: 0.73266
Epoch: 8400 Train: 0.12709 Test: 0.49130
Epoch: 8500 Train: 0.20515 Test: 0.51164
Epoch: 8600 Train: 0.22682 Test: 0.63472
Epoch: 8700 Train: 0.13378 Test: 0.50852
Epoch: 8800 Train: 0.12176 Test: 0.47610
Epoch: 8900 Train: 0.12045 Test: 0.47591
Epoch: 9000 Train: 0.20733 Test: 0.57780
Epoch: 9100 Train: 0.12159 Test: 0.47887
Epoch 9200: New minimal relative error: 9.60%, model saved.
Epoch: 9200 Train: 0.11686 Test: 0.46365
Epoch: 9300 Train: 0.11675 Test: 0.46584
Epoch: 9400 Train: 0.16335 Test: 0.48978
Epoch: 9500 Train: 0.11358 Test: 0.45369
Epoch: 9600 Train: 0.18370 Test: 0.56631
Epoch: 9700 Train: 0.11152 Test: 0.44856
Epoch: 9800 Train: 0.11075 Test: 0.44463
Epoch: 9900 Train: 0.11112 Test: 0.43933
Epoch: 9999 Train: 0.10867 Test: 0.44057
Training Loss: tensor(0.1087)
Test Loss: tensor(0.4406)
Learned LE: [ 0.8468398  -0.01652965 -4.4568954 ]
True LE: [ 8.6596406e-01 -2.6797608e-03 -1.4538744e+01]
Relative Error: [31.994387  31.107918  30.322334  29.63613   29.046553  28.548967
 28.13699   27.813425  27.524052  26.907372  26.291796  25.510853
 24.710993  24.2733    23.935257  23.713797  24.057041  24.558516
 24.893791  25.1896    25.531414  25.816103  25.647406  25.51572
 25.304571  24.949673  24.71147   24.5878    24.594671  24.739689
 25.023613  25.445286  26.002607  26.691029  27.506845  28.445889
 29.50271   30.670807  31.942623  33.305286  34.742756  36.23159
 37.739197  39.205887  40.563747  41.75306   42.710274  43.377205
 43.706936  43.583424  42.986763  42.41932   41.80135   40.933514
 39.81893   38.469666  37.103073  35.760162  34.47968   33.268097
 32.138744  31.100481  30.157476  29.31135   28.562084  27.907818
 27.345472  26.870419  26.475859  26.182026  25.969383  25.358604
 24.717915  23.848701  23.048468  22.590687  22.243713  22.038647
 22.476631  22.96103   23.248035  23.571814  23.951319  24.139614
 24.007816  23.926003  23.58474   23.298082  23.13444   23.085918
 23.164253  23.379023  23.74849   24.277063  24.910559  25.644115
 26.471052  27.39855   28.482637  29.669004  30.948792  32.30872
 33.73159   35.191605  36.65508   38.052864  39.32568   40.412556
 41.25326   41.795845  42.061035  41.678623  41.019188  40.61311
 39.919067  38.997128  37.773277  36.436825  35.101097  33.808525
 32.57048   31.403446  30.31868   29.323298  28.420008  27.610275
 26.89387   26.268425  25.730955  25.276384  24.915936  24.684498
 24.506498  23.878332  23.151531  22.256786  21.46411   20.993849
 20.646921  20.549355  21.009106  21.434965  21.72535   22.081467
 22.502903  22.589014  22.49608   22.332361  21.99609   21.778263
 21.688314  21.71225   21.878254  22.24257   22.71575   23.295006
 23.974926  24.749672  25.612003  26.554125  27.5654    28.759777
 30.04345   31.396175  32.79909   34.225742  35.637363  36.96516
 38.14966   39.13252   39.857197  40.28432   40.448616  39.872986
 39.385677  38.886703  38.119736  37.125515  35.834427  34.517242
 33.22298   31.966421  30.768507  29.64282   28.598955  27.642069
 26.774723  25.997793  25.310946  24.71212   24.196978  23.769876
 23.47075   23.300938  23.109364  22.463867  21.645542  20.733467
 19.956924  19.48275   19.145533  19.164436  19.650173  20.0027
 20.321026  20.71368   21.178482  21.157139  21.105408  20.807283
 20.531807  20.3835    20.366278  20.52695   20.83468   21.25148
 21.77627   22.399286  23.003065  23.774582  24.701183  25.768873
 26.833456  27.937117  29.220135  30.561287  31.939968  33.32861
 34.68528   35.938602  37.032017  37.90921   38.519497  38.94127
 38.682983  38.1402    37.82118   37.232334  36.400105  35.285942
 33.99949   32.71478   31.448915  30.227295  29.066822  27.979076
 26.97216   26.049992  25.21462   24.46686   23.806795  23.231972
 22.73819   22.360292  22.149006  21.987295  21.774645  21.112627
 20.204268  19.278255  18.52797   18.058685  17.741425  17.883896
 18.375937  18.68217   19.031712  19.462425  19.911013  19.837503
 19.732647  19.39878   19.185501  19.106676  19.254084  19.521084
 19.880232  20.332254  20.662018  21.172575  21.85242   22.68854
 23.667236  24.7762    26.011833  27.256426  28.47544   29.80003
 31.150173  32.496555  33.793423  34.96841   35.968113  36.739464
 37.238182  37.59713   37.148373  36.70709   36.317844  35.64667
 34.756775  33.539726  32.27586   31.01094   29.773403  28.584667
 27.458614  26.405216  25.431171  24.53966   23.740213  23.016647
 22.375723  21.823122  21.376312  21.090012  20.893744  20.739933
 20.502163  19.771059  18.82752   17.892027  17.178324  16.72346
 16.435871  16.706278  17.176123  17.470154  17.852766  18.321915
 18.666883  18.625051  18.382463  18.100895  17.951117  18.04524
 18.281387  18.600502  18.839985  19.059402  19.467655  20.053308
 20.803701  21.704718  22.741232  23.895416  25.15077   26.509493
 27.802315  29.106367  30.422762  31.725424  32.956894  34.050137
 34.954174  35.620983  36.109264  36.037125  35.65812   35.361145
 34.874336  34.128883  33.1317    31.898998  30.645605  29.401073
 28.190617  27.032059  25.937456  24.940434  24.042473  23.22053
 22.457653  21.75216   21.103786  20.531845  20.122908  19.885267
 19.70157   19.55568   19.290424  18.478136  17.4897    16.523115
 15.844585  15.4529705 15.229063  15.62898   16.037437  16.360254
 16.776154  17.283813  17.519922  17.486235  17.134127  16.907358
 16.91927   17.099422  17.392511  17.483265 ]
