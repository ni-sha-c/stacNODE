time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 106.40%, model saved.
Epoch: 0 Train: 3939.63037 Test: 3893.21631
Epoch 100: New minimal relative error: 22.79%, model saved.
Epoch: 100 Train: 27.44300 Test: 29.78630
Epoch 200: New minimal relative error: 12.75%, model saved.
Epoch: 200 Train: 10.42451 Test: 12.45960
Epoch: 300 Train: 6.68847 Test: 8.00660
Epoch: 400 Train: 70.24333 Test: 39.27630
Epoch: 500 Train: 2.91256 Test: 3.51124
Epoch 600: New minimal relative error: 10.96%, model saved.
Epoch: 600 Train: 2.33099 Test: 2.84675
Epoch: 700 Train: 11.30198 Test: 11.41280
Epoch: 800 Train: 2.66803 Test: 2.54212
Epoch: 900 Train: 3.14298 Test: 3.16823
Epoch: 1000 Train: 5.28594 Test: 7.55949
Epoch: 1100 Train: 8.45412 Test: 6.97523
Epoch: 1200 Train: 1.70065 Test: 1.98607
Epoch 1300: New minimal relative error: 9.25%, model saved.
Epoch: 1300 Train: 0.93347 Test: 1.22178
Epoch: 1400 Train: 3.61709 Test: 4.17442
Epoch: 1500 Train: 0.81678 Test: 1.01664
Epoch: 1600 Train: 2.70842 Test: 3.85296
Epoch: 1700 Train: 1.71650 Test: 1.64427
Epoch 1800: New minimal relative error: 7.81%, model saved.
Epoch: 1800 Train: 0.44552 Test: 0.61324
Epoch: 1900 Train: 0.44688 Test: 0.62843
Epoch: 2000 Train: 7.26885 Test: 6.24959
Epoch: 2100 Train: 1.42548 Test: 1.75436
Epoch: 2200 Train: 3.11007 Test: 3.90381
Epoch: 2300 Train: 0.88569 Test: 0.89010
Epoch 2400: New minimal relative error: 7.02%, model saved.
Epoch: 2400 Train: 0.35101 Test: 0.50763
Epoch: 2500 Train: 0.86902 Test: 0.52521
Epoch: 2600 Train: 0.61874 Test: 0.81897
Epoch: 2700 Train: 0.31415 Test: 0.37049
Epoch: 2800 Train: 0.22938 Test: 0.32411
Epoch: 2900 Train: 0.72740 Test: 0.79852
Epoch: 3000 Train: 1.60945 Test: 1.54134
Epoch: 3100 Train: 0.45726 Test: 0.62639
Epoch: 3200 Train: 0.37795 Test: 0.40241
Epoch: 3300 Train: 0.22925 Test: 0.30881
Epoch: 3400 Train: 0.27052 Test: 0.38897
Epoch: 3500 Train: 0.74668 Test: 0.91982
Epoch: 3600 Train: 0.60996 Test: 0.86477
Epoch: 3700 Train: 0.25037 Test: 0.25943
Epoch: 3800 Train: 0.26813 Test: 0.40086
Epoch: 3900 Train: 0.77719 Test: 1.17717
Epoch: 4000 Train: 0.24144 Test: 0.32581
Epoch 4100: New minimal relative error: 6.95%, model saved.
Epoch: 4100 Train: 0.38177 Test: 0.42896
Epoch: 4200 Train: 2.88938 Test: 3.77603
Epoch: 4300 Train: 1.02653 Test: 1.29279
Epoch: 4400 Train: 0.31744 Test: 0.39519
Epoch: 4500 Train: 0.32846 Test: 0.33983
Epoch: 4600 Train: 0.13186 Test: 0.20434
Epoch: 4700 Train: 0.90541 Test: 0.61275
Epoch: 4800 Train: 0.37262 Test: 0.47316
Epoch: 4900 Train: 0.22561 Test: 0.35737
Epoch: 5000 Train: 0.24093 Test: 0.27442
Epoch 5100: New minimal relative error: 5.20%, model saved.
Epoch: 5100 Train: 0.23476 Test: 0.28279
Epoch: 5200 Train: 0.23245 Test: 0.26293
Epoch: 5300 Train: 0.62681 Test: 0.79699
Epoch: 5400 Train: 0.70242 Test: 0.85658
Epoch: 5500 Train: 2.55202 Test: 2.15375
Epoch: 5600 Train: 0.15789 Test: 0.21350
Epoch: 5700 Train: 0.15666 Test: 0.18256
Epoch: 5800 Train: 0.08547 Test: 0.13945
Epoch: 5900 Train: 0.11785 Test: 0.13651
Epoch: 6000 Train: 0.09214 Test: 0.13137
Epoch: 6100 Train: 1.33298 Test: 1.01500
Epoch: 6200 Train: 0.71844 Test: 0.95958
Epoch: 6300 Train: 0.19381 Test: 0.28136
Epoch: 6400 Train: 0.16867 Test: 0.27205
Epoch 6500: New minimal relative error: 4.98%, model saved.
Epoch: 6500 Train: 0.06225 Test: 0.09907
Epoch: 6600 Train: 0.07749 Test: 0.12360
Epoch: 6700 Train: 0.43638 Test: 0.49561
Epoch: 6800 Train: 0.13736 Test: 0.16626
Epoch: 6900 Train: 0.06295 Test: 0.09774
Epoch: 7000 Train: 0.21240 Test: 0.27822
Epoch: 7100 Train: 0.09528 Test: 0.13137
Epoch: 7200 Train: 0.11453 Test: 0.13164
Epoch: 7300 Train: 0.05953 Test: 0.08897
Epoch: 7400 Train: 0.05458 Test: 0.08653
Epoch: 7500 Train: 0.06593 Test: 0.09635
Epoch: 7600 Train: 0.08036 Test: 0.12622
Epoch: 7700 Train: 0.17091 Test: 0.32566
Epoch: 7800 Train: 0.05005 Test: 0.08077
Epoch: 7900 Train: 0.05801 Test: 0.09330
Epoch: 8000 Train: 0.05228 Test: 0.08671
Epoch: 8100 Train: 0.05886 Test: 0.08957
Epoch: 8200 Train: 0.24224 Test: 0.29697
Epoch: 8300 Train: 0.10105 Test: 0.15138
Epoch: 8400 Train: 0.17193 Test: 0.22753
Epoch: 8500 Train: 0.06779 Test: 0.09713
Epoch: 8600 Train: 0.04688 Test: 0.07446
Epoch: 8700 Train: 0.32865 Test: 0.35048
Epoch: 8800 Train: 0.10501 Test: 0.19293
Epoch: 8900 Train: 0.06365 Test: 0.10460
Epoch: 9000 Train: 0.35236 Test: 0.39336
Epoch: 9100 Train: 0.08165 Test: 0.11691
Epoch: 9200 Train: 0.12819 Test: 0.20931
Epoch: 9300 Train: 0.34053 Test: 0.38357
Epoch: 9400 Train: 0.04026 Test: 0.06538
Epoch: 9500 Train: 0.05275 Test: 0.08520
Epoch: 9600 Train: 0.12148 Test: 0.14654
Epoch: 9700 Train: 0.04212 Test: 0.06656
Epoch 9800: New minimal relative error: 4.07%, model saved.
Epoch: 9800 Train: 0.13000 Test: 0.16710
Epoch: 9900 Train: 0.03985 Test: 0.06877
Epoch: 9999 Train: 0.03821 Test: 0.06677
Training Loss: tensor(0.0382)
Test Loss: tensor(0.0668)
Learned LE: [ 0.81569487  0.04542343 -4.318876  ]
True LE: [ 8.6307210e-01  8.8250050e-03 -1.4539792e+01]
Relative Error: [ 9.161011    9.660395    9.555396   10.197572   10.531256   11.353683
 12.228663   12.274018   12.011236   11.706378   11.286339   10.538838
  9.876856    9.021788    8.194082    7.427842    6.4820023   5.519661
  4.1592426   3.133419    2.4084368   1.9578847   1.6624756   1.3642817
  1.243201    1.4340454   1.7613573   2.328007    3.1666386   3.7407544
  3.7536736   3.5401437   3.1209378   2.093811    1.3811452   0.9191731
  1.3972063   1.7530252   1.6586592   1.6474515   1.913836    2.2711222
  2.4416664   2.5771332   2.9678404   3.4641738   4.0870275   4.7250485
  5.090604    5.4324617   5.899791    6.1226225   6.328541    5.992081
  5.8459077   5.932994    6.1519365   6.466594    7.0259957   7.3378553
  7.3968143   7.564886    8.141661    8.773979    8.6558895   9.219747
  9.800442   10.500278   11.586318   12.008951   11.821367   11.54488
 11.239028   10.475157    9.53896     8.655296    7.6996856   6.759332
  5.636964    4.551921    3.2500248   2.302757    1.742261    1.4527982
  1.2557356   1.104155    1.1027544   1.22681     1.5056617   1.9427654
  2.697703    3.393013    3.316785    3.3292348   3.138168    2.1417964
  1.4664283   0.86571646  1.0857788   1.5291893   1.5598837   1.6872617
  1.8832889   2.125566    2.282969    2.3216171   2.686148    3.1367536
  3.7065048   4.127857    4.382157    4.784579    5.2395964   5.400134
  5.3497205   5.097749    5.0550294   5.116654    5.268246    5.530988
  5.8714657   6.2292566   6.372872    6.3129077   6.768243    7.5829687
  7.7069798   8.181843    8.672371    9.487555   10.61287    11.468944
 11.643436   11.34297    11.083892   10.256674    9.283899    8.262112
  7.152071    5.819339    4.658914    3.440176    2.3500543   1.6460568
  1.4559933   1.4168042   1.3481829   1.1462612   1.088368    1.1503359
  1.2814317   1.5475342   2.1123328   2.9196863   3.0068126   3.1148012
  3.072742    2.3845105   1.5849086   0.98140466  0.6797199   1.1331111
  1.3504539   1.6558707   1.8992106   2.024606    2.057316    2.1236472
  2.3650236   2.781368    3.2444026   3.462804    3.7047327   4.1239305
  4.4891763   4.585477    4.527263    4.279653    4.2193437   4.2522964
  4.4046183   4.5440993   4.578656    4.9443      5.232291    5.3164477
  5.3774524   6.012596    6.593405    7.0295205   7.5541215   8.206267
  9.397454   10.405416   11.244659   11.08185    10.784182   10.058687
  9.079808    7.923002    6.4458003   4.8992386   3.6942258   2.482723
  1.7650726   1.8565079   2.2314212   2.4062939   2.256964    1.8731422
  1.4339228   1.1900057   1.1380893   1.2625519   1.5210537   2.2098475
  2.9149997   2.8000572   2.8996794   2.6252182   1.7895749   1.2799864
  0.4509233   0.7526816   0.94176495  1.4194111   1.8127015   1.9337814
  1.829469    1.8812805   2.0791745   2.3851943   2.7402422   2.9129024
  3.031972    3.3581276   3.7525382   3.7678797   3.7209983   3.5780628
  3.465905    3.480298    3.521304    3.57275     3.5364065   3.7279108
  3.9783716   4.1854086   4.3073874   4.555019    5.323062    5.7282696
  6.2482724   6.805457    7.954844    9.01815    10.075703   10.671936
 10.518303    9.882645    8.830607    7.4754634   5.852666    4.2299614
  2.7629404   1.7949369   1.8894534   2.6008847   3.2899332   3.7142608
  3.7545311   3.2733262   2.554765    1.6747127   1.1723388   1.0268797
  1.1163924   1.5078812   2.3968904   2.5906153   2.5340261   2.6204405
  1.9201322   1.6113927   0.78235877  0.27386227  0.6253454   1.0614699
  1.536656    1.7541021   1.6525198   1.5989      1.851627    2.0815687
  2.3363612   2.4380682   2.494582    2.7735493   3.072479    3.005639
  2.9130597   2.8956492   2.771258    2.7812333   2.8430934   2.7333844
  2.69121     2.7113059   2.8671422   3.0977342   3.2486136   3.3197007
  3.8740356   4.5505924   5.0867715   5.44636     6.280561    7.552331
  8.579451    9.39831    10.017279    9.653747    8.782402    7.1359754
  5.4513044   3.6325257   2.1116762   1.6611944   2.6287177   3.749472
  4.698777    5.229574    5.14571     4.6833825   4.0435038   3.047544
  1.8686825   1.0235847   0.74932075  1.0328102   1.6432885   2.1914444
  2.3087404   2.2333057   2.14064     1.7852647   1.2853199   0.40897974
  0.2054527   0.7343583   1.1280942   1.5878769   1.5084397   1.4699624
  1.5895102   1.8082887   1.9502535   1.9845495   1.8928143   2.1536627
  2.455507    2.4099574   2.2271917   2.166596    2.098244    2.0595639
  2.1992798   2.1311305   2.054885    1.9486064   1.9951715   2.1581068
  2.322283    2.4361975   2.503145    3.2249048   3.7767296   4.355734
  4.736148    5.8075376   6.8515906   7.906511    8.661901    9.0322895
  8.654867    7.199861    5.16125     3.3280637   1.8344758   1.7548088
  2.758483    4.0790863   5.0761065   5.9804764   6.3309274   6.1734023
  5.5221567   4.5348024   3.430777    2.0063493 ]
