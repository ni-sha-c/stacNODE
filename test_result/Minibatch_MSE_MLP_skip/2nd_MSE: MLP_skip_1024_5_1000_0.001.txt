time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 102.17%, model saved.
Epoch: 0 Train: 4168.24561 Test: 4105.33740
Epoch 100: New minimal relative error: 37.89%, model saved.
Epoch: 100 Train: 43.64124 Test: 55.12304
Epoch 200: New minimal relative error: 25.51%, model saved.
Epoch: 200 Train: 29.10069 Test: 45.03248
Epoch: 300 Train: 48.93380 Test: 32.53440
Epoch 400: New minimal relative error: 19.93%, model saved.
Epoch: 400 Train: 7.31273 Test: 7.71097
Epoch: 500 Train: 4.20269 Test: 5.45321
Epoch: 600 Train: 6.32633 Test: 10.71230
Epoch: 700 Train: 5.22301 Test: 5.74146
Epoch 800: New minimal relative error: 15.16%, model saved.
Epoch: 800 Train: 10.53360 Test: 9.02796
Epoch: 900 Train: 11.55686 Test: 6.70225
Epoch: 1000 Train: 5.14161 Test: 2.77531
Epoch: 1100 Train: 16.09221 Test: 19.42450
Epoch: 1200 Train: 6.11134 Test: 6.59419
Epoch: 1300 Train: 8.84966 Test: 9.24261
Epoch: 1400 Train: 2.69526 Test: 2.84224
Epoch 1500: New minimal relative error: 10.71%, model saved.
Epoch: 1500 Train: 3.12408 Test: 4.75015
Epoch: 1600 Train: 3.63862 Test: 3.20030
Epoch: 1700 Train: 2.59766 Test: 3.19174
Epoch: 1800 Train: 1.20607 Test: 1.68721
Epoch: 1900 Train: 2.26219 Test: 2.27207
Epoch: 2000 Train: 3.52318 Test: 3.15057
Epoch: 2100 Train: 5.84090 Test: 5.14782
Epoch: 2200 Train: 1.89908 Test: 2.29190
Epoch: 2300 Train: 0.99243 Test: 0.87293
Epoch: 2400 Train: 2.24716 Test: 2.97157
Epoch: 2500 Train: 0.77399 Test: 1.40776
Epoch: 2600 Train: 0.65744 Test: 0.72229
Epoch: 2700 Train: 3.79901 Test: 4.18252
Epoch 2800: New minimal relative error: 9.67%, model saved.
Epoch: 2800 Train: 1.43824 Test: 1.89677
Epoch: 2900 Train: 1.43268 Test: 1.85564
Epoch: 3000 Train: 0.31976 Test: 0.72487
Epoch: 3100 Train: 1.96852 Test: 2.81937
Epoch: 3200 Train: 1.24997 Test: 2.02788
Epoch: 3300 Train: 2.01263 Test: 2.23676
Epoch: 3400 Train: 0.37895 Test: 0.85073
Epoch: 3500 Train: 0.83705 Test: 1.06010
Epoch: 3600 Train: 0.32588 Test: 0.65712
Epoch: 3700 Train: 0.95655 Test: 1.21837
Epoch: 3800 Train: 0.63090 Test: 0.68670
Epoch 3900: New minimal relative error: 8.47%, model saved.
Epoch: 3900 Train: 0.31483 Test: 0.61565
Epoch: 4000 Train: 0.19724 Test: 0.35445
Epoch: 4100 Train: 0.68460 Test: 0.85430
Epoch: 4200 Train: 1.04231 Test: 1.03252
Epoch: 4300 Train: 0.24375 Test: 0.40897
Epoch: 4400 Train: 0.17818 Test: 0.43797
Epoch: 4500 Train: 0.97530 Test: 1.26573
Epoch: 4600 Train: 0.51448 Test: 0.51449
Epoch: 4700 Train: 1.17120 Test: 1.13437
Epoch: 4800 Train: 1.23693 Test: 1.47742
Epoch: 4900 Train: 0.19020 Test: 0.41049
Epoch: 5000 Train: 0.46358 Test: 0.63600
Epoch: 5100 Train: 0.38883 Test: 0.60882
Epoch: 5200 Train: 0.88815 Test: 1.23011
Epoch: 5300 Train: 0.75835 Test: 1.02532
Epoch: 5400 Train: 0.59663 Test: 0.66547
Epoch: 5500 Train: 0.36870 Test: 0.58271
Epoch: 5600 Train: 0.42038 Test: 0.52105
Epoch: 5700 Train: 0.23079 Test: 0.58539
Epoch: 5800 Train: 0.22774 Test: 0.38584
Epoch: 5900 Train: 0.35425 Test: 0.59448
Epoch: 6000 Train: 0.68022 Test: 0.91884
Epoch: 6100 Train: 0.34667 Test: 0.56373
Epoch: 6200 Train: 0.27924 Test: 0.55862
Epoch: 6300 Train: 0.30500 Test: 0.50711
Epoch: 6400 Train: 0.69196 Test: 0.66224
Epoch: 6500 Train: 0.51829 Test: 0.55561
Epoch: 6600 Train: 0.17923 Test: 0.38491
Epoch: 6700 Train: 0.64457 Test: 0.49306
Epoch: 6800 Train: 0.35180 Test: 0.45873
Epoch: 6900 Train: 0.09422 Test: 0.23955
Epoch: 7000 Train: 0.06791 Test: 0.21857
Epoch: 7100 Train: 0.06951 Test: 0.20756
Epoch: 7200 Train: 0.09591 Test: 0.23588
Epoch: 7300 Train: 0.18928 Test: 0.36260
Epoch: 7400 Train: 0.07492 Test: 0.24133
Epoch: 7500 Train: 0.14780 Test: 0.30065
Epoch: 7600 Train: 0.03695 Test: 0.18926
Epoch: 7700 Train: 0.21283 Test: 0.37177
Epoch: 7800 Train: 0.23742 Test: 0.30939
Epoch: 7900 Train: 0.96047 Test: 1.06260
Epoch: 8000 Train: 0.04924 Test: 0.17770
Epoch: 8100 Train: 0.02149 Test: 0.15343
Epoch: 8200 Train: 0.10714 Test: 0.24766
Epoch: 8300 Train: 0.08883 Test: 0.23342
Epoch: 8400 Train: 0.10436 Test: 0.23283
Epoch: 8500 Train: 0.41609 Test: 0.61668
Epoch: 8600 Train: 0.13603 Test: 0.24476
Epoch: 8700 Train: 0.03823 Test: 0.15867
Epoch: 8800 Train: 1.16850 Test: 1.04702
Epoch: 8900 Train: 0.02715 Test: 0.16469
Epoch: 9000 Train: 0.04874 Test: 0.18819
Epoch: 9100 Train: 0.17056 Test: 0.29570
Epoch: 9200 Train: 0.13284 Test: 0.25531
Epoch: 9300 Train: 0.04184 Test: 0.16758
Epoch: 9400 Train: 0.02710 Test: 0.14496
Epoch: 9500 Train: 0.02567 Test: 0.14446
Epoch: 9600 Train: 0.02479 Test: 0.14709
Epoch: 9700 Train: 0.40300 Test: 0.40908
Epoch: 9800 Train: 0.03744 Test: 0.18683
Epoch: 9900 Train: 0.04795 Test: 0.16555
Epoch 9999: New minimal relative error: 8.11%, model saved.
Epoch: 9999 Train: 0.10857 Test: 0.24290
Training Loss: tensor(0.1086)
Test Loss: tensor(0.2429)
Learned LE: [ 0.81680137  0.05371032 -4.1876535 ]
True LE: [ 8.6332804e-01  5.3717817e-05 -1.4540423e+01]
Relative Error: [2.008761   1.5810075  1.2115047  0.70496213 0.32330108 0.30550802
 0.38666925 0.47952598 0.43100208 0.48438346 0.7794247  0.8686311
 1.0449067  1.1147854  1.1519761  1.2345482  1.4035298  1.2167264
 1.3323405  1.4868411  1.8103597  2.2112179  1.8780738  1.4419065
 1.227602   0.9460557  0.5599168  0.18070507 0.24332082 0.4248408
 0.38580218 0.240006   0.09224106 0.17751968 0.35846072 0.56359565
 0.78974015 1.42296    1.9476649  1.6156219  0.72650623 0.69101954
 0.5968108  0.6825279  0.9340684  1.620496   2.380665   2.4854612
 2.2319424  1.7317859  1.564958   1.8657109  2.0208735  1.9943831
 1.994473   2.3209405  3.05249    3.540666   3.6176329  3.327094
 3.0310678  2.5004869  2.1715474  1.6552646  1.3135713  1.0427932
 0.61252755 0.43422627 0.29903993 0.28431314 0.2793782  0.31669378
 0.56657237 0.7121102  0.75390905 0.9094011  1.0015157  0.8701232
 0.8036555  0.8135209  0.8541894  1.1049373  1.2601094  1.6255459
 1.6103163  1.1874074  0.9009328  0.5928656  0.42319307 0.42547867
 0.5381396  0.5837912  0.57478225 0.5674968  0.47510165 0.40099263
 0.29051372 0.21060911 0.48497096 0.76551604 1.1949089  1.0189464
 0.42014754 0.34301844 0.45119515 0.75691533 0.8885215  1.5120844
 2.4807277  2.7769952  2.5938797  1.8848217  1.6195695  1.5217193
 1.7249888  1.8875333  2.0040128  2.026648   2.6165092  3.0451374
 3.2808197  3.0313518  2.7008286  2.3792696  2.0519693  1.71799
 1.319322   1.2008183  0.98115325 0.6478046  0.5210943  0.3439996
 0.16662024 0.2289065  0.34129095 0.73500466 0.56964105 0.5253074
 0.78732085 0.6818434  0.33535582 0.34772834 0.51143736 0.6037892
 0.8522285  1.01261    1.154662   0.99224776 0.66648227 0.4619933
 0.5115935  0.5774559  0.75312823 0.629238   0.5855778  0.67939067
 0.68624854 0.84612465 0.82341236 0.43613607 0.15528576 0.33947393
 0.49478775 0.42890766 0.09632056 0.20151068 0.43793666 0.79275715
 1.1342586  1.4128393  2.1328228  2.7941003  2.666948   2.1878216
 1.809584   1.6233842  1.3976452  1.7100421  1.9181248  1.934346
 2.2711842  2.485213   2.6893108  2.6523023  2.4017751  2.1275065
 1.7422507  1.5381721  1.3271167  1.2612514  1.2037381  1.0165352
 0.71321034 0.6491098  0.398184   0.07378296 0.20304836 0.43490714
 0.668185   0.420449   0.22657894 0.46073976 0.2430105  0.37834513
 0.4057168  0.32166246 0.29147515 0.5261151  0.6046315  0.5991326
 0.43661842 0.6232857  0.60077935 0.7315843  0.80879974 0.7159887
 0.7324601  0.8947318  0.7694937  0.88476485 0.9589835  0.8487369
 0.51517797 0.313935   0.24856487 0.2591116  0.26505962 0.546199
 0.5487786  0.86269325 1.3636668  1.3414942  1.6734855  2.3478804
 2.5444102  2.3418336  2.1302662  1.8241564  1.4042646  1.6383157
 1.866344   1.7700732  1.8803991  2.0610604  2.1140256  2.1311545
 2.084428   1.7884182  1.6445805  1.3868994  1.2245646  1.1347936
 1.2379978  1.117229   1.0243895  0.7411892  0.7444469  0.4297793
 0.22386806 0.34882623 0.6859951  0.6417763  0.41412878 0.06291556
 0.12474822 0.36010525 0.6966851  0.60246724 0.53094244 0.42270225
 0.360708   0.35283113 0.3539103  0.7437556  0.8661075  0.82813585
 0.75118077 0.93014306 0.94233334 1.0676458  1.0570337  0.98102075
 1.1389081  1.0919181  0.93938726 0.74075544 0.6686109  0.7342561
 0.6820293  0.7261981  0.7558429  0.9622259  1.1675673  1.600453
 1.4932584  1.6509514  2.1125503  2.056857   2.0279484  1.8061402
 1.4065453  1.3877839  1.623833   1.776217   1.5331644  1.5935538
 1.5916486  1.6401758  1.6061723  1.4983165  1.4118576  1.2754327
 1.152697   1.0383736  0.96160436 1.077425   1.0882825  1.0227098
 0.80438006 0.8136346  0.5312125  0.37601793 0.456659   0.8255138
 0.67392385 0.45799002 0.2689616  0.5080587  0.88316566 0.9719373
 0.9453169  0.843894   0.84521985 0.81256413 0.8614532  0.8504449
 1.1839274  1.0604059  0.8940869  1.005176   0.9695212  1.0572137
 1.1153896  1.1161888  1.1382493  1.1347002  1.12152    1.1352295
 0.9834162  0.9420554  0.9441429  0.95815265 1.0251116  0.9824289
 1.0098838  1.1665599  1.330249   1.3533785  1.5883662  1.5472327
 1.4572805  1.5346278  1.3672856  0.99094075 1.2581217  1.5478415
 1.3993318  1.2483053  1.2258514  1.162186   1.2558807  1.1535932
 0.99807453 1.0240881  0.98913515 0.9206664  0.8948041  0.9199889
 0.9224271  1.0103451  0.94881153 0.7688357  0.8116125  0.57917655
 0.44922373 0.48651448 0.8490013  0.7521126  0.66371113 0.5948672
 0.9083344  1.1945517  1.1229458  1.1116142  0.9842594  1.0558207
 1.1181065  1.1953896  1.0473119  1.1477034 ]
