time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 1000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 103.75%, model saved.
Epoch: 0 Train: 3987.26245 Test: 4160.72754
Epoch 100: New minimal relative error: 47.07%, model saved.
Epoch: 100 Train: 40.10990 Test: 31.22549
Epoch 200: New minimal relative error: 19.44%, model saved.
Epoch: 200 Train: 9.99670 Test: 8.29301
Epoch: 300 Train: 6.09413 Test: 4.37572
Epoch 400: New minimal relative error: 14.69%, model saved.
Epoch: 400 Train: 4.71735 Test: 3.65856
Epoch: 500 Train: 15.67369 Test: 21.37029
Epoch: 600 Train: 13.09507 Test: 13.29158
Epoch: 700 Train: 6.79064 Test: 4.15617
Epoch: 800 Train: 6.02233 Test: 6.25065
Epoch 900: New minimal relative error: 14.01%, model saved.
Epoch: 900 Train: 2.02421 Test: 1.89035
Epoch: 1000 Train: 3.21135 Test: 2.37600
Epoch 1100: New minimal relative error: 10.25%, model saved.
Epoch: 1100 Train: 1.36614 Test: 0.87852
Epoch: 1200 Train: 1.55892 Test: 1.19865
Epoch: 1300 Train: 1.34982 Test: 1.09546
Epoch: 1400 Train: 1.11964 Test: 0.81210
Epoch: 1500 Train: 2.77106 Test: 3.43949
Epoch: 1600 Train: 1.38412 Test: 1.21369
Epoch: 1700 Train: 2.53607 Test: 3.29720
Epoch 1800: New minimal relative error: 9.58%, model saved.
Epoch: 1800 Train: 1.32573 Test: 1.34199
Epoch: 1900 Train: 1.76021 Test: 2.28640
Epoch: 2000 Train: 7.29197 Test: 4.87771
Epoch: 2100 Train: 1.46333 Test: 1.26118
Epoch 2200: New minimal relative error: 8.87%, model saved.
Epoch: 2200 Train: 1.30755 Test: 0.82205
Epoch: 2300 Train: 0.48924 Test: 0.31414
Epoch 2400: New minimal relative error: 8.81%, model saved.
Epoch: 2400 Train: 0.36056 Test: 0.27262
Epoch: 2500 Train: 1.93335 Test: 1.88168
Epoch: 2600 Train: 0.72725 Test: 0.61650
Epoch: 2700 Train: 0.81898 Test: 0.73446
Epoch: 2800 Train: 1.57646 Test: 1.55189
Epoch 2900: New minimal relative error: 5.99%, model saved.
Epoch: 2900 Train: 0.31412 Test: 0.29108
Epoch: 3000 Train: 1.07674 Test: 1.02306
Epoch: 3100 Train: 0.20427 Test: 0.13545
Epoch: 3200 Train: 1.07085 Test: 1.33822
Epoch: 3300 Train: 0.26098 Test: 0.21564
Epoch: 3400 Train: 0.27918 Test: 0.25147
Epoch: 3500 Train: 1.58849 Test: 1.95050
Epoch: 3600 Train: 2.05637 Test: 2.39954
Epoch 3700: New minimal relative error: 5.31%, model saved.
Epoch: 3700 Train: 0.28049 Test: 0.17808
Epoch: 3800 Train: 0.78228 Test: 0.95224
Epoch 3900: New minimal relative error: 5.18%, model saved.
Epoch: 3900 Train: 0.18064 Test: 0.16510
Epoch: 4000 Train: 0.63636 Test: 0.57708
Epoch: 4100 Train: 0.17967 Test: 0.18208
Epoch: 4200 Train: 0.32876 Test: 0.28104
Epoch: 4300 Train: 0.24607 Test: 0.19320
Epoch: 4400 Train: 0.12816 Test: 0.09589
Epoch: 4500 Train: 0.39234 Test: 0.46305
Epoch: 4600 Train: 0.31472 Test: 0.37897
Epoch: 4700 Train: 0.45002 Test: 0.30729
Epoch: 4800 Train: 0.28937 Test: 0.32603
Epoch: 4900 Train: 0.21173 Test: 0.08810
Epoch: 5000 Train: 2.72739 Test: 1.60641
Epoch: 5100 Train: 0.65252 Test: 0.70655
Epoch: 5200 Train: 0.10204 Test: 0.10306
Epoch: 5300 Train: 1.08444 Test: 1.00831
Epoch: 5400 Train: 0.35493 Test: 0.19357
Epoch 5500: New minimal relative error: 3.04%, model saved.
Epoch: 5500 Train: 0.10736 Test: 0.10968
Epoch 5600: New minimal relative error: 2.74%, model saved.
Epoch: 5600 Train: 0.14782 Test: 0.13215
Epoch: 5700 Train: 1.87926 Test: 1.95328
Epoch: 5800 Train: 0.49194 Test: 0.60418
Epoch: 5900 Train: 0.09170 Test: 0.07659
Epoch: 6000 Train: 0.09294 Test: 0.10627
Epoch: 6100 Train: 0.20757 Test: 0.20638
Epoch: 6200 Train: 0.20875 Test: 0.14628
Epoch: 6300 Train: 0.25235 Test: 0.22343
Epoch: 6400 Train: 0.07921 Test: 0.10766
Epoch: 6500 Train: 0.11392 Test: 0.09216
Epoch: 6600 Train: 0.26330 Test: 0.18350
Epoch: 6700 Train: 1.03200 Test: 1.47773
Epoch: 6800 Train: 0.06908 Test: 0.06375
Epoch: 6900 Train: 0.10737 Test: 0.11227
Epoch: 7000 Train: 0.14568 Test: 0.15605
Epoch: 7100 Train: 0.54341 Test: 0.68098
Epoch: 7200 Train: 0.33889 Test: 0.17644
Epoch: 7300 Train: 0.42748 Test: 0.54724
Epoch: 7400 Train: 0.05887 Test: 0.05294
Epoch: 7500 Train: 0.94728 Test: 1.17190
Epoch: 7600 Train: 0.05540 Test: 0.04953
Epoch 7700: New minimal relative error: 2.11%, model saved.
Epoch: 7700 Train: 0.05489 Test: 0.05038
Epoch: 7800 Train: 0.08049 Test: 0.05091
Epoch: 7900 Train: 0.05199 Test: 0.04732
Epoch: 8000 Train: 0.05146 Test: 0.04763
Epoch: 8100 Train: 0.05049 Test: 0.04628
Epoch: 8200 Train: 0.05425 Test: 0.05109
Epoch: 8300 Train: 0.06277 Test: 0.07127
Epoch: 8400 Train: 0.05572 Test: 0.04576
Epoch: 8500 Train: 0.09844 Test: 0.12059
Epoch: 8600 Train: 0.04741 Test: 0.04677
Epoch: 8700 Train: 0.04601 Test: 0.04421
Epoch: 8800 Train: 0.04594 Test: 0.04464
Epoch: 8900 Train: 0.04388 Test: 0.04203
Epoch: 9000 Train: 0.05552 Test: 0.05862
Epoch: 9100 Train: 0.25755 Test: 0.24537
Epoch: 9200 Train: 0.07389 Test: 0.11738
Epoch: 9300 Train: 0.04163 Test: 0.04572
Epoch: 9400 Train: 0.07610 Test: 0.09334
Epoch: 9500 Train: 0.03998 Test: 0.03957
Epoch: 9600 Train: 0.04209 Test: 0.04213
Epoch: 9700 Train: 0.04700 Test: 0.04494
Epoch: 9800 Train: 0.04040 Test: 0.04011
Epoch: 9900 Train: 0.04038 Test: 0.04153
Epoch: 9999 Train: 0.05647 Test: 0.04327
Training Loss: tensor(0.0565)
Test Loss: tensor(0.0433)
Learned LE: [ 8.6946136e-01 -4.7454983e-03 -4.8188715e+00]
True LE: [ 8.6714894e-01 -8.2146889e-03 -1.4529756e+01]
Relative Error: [2.3251019  2.1619103  1.9224131  1.7581618  1.6804675  1.8944793
 2.1737263  1.7604733  1.1522342  0.93915284 1.449751   1.9530843
 2.3368871  3.0696328  3.8440242  4.337958   4.932951   5.2557874
 5.1390543  4.9292316  4.5696073  4.1561017  3.7119348  3.473426
 3.4159303  3.2817972  3.3033345  3.040178   2.4732025  2.614598
 2.4329858  2.4152663  2.5811377  2.1445851  1.6098337  1.4926883
 1.3302904  1.2841684  1.4190966  1.3456136  1.2619363  1.3644152
 1.4743071  1.3193622  0.9950504  0.8452508  0.87468743 0.9895696
 1.1268791  1.2030572  1.1566603  1.0868343  1.0183947  0.95974535
 0.9014601  0.83227766 0.7448417  0.81651014 0.95811796 1.4021441
 1.5733786  1.9004343  2.023858   1.9218187  1.665124   1.4917277
 1.561407   1.7491354  1.8549144  1.5703334  1.0499407  0.8339208
 1.6408632  2.095191   2.46439    3.230193   4.0276074  4.4552827
 4.8073726  4.8871474  5.0962453  4.9876804  4.4899793  4.0026464
 3.462243   3.238019   3.183084   2.9602933  2.891636   2.8277807
 2.2221184  2.1579611  2.1666012  2.1103034  2.2965858  1.9972954
 1.5455904  1.2477962  1.1026455  1.0355107  1.1325047  1.0659001
 0.98179495 1.0793839  1.1279863  0.9459264  0.7052132  0.5194802
 0.57519335 0.7594811  0.912312   0.98349315 0.96135885 0.88083285
 0.7976595  0.77049404 0.7197394  0.60311204 0.46134204 0.49764854
 0.5279832  0.9501893  1.1491029  1.4199581  1.7945176  1.7656758
 1.451865   1.3070912  1.458731   1.6826613  1.6189029  1.346742
 0.9126425  0.89824593 1.6774257  2.2289717  2.529523   3.3051147
 4.0951     4.483857   4.601418   4.6600704  4.5809536  4.4224744
 4.4020033  3.832351   3.350575   2.9298024  2.8615327  2.6672568
 2.4075997  2.4273107  2.0254421  1.7148607  1.7558773  1.710854
 1.9474146  1.8119382  1.4660072  1.0371535  0.8407212  0.787076
 0.83236295 0.77565837 0.683913   0.74884385 0.734798   0.60119116
 0.42836213 0.30806395 0.2900917  0.5256271  0.7113919  0.79054403
 0.7918914  0.7391393  0.62472105 0.5880502  0.54004663 0.45308828
 0.35581338 0.30012298 0.2906027  0.45161062 0.71177506 0.89684457
 1.4145133  1.6699181  1.3900629  1.1904838  1.3289949  1.5828927
 1.5048379  1.0477687  0.72425485 0.8868716  1.5940927  2.1777153
 2.298977   2.9357846  3.3467963  3.633191   3.7810133  3.9910898
 4.101094   3.8157985  3.431885   3.364635   3.0146139  2.6625679
 2.3550572  2.3795238  1.9996673  1.7949375  1.7252138  1.2172842
 1.2715998  1.2838118  1.5210377  1.513438   1.2892934  0.85688585
 0.5721117  0.54935974 0.5550422  0.486231   0.36968732 0.40764385
 0.3418146  0.30072954 0.17973214 0.31422147 0.254734   0.33639717
 0.5145517  0.6188624  0.66811734 0.6530067  0.56112474 0.45582446
 0.3921588  0.28132173 0.23157476 0.23298757 0.19406292 0.18018559
 0.17785741 0.3423304  0.7969586  1.2694424  1.3567019  1.0539947
 1.1835489  1.5235317  1.5538956  0.9538702  0.6463801  0.8668803
 1.3829114  1.6757383  1.9740783  2.4734373  2.7097385  2.6769145
 2.520148   2.7025125  2.8200595  2.7500215  2.5980806  2.2622933
 2.2339144  2.0478911  1.7657559  1.7836941  1.6971585  1.264176
 1.1369212  0.8620736  0.8107053  0.8100485  0.9066388  1.1156123
 0.99769115 0.7591709  0.4111699  0.21366377 0.39918363 0.31478664
 0.25197455 0.18934457 0.12716283 0.06972893 0.14524978 0.27484336
 0.2684816  0.23904206 0.34726417 0.48783854 0.5593624  0.5548423
 0.5266     0.40286765 0.25934103 0.14822447 0.10663588 0.12529287
 0.2744089  0.345246   0.26541936 0.09227614 0.23878528 0.71816176
 1.1219493  0.9854005  1.0011854  1.4813223  1.5533773  1.0306293
 0.735729   0.91789347 1.2027129  1.4221349  2.0245576  2.059832
 2.1641846  2.0107872  1.7260251  1.6528044  1.5207702  1.5238569
 1.5631577  1.4671806  1.2147021  1.1965142  1.1327745  0.92681813
 0.9987849  1.0099571  0.6114847  0.6198668  0.29761034 0.24430966
 0.32728136 0.6163657  0.6327827  0.5703348  0.27161214 0.20487122
 0.282059   0.34925655 0.26108018 0.21712813 0.20643356 0.16038382
 0.2795471  0.36816475 0.39279318 0.35468945 0.24037369 0.30381516
 0.40833637 0.46487758 0.48405585 0.33144647 0.1695374  0.13697797
 0.23071349 0.16935678 0.1831094  0.3751999  0.44622076 0.50105757
 0.46568283 0.34989268 0.6505135  0.901842   0.75781214 1.122991
 1.4139122  1.159457   0.6617625  1.1090928  1.3341565  1.8361472
 2.0243883  1.7743206  1.4698575  1.3761     1.2259156  0.9227004
 0.8601094  0.7058218  0.4404969  0.59247494 0.6042261  0.52985275
 0.3759837  0.54853785 0.5603674  0.5120089 ]
