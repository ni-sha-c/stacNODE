time_step: 0.01
lr: 0.001
weight_decay: 0.001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 102.04%, model saved.
Epoch: 0 Train: 3500.07446 Test: 3955.40210
Epoch 100: New minimal relative error: 49.06%, model saved.
Epoch: 100 Train: 170.22681 Test: 161.09224
Epoch 200: New minimal relative error: 28.27%, model saved.
Epoch: 200 Train: 26.87198 Test: 30.52448
Epoch 300: New minimal relative error: 20.71%, model saved.
Epoch: 300 Train: 11.16411 Test: 10.94235
Epoch 400: New minimal relative error: 12.30%, model saved.
Epoch: 400 Train: 10.37085 Test: 9.01011
Epoch 500: New minimal relative error: 11.30%, model saved.
Epoch: 500 Train: 6.23113 Test: 5.74817
Epoch: 600 Train: 14.69792 Test: 14.68354
Epoch: 700 Train: 46.60767 Test: 59.05145
Epoch 800: New minimal relative error: 10.13%, model saved.
Epoch: 800 Train: 4.10624 Test: 3.83084
Epoch: 900 Train: 5.51954 Test: 6.30818
Epoch 1000: New minimal relative error: 9.23%, model saved.
Epoch: 1000 Train: 3.84652 Test: 3.22026
Epoch: 1100 Train: 6.02569 Test: 6.79800
Epoch: 1200 Train: 2.70648 Test: 2.58794
Epoch: 1300 Train: 15.89872 Test: 17.25411
Epoch: 1400 Train: 1.60334 Test: 1.96422
Epoch 1500: New minimal relative error: 7.78%, model saved.
Epoch: 1500 Train: 1.41841 Test: 1.42969
Epoch: 1600 Train: 5.59372 Test: 4.27599
Epoch: 1700 Train: 2.35627 Test: 2.58979
Epoch: 1800 Train: 2.48279 Test: 2.45697
Epoch: 1900 Train: 0.81264 Test: 0.92640
Epoch: 2000 Train: 0.84001 Test: 0.85401
Epoch: 2100 Train: 1.04195 Test: 0.90138
Epoch: 2200 Train: 2.05920 Test: 2.07373
Epoch 2300: New minimal relative error: 7.56%, model saved.
Epoch: 2300 Train: 0.63650 Test: 0.62605
Epoch: 2400 Train: 0.97977 Test: 1.10323
Epoch: 2500 Train: 0.87873 Test: 0.84814
Epoch: 2600 Train: 1.42998 Test: 1.69715
Epoch: 2700 Train: 0.93460 Test: 0.98984
Epoch: 2800 Train: 0.86717 Test: 0.89369
Epoch: 2900 Train: 2.44923 Test: 2.78010
Epoch: 3000 Train: 0.58868 Test: 0.66574
Epoch: 3100 Train: 0.43632 Test: 0.41431
Epoch: 3200 Train: 1.68628 Test: 1.49976
Epoch: 3300 Train: 2.19017 Test: 2.33435
Epoch: 3400 Train: 0.43669 Test: 0.42171
Epoch: 3500 Train: 0.42769 Test: 0.39224
Epoch 3600: New minimal relative error: 7.34%, model saved.
Epoch: 3600 Train: 0.31824 Test: 0.30439
Epoch: 3700 Train: 0.35120 Test: 0.35068
Epoch 3800: New minimal relative error: 5.91%, model saved.
Epoch: 3800 Train: 2.07787 Test: 1.60817
Epoch: 3900 Train: 1.37392 Test: 1.62454
Epoch: 4000 Train: 1.87089 Test: 2.36076
Epoch: 4100 Train: 0.78172 Test: 0.52037
Epoch: 4200 Train: 0.84798 Test: 0.93389
Epoch: 4300 Train: 0.34309 Test: 0.33653
Epoch: 4400 Train: 0.46892 Test: 0.35604
Epoch: 4500 Train: 0.33851 Test: 0.35401
Epoch: 4600 Train: 0.91155 Test: 1.01255
Epoch: 4700 Train: 0.27440 Test: 0.26251
Epoch: 4800 Train: 0.23133 Test: 0.23805
Epoch: 4900 Train: 0.68541 Test: 0.64844
Epoch: 5000 Train: 0.45285 Test: 0.44141
Epoch 5100: New minimal relative error: 5.23%, model saved.
Epoch: 5100 Train: 0.40904 Test: 0.37058
Epoch: 5200 Train: 0.29932 Test: 0.31934
Epoch: 5300 Train: 0.23333 Test: 0.24427
Epoch: 5400 Train: 0.21367 Test: 0.22200
Epoch: 5500 Train: 0.33678 Test: 0.39626
Epoch: 5600 Train: 0.20394 Test: 0.21597
Epoch: 5700 Train: 0.24994 Test: 0.26963
Epoch: 5800 Train: 1.00895 Test: 1.11449
Epoch: 5900 Train: 0.44425 Test: 0.47600
Epoch: 6000 Train: 0.22636 Test: 0.22158
Epoch: 6100 Train: 0.20954 Test: 0.22330
Epoch: 6200 Train: 0.17775 Test: 0.18803
Epoch 6300: New minimal relative error: 3.89%, model saved.
Epoch: 6300 Train: 0.19407 Test: 0.21381
Epoch: 6400 Train: 0.19667 Test: 0.22162
Epoch: 6500 Train: 0.20550 Test: 0.22406
Epoch: 6600 Train: 0.42312 Test: 0.43006
Epoch: 6700 Train: 0.77965 Test: 0.85809
Epoch: 6800 Train: 0.23537 Test: 0.23413
Epoch: 6900 Train: 0.17159 Test: 0.18513
Epoch: 7000 Train: 0.35467 Test: 0.42094
Epoch: 7100 Train: 0.41348 Test: 0.52962
Epoch: 7200 Train: 0.15192 Test: 0.16439
Epoch: 7300 Train: 0.14799 Test: 0.16181
Epoch: 7400 Train: 0.14957 Test: 0.16193
Epoch: 7500 Train: 0.14613 Test: 0.15877
Epoch: 7600 Train: 0.15202 Test: 0.16637
Epoch: 7700 Train: 0.17544 Test: 0.17960
Epoch: 7800 Train: 0.17167 Test: 0.20291
Epoch: 7900 Train: 0.13547 Test: 0.15005
Epoch: 8000 Train: 0.14424 Test: 0.15746
Epoch: 8100 Train: 0.13466 Test: 0.14880
Epoch: 8200 Train: 0.13033 Test: 0.14444
Epoch: 8300 Train: 0.13269 Test: 0.14332
Epoch: 8400 Train: 0.13729 Test: 0.15167
Epoch: 8500 Train: 0.12570 Test: 0.14144
Epoch: 8600 Train: 0.27853 Test: 0.18787
Epoch: 8700 Train: 0.57234 Test: 0.59648
Epoch: 8800 Train: 0.21245 Test: 0.21037
Epoch: 8900 Train: 0.21686 Test: 0.17095
Epoch: 9000 Train: 0.43651 Test: 0.46921
Epoch: 9100 Train: 0.12115 Test: 0.14283
Epoch: 9200 Train: 0.12228 Test: 0.13619
Epoch: 9300 Train: 0.11462 Test: 0.13023
Epoch: 9400 Train: 0.11507 Test: 0.12928
Epoch: 9500 Train: 0.11603 Test: 0.12949
Epoch: 9600 Train: 0.11101 Test: 0.12718
Epoch: 9700 Train: 0.11070 Test: 0.12749
Epoch: 9800 Train: 0.10872 Test: 0.12479
Epoch: 9900 Train: 0.10766 Test: 0.12343
Epoch: 9999 Train: 0.17467 Test: 0.18616
Training Loss: tensor(0.1747)
Test Loss: tensor(0.1862)
Learned LE: [ 0.76852113 -0.0439725  -3.2305603 ]
True LE: [ 8.5599059e-01  5.2578450e-04 -1.4529522e+01]
Relative Error: [3.837955   4.2057967  4.047357   4.2054276  4.1502886  4.046622
 4.2367115  3.6895156  2.9641461  2.0791364  1.5298501  1.5850276
 1.9088182  2.1425605  2.3257284  2.5834906  2.6319594  2.4630973
 2.1412418  1.7537501  1.5108864  1.4233464  1.2978344  1.1031489
 1.0641297  1.4145806  1.5548078  1.624343   1.897293   2.1548786
 2.1450884  2.1450443  2.383424   2.5352178  3.2602673  3.7258058
 3.2247648  2.5887153  1.9831996  1.4994074  1.1723099  1.1393535
 1.4271764  1.5541424  1.561713   1.586322   1.9328026  1.9988213
 1.6085367  1.3087063  1.0475358  0.8547063  0.72380275 0.72998464
 0.86761236 0.95755106 1.0522908  1.1470082  1.3203298  1.4726228
 1.8245794  2.592993   3.150193   3.475607   3.4412298  3.3801305
 3.654403   3.426956   3.4249227  3.2707672  2.9412677  2.181439
 1.2299093  2.1492476  3.0087745  2.4651754  1.9598498  1.9261222
 2.0934713  2.3487387  2.3767474  2.0730593  1.5542163  1.2206439
 1.0647764  0.9517277  0.78287756 0.6603341  1.5296763  1.2317113
 1.4468676  1.8507074  1.7598572  1.7249352  1.9528621  2.0869777
 2.6263895  3.1852736  2.7494562  2.2168803  1.4477158  1.2052028
 1.0335549  0.95988077 1.1527945  1.3096962  1.4092206  1.3452426
 1.4136988  1.4623725  1.3029904  1.0699189  0.896286   0.8027673
 0.77281743 0.6230325  0.46860123 0.5132833  0.64240897 0.8176148
 0.9563484  1.0931958  1.3224719  1.9032624  2.652431   2.8119538
 3.0155864  2.6885657  2.7014244  2.903023   2.7653441  2.6877437
 2.794491   2.2364419  1.0793662  3.0233514  2.6372046  2.2946854
 1.7892199  1.5792916  1.712604   1.6673423  1.695369   2.001707
 1.9356357  1.5136473  0.98008716 0.7346513  0.595669   0.442643
 0.4793306  0.98617256 1.1075575  1.3084567  1.5130181  1.3911756
 1.4062515  1.3133619  1.943666   2.618989   2.329917   1.8378093
 1.2420869  0.9460791  0.8281204  0.81388533 0.8992558  1.0490159
 1.179441   0.9051078  0.89401424 0.9975493  0.9912501  0.76564723
 0.68433267 0.6729627  0.729494   0.8157798  0.7821044  0.5881065
 0.40540546 0.52299446 0.73667186 0.8606801  1.0588408  1.3076932
 2.0336258  2.4556777  2.4774232  2.5118792  1.9663029  2.1107762
 2.2430446  2.275417   2.4311879  1.7809318  1.6207639  2.471036
 2.2246902  1.8183559  1.3776522  1.5132573  1.8995653  1.9013051
 1.4343601  1.1506522  1.1001388  1.3877776  1.513835   0.81047416
 0.49641672 0.46582463 0.28875595 0.519786   0.5776392  0.9999821
 1.1515183  1.1129363  1.0604217  1.0852301  0.9671821  1.598424
 2.0595942  1.6082246  1.2479025  0.61494166 0.58244973 0.7160129
 0.8276511  0.86239076 0.8125755  0.67120314 0.57747424 0.63482463
 0.633337   0.5529928  0.3268926  0.4329932  0.64036614 0.84973073
 1.0934     1.2073067  1.1882205  0.9732452  0.9840035  1.0755284
 1.0800227  1.2824215  1.48941    2.1731715  2.3577967  2.227999
 2.0481682  1.3204287  1.6577227  1.7492553  2.0701404  1.4027536
 1.7098287  1.8453313  1.2090571  0.9689443  1.0256114  1.4420047
 1.7322166  2.0340278  1.5348666  1.1680251  0.84003705 0.5556341
 0.7245653  0.69240016 0.85296285 0.5593021  0.5153851  0.3067228
 0.54297775 0.44553912 0.8680216  0.9758318  0.7694142  0.65100574
 0.7110165  0.67332554 1.0913042  1.3805659  1.2518953  0.76209337
 0.49068603 0.60744643 0.81738895 0.90610504 0.9332867  0.7810336
 0.82912284 0.7997978  0.6168936  0.56819457 0.44287872 0.24960227
 0.24024282 0.62049377 1.0770378  1.5406021  1.8462175  1.9764421
 1.8119487  1.604828   1.5958402  1.5669061  1.62973    1.884238
 2.369353   2.4356062  2.175267   1.7770251  1.039265   1.3182462
 1.6788753  1.6118155  1.0792589  2.1387157  0.96145934 0.48019812
 0.69094706 1.366624   1.3144103  1.5245533  1.6971158  1.2054795
 0.92315066 0.8242938  0.49731863 0.11638684 0.40840152 0.31815964
 0.79802644 0.6956325  0.45310068 0.37501475 0.45142388 0.56736267
 0.8617592  0.623962   0.39318717 0.63204837 0.65717393 0.64351773
 0.8969312  1.0829809  1.164874   0.7598112  0.65664166 0.87143433
 0.94745123 0.93135285 0.84068376 0.99367213 0.8822835  0.8173267
 0.6152561  0.3991965  0.15270782 0.28182915 0.7067685  1.1778297
 1.8008357  2.266684   2.3609362  2.0635636  1.5405124  1.2277559
 1.0932174  1.1658905  1.4987395  2.1949823  2.5718737  2.4438176
 1.9231325  1.1884139  1.4602054  1.5588863  0.7549987  2.457265
 2.0084405  0.5406267  0.45494348 0.60389477 1.068675   0.9023176
 1.0908082  1.1351154  0.8711169  0.7316656  0.76258135 0.6221788
 0.5791859  0.47331303 0.4115888  0.41940528]
