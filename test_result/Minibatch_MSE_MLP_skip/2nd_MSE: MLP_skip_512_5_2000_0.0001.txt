time_step: 0.01
lr: 0.001
weight_decay: 0.0001
num_epoch: 10000
num_train: 10000
num_test: 8000
num_trans: 1000
batch_size: 2000
loss_type: MSE
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 110.97%, model saved.
Epoch: 0 Train: 4170.56299 Test: 4279.91797
Epoch 100: New minimal relative error: 47.53%, model saved.
Epoch: 100 Train: 44.09165 Test: 46.75490
Epoch 200: New minimal relative error: 35.35%, model saved.
Epoch: 200 Train: 27.34374 Test: 29.01877
Epoch 300: New minimal relative error: 22.61%, model saved.
Epoch: 300 Train: 7.38691 Test: 8.30092
Epoch: 400 Train: 5.27772 Test: 6.32802
Epoch: 500 Train: 10.15621 Test: 10.00019
Epoch: 600 Train: 3.59712 Test: 3.97120
Epoch: 700 Train: 7.43397 Test: 8.16248
Epoch 800: New minimal relative error: 11.38%, model saved.
Epoch: 800 Train: 3.94541 Test: 3.65239
Epoch: 900 Train: 8.58978 Test: 7.92030
Epoch: 1000 Train: 2.14480 Test: 2.36748
Epoch: 1100 Train: 3.11069 Test: 3.90063
Epoch: 1200 Train: 6.45768 Test: 7.31925
Epoch: 1300 Train: 1.26475 Test: 1.47415
Epoch: 1400 Train: 4.01810 Test: 4.90381
Epoch: 1500 Train: 0.73311 Test: 1.53785
Epoch: 1600 Train: 1.42748 Test: 1.94450
Epoch: 1700 Train: 11.67417 Test: 5.36214
Epoch: 1800 Train: 0.47930 Test: 0.58913
Epoch: 1900 Train: 0.85444 Test: 1.30310
Epoch: 2000 Train: 2.27057 Test: 2.56514
Epoch: 2100 Train: 2.63918 Test: 3.40876
Epoch: 2200 Train: 0.50839 Test: 1.03018
Epoch: 2300 Train: 1.10884 Test: 0.93274
Epoch: 2400 Train: 0.90289 Test: 1.17671
Epoch 2500: New minimal relative error: 9.50%, model saved.
Epoch: 2500 Train: 5.37305 Test: 4.59562
Epoch: 2600 Train: 0.61310 Test: 0.72974
Epoch: 2700 Train: 0.54160 Test: 0.46007
Epoch: 2800 Train: 1.51680 Test: 1.31861
Epoch: 2900 Train: 0.88919 Test: 1.08152
Epoch: 3000 Train: 1.66636 Test: 1.63088
Epoch: 3100 Train: 0.65972 Test: 0.93460
Epoch: 3200 Train: 0.40420 Test: 0.47985
Epoch: 3300 Train: 0.28682 Test: 0.29897
Epoch: 3400 Train: 0.40164 Test: 0.31873
Epoch: 3500 Train: 0.66600 Test: 0.86695
Epoch: 3600 Train: 0.87782 Test: 0.72430
Epoch: 3700 Train: 0.31513 Test: 0.34728
Epoch: 3800 Train: 0.98648 Test: 1.69868
Epoch: 3900 Train: 1.04641 Test: 0.56286
Epoch: 4000 Train: 0.36997 Test: 0.30364
Epoch: 4100 Train: 0.31428 Test: 0.36645
Epoch: 4200 Train: 0.19269 Test: 0.23827
Epoch: 4300 Train: 0.21206 Test: 0.21956
Epoch: 4400 Train: 0.40423 Test: 0.27138
Epoch: 4500 Train: 0.17016 Test: 0.21611
Epoch: 4600 Train: 0.11722 Test: 0.18548
Epoch 4700: New minimal relative error: 7.77%, model saved.
Epoch: 4700 Train: 0.35197 Test: 0.43738
Epoch 4800: New minimal relative error: 7.22%, model saved.
Epoch: 4800 Train: 0.18437 Test: 0.35187
Epoch: 4900 Train: 0.68741 Test: 0.72357
Epoch: 5000 Train: 0.39442 Test: 0.37306
Epoch: 5100 Train: 0.13886 Test: 0.25334
Epoch: 5200 Train: 0.28586 Test: 0.38457
Epoch: 5300 Train: 1.12552 Test: 0.82615
Epoch: 5400 Train: 0.19681 Test: 0.27565
Epoch: 5500 Train: 0.25457 Test: 0.28143
Epoch 5600: New minimal relative error: 5.23%, model saved.
Epoch: 5600 Train: 0.11680 Test: 0.20950
Epoch: 5700 Train: 0.10075 Test: 0.17730
Epoch: 5800 Train: 0.31424 Test: 0.38113
Epoch: 5900 Train: 0.21850 Test: 0.34420
Epoch: 6000 Train: 0.11417 Test: 0.18074
Epoch: 6100 Train: 0.07925 Test: 0.13731
Epoch: 6200 Train: 0.88340 Test: 1.14020
Epoch: 6300 Train: 0.36636 Test: 0.69570
Epoch: 6400 Train: 0.12504 Test: 0.13591
Epoch: 6500 Train: 0.18083 Test: 0.22738
Epoch: 6600 Train: 0.72522 Test: 0.86512
Epoch: 6700 Train: 0.10197 Test: 0.17732
Epoch: 6800 Train: 0.13160 Test: 0.16273
Epoch: 6900 Train: 0.10278 Test: 0.15091
Epoch: 7000 Train: 0.79224 Test: 0.71914
Epoch: 7100 Train: 0.13795 Test: 0.19913
Epoch: 7200 Train: 0.11007 Test: 0.18396
Epoch: 7300 Train: 0.07976 Test: 0.11730
Epoch: 7400 Train: 0.07052 Test: 0.12453
Epoch: 7500 Train: 0.92722 Test: 1.15587
Epoch: 7600 Train: 0.81629 Test: 1.32930
Epoch: 7700 Train: 0.14416 Test: 0.11454
Epoch: 7800 Train: 0.16868 Test: 0.25193
Epoch: 7900 Train: 0.16800 Test: 0.13344
Epoch: 8000 Train: 0.05506 Test: 0.13715
Epoch: 8100 Train: 0.10468 Test: 0.14206
Epoch: 8200 Train: 0.06816 Test: 0.10758
Epoch: 8300 Train: 0.37731 Test: 0.27467
Epoch: 8400 Train: 0.38919 Test: 0.41502
Epoch: 8500 Train: 0.07819 Test: 0.12507
Epoch: 8600 Train: 0.19728 Test: 0.25923
Epoch: 8700 Train: 0.09767 Test: 0.14669
Epoch: 8800 Train: 0.07407 Test: 0.12142
Epoch: 8900 Train: 0.05520 Test: 0.09173
Epoch: 9000 Train: 0.09567 Test: 0.14916
Epoch: 9100 Train: 0.06297 Test: 0.11635
Epoch: 9200 Train: 0.16811 Test: 0.16486
Epoch: 9300 Train: 0.06105 Test: 0.10906
Epoch: 9400 Train: 0.04381 Test: 0.08930
Epoch: 9500 Train: 0.14368 Test: 0.18176
Epoch: 9600 Train: 0.15921 Test: 0.22017
Epoch: 9700 Train: 0.06091 Test: 0.09952
Epoch: 9800 Train: 0.04091 Test: 0.08182
Epoch: 9900 Train: 0.04039 Test: 0.08049
Epoch: 9999 Train: 0.04600 Test: 0.08223
Training Loss: tensor(0.0460)
Test Loss: tensor(0.0822)
Learned LE: [ 0.8543609  -0.01368101 -3.1795177 ]
True LE: [ 8.7510115e-01  1.8200427e-03 -1.4554021e+01]
Relative Error: [ 5.106528   5.7027674  6.544684   7.7059827  8.947041  10.075375
 11.50672   12.483816  13.099937  13.785632  13.521704  12.991164
 12.176662  11.200239  10.502581   9.800136   9.240631   8.758842
  8.515537   8.31196    7.930978   7.508291   7.155935   6.7835517
  6.63134    6.6565647  6.701737   6.7655525  6.9795437  7.268777
  7.5814986  7.832886   8.123988   8.501984   8.876257   9.340697
  9.6861315 10.008344  10.517036  10.836197  11.049489  11.061785
 10.556864   9.828293   9.127108   8.23883    7.287591   6.1968207
  5.128263   4.2544765  3.6217458  3.2740803  3.1755812  3.2008886
  3.1923048  3.2130253  3.169737   3.156131   3.3519163  3.6385217
  3.850178   3.979526   4.255546   4.818127   5.6726575  6.6789107
  8.057138   9.105049  10.463802  11.606619  12.176882  12.816761
 12.568861  11.98638   11.392911  10.486723   9.778866   9.09412
  8.612214   8.232946   7.8923917  7.5318413  7.1712656  6.8946323
  6.432654   6.095523   6.0083117  6.05253    6.1481504  6.294304
  6.4424133  6.7204566  6.987838   7.228731   7.430045   7.751467
  8.105497   8.527972   8.99254    9.493897  10.103803  10.5129
 10.740561  10.721874  10.264412   9.492182   8.92186    8.169557
  7.175132   6.148722   4.967673   4.0003467  3.448667   3.108455
  2.9494228  2.919627   2.9202487  2.8479335  2.8092027  2.7742577
  2.9422617  3.0829506  3.2809713  3.4345462  3.556041   3.9449704
  4.72067    5.7450337  6.9399724  8.184983   9.307072  10.614387
 11.1610775 11.811063  11.587464  11.097622  10.645494   9.803638
  9.159506   8.49553    8.041211   7.5705385  7.161694   6.799697
  6.4744673  6.166895   5.73603    5.4440556  5.4276853  5.501337
  5.6620517  5.725234   5.823756   6.120326   6.3536167  6.548928
  6.7366214  7.001776   7.29384    7.7086306  8.205539   8.78682
  9.526898  10.1528015 10.428216  10.468815  10.092989   9.279793
  8.755918   8.200369   7.276674   6.2693105  4.985122   3.9866965
  3.4168925  3.1577642  2.892738   2.7828581  2.6893928  2.6113882
  2.584874   2.529676   2.6060202  2.691961   2.743545   2.868041
  3.0517461  3.2460966  3.788025   4.6972046  5.8919544  7.1736226
  8.163927   9.429762  10.1350565 10.712848  10.629165  10.199313
  9.92036    9.1513     8.564251   8.060005   7.3236995  6.877038
  6.379667   6.054724   5.7669735  5.457196   5.1089506  4.8890553
  4.936105   5.0689015  5.201142   5.2746687  5.338465   5.4698954
  5.708232   5.8264894  5.976258   6.2247148  6.5567493  6.8954306
  7.3096623  7.9775734  8.709076   9.490585  10.006937  10.157442
 10.019651   9.126314   8.736948   8.18817    7.616768   6.5150037
  5.3373823  4.2054386  3.5460572  3.3075347  2.985317   2.719876
  2.5502658  2.490668   2.462047   2.3675728  2.3470547  2.3997524
  2.374393   2.3500087  2.5355403  2.69359    3.0155861  3.6911542
  4.786528   6.0672565  7.0443263  8.164106   9.079222   9.586274
  9.706483   9.287318   9.048613   8.524634   8.023644   7.6227226
  6.768775   6.19358    5.6477723  5.2854514  5.076503   4.7923813
  4.514361   4.399215   4.4665823  4.660702   4.7543693  4.8833256
  4.973887   4.985444   5.0663943  5.217205   5.308569   5.4693384
  5.749076   6.050434   6.4908357  7.1841307  7.9610415  8.68433
  9.384431   9.737646   9.726458   9.030583   8.712992   8.279295
  7.794939   6.998063   5.9970512  4.741006   3.923773   3.4485388
  3.075217   2.6698208  2.4312723  2.4393     2.463302   2.3586388
  2.2723005  2.2057452  2.1371577  2.0144124  2.0856175  2.2505696
  2.3707829  2.8351498  3.5753808  4.7440505  5.929361   6.824334
  7.961623   8.436519   8.708945   8.396836   8.157539   7.8781075
  7.476637   7.008896   6.2166576  5.6044416  4.8744907  4.5315375
  4.4033527  4.1651     3.9705873  3.943556   4.0174384  4.2161407
  4.326612   4.502883   4.6613564  4.643636   4.549421   4.6350255
  4.733813   4.7747602  4.919818   5.1643577  5.5538454  6.335137
  7.1997285  7.914449   8.57562    9.118899   9.378396   8.902387
  8.466472   8.369005   7.9160137  7.393034   6.7332044  5.66844
  4.4666753  3.6944761  3.1857784  2.6671343  2.352106   2.370038
  2.4530866  2.425722   2.3489218  2.2051406  2.0606894  1.8631346
  1.8103467  1.9062344  2.04254    2.2043636  2.665833   3.4110804
  4.539808   5.431993   6.4714155  7.2339573  7.48441    7.482776
  7.2903075  7.0362687  6.7799277  6.4532123  5.7236123  5.123915
  4.2190356  3.8034682  3.7240295  3.549062   3.4334693  3.4851692
  3.5953097  3.7297993  3.895733   4.047215 ]
