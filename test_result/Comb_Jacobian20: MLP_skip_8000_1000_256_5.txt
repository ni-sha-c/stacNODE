time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 5
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.16%, model saved.
Epoch: 0 Train: 175851.81250 Test: 3908.08398
Epoch: 80 Train: 47449.22266 Test: 1644.02759
Epoch 160: New minimal relative error: 81.40%, model saved.
Epoch: 160 Train: 45057.67188 Test: 1595.94312
Epoch: 240 Train: 43089.23438 Test: 1381.39783
Epoch: 320 Train: 43549.48828 Test: 1495.42969
Epoch 400: New minimal relative error: 57.05%, model saved.
Epoch: 400 Train: 43148.20703 Test: 1445.63171
Epoch: 480 Train: 44572.66797 Test: 1544.10547
Epoch: 560 Train: 43738.05859 Test: 1500.02588
Epoch: 640 Train: 48416.59766 Test: 1573.99219
Epoch: 720 Train: 43712.42578 Test: 1481.56287
Epoch: 800 Train: 48405.30469 Test: 1497.82776
Epoch: 880 Train: 43221.73828 Test: 1550.10840
Epoch: 960 Train: 47823.83203 Test: 1562.03333
Epoch: 1040 Train: 44029.65625 Test: 1499.01135
Epoch: 1120 Train: 48304.88281 Test: 1469.76843
Epoch: 1200 Train: 45798.90625 Test: 1526.65210
Epoch: 1280 Train: 44579.75391 Test: 1473.46375
Epoch: 1360 Train: 46783.88281 Test: 1522.28235
Epoch: 1440 Train: 45342.52734 Test: 1514.96155
Epoch: 1520 Train: 43342.21875 Test: 1474.15857
Epoch: 1600 Train: 43082.48438 Test: 1504.98132
Epoch: 1680 Train: 43966.72656 Test: 1425.45422
Epoch: 1760 Train: 43456.26953 Test: 1449.29529
Epoch: 1840 Train: 46584.80469 Test: 1410.09900
Epoch: 1920 Train: 39542.80469 Test: 1367.95435
Epoch: 2000 Train: 44182.32422 Test: 1397.20166
Epoch: 2080 Train: 36987.22266 Test: 1226.61707
Epoch: 2160 Train: 40866.01562 Test: 1269.31799
Epoch: 2240 Train: 35948.75391 Test: 1154.21655
Epoch: 2320 Train: 39646.67578 Test: 1229.15259
Epoch: 2400 Train: 36190.35547 Test: 1155.45239
Epoch: 2480 Train: 32177.12695 Test: 889.72992
Epoch: 2560 Train: 34158.45312 Test: 932.20123
Epoch: 2640 Train: 34794.16406 Test: 1043.81201
Epoch: 2720 Train: 31971.36328 Test: 983.47241
Epoch: 2800 Train: 32620.43359 Test: 884.84460
Epoch: 2880 Train: 31132.26172 Test: 838.71277
Epoch: 2960 Train: 30223.60352 Test: 736.24268
Epoch: 3040 Train: 29415.39844 Test: 694.74591
Epoch: 3120 Train: 28874.27344 Test: 688.16943
Epoch: 3200 Train: 29956.55859 Test: 681.24280
Epoch: 3280 Train: 29317.89258 Test: 751.10004
Epoch: 3360 Train: 26546.91406 Test: 631.94305
Epoch: 3440 Train: 25797.50586 Test: 625.59528
Epoch: 3520 Train: 24263.14258 Test: 517.59448
Epoch: 3600 Train: 24088.99023 Test: 503.19147
Epoch: 3680 Train: 23596.16992 Test: 463.84100
Epoch: 3760 Train: 22506.73828 Test: 442.03305
Epoch: 3840 Train: 20402.20703 Test: 392.39490
Epoch: 3920 Train: 20502.94727 Test: 343.64426
Epoch: 4000 Train: 19421.15820 Test: 335.70319
Epoch: 4080 Train: 18610.07812 Test: 293.73218
Epoch: 4160 Train: 17048.58789 Test: 237.93703
Epoch: 4240 Train: 13341.39355 Test: 166.93462
Epoch: 4320 Train: 7986.06250 Test: 117.62315
Epoch: 4400 Train: 5645.33008 Test: 70.69644
Epoch 4480: New minimal relative error: 52.18%, model saved.
Epoch: 4480 Train: 3879.02759 Test: 28.95478
Epoch 4560: New minimal relative error: 31.24%, model saved.
Epoch: 4560 Train: 3156.64551 Test: 18.67804
Epoch 4640: New minimal relative error: 12.19%, model saved.
Epoch: 4640 Train: 2653.22266 Test: 13.99981
Epoch: 4720 Train: 2224.77466 Test: 11.19926
Epoch: 4800 Train: 2063.61987 Test: 9.75974
Epoch: 4880 Train: 1916.49866 Test: 9.88582
Epoch: 4960 Train: 1759.46045 Test: 8.04018
Epoch: 5040 Train: 1683.74084 Test: 6.93081
Epoch: 5120 Train: 1629.30371 Test: 7.12606
Epoch: 5200 Train: 1712.33240 Test: 9.59616
Epoch: 5280 Train: 1810.72058 Test: 13.48746
Epoch: 5360 Train: 1554.09180 Test: 8.06435
Epoch: 5440 Train: 1480.44495 Test: 6.31944
Epoch: 5520 Train: 1382.65833 Test: 7.20078
Epoch: 5600 Train: 1343.27515 Test: 8.46982
Epoch 5680: New minimal relative error: 8.63%, model saved.
Epoch: 5680 Train: 1322.94580 Test: 8.39387
Epoch: 5760 Train: 1215.43433 Test: 4.91351
Epoch: 5840 Train: 1312.25195 Test: 7.77184
Epoch: 5920 Train: 1240.57983 Test: 5.98756
Epoch: 6000 Train: 1179.80273 Test: 4.92837
Epoch 6080: New minimal relative error: 6.59%, model saved.
Epoch: 6080 Train: 1122.55017 Test: 4.15989
Epoch: 6160 Train: 1067.86792 Test: 4.27358
Epoch: 6240 Train: 1065.30823 Test: 4.51423
Epoch: 6320 Train: 1072.31104 Test: 3.58743
Epoch: 6400 Train: 996.31134 Test: 3.99379
Epoch: 6480 Train: 1036.80017 Test: 3.73921
Epoch: 6560 Train: 1000.50085 Test: 3.59700
Epoch: 6640 Train: 977.12994 Test: 3.73663
Epoch: 6720 Train: 917.91730 Test: 2.87408
Epoch: 6800 Train: 919.17950 Test: 2.98348
Epoch: 6880 Train: 923.09827 Test: 3.02281
Epoch: 6960 Train: 956.34448 Test: 3.56783
Epoch: 7040 Train: 938.62146 Test: 3.49269
Epoch: 7120 Train: 970.01617 Test: 4.76403
Epoch: 7200 Train: 988.52710 Test: 4.62295
Epoch: 7280 Train: 928.82947 Test: 4.11675
Epoch: 7360 Train: 974.10687 Test: 5.11679
Epoch: 7440 Train: 975.17493 Test: 3.78195
Epoch: 7520 Train: 988.34583 Test: 4.00266
Epoch: 7600 Train: 895.47345 Test: 3.94545
Epoch: 7680 Train: 907.75092 Test: 3.55381
Epoch: 7760 Train: 942.46429 Test: 3.94857
Epoch: 7840 Train: 865.73602 Test: 3.14457
Epoch: 7920 Train: 865.06927 Test: 3.12523
Epoch 7999: New minimal relative error: 6.33%, model saved.
Epoch: 7999 Train: 853.79907 Test: 2.94550
Training Loss: tensor(853.7991)
Test Loss: tensor(2.9455)
Learned LE: [  0.6714158    0.17172767 -14.523801  ]
True LE: [ 8.7933278e-01  1.2199946e-03 -1.4557536e+01]
Relative Error: [10.696825  10.707503  10.800909  11.021581  10.971655  10.900203
 10.963534  11.166009  11.436675  11.767403  12.22064   12.886426
 13.57318   14.286858  14.828833  15.284656  15.518912  15.500145
 15.375291  15.196764  14.970638  14.706779  14.419103  14.123684
 13.836537  13.564855  13.26206   12.70059   11.6425085 10.48572
  9.2332115  7.4926777  5.6456485  4.4562006  3.3864102  2.5239458
  1.9480113  1.7406011  1.7991731  1.9233518  2.257758   3.4206889
  4.492528   5.5752716  6.8034196  7.9207606  8.86372    9.58218
 10.104844  10.437159  10.555455  10.540652  10.414891  10.230815
 10.1157    10.066902   9.948211   9.818545   9.576948   9.255755
  8.979338   8.890889   9.090078   9.093008   9.145141   9.339129
  9.374038   9.347959   9.425629   9.629044   9.932492  10.324107
 10.822801  11.486554  12.30931   13.116603  13.767465  14.278625
 14.46777   14.496082  14.400294  14.220257  14.035757  13.857264
 13.698086  13.525842  13.295012  13.093694  12.933676  12.257301
 11.216969  10.055012   8.805854   7.3231792  5.527541   4.460378
  3.528985   2.8509848  2.5290937  2.4766457  2.4376457  2.490125
  2.7258193  3.1649346  3.7436697  4.781255   6.0291533  7.1046762
  7.9580603  8.648704   9.180758   9.550059   9.739715   9.826551
  9.8075905  9.5701065  9.557323   9.497746   9.305581   9.012237
  8.6376     8.135273   7.711748   7.375137   7.3755355  7.494583
  7.495603   7.653154   7.8223696  7.7398562  7.8209715  8.052736
  8.369091   8.803058   9.374281  10.045762  10.956864  11.846581
 12.625095  13.157018  13.360233  13.337726  13.268471  13.174601
 13.063661  12.949292  12.852013  12.790038  12.777671  12.709697
 12.629282  11.87181   10.935216   9.8006315  8.565268   7.286135
  5.678513   4.704655   3.9374194  3.4376047  3.2715168  3.1998024
  3.1251855  3.1448529  3.209163   3.2512321  3.231708   4.1104293
  5.1489244  6.131687   6.991699   7.7096596  8.264935   8.646189
  9.019349   9.236326   9.205587   9.088395   9.15346    9.014796
  8.729961   8.327706   7.8338943  7.2816467  6.5945625  5.9914036
  5.7073255  5.824963   5.858538   6.009815   6.266659   6.2345786
  6.209634   6.392315   6.751259   7.1887226  7.808134   8.561735
  9.470771  10.450856  11.340016  11.886585  12.148433  12.277939
 12.329048  12.298713  12.160991  11.936333  11.832409  11.81426
 11.906292  12.114398  12.268618  11.509908  10.675046   9.716286
  8.544147   7.3014507  6.0821514  5.1578727  4.540915   4.171858
  4.061121   3.9926343  3.8291342  3.7117414  3.5146163  3.5458672
  3.344869   3.5340254  4.240638   5.098824   5.9304895  6.6673646
  7.2624707  7.841283   8.273273   8.480445   8.519887   8.510101
  8.594397   8.482817   8.178512   7.7274694  7.119845   6.4295926
  5.800216   5.1751595  4.531098   4.3980265  4.5461807  4.6399684
  5.029714   5.2325463  5.1060066  5.1273327  5.361042   5.6925764
  6.112553   6.9099345  7.9218497  8.874914   9.97104   10.634134
 10.984895  11.273873  11.474942  11.580968  11.426926  11.205711
 10.978243  10.906374  11.056452  11.341401  11.534542  11.166451
 10.412518   9.549232   8.592295   7.555395   6.462705   5.777204
  5.2581677  5.0026464  4.854717   4.6853557  4.505172   4.129586
  3.826111   3.8216002  3.6986635  3.3959367  3.4575706  4.0655484
  4.83806    5.6018176  6.2613344  6.957499   7.4209223  7.68952
  7.8593025  7.9681816  8.014196   7.878135   7.558848   7.0254087
  6.411914   5.9633517  5.4181986  4.7753267  3.9917576  3.2470622
  3.3560858  3.6718209  3.9367442  4.355931   4.354427   4.1346483
  4.100122   4.343971   4.8228207  5.3935037  6.182421   7.263173
  8.450333   9.138689   9.665833  10.138874  10.518935  10.740388
 10.678977  10.556453  10.40859   10.292645  10.254527  10.5811825
 10.867321  10.508061  10.142716   9.421117   8.582964   7.773647
  6.95284    6.3849673  6.07622    5.820152   5.1475534  4.7722063
  4.648155   4.5236206  4.3198767  4.157395   3.9247465  3.757263
  3.242853   3.2610714  3.7702553  4.4983416  5.2641644  6.034309
  6.5642605  6.9632754  7.16357    7.32611    7.489337   7.30467
  6.933102   6.4349933  5.981723   5.5845757  5.086393   4.4296513
  3.6315744  2.8824215  2.4527564  2.5773935  2.8207638  3.1405733
  3.5560627  3.5025995  3.2399542  3.1101336  3.3116558  3.9114923
  4.681116   5.7645025  6.72211    7.387124   8.088092   8.771032
  9.366292   9.74935    9.832076   9.829383   9.793668   9.767383
  9.80165    9.94821   10.027934   9.559584 ]
