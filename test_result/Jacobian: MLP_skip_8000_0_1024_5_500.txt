time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 1024
n_layers: 5
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 99.47%, model saved.
Epoch: 0 Train: 31715.04102 Test: 4314.01270
Epoch 80: New minimal relative error: 79.07%, model saved.
Epoch: 80 Train: 7686.06299 Test: 1442.51477
Epoch: 160 Train: 6789.65430 Test: 1041.25366
Epoch: 240 Train: 5423.93408 Test: 1023.57068
Epoch: 320 Train: 5382.63623 Test: 1032.59778
Epoch: 400 Train: 5037.85449 Test: 1473.54895
Epoch: 480 Train: 3965.12964 Test: 347.13379
Epoch 560: New minimal relative error: 37.27%, model saved.
Epoch: 560 Train: 1483.14270 Test: 110.90587
Epoch: 640 Train: 1592.75586 Test: 58.72839
Epoch 720: New minimal relative error: 19.07%, model saved.
Epoch: 720 Train: 355.24191 Test: 8.59229
Epoch: 800 Train: 345.46704 Test: 48.93354
Epoch: 880 Train: 226.11369 Test: 14.90321
Epoch 960: New minimal relative error: 3.13%, model saved.
Epoch: 960 Train: 133.26830 Test: 2.66090
Epoch: 1040 Train: 120.75372 Test: 2.75226
Epoch: 1120 Train: 122.54050 Test: 14.81528
Epoch: 1200 Train: 139.44095 Test: 12.86582
Epoch 1280: New minimal relative error: 2.74%, model saved.
Epoch: 1280 Train: 95.00156 Test: 1.32711
Epoch: 1360 Train: 101.59063 Test: 8.65352
Epoch: 1440 Train: 101.45834 Test: 7.00794
Epoch: 1520 Train: 103.06951 Test: 9.47745
Epoch: 1600 Train: 89.16949 Test: 7.22527
Epoch 1680: New minimal relative error: 2.25%, model saved.
Epoch: 1680 Train: 58.80309 Test: 2.41078
Epoch: 1760 Train: 84.26764 Test: 6.69633
Epoch: 1840 Train: 52.97309 Test: 1.41030
Epoch: 1920 Train: 55.91835 Test: 6.38352
Epoch: 2000 Train: 46.74826 Test: 0.46382
Epoch: 2080 Train: 48.16367 Test: 3.38840
Epoch: 2160 Train: 42.46547 Test: 0.82702
Epoch: 2240 Train: 67.34589 Test: 7.42696
Epoch: 2320 Train: 40.04506 Test: 1.65069
Epoch: 2400 Train: 47.23099 Test: 3.81687
Epoch: 2480 Train: 63.07712 Test: 3.52733
Epoch: 2560 Train: 58.20486 Test: 5.89578
Epoch: 2640 Train: 46.44434 Test: 2.39598
Epoch: 2720 Train: 45.47893 Test: 4.65294
Epoch: 2800 Train: 53.45075 Test: 4.07255
Epoch: 2880 Train: 45.32375 Test: 6.59938
Epoch: 2960 Train: 35.90226 Test: 3.09635
Epoch: 3040 Train: 33.19461 Test: 0.56623
Epoch: 3120 Train: 29.43000 Test: 0.47972
Epoch: 3200 Train: 42.19700 Test: 3.12412
Epoch: 3280 Train: 40.44278 Test: 3.39148
Epoch: 3360 Train: 39.80517 Test: 3.24160
Epoch: 3440 Train: 27.09103 Test: 0.78277
Epoch: 3520 Train: 81.88100 Test: 7.81169
Epoch: 3600 Train: 30.04246 Test: 0.26168
Epoch 3680: New minimal relative error: 1.71%, model saved.
Epoch: 3680 Train: 27.26409 Test: 0.21537
Epoch: 3760 Train: 26.32803 Test: 0.35397
Epoch: 3840 Train: 27.22310 Test: 0.67606
Epoch: 3920 Train: 24.59727 Test: 0.67056
Epoch: 4000 Train: 32.41213 Test: 3.86834
Epoch: 4080 Train: 30.23823 Test: 1.99044
Epoch: 4160 Train: 26.97126 Test: 1.92791
Epoch: 4240 Train: 41.93624 Test: 1.37377
Epoch: 4320 Train: 22.23554 Test: 0.61101
Epoch: 4400 Train: 24.60852 Test: 1.00658
Epoch 4480: New minimal relative error: 1.21%, model saved.
Epoch: 4480 Train: 20.72701 Test: 0.10050
Epoch: 4560 Train: 21.05725 Test: 0.22239
Epoch: 4640 Train: 20.52784 Test: 0.28203
Epoch: 4720 Train: 19.80996 Test: 0.04376
Epoch: 4800 Train: 20.39376 Test: 0.18124
Epoch: 4880 Train: 20.09843 Test: 0.08352
Epoch: 4960 Train: 21.99383 Test: 0.79675
Epoch 5040: New minimal relative error: 0.84%, model saved.
Epoch: 5040 Train: 19.01266 Test: 0.04475
Epoch: 5120 Train: 20.98679 Test: 0.18435
Epoch: 5200 Train: 18.50443 Test: 0.41266
Epoch: 5280 Train: 26.48749 Test: 0.90311
Epoch: 5360 Train: 18.72443 Test: 0.11895
Epoch: 5440 Train: 17.73736 Test: 0.09629
Epoch: 5520 Train: 19.01287 Test: 0.15533
Epoch: 5600 Train: 25.08557 Test: 1.64181
Epoch: 5680 Train: 17.67806 Test: 0.15802
Epoch: 5760 Train: 19.71295 Test: 0.32050
Epoch: 5840 Train: 20.08429 Test: 0.47833
Epoch: 5920 Train: 18.31969 Test: 0.21622
Epoch: 6000 Train: 22.09368 Test: 2.32281
Epoch: 6080 Train: 16.85794 Test: 0.10779
Epoch: 6160 Train: 22.19869 Test: 0.61061
Epoch: 6240 Train: 17.09997 Test: 0.18147
Epoch: 6320 Train: 17.43438 Test: 0.10181
Epoch: 6400 Train: 16.80530 Test: 0.21026
Epoch: 6480 Train: 16.15373 Test: 0.27267
Epoch: 6560 Train: 15.63037 Test: 0.23087
Epoch: 6640 Train: 15.64548 Test: 0.17049
Epoch: 6720 Train: 15.19869 Test: 0.03397
Epoch: 6800 Train: 15.92191 Test: 0.04882
Epoch: 6880 Train: 16.56520 Test: 0.05586
Epoch: 6960 Train: 27.15802 Test: 2.39583
Epoch: 7040 Train: 19.69216 Test: 1.53968
Epoch: 7120 Train: 16.21092 Test: 0.49150
Epoch: 7200 Train: 15.41254 Test: 0.08042
Epoch: 7280 Train: 15.16781 Test: 0.23348
Epoch: 7360 Train: 16.67864 Test: 0.92138
Epoch: 7440 Train: 18.21960 Test: 0.27705
Epoch: 7520 Train: 16.49700 Test: 0.43873
Epoch: 7600 Train: 17.54029 Test: 0.14594
Epoch: 7680 Train: 16.51390 Test: 0.09194
Epoch: 7760 Train: 15.37286 Test: 0.06954
Epoch: 7840 Train: 20.22692 Test: 3.23193
Epoch: 7920 Train: 15.22378 Test: 0.05982
Epoch: 7999 Train: 14.64677 Test: 0.06217
Training Loss: tensor(14.6468)
Test Loss: tensor(0.0622)
Learned LE: [  0.84591144   0.01599342 -14.533947  ]
True LE: [ 8.61350417e-01  5.21556521e-03 -1.45379305e+01]
Relative Error: [1.1101506  1.1578087  1.1105673  1.0241969  0.9373123  0.744426
 0.5263834  0.37289748 0.36536592 0.40704864 0.49870542 0.7225566
 0.9032973  1.0235388  1.0184313  1.1111262  1.2541796  1.2641577
 1.1754295  1.2561741  1.015852   0.8665342  0.8795592  0.9770499
 0.91914225 0.89828444 0.9056659  0.9656492  1.0551875  1.1336136
 1.2605196  1.271975   1.2513905  1.1771128  1.0958322  1.0489877
 1.0415705  1.0406328  1.1486286  1.1907285  1.1281724  1.0044029
 0.94994307 0.98684484 1.0620222  1.1806283  1.2432672  1.5116332
 1.3910152  1.1107414  0.92822576 0.8469401  0.90765685 0.6322949
 0.50048566 0.5197831  0.6246903  0.7582721  0.8806846  1.0327739
 1.0743301  1.0022438  1.0850259  1.142614   1.048588   1.0042751
 0.8789167  0.7254789  0.52796817 0.38795647 0.30788457 0.3109841
 0.41531277 0.6039004  0.80771583 0.8847655  0.8561303  0.95306814
 1.0700991  1.0303338  0.9783833  1.0853487  0.91291183 0.7800203
 0.81638557 0.8345981  0.7900682  0.8111777  0.91933125 1.0152023
 1.1533778  1.2729911  1.3034096  1.333907   1.3382378  1.2627212
 1.1508279  1.0928314  1.0714095  1.0751375  1.1470227  1.1173177
 1.0215815  0.91393393 0.8344792  0.85411966 0.9431357  1.0307376
 1.1465659  1.411673   1.217927   0.9430945  0.7459005  0.715153
 0.706802   0.48643556 0.42272335 0.49379742 0.612185   0.80309415
 1.0294789  1.1287467  1.1155581  1.0761607  1.1144966  1.0648879
 0.9875864  0.9613244  0.8195404  0.6731303  0.5388957  0.43162304
 0.31185845 0.31552967 0.40493494 0.46086976 0.6314018  0.7748603
 0.70739806 0.73311627 0.8675706  0.87253237 0.89105535 1.0464324
 0.9023427  0.7789546  0.7751787  0.7086395  0.69457877 0.76759726
 0.9092462  1.0715706  1.2290044  1.2843394  1.3809917  1.4276294
 1.4268636  1.298718   1.1013879  1.068177   1.1051255  1.1067549
 1.1210732  1.056466   0.9263698  0.81560117 0.72158355 0.70595855
 0.8257209  0.9158252  1.0409504  1.2813681  1.01784    0.70322233
 0.5453842  0.5827939  0.50870055 0.3289465  0.29456183 0.41124183
 0.60670114 0.8536012  1.1116867  1.1525288  1.1875968  1.152017
 1.0333883  0.96220326 0.9561378  0.94556904 0.7985746  0.6320588
 0.54177696 0.4316228  0.3610634  0.36606404 0.39502275 0.36089095
 0.462613   0.66974044 0.51398563 0.5303206  0.7711403  0.834847
 0.88453025 1.0640237  0.9265905  0.85925114 0.8853252  0.68878627
 0.5894181  0.64227754 0.80846447 1.0585144  1.1700684  1.2804327
 1.4313533  1.4273297  1.2667636  1.0534285  0.92118055 0.9142566
 0.9547396  1.0413672  1.1026722  0.9991126  0.84531707 0.6901444
 0.60675114 0.5888335  0.740442   0.8200437  0.95352083 1.1425912
 0.8119246  0.50000966 0.3777264  0.46782923 0.3065477  0.17514414
 0.21218027 0.38657194 0.63968813 0.9729528  1.1205703  1.242324
 1.1949936  1.0619136  0.98718023 0.94375086 0.923621   0.97008264
 0.81163895 0.6194475  0.55319077 0.4604563  0.39570555 0.38586286
 0.3352431  0.29005447 0.32962433 0.41898322 0.365018   0.44088826
 0.74522144 0.8399644  0.8756902  1.0084593  0.853738   0.88122946
 0.9289789  0.7012623  0.49321836 0.52696705 0.7430163  0.9506952
 1.0992484  1.17461    1.1464925  1.1191797  0.9909514  0.7815861
 0.6523212  0.7624051  0.85894763 0.9040084  0.949007   0.964352
 0.773702   0.59684396 0.5053708  0.49345818 0.6286899  0.7547099
 0.8715323  1.0081955  0.6447787  0.3723266  0.2830151  0.36168814
 0.1425158  0.07957604 0.20015936 0.41521704 0.72280926 0.9749356
 1.1489346  1.2134252  1.2067341  1.0125014  0.9412398  0.9074221
 0.89040923 0.9540623  0.84022677 0.6409237  0.5785408  0.51123595
 0.4361786  0.39978713 0.2748991  0.2475496  0.23983528 0.21509051
 0.2922558  0.47209948 0.74227524 0.77883065 0.77026904 0.87982285
 0.76145095 0.7882853  0.8005826  0.6559196  0.5199268  0.48117673
 0.67750055 0.8805232  0.9541184  0.8869457  0.78543985 0.79429764
 0.7916139  0.59455216 0.48882416 0.5813887  0.7349875  0.80265033
 0.7848733  0.77418417 0.71637034 0.5231002  0.41768575 0.4350914
 0.5531492  0.68469495 0.7564585  0.86770606 0.4937585  0.2517675
 0.20812473 0.29509595 0.14557265 0.0198619  0.16957279 0.45069984
 0.83083373 0.99658144 1.0837507  1.1694292  1.0836613  0.90832525
 0.84018165 0.8218645  0.7989035  0.8984722  0.85571975 0.6150245
 0.59413534 0.57087415 0.4763165  0.3787598  0.25012866 0.21078786
 0.24787994 0.12653285 0.26202786 0.5055124  0.6520978  0.7003734
 0.6407861  0.745429   0.6920516  0.6905512  0.67628753 0.4928222
 0.519425   0.56651527 0.5866839  0.7178868 ]
