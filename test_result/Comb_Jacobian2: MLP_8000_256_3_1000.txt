time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
batch_size: None
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP
n_hidden: 256
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/gs/train_MLPskip_MSE_new/
Epoch 0: New minimal relative error: 99.57%, model saved.
Epoch: 0 Train: 59129.35938 Test: 4179.38281
Epoch: 80 Train: 10442.80859 Test: 1326.81433
Epoch: 160 Train: 2850.80347 Test: 203.30043
Epoch 240: New minimal relative error: 13.96%, model saved.
Epoch: 240 Train: 454.51544 Test: 17.42752
Epoch: 320 Train: 173.74240 Test: 5.34702
Epoch 400: New minimal relative error: 5.20%, model saved.
Epoch: 400 Train: 94.36439 Test: 2.06364
Epoch: 480 Train: 63.23416 Test: 2.82638
Epoch: 560 Train: 60.93757 Test: 0.74329
Epoch: 640 Train: 34.55103 Test: 0.67200
Epoch: 720 Train: 46.42148 Test: 2.69396
Epoch 800: New minimal relative error: 3.03%, model saved.
Epoch: 800 Train: 23.02752 Test: 0.67009
Epoch: 880 Train: 49.25806 Test: 2.67014
Epoch 960: New minimal relative error: 2.33%, model saved.
Epoch: 960 Train: 39.99055 Test: 3.83193
Epoch: 1040 Train: 15.57144 Test: 0.27172
Epoch: 1120 Train: 18.32366 Test: 2.44216
Epoch: 1200 Train: 13.28657 Test: 0.21740
Epoch: 1280 Train: 24.12888 Test: 3.04913
Epoch: 1360 Train: 10.41370 Test: 0.08333
Epoch 1440: New minimal relative error: 2.07%, model saved.
Epoch: 1440 Train: 8.87273 Test: 0.12402
Epoch: 1520 Train: 8.20265 Test: 0.27585
Epoch: 1600 Train: 8.11866 Test: 0.65977
Epoch 1680: New minimal relative error: 1.79%, model saved.
Epoch: 1680 Train: 6.58326 Test: 0.06615
Epoch: 1760 Train: 7.70214 Test: 0.97679
Epoch: 1840 Train: 13.58835 Test: 1.00183
Epoch: 1920 Train: 6.15804 Test: 0.13674
Epoch: 2000 Train: 9.50434 Test: 0.44426
Epoch: 2080 Train: 7.17172 Test: 0.38362
Epoch: 2160 Train: 4.98550 Test: 0.08725
Epoch: 2240 Train: 4.45882 Test: 0.11820
Epoch: 2320 Train: 6.43366 Test: 1.31218
Epoch: 2400 Train: 4.51395 Test: 0.34849
Epoch: 2480 Train: 7.07273 Test: 0.29586
Epoch 2560: New minimal relative error: 1.33%, model saved.
Epoch: 2560 Train: 4.26221 Test: 0.04476
Epoch: 2640 Train: 3.54809 Test: 0.43365
Epoch: 2720 Train: 3.43594 Test: 0.10536
Epoch: 2800 Train: 3.51673 Test: 0.08639
Epoch: 2880 Train: 7.90357 Test: 0.41467
Epoch: 2960 Train: 10.07048 Test: 3.52529
Epoch: 3040 Train: 2.88675 Test: 0.06383
Epoch: 3120 Train: 5.72688 Test: 0.57842
Epoch: 3200 Train: 2.55142 Test: 0.01156
Epoch: 3280 Train: 2.49402 Test: 0.02465
Epoch 3360: New minimal relative error: 0.43%, model saved.
Epoch: 3360 Train: 2.36843 Test: 0.01021
Epoch: 3440 Train: 2.92678 Test: 0.17005
Epoch: 3520 Train: 2.90611 Test: 0.05655
Epoch: 3600 Train: 2.74854 Test: 0.18669
Epoch: 3680 Train: 2.12326 Test: 0.02979
Epoch: 3760 Train: 2.82004 Test: 0.22073
Epoch: 3840 Train: 1.98297 Test: 0.02884
Epoch: 3920 Train: 2.24249 Test: 0.20225
Epoch: 4000 Train: 1.86758 Test: 0.02897
Epoch: 4080 Train: 1.81842 Test: 0.02410
Epoch: 4160 Train: 1.72582 Test: 0.00728
Epoch 4240: New minimal relative error: 0.32%, model saved.
Epoch: 4240 Train: 1.78961 Test: 0.02211
Epoch: 4320 Train: 2.39260 Test: 0.35483
Epoch: 4400 Train: 1.61553 Test: 0.02229
Epoch: 4480 Train: 1.58068 Test: 0.01374
Epoch: 4560 Train: 1.98001 Test: 0.09989
Epoch: 4640 Train: 2.75667 Test: 0.23429
Epoch: 4720 Train: 5.03260 Test: 1.30464
Epoch: 4800 Train: 2.84216 Test: 0.48293
Epoch: 4880 Train: 1.35270 Test: 0.00591
Epoch: 4960 Train: 1.32771 Test: 0.00616
Epoch: 5040 Train: 6.07879 Test: 1.45355
Epoch: 5120 Train: 1.25401 Test: 0.00522
Epoch: 5200 Train: 1.24865 Test: 0.01264
Epoch: 5280 Train: 1.21225 Test: 0.01108
Epoch: 5360 Train: 1.18094 Test: 0.00615
Epoch: 5440 Train: 1.61627 Test: 0.13005
Epoch: 5520 Train: 1.18945 Test: 0.02996
Epoch: 5600 Train: 1.09222 Test: 0.00465
Epoch: 5680 Train: 1.24103 Test: 0.04891
Epoch: 5760 Train: 3.00525 Test: 0.59484
Epoch: 5840 Train: 1.02325 Test: 0.00497
Epoch: 5920 Train: 1.03404 Test: 0.02608
Epoch: 6000 Train: 1.46911 Test: 0.01797
Epoch: 6080 Train: 0.95927 Test: 0.00424
Epoch: 6160 Train: 1.11247 Test: 0.07670
Epoch: 6240 Train: 3.71514 Test: 0.86580
Epoch: 6320 Train: 0.90353 Test: 0.00441
Epoch: 6400 Train: 3.51026 Test: 0.72941
Epoch: 6480 Train: 0.95427 Test: 0.04079
Epoch: 6560 Train: 0.85427 Test: 0.00452
Epoch: 6640 Train: 2.72075 Test: 0.50329
Epoch: 6720 Train: 2.12173 Test: 0.48732
Epoch: 6800 Train: 0.80426 Test: 0.00399
Epoch: 6880 Train: 2.51573 Test: 0.18748
Epoch: 6960 Train: 0.88380 Test: 0.05226
Epoch: 7040 Train: 0.76155 Test: 0.00389
Epoch: 7120 Train: 0.78712 Test: 0.00831
Epoch: 7200 Train: 0.74210 Test: 0.00617
Epoch: 7280 Train: 0.72133 Test: 0.00349
Epoch: 7360 Train: 1.42589 Test: 0.08075
Epoch: 7440 Train: 0.71106 Test: 0.00932
Epoch: 7520 Train: 0.68630 Test: 0.00461
Epoch: 7600 Train: 0.67526 Test: 0.00558
Epoch: 7680 Train: 1.16821 Test: 0.22374
Epoch: 7760 Train: 0.64909 Test: 0.00325
Epoch 7840: New minimal relative error: 0.26%, model saved.
Epoch: 7840 Train: 0.63733 Test: 0.00317
Epoch: 7920 Train: 0.63574 Test: 0.00661
Epoch: 7999 Train: 0.61697 Test: 0.00312
Training Loss: tensor(0.6170)
Test Loss: tensor(0.0031)
Learned LE: [ 8.6127555e-01  8.8001788e-03 -1.4546600e+01]
True LE: [ 8.6776513e-01  1.2984311e-03 -1.4540400e+01]
Relative Error: [3.2330632  3.2605655  3.2581058  3.2528195  3.2647028  3.2983205
 3.344087   3.385929   3.4078333  3.3992872  3.3550751  3.273464
 3.1534214  2.9919407  2.7848809  2.5307515  2.237745   1.9316293
 1.6615814  1.5055362  1.5320141  1.6908355  1.839732   1.8983146
 1.901318   1.9290296  2.0236723  2.1696427  2.3322434  2.4838543
 2.6005392  2.6549072  2.6230283  2.4951744  2.2781458  1.992381
 1.6637707  1.3213408  1.006819   0.79947126 0.8124378  1.0371735
 1.3482625  1.6481704  1.8755922  2.0057764  2.0767689  2.1735468
 2.3381662  2.5083385  2.5961225  2.6377714  2.7211773  2.7855392
 2.7521658  2.650554   2.536941   2.4447248  2.4078612  2.4507089
 2.560467   2.6877139  2.7796555  2.815585   2.812358   2.8045735
 2.8202486  2.8673372  2.9351597  3.0042143  3.055788   3.0777035
 3.0651262  3.0177572  2.9359748  2.817473   2.6558585  2.443322
 2.1784909  1.8762007  1.5749514  1.3465961  1.2925024  1.4300073
 1.6160303  1.7181225  1.7360469  1.7536908  1.8315036  1.9676397
 2.1364837  2.319082   2.4922     2.616825   2.6511707  2.5729196
 2.3882277  2.1244097  1.8141413  1.4866505  1.1694223  0.8999538
 0.7494     0.80098164 1.0244044  1.3108451  1.5719904  1.7469423
 1.8180013  1.8461654  1.9308934  2.0857203  2.2020333  2.2314458
 2.2850802  2.3627703  2.357674   2.2687533  2.1489968  2.0302072
 1.9549516  1.9691567  2.076777   2.2264783  2.3467572  2.39884
 2.3953016  2.378884   2.3887954  2.4403021  2.523076   2.6139865
 2.6902475  2.7380497  2.7528353  2.7360895  2.690913   2.6176312
 2.5104227  2.3585324  2.1499612  1.8839601  1.5820583  1.294691
 1.1196438  1.1620404  1.3535802  1.5100135  1.5584831  1.563972
 1.6104879  1.718799   1.8730989  2.065439   2.28047    2.4723883
 2.5768714  2.5509386  2.3968427  2.1505156  1.8561635  1.5487783
 1.2531074  0.9900909  0.78658366 0.69109815 0.75671846 0.9592071
 1.2121259  1.4343878  1.5637683  1.587638   1.5839573  1.662842
 1.7998481  1.8536919  1.8615259  1.9285848  1.9623471  1.9055499
 1.7985001  1.6677735  1.5532603  1.521774   1.6084439  1.77519
 1.9337926  2.0165515  2.019387   1.9891268  1.9801878  2.021312
 2.106776   2.2101495  2.303094   2.3686008  2.4018924  2.4072285
 2.3906236  2.356123   2.3019967  2.2192984  2.0911772  1.9017273
 1.6497817  1.3597811  1.0900303  0.95890236 1.0574974  1.2491825
 1.3573316  1.3696833  1.3787332  1.4421078  1.558351   1.7279614
 1.9536561  2.1963544  2.3733714  2.4110186  2.2943168  2.0650423
 1.7819722  1.4897387  1.215405   0.9771944  0.7907454  0.66800386
 0.62255335 0.681306   0.845831   1.0591849  1.2464695  1.3441164
 1.3369137  1.3089931  1.3819313  1.4925741  1.4949483  1.5023428
 1.5562947  1.5486145  1.476972   1.3635004  1.2267622  1.1381711
 1.1715802  1.3294215  1.5272657  1.6613494  1.6905506  1.651096
 1.6116698  1.6224463  1.6916907  1.7934002  1.8925911  1.9658611
 2.0066564  2.0210764  2.0191212  2.008539   1.9927936  1.9682107
 1.9227147  1.8354563  1.6854382  1.468444   1.2053888  0.9524391
 0.83297956 0.93402404 1.0979389  1.1653504  1.1579034  1.1682953
 1.2285986  1.3417884  1.5287848  1.7809887  2.020326   2.13976
 2.0854442  1.8884547  1.6202414  1.3396053  1.0774263  0.8494425
 0.6731484  0.5684939  0.5332925  0.5387223  0.5839538  0.6977998
 0.8639177  1.0214164  1.1049052  1.0878749  1.0394744  1.0983456
 1.1855277  1.1550769  1.1544997  1.1839954  1.1613213  1.0956182
 0.980266   0.8522335  0.80388474 0.89920723 1.103407   1.3005025
 1.3919866  1.3713745  1.3051089  1.267955   1.2957634  1.3755244
 1.4685645  1.5413864  1.5805042  1.5911505  1.5872737  1.5815781
 1.5811846  1.5874082  1.596003   1.5948879  1.5619569  1.4723499
 1.3150054  1.101211   0.8716186  0.728366   0.77883583 0.9045207
 0.94663435 0.9274137  0.9288861  0.9707981  1.0723459  1.264902
 1.516903   1.7196844  1.7667953  1.6438073  1.4172238  1.1615727
 0.9177903  0.69833755 0.50878966 0.36833313 0.31807652 0.35904524
 0.4227082  0.47346056 0.5397461  0.6475948  0.7725693  0.85720736
 0.8545557  0.7954578  0.8220126  0.9010827  0.8562699  0.8302666
 0.8392306  0.8221464  0.77228105 0.66696334 0.55822134 0.54135644
 0.66894716 0.8866184  1.0664504  1.1201078  1.0668361  0.98851806
 0.95485234 0.9858339  1.0548964  1.1205916  1.1562908  1.1585919
 1.1408548  1.1217824  1.1143346  1.1227337  1.1461116  1.1806155
 1.2176933  1.2391053  1.2198076  1.1423563  1.0056034  0.8273691
 0.66263705 0.6180783  0.68647254 0.7158166 ]
