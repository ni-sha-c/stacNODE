time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.33%, model saved.
Epoch: 0 Train: 59696.45312 Test: 3874.70337
Epoch 80: New minimal relative error: 101.87%, model saved.
Epoch: 80 Train: 16696.36719 Test: 1635.45032
Epoch: 160 Train: 15191.73242 Test: 1315.09851
Epoch 240: New minimal relative error: 51.49%, model saved.
Epoch: 240 Train: 15069.23633 Test: 1304.54773
Epoch: 320 Train: 14636.90918 Test: 1391.44238
Epoch: 400 Train: 14989.10547 Test: 3119.92627
Epoch: 480 Train: 15848.12305 Test: 1276.46362
Epoch: 560 Train: 13915.60547 Test: 1382.24829
Epoch: 640 Train: 15232.40625 Test: 1347.78467
Epoch: 720 Train: 14346.49805 Test: 1218.79480
Epoch: 800 Train: 14021.43555 Test: 1277.04456
Epoch: 880 Train: 13528.72266 Test: 1330.49756
Epoch: 960 Train: 13756.27930 Test: 1306.01880
Epoch: 1040 Train: 12793.97363 Test: 1088.99756
Epoch: 1120 Train: 13705.75977 Test: 1051.34863
Epoch: 1200 Train: 12523.28711 Test: 1084.40564
Epoch: 1280 Train: 11263.99512 Test: 970.09467
Epoch: 1360 Train: 11339.33203 Test: 917.96094
Epoch: 1440 Train: 9980.04492 Test: 661.81970
Epoch: 1520 Train: 9537.82324 Test: 567.71265
Epoch: 1600 Train: 8763.03613 Test: 501.56888
Epoch: 1680 Train: 6889.93408 Test: 339.94135
Epoch: 1760 Train: 3937.54663 Test: 159.63742
Epoch 1840: New minimal relative error: 50.61%, model saved.
Epoch: 1840 Train: 2433.24609 Test: 82.79007
Epoch 1920: New minimal relative error: 21.77%, model saved.
Epoch: 1920 Train: 1456.06836 Test: 27.76352
Epoch 2000: New minimal relative error: 11.26%, model saved.
Epoch: 2000 Train: 1104.54688 Test: 16.76604
Epoch: 2080 Train: 871.79926 Test: 12.17235
Epoch: 2160 Train: 765.50702 Test: 10.14093
Epoch: 2240 Train: 623.97144 Test: 8.99603
Epoch 2320: New minimal relative error: 7.19%, model saved.
Epoch: 2320 Train: 567.14447 Test: 6.04382
Epoch: 2400 Train: 521.35413 Test: 6.16823
Epoch: 2480 Train: 464.67380 Test: 4.27809
Epoch: 2560 Train: 441.83145 Test: 3.85015
Epoch: 2640 Train: 415.70627 Test: 3.55018
Epoch: 2720 Train: 399.73123 Test: 3.38039
Epoch: 2800 Train: 368.26868 Test: 5.51920
Epoch 2880: New minimal relative error: 6.01%, model saved.
Epoch: 2880 Train: 341.44238 Test: 2.52402
Epoch: 2960 Train: 328.80432 Test: 2.40442
Epoch: 3040 Train: 298.69498 Test: 2.07399
Epoch: 3120 Train: 277.35226 Test: 3.35220
Epoch: 3200 Train: 266.53195 Test: 1.82176
Epoch: 3280 Train: 261.12268 Test: 4.77626
Epoch 3360: New minimal relative error: 5.21%, model saved.
Epoch: 3360 Train: 250.43192 Test: 1.52145
Epoch: 3440 Train: 236.49890 Test: 1.43571
Epoch: 3520 Train: 256.14752 Test: 1.91960
Epoch: 3600 Train: 311.19440 Test: 3.17709
Epoch: 3680 Train: 308.28363 Test: 3.33713
Epoch 3760: New minimal relative error: 5.05%, model saved.
Epoch: 3760 Train: 275.19302 Test: 3.08592
Epoch: 3840 Train: 286.70111 Test: 21.78688
Epoch: 3920 Train: 248.84209 Test: 1.97043
Epoch: 4000 Train: 214.96057 Test: 1.45320
Epoch: 4080 Train: 208.80220 Test: 1.44742
Epoch: 4160 Train: 200.68971 Test: 1.20116
Epoch: 4240 Train: 203.59792 Test: 5.56620
Epoch: 4320 Train: 198.06050 Test: 1.48058
Epoch: 4400 Train: 201.91107 Test: 1.50281
Epoch: 4480 Train: 200.15848 Test: 1.55872
Epoch: 4560 Train: 201.62939 Test: 1.46558
Epoch: 4640 Train: 184.75024 Test: 1.21802
Epoch: 4720 Train: 176.52477 Test: 1.04574
Epoch: 4800 Train: 166.75883 Test: 1.22093
Epoch: 4880 Train: 180.17085 Test: 2.04527
Epoch: 4960 Train: 176.63289 Test: 1.35730
Epoch: 5040 Train: 168.27147 Test: 1.07832
Epoch: 5120 Train: 169.99385 Test: 1.18079
Epoch: 5200 Train: 171.45090 Test: 1.33135
Epoch 5280: New minimal relative error: 4.39%, model saved.
Epoch: 5280 Train: 175.66971 Test: 1.24517
Epoch: 5360 Train: 182.53960 Test: 1.53179
Epoch: 5440 Train: 189.46707 Test: 1.89102
Epoch: 5520 Train: 180.92711 Test: 1.30723
Epoch: 5600 Train: 158.87726 Test: 0.98278
Epoch: 5680 Train: 158.63115 Test: 1.40737
Epoch: 5760 Train: 160.60915 Test: 1.12566
Epoch: 5840 Train: 158.06320 Test: 1.67703
Epoch: 5920 Train: 149.54402 Test: 0.93666
Epoch: 6000 Train: 147.28656 Test: 0.85195
Epoch: 6080 Train: 148.46759 Test: 0.73466
Epoch: 6160 Train: 150.55576 Test: 0.85143
Epoch: 6240 Train: 137.03664 Test: 0.74697
Epoch 6320: New minimal relative error: 4.13%, model saved.
Epoch: 6320 Train: 131.54575 Test: 0.65388
Epoch: 6400 Train: 130.60490 Test: 0.60208
Epoch: 6480 Train: 133.09583 Test: 0.65565
Epoch: 6560 Train: 128.02818 Test: 0.68258
Epoch 6640: New minimal relative error: 2.48%, model saved.
Epoch: 6640 Train: 124.22318 Test: 0.60580
Epoch: 6720 Train: 116.42281 Test: 0.65582
Epoch: 6800 Train: 114.82487 Test: 0.49378
Epoch: 6880 Train: 113.62835 Test: 0.50802
Epoch: 6960 Train: 118.30886 Test: 0.59700
Epoch: 7040 Train: 113.18053 Test: 0.62602
Epoch: 7120 Train: 110.64886 Test: 0.61529
Epoch: 7200 Train: 107.73730 Test: 0.50331
Epoch: 7280 Train: 104.19807 Test: 0.49569
Epoch: 7360 Train: 106.72162 Test: 0.55227
Epoch: 7440 Train: 107.86369 Test: 0.55299
Epoch: 7520 Train: 101.99671 Test: 0.71634
Epoch: 7600 Train: 100.23042 Test: 0.52065
Epoch: 7680 Train: 102.50131 Test: 0.73768
Epoch: 7760 Train: 98.59498 Test: 0.44650
Epoch: 7840 Train: 97.19836 Test: 0.44932
Epoch: 7920 Train: 98.10991 Test: 0.48574
Epoch: 7999 Train: 114.16790 Test: 0.62186
Training Loss: tensor(114.1679)
Test Loss: tensor(0.6219)
Learned LE: [ 8.6851430e-01 -1.2207375e-02 -1.4529455e+01]
True LE: [ 8.70241880e-01  3.42352944e-03 -1.45431595e+01]
Relative Error: [7.564995  7.514607  7.27096   6.882147  6.3496733 5.7010717 5.025518
 4.777964  4.657998  4.5087705 4.3585143 4.228997  4.1040015 4.023796
 4.15746   4.3813806 4.526066  4.9129753 5.1392155 5.4852624 6.1062975
 6.942603  7.688524  7.0783563 6.4544687 5.9846325 5.534323  5.2071905
 5.032536  5.030575  5.1112256 5.048177  5.099538  4.7477837 4.4073725
 4.0946198 3.6983562 3.4897373 3.5463617 3.45133   3.4570608 3.889156
 4.448742  4.9130907 5.244341  5.5084567 5.746141  5.903696  6.079278
 5.9604096 5.441842  4.960275  4.671231  4.6472754 4.925005  5.3565755
 5.824293  6.236369  6.5807195 6.91418   7.263573  7.648411  7.8302236
 7.787886  7.5368176 7.1283283 6.5452123 5.8415866 5.076873  4.609821
 4.341774  4.1159077 3.9231274 3.9506245 3.9491768 3.8395774 3.8940954
 4.0456033 4.1247997 4.4238133 4.6946416 5.04954   5.7079506 6.5935864
 6.7098846 6.080378  5.500026  5.1034193 4.7549987 4.4795923 4.330646
 4.3940325 4.414041  4.329535  4.359355  4.3514595 4.0112095 3.7307389
 3.5852644 3.3536148 3.1863284 3.118588  3.212901  3.672772  4.257303
 4.716506  5.022498  5.2213593 5.3517213 5.3818817 5.4451985 5.420631
 5.0741663 4.5481763 4.260541  4.328361  4.737451  5.243229  5.891743
 6.477042  6.929036  7.299594  7.6074867 7.960305  8.05267   7.9607897
 7.6956944 7.312961  6.7887387 5.889053  4.896408  4.3168044 3.9917576
 3.7103705 3.5208743 3.5425975 3.5624034 3.5184515 3.6857572 3.717262
 3.7417088 3.942021  4.209707  4.5914726 5.279277  5.989382  5.749532
 5.1954417 4.671407  4.300755  4.094676  3.8696368 3.7731524 3.8843672
 3.8480904 3.7694402 3.7719731 3.8459003 3.714284  3.464667  3.3761353
 3.152613  2.8700736 2.9025755 3.0238068 3.4641113 4.090916  4.5720277
 4.847828  4.9731793 5.0067463 4.910053  4.8278666 4.757354  4.5756207
 4.1895638 3.9179935 4.0250487 4.5054975 5.162485  5.9693475 6.8126626
 7.3918724 7.7942133 7.9479175 8.08251   8.199079  8.120528  7.8501463
 7.4486003 6.584511  5.6071887 4.7273107 4.146022  3.7455077 3.446914
 3.2192056 3.2777092 3.3132787 3.2157128 3.244626  3.3923469 3.410022
 3.4857764 3.672964  4.1129494 4.8337483 5.2423043 4.891368  4.4272532
 3.9607062 3.6251192 3.451363  3.3582401 3.3253975 3.4690454 3.4475725
 3.3951435 3.3420296 3.4321692 3.441307  3.2871344 3.1432438 2.807815
 2.654028  2.7136526 2.8610258 3.2502825 3.9455314 4.455574  4.7615523
 4.845784  4.7674704 4.5460963 4.3507576 4.2254686 3.9514554 3.8035047
 3.5912247 3.813196  4.358897  5.134169  6.0746646 7.068173  7.9302626
 8.174579  8.323291  8.390487  8.515098  8.459508  8.196581  7.51733
 6.5631094 5.522562  4.5350184 3.8083777 3.5529938 3.2829385 3.0568647
 3.1318142 3.2098298 3.135141  3.0118585 2.915817  3.0031228 3.0343835
 3.1234217 3.599     4.333805  4.4723086 4.1132793 3.7923715 3.312495
 3.05462   2.839837  2.6719558 2.6830568 2.855424  2.9468238 3.0824108
 3.179514  3.2461762 3.2369056 3.1835146 2.9392974 2.6040864 2.4839349
 2.5582292 2.7182543 3.0857592 3.7938144 4.4021626 4.7265177 4.776872
 4.606207  4.29301   3.926574  3.686816  3.3331926 3.195062  3.247906
 3.5454047 4.2113457 5.1447897 6.2210336 7.324338  8.123172  8.7114
 8.897624  9.017387  9.051003  9.039478  8.728466  7.835469  6.8453774
 5.748729  4.6290936 3.6554    3.0812259 2.9196289 2.8762932 2.9598496
 3.1403103 3.126131  3.0150156 2.7894456 2.5789394 2.584948  2.6327407
 2.9655044 3.7417722 3.7711735 3.4160013 3.2740088 2.799971  2.467534
 2.1407607 1.9863532 2.112359  2.3465004 2.4605236 2.6946745 2.8826942
 3.082535  3.2372105 3.2098625 2.8200831 2.50164   2.3077073 2.4463122
 2.5949514 2.9511235 3.548748  4.2103796 4.600331  4.698097  4.5709295
 4.182839  3.6444857 3.198143  2.7509565 2.483113  2.706368  3.2653465
 4.0609913 5.1712403 6.4088163 7.4251304 8.329089  9.077925  9.421503
 9.476317  9.509735  9.551379  9.272064  8.48436   7.4554324 6.3032775
 4.9578247 3.797794  2.8509927 2.5337334 2.5775733 2.7871284 3.0754685
 3.177811  3.058841  2.8589673 2.488848  2.1775312 2.197022  2.3977802
 3.0918477 3.1914878 2.8057542 2.6556952 2.3868158 1.8736897 1.5687647
 1.4443036]
