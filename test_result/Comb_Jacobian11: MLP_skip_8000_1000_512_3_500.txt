time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 500
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 100.00%, model saved.
Epoch: 0 Train: 32085.40430 Test: 3535.82959
Epoch 80: New minimal relative error: 91.37%, model saved.
Epoch: 80 Train: 8767.54395 Test: 1250.80872
Epoch: 160 Train: 8260.79297 Test: 1052.57129
Epoch 240: New minimal relative error: 62.93%, model saved.
Epoch: 240 Train: 7970.89404 Test: 1038.98206
Epoch: 320 Train: 7790.97656 Test: 930.23895
Epoch: 400 Train: 6190.75342 Test: 727.18842
Epoch: 480 Train: 4731.19189 Test: 512.85516
Epoch: 560 Train: 2820.95801 Test: 223.35330
Epoch 640: New minimal relative error: 49.71%, model saved.
Epoch: 640 Train: 1338.93018 Test: 74.39089
Epoch: 720 Train: 756.27008 Test: 51.53227
Epoch 800: New minimal relative error: 33.44%, model saved.
Epoch: 800 Train: 447.44214 Test: 16.53897
Epoch 880: New minimal relative error: 15.65%, model saved.
Epoch: 880 Train: 337.46494 Test: 12.58116
Epoch: 960 Train: 280.21243 Test: 9.80016
Epoch: 1040 Train: 320.87695 Test: 105.57056
Epoch 1120: New minimal relative error: 6.20%, model saved.
Epoch: 1120 Train: 206.97279 Test: 4.78293
Epoch: 1200 Train: 224.54553 Test: 16.76604
Epoch: 1280 Train: 173.17609 Test: 4.29866
Epoch 1360: New minimal relative error: 5.59%, model saved.
Epoch: 1360 Train: 154.04999 Test: 3.51325
Epoch: 1440 Train: 149.83022 Test: 5.27756
Epoch: 1520 Train: 139.22760 Test: 3.38421
Epoch: 1600 Train: 176.85898 Test: 55.60464
Epoch 1680: New minimal relative error: 5.34%, model saved.
Epoch: 1680 Train: 120.87840 Test: 2.34747
Epoch: 1760 Train: 115.58479 Test: 2.44433
Epoch: 1840 Train: 111.04570 Test: 3.81287
Epoch 1920: New minimal relative error: 4.75%, model saved.
Epoch: 1920 Train: 108.03462 Test: 2.09308
Epoch: 2000 Train: 103.10622 Test: 2.00306
Epoch: 2080 Train: 116.09956 Test: 16.50646
Epoch: 2160 Train: 96.40977 Test: 5.11097
Epoch: 2240 Train: 88.26720 Test: 1.82882
Epoch: 2320 Train: 87.20197 Test: 3.89053
Epoch 2400: New minimal relative error: 3.72%, model saved.
Epoch: 2400 Train: 83.45524 Test: 1.63921
Epoch: 2480 Train: 80.77277 Test: 1.34812
Epoch: 2560 Train: 81.07484 Test: 1.39426
Epoch: 2640 Train: 76.05339 Test: 1.64477
Epoch: 2720 Train: 72.82930 Test: 1.46003
Epoch: 2800 Train: 73.58537 Test: 1.96134
Epoch: 2880 Train: 73.70393 Test: 1.85760
Epoch: 2960 Train: 72.61252 Test: 1.42990
Epoch: 3040 Train: 70.01292 Test: 1.55739
Epoch: 3120 Train: 68.88569 Test: 1.27654
Epoch 3200: New minimal relative error: 1.78%, model saved.
Epoch: 3200 Train: 67.95075 Test: 1.17401
Epoch: 3280 Train: 67.74453 Test: 1.33328
Epoch: 3360 Train: 71.95907 Test: 6.25014
Epoch: 3440 Train: 65.29768 Test: 1.17302
Epoch: 3520 Train: 64.72505 Test: 5.62795
Epoch: 3600 Train: 59.44148 Test: 0.95417
Epoch: 3680 Train: 59.37048 Test: 1.09161
Epoch: 3760 Train: 57.24702 Test: 1.06595
Epoch: 3840 Train: 56.77650 Test: 1.09976
Epoch: 3920 Train: 56.51856 Test: 2.18846
Epoch: 4000 Train: 54.53601 Test: 1.14882
Epoch: 4080 Train: 51.20106 Test: 0.67067
Epoch: 4160 Train: 50.72945 Test: 0.67122
Epoch 4240: New minimal relative error: 1.42%, model saved.
Epoch: 4240 Train: 51.21280 Test: 0.67897
Epoch: 4320 Train: 51.07849 Test: 0.97957
Epoch: 4400 Train: 50.59486 Test: 0.70576
Epoch: 4480 Train: 50.35190 Test: 0.70784
Epoch: 4560 Train: 55.12288 Test: 1.71874
Epoch: 4640 Train: 51.64099 Test: 0.76791
Epoch: 4720 Train: 50.09421 Test: 0.76440
Epoch: 4800 Train: 49.80880 Test: 0.73824
Epoch: 4880 Train: 50.68618 Test: 0.79043
Epoch: 4960 Train: 50.76546 Test: 1.13343
Epoch: 5040 Train: 51.55034 Test: 1.05564
Epoch: 5120 Train: 49.57362 Test: 0.74357
Epoch: 5200 Train: 49.31245 Test: 0.82672
Epoch: 5280 Train: 49.78460 Test: 1.36163
Epoch: 5360 Train: 49.32460 Test: 1.24685
Epoch: 5440 Train: 48.82094 Test: 0.81549
Epoch: 5520 Train: 49.26656 Test: 1.09629
Epoch: 5600 Train: 47.90528 Test: 0.69619
Epoch: 5680 Train: 47.69477 Test: 0.82304
Epoch: 5760 Train: 45.81585 Test: 0.67450
Epoch: 5840 Train: 44.72886 Test: 0.60798
Epoch: 5920 Train: 43.86728 Test: 0.59943
Epoch: 6000 Train: 44.22053 Test: 0.56794
Epoch: 6080 Train: 45.30574 Test: 0.59104
Epoch: 6160 Train: 45.46841 Test: 0.70734
Epoch: 6240 Train: 45.16962 Test: 0.61261
Epoch: 6320 Train: 46.12623 Test: 1.10579
Epoch: 6400 Train: 46.74812 Test: 0.63345
Epoch: 6480 Train: 46.62462 Test: 0.69295
Epoch: 6560 Train: 47.04145 Test: 0.68975
Epoch: 6640 Train: 45.74867 Test: 0.84273
Epoch: 6720 Train: 43.91320 Test: 0.75828
Epoch: 6800 Train: 43.88408 Test: 1.25422
Epoch: 6880 Train: 42.19018 Test: 0.59471
Epoch: 6960 Train: 43.33443 Test: 0.63228
Epoch: 7040 Train: 42.18613 Test: 0.55591
Epoch: 7120 Train: 42.33161 Test: 0.53279
Epoch: 7200 Train: 41.46317 Test: 0.50449
Epoch: 7280 Train: 41.41286 Test: 0.79242
Epoch: 7360 Train: 40.05253 Test: 0.44809
Epoch: 7440 Train: 39.39386 Test: 0.44589
Epoch: 7520 Train: 39.50063 Test: 0.66102
Epoch: 7600 Train: 38.17336 Test: 0.45456
Epoch: 7680 Train: 36.94354 Test: 0.42660
Epoch: 7760 Train: 35.86404 Test: 0.40447
Epoch: 7840 Train: 36.53021 Test: 0.43595
Epoch: 7920 Train: 38.95928 Test: 0.48421
Epoch: 7999 Train: 38.56574 Test: 0.49710
Training Loss: tensor(38.5657)
Test Loss: tensor(0.4971)
Learned LE: [  0.9294994   -0.05538262 -14.54174   ]
True LE: [ 8.7361139e-01 -1.4432530e-03 -1.4543827e+01]
Relative Error: [0.27354473 0.3409563  0.50128627 0.8371661  1.3098546  1.5531185
 1.2825234  1.2880995  1.3220222  1.4846203  1.7380457  2.1969485
 2.2873654  1.8768245  1.1875539  0.9578657  1.04643    1.2082144
 1.2362891  0.59809124 0.7089429  1.2142414  1.3681369  1.1520661
 0.8481244  1.0827042  1.2477081  0.84886605 0.53220576 0.22208732
 0.05620634 0.21631175 0.36299595 0.13863802 0.2109208  0.55405635
 0.6529255  0.84688216 0.6332115  0.23228747 0.5486881  0.34764057
 0.8002559  1.4937493  1.7017063  1.3488244  0.6395476  0.9614454
 0.70958364 0.39558607 0.58252054 0.771495   0.95189065 0.7788546
 0.8205702  1.0538596  1.1064742  1.3278952  1.3462602  1.2276046
 0.7963474  0.36836022 0.108879   0.11548359 0.22756404 0.31719372
 0.3803181  0.8290371  1.3317144  1.3980204  1.3950462  1.3127489
 1.088871   1.031583   1.3320841  1.4351348  1.33861    1.4860268
 1.0116725  0.7208196  0.79689044 1.2297745  1.420886   0.651815
 0.7817714  0.97209245 1.0117723  0.90588576 1.218423   1.4036536
 0.8271915  0.53747714 0.24474409 0.10186975 0.30642655 0.26160356
 0.15604436 0.09755209 0.43527243 0.6473816  0.9024868  0.7892945
 0.4049371  0.40965444 0.7747192  0.3797439  1.0851024  1.3598175
 1.189034   0.7100135  0.5879977  1.0134981  0.34376898 0.5175346
 0.6612973  0.9443771  0.7499287  0.78488076 1.1402377  0.9993336
 1.1488266  1.4173882  1.2536697  0.858113   0.50961    0.24556509
 0.23166955 0.29925692 0.3179266  0.24764445 0.3201163  0.725559
 1.3227115  1.1564809  0.897735   0.69681036 0.6750934  0.7438203
 1.0124371  1.0146286  0.8477262  1.0526346  1.0279522  0.60092944
 0.6358865  1.0876856  1.6300626  0.65133286 0.59037685 0.9143496
 0.9554676  0.9601659  1.4421444  1.0337162  0.52806324 0.11751255
 0.1444763  0.150584   0.36361575 0.50965834 0.46716872 0.35218692
 0.5660566  0.9551744  1.1022285  0.96058416 0.55552715 0.38975295
 0.50345594 0.38524973 0.8427689  1.0089228  0.83845866 0.40181884
 0.5891677  0.6228473  0.3906083  0.6260956  0.5871023  0.8045717
 0.7017164  0.8942796  0.94174427 0.988017   1.2226801  1.1810247
 0.8639291  0.65366584 0.47870445 0.2512279  0.33441755 0.5863919
 0.666246   0.73638046 0.7738262  0.87639976 0.908565   0.8053366
 0.7797892  0.8263899  0.79576904 0.7485837  0.8272889  0.93419266
 0.824753   0.6674938  0.80808747 0.7858717  0.6123897  0.94847596
 1.3105029  0.70535964 0.24079308 0.60125905 0.5743385  0.6115103
 1.280391   0.65936095 0.5163832  0.23942481 0.48880726 0.2000255
 0.33372185 0.7157004  0.7422844  0.36380222 0.44147983 0.8989483
 1.0053517  0.95894665 0.6362657  0.4968957  0.3184239  0.30489007
 0.5512864  0.69026977 0.78042674 0.5133258  0.35288998 0.53318447
 0.4714619  0.641206   0.36361098 0.757743   0.75564396 1.0078292
 0.77904165 0.9439538  1.3840518  1.2453699  1.0938188  0.7090377
 0.44741476 0.48872083 0.7774312  0.8677071  1.0450513  1.0591493
 0.9616421  0.88760173 0.6853085  0.42302454 0.7016309  0.87394446
 0.84569436 0.7830212  0.67235124 0.82623523 0.7316303  0.6771527
 0.3220625  0.46049473 0.6760106  0.7311145  0.6557034  0.6080456
 0.5676492  0.50727326 0.6254962  0.37467867 0.6769549  0.3496187
 0.52056813 0.4602366  0.09108274 0.55391693 0.7537023  0.87327695
 0.6934784  0.3624201  0.24324378 0.7620334  0.8298665  0.9832747
 0.79577875 0.51353043 0.31727046 0.21256962 0.40999073 0.41274628
 0.63166267 0.7685885  0.73073924 0.295143   0.36640805 0.6079634
 0.47502416 0.5039815  0.6704377  0.78603745 0.50840795 0.6390243
 1.006623   1.1771106  1.1957388  0.8580138  0.6241335  0.70795935
 0.8027575  0.5944626  0.47351146 0.44033596 0.6262197  1.0860982
 1.2562528  1.2600058  0.9338462  0.8857249  0.87605876 0.87618595
 0.73925567 0.6241694  0.5465665  0.70611817 0.41376558 0.30178508
 0.22818509 0.3872946  0.39449245 0.36498758 0.36043906 0.33267945
 0.36379623 0.42606044 0.39710718 0.39078408 0.24740948 0.788012
 0.6974975  0.02935273 0.64678895 0.6540852  0.76551193 0.06058694
 0.44107357 0.55277145 0.8901642  1.0776726  1.1178269  0.86263174
 0.66870785 0.36564893 0.4639786  0.47022352 0.42621973 0.5083226
 0.7670161  1.0038602  0.48303652 0.24043702 0.45796722 0.4679739
 0.35360143 0.53466207 0.5561501  0.30525944 0.50193465 0.7692776
 1.138382   0.79562765 0.7304752  0.8918169  1.0139546  0.51821035
 0.4879304  0.42656717 0.1682022  0.33284295 0.68772876 1.1520188
 1.191389   0.9784043  1.0226132  0.9477312  0.8434306  0.807089
 0.5029566  0.5024044  0.46645296 0.67025733]
