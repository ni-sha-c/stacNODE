time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 5
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.47%, model saved.
Epoch: 0 Train: 60785.43750 Test: 4027.20654
Epoch: 100 Train: 15169.77637 Test: 1733.20471
Epoch 200: New minimal relative error: 92.22%, model saved.
Epoch: 200 Train: 14622.48242 Test: 1246.61462
Epoch: 300 Train: 13546.15527 Test: 1261.77710
Epoch: 400 Train: 11517.83789 Test: 1094.61060
Epoch 500: New minimal relative error: 65.96%, model saved.
Epoch: 500 Train: 11292.39160 Test: 1032.23279
Epoch 600: New minimal relative error: 52.81%, model saved.
Epoch: 600 Train: 10199.34863 Test: 817.56897
Epoch: 700 Train: 10360.79004 Test: 738.57446
Epoch 800: New minimal relative error: 48.76%, model saved.
Epoch: 800 Train: 9069.95215 Test: 619.92755
Epoch: 900 Train: 8401.56152 Test: 555.91028
Epoch: 1000 Train: 5147.26270 Test: 209.12033
Epoch: 1100 Train: 2260.70386 Test: 73.48870
Epoch 1200: New minimal relative error: 38.05%, model saved.
Epoch: 1200 Train: 2257.66650 Test: 82.81814
Epoch: 1300 Train: 1682.05151 Test: 35.55235
Epoch 1400: New minimal relative error: 11.77%, model saved.
Epoch: 1400 Train: 1090.39758 Test: 16.32174
Epoch: 1500 Train: 788.21228 Test: 10.46335
Epoch: 1600 Train: 724.08319 Test: 11.72609
Epoch: 1700 Train: 678.85406 Test: 13.58471
Epoch: 1800 Train: 650.44098 Test: 17.33931
Epoch: 1900 Train: 638.55042 Test: 15.50488
Epoch: 2000 Train: 432.91397 Test: 4.70864
Epoch: 2100 Train: 410.94131 Test: 9.18383
Epoch 2200: New minimal relative error: 9.93%, model saved.
Epoch: 2200 Train: 354.63617 Test: 4.37286
Epoch: 2300 Train: 372.92102 Test: 24.16475
Epoch: 2400 Train: 450.39349 Test: 21.92163
Epoch: 2500 Train: 367.43369 Test: 6.79807
Epoch: 2600 Train: 331.69211 Test: 15.22706
Epoch 2700: New minimal relative error: 5.86%, model saved.
Epoch: 2700 Train: 332.43332 Test: 4.41576
Epoch: 2800 Train: 281.99023 Test: 2.93658
Epoch: 2900 Train: 316.10028 Test: 7.05102
Epoch: 3000 Train: 251.71367 Test: 4.88634
Epoch: 3100 Train: 236.79665 Test: 3.84337
Epoch: 3200 Train: 224.97853 Test: 2.70406
Epoch: 3300 Train: 196.15504 Test: 1.25112
Epoch: 3400 Train: 185.72560 Test: 2.24787
Epoch: 3500 Train: 182.90605 Test: 0.95277
Epoch 3600: New minimal relative error: 3.52%, model saved.
Epoch: 3600 Train: 247.27669 Test: 5.52033
Epoch: 3700 Train: 213.40651 Test: 1.57852
Epoch: 3800 Train: 184.90404 Test: 1.88329
Epoch 3900: New minimal relative error: 2.98%, model saved.
Epoch: 3900 Train: 179.24596 Test: 1.35691
Epoch: 4000 Train: 165.08380 Test: 1.25477
Epoch: 4100 Train: 201.47208 Test: 1.90791
Epoch: 4200 Train: 205.32465 Test: 14.73897
Epoch: 4300 Train: 152.61809 Test: 0.87996
Epoch: 4400 Train: 147.22395 Test: 1.00190
Epoch: 4500 Train: 183.59210 Test: 3.44556
Epoch: 4600 Train: 155.03674 Test: 1.13570
Epoch: 4700 Train: 160.54277 Test: 2.69965
Epoch: 4800 Train: 164.21625 Test: 1.08595
Epoch: 4900 Train: 146.68283 Test: 2.01245
Epoch: 5000 Train: 127.23907 Test: 0.99576
Epoch: 5100 Train: 128.01468 Test: 1.21800
Epoch: 5200 Train: 119.05315 Test: 1.64099
Epoch: 5300 Train: 104.46368 Test: 0.60508
Epoch: 5400 Train: 103.76069 Test: 0.41095
Epoch: 5500 Train: 102.57373 Test: 0.50207
Epoch: 5600 Train: 90.99545 Test: 0.92861
Epoch 5700: New minimal relative error: 2.71%, model saved.
Epoch: 5700 Train: 92.28932 Test: 0.41922
Epoch: 5800 Train: 111.86785 Test: 1.41634
Epoch: 5900 Train: 116.95967 Test: 1.18738
Epoch: 6000 Train: 98.04948 Test: 0.71300
Epoch: 6100 Train: 91.47261 Test: 1.16464
Epoch: 6200 Train: 98.65386 Test: 1.14727
Epoch 6300: New minimal relative error: 2.57%, model saved.
Epoch: 6300 Train: 98.98647 Test: 0.78744
Epoch: 6400 Train: 88.42786 Test: 0.51497
Epoch: 6500 Train: 88.17034 Test: 0.58735
Epoch: 6600 Train: 92.17720 Test: 0.85445
Epoch: 6700 Train: 87.73301 Test: 1.48698
Epoch: 6800 Train: 92.74641 Test: 0.94344
Epoch: 6900 Train: 101.17941 Test: 0.74640
Epoch: 7000 Train: 86.80685 Test: 0.71161
Epoch 7100: New minimal relative error: 2.00%, model saved.
Epoch: 7100 Train: 80.47108 Test: 0.39963
Epoch: 7200 Train: 84.78757 Test: 0.48355
Epoch: 7300 Train: 76.82585 Test: 0.31029
Epoch: 7400 Train: 75.88035 Test: 0.32868
Epoch: 7500 Train: 73.87959 Test: 0.26018
Epoch: 7600 Train: 75.06233 Test: 0.32211
Epoch: 7700 Train: 72.53841 Test: 0.29692
Epoch: 7800 Train: 82.46642 Test: 0.42265
Epoch: 7900 Train: 72.49614 Test: 0.28112
Epoch: 8000 Train: 69.15395 Test: 0.45258
Epoch: 8100 Train: 64.27993 Test: 0.77551
Epoch: 8200 Train: 63.87466 Test: 0.22976
Epoch: 8300 Train: 62.40136 Test: 0.23426
Epoch: 8400 Train: 67.01265 Test: 0.36918
Epoch: 8500 Train: 67.35447 Test: 0.30547
Epoch: 8600 Train: 74.62673 Test: 0.75322
Epoch: 8700 Train: 70.31658 Test: 0.33710
Epoch: 8800 Train: 67.31006 Test: 0.41055
Epoch: 8900 Train: 69.06313 Test: 0.52845
Epoch: 9000 Train: 75.09940 Test: 0.65089
Epoch: 9100 Train: 76.54366 Test: 0.60168
Epoch: 9200 Train: 80.21813 Test: 0.71372
Epoch: 9300 Train: 79.83749 Test: 0.36548
Epoch 9400: New minimal relative error: 1.74%, model saved.
Epoch: 9400 Train: 80.35040 Test: 0.54425
Epoch: 9500 Train: 77.65056 Test: 0.44743
Epoch: 9600 Train: 75.31529 Test: 0.39954
Epoch: 9700 Train: 70.24056 Test: 0.37401
Epoch: 9800 Train: 74.22333 Test: 0.47447
Epoch: 9900 Train: 84.86419 Test: 0.41187
Epoch: 9999 Train: 74.58838 Test: 0.36931
Training Loss: tensor(74.5884)
Test Loss: tensor(0.3693)
Learned LE: [ 8.7980449e-01 -8.7879999e-03 -1.4550254e+01]
True LE: [ 8.7599558e-01 -1.8137411e-04 -1.4552431e+01]
Relative Error: [3.7238877 3.413779  3.2459993 3.329118  3.451778  3.6745772 3.9165668
 4.184728  4.177335  4.3536434 4.327741  4.0665836 3.6204598 3.4153864
 3.2217531 3.0424542 3.1552916 3.5048645 3.551962  3.2484975 2.8419476
 2.5361083 2.4672356 2.466476  2.3820474 2.1588755 1.9859878 2.1878998
 2.7493742 3.317904  3.8127809 3.8542163 4.019583  4.2429805 4.1321864
 4.0463185 4.043087  3.9481442 3.9900768 3.9758344 4.044467  4.2178526
 4.5402846 4.9064217 5.362201  5.691009  5.8550415 5.9771824 6.189015
 6.131118  5.9522114 5.8031964 5.633278  5.395387  5.4310513 5.5599933
 5.5562153 5.836136  5.545279  5.0460024 4.382978  3.8608086 3.4677415
 3.2203398 2.9354012 2.9264956 3.0880566 3.2311532 3.4856994 3.5263948
 3.7215095 3.7487738 3.9508924 3.759061  3.392546  3.005741  2.836246
 2.9353004 2.875281  3.0566657 3.3091242 3.0216844 2.866221  2.5298727
 2.390101  2.359949  2.285382  2.011621  1.8214782 2.004701  2.4833763
 3.1036572 3.4832983 3.470003  3.6960852 3.8078215 3.622441  3.6614597
 3.5213325 3.4403915 3.4375381 3.4207716 3.422959  3.4733605 3.7297091
 3.9761264 4.4180737 4.934404  5.3494797 5.5307565 5.6843867 5.7542744
 5.6510286 5.4339585 5.204816  5.053506  4.947205  5.089363  5.1660995
 5.344122  5.4768953 4.9259033 4.2620616 3.7031114 3.2667403 2.932402
 2.7565386 2.6041608 2.5650103 2.6800785 2.7881327 2.8309681 2.9451237
 3.1829052 3.4879978 3.5577319 3.1704123 2.8443818 2.4876025 2.4633522
 2.6927505 2.7433708 2.8789518 2.9442165 2.6867316 2.570758  2.3517737
 2.2863224 2.1842859 1.8967522 1.6467443 1.7467889 2.1558213 2.752458
 3.066622  3.2271926 3.4171298 3.6379855 3.2628114 3.0438006 3.0784469
 3.0492854 3.0866191 3.0726502 3.1147852 3.082573  3.0653539 3.3029106
 3.5748188 3.931861  4.329898  4.8309855 5.2552404 5.392291  5.3435726
 5.2754855 4.979242  4.604403  4.480324  4.538643  4.674773  4.8209
 5.100037  4.9001384 4.3097215 3.6243424 3.1443455 2.7938046 2.57043
 2.3442209 2.233419  2.270948  2.2511342 2.2573698 2.3214395 2.5199938
 2.7652013 3.1259165 3.287234  2.7839484 2.424971  2.1776807 2.208958
 2.447953  2.6001968 2.6361663 2.6694005 2.478346  2.3479815 2.1360488
 2.0903661 1.8191668 1.4865161 1.4797903 1.7560523 2.2668362 2.5132859
 2.848819  3.0855577 3.4326627 3.308254  2.832036  2.5529366 2.7404344
 2.796873  2.8222861 2.726991  2.752198  2.7500148 2.7956035 2.9982634
 3.0905254 3.367785  3.814048  4.218787  4.7147303 5.0300727 5.0102396
 4.964874  4.5893803 4.117708  3.944112  4.092599  4.2277837 4.4830317
 4.739527  4.3938046 3.7190523 3.0584347 2.6997173 2.5167751 2.2349648
 2.0055637 1.9904025 1.9078394 1.8556267 1.8878052 1.9907418 2.1206424
 2.3452797 2.664501  2.969794  2.6126547 2.222934  2.0027337 2.026199
 2.250195  2.500408  2.551827  2.4654415 2.2038395 2.1045086 1.9373542
 1.8830905 1.4779545 1.2527133 1.272295  1.6690122 2.0082016 2.2283626
 2.657682  3.0065293 3.148982  2.8356352 2.5726917 2.2467444 2.2954073
 2.5377219 2.6179392 2.5526972 2.5412185 2.591223  2.665969  2.6392765
 2.580479  2.8928893 3.3515952 3.6851716 3.9713728 4.5605283 4.7559404
 4.6693497 4.300022  3.706458  3.4447842 3.5056229 3.7434134 4.1050277
 4.381805  3.9460175 3.2466147 2.6703472 2.3959143 2.2240162 2.0392227
 1.8341459 1.8227147 1.6287727 1.5948293 1.6088986 1.5699781 1.6838676
 1.9058529 2.1494308 2.4186351 2.4153697 2.2170317 2.0571413 1.9687902
 2.0997093 2.4145079 2.5317144 2.3664439 2.0923223 1.9540521 1.897795
 1.798277  1.3019745 1.0383413 1.088387  1.490649  1.5472068 1.9233564
 2.460232  2.8305864 2.766137  2.495162  2.2386053 2.0550385 1.9419808
 2.1896884 2.4565344 2.5043085 2.4584825 2.3833914 2.2961018 2.1962626
 2.089938  2.2418659 2.7165895 3.1412494 3.3835237 3.760726  4.368332
 4.5062675 4.2026253 3.493158  3.0261424 2.8837032 3.1764462 3.5199358
 4.0103087 3.7292063 2.9903555 2.4401593 2.2386496 2.0373535 1.867149
 1.7842656 1.687641  1.5378728 1.4945755 1.3745807 1.3332205 1.4176223
 1.6428517 1.8248181 2.0738592 2.0709808 2.1439176 2.002106  2.0971115
 2.1890712 2.3705335 2.5835185 2.4776514 2.1145332 2.0130982 1.831014
 1.7872953]
