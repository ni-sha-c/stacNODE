time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 0
loss_type: MSE
dyn_sys: lorenz
model_type: MLP
n_hidden: 1024
n_layers: 3
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_MSE/
Epoch 0: New minimal relative error: 99.39%, model saved.
Epoch: 0 Train: 3748.24902 Test: 4047.86597
Epoch 80: New minimal relative error: 16.42%, model saved.
Epoch: 80 Train: 49.33078 Test: 38.79800
Epoch: 160 Train: 28.61834 Test: 25.06478
Epoch 240: New minimal relative error: 8.59%, model saved.
Epoch: 240 Train: 5.05401 Test: 3.57374
Epoch: 320 Train: 3.43582 Test: 2.31787
Epoch: 400 Train: 2.97812 Test: 2.14487
Epoch 480: New minimal relative error: 7.85%, model saved.
Epoch: 480 Train: 1.88675 Test: 1.20901
Epoch: 560 Train: 1.58955 Test: 0.98748
Epoch: 640 Train: 1.10558 Test: 0.67459
Epoch: 720 Train: 0.90800 Test: 0.67711
Epoch 800: New minimal relative error: 5.45%, model saved.
Epoch: 800 Train: 1.02427 Test: 0.43597
Epoch: 880 Train: 2.27631 Test: 2.38687
Epoch: 960 Train: 5.49180 Test: 5.14036
Epoch: 1040 Train: 1.57559 Test: 1.33634
Epoch: 1120 Train: 0.51894 Test: 0.34055
Epoch: 1200 Train: 0.82577 Test: 0.87318
Epoch: 1280 Train: 2.81586 Test: 3.07003
Epoch: 1360 Train: 0.53973 Test: 0.50032
Epoch: 1440 Train: 2.96501 Test: 3.11435
Epoch: 1520 Train: 0.56876 Test: 0.58675
Epoch: 1600 Train: 2.27047 Test: 2.79452
Epoch: 1680 Train: 0.96069 Test: 1.02400
Epoch: 1760 Train: 0.30006 Test: 0.33177
Epoch: 1840 Train: 1.25510 Test: 1.64185
Epoch: 1920 Train: 0.59983 Test: 0.67877
Epoch: 2000 Train: 1.06291 Test: 1.12577
Epoch: 2080 Train: 0.68348 Test: 0.71307
Epoch: 2160 Train: 0.22590 Test: 0.21063
Epoch: 2240 Train: 0.31544 Test: 0.29437
Epoch: 2320 Train: 0.54113 Test: 0.60146
Epoch: 2400 Train: 0.33187 Test: 0.42899
Epoch: 2480 Train: 0.22886 Test: 0.25868
Epoch: 2560 Train: 1.50729 Test: 1.75764
Epoch: 2640 Train: 0.14207 Test: 0.13692
Epoch: 2720 Train: 0.77805 Test: 0.86345
Epoch: 2800 Train: 0.55303 Test: 0.59205
Epoch: 2880 Train: 1.34779 Test: 1.22123
Epoch: 2960 Train: 0.08510 Test: 0.11517
Epoch: 3040 Train: 0.21461 Test: 0.22331
Epoch: 3120 Train: 2.83309 Test: 3.00431
Epoch: 3200 Train: 0.10904 Test: 0.12030
Epoch 3280: New minimal relative error: 5.10%, model saved.
Epoch: 3280 Train: 0.68587 Test: 0.74760
Epoch: 3360 Train: 0.61062 Test: 0.47894
Epoch: 3440 Train: 0.28077 Test: 0.33085
Epoch: 3520 Train: 0.66560 Test: 0.46746
Epoch: 3600 Train: 0.10454 Test: 0.11869
Epoch: 3680 Train: 0.12545 Test: 0.13555
Epoch: 3760 Train: 0.28371 Test: 0.30097
Epoch: 3840 Train: 0.47000 Test: 0.30152
Epoch: 3920 Train: 0.21801 Test: 0.21488
Epoch: 4000 Train: 0.13886 Test: 0.15208
Epoch: 4080 Train: 0.48915 Test: 0.71059
Epoch: 4160 Train: 0.05539 Test: 0.06974
Epoch: 4240 Train: 0.46084 Test: 0.48717
Epoch: 4320 Train: 0.19193 Test: 0.21727
Epoch: 4400 Train: 0.09646 Test: 0.12092
Epoch: 4480 Train: 0.08993 Test: 0.10245
Epoch 4560: New minimal relative error: 4.05%, model saved.
Epoch: 4560 Train: 0.30464 Test: 0.35994
Epoch: 4640 Train: 0.09988 Test: 0.11887
Epoch: 4720 Train: 0.04323 Test: 0.05860
Epoch: 4800 Train: 0.07173 Test: 0.09279
Epoch: 4880 Train: 0.07871 Test: 0.08837
Epoch 4960: New minimal relative error: 2.05%, model saved.
Epoch: 4960 Train: 0.05828 Test: 0.07419
Epoch: 5040 Train: 0.09402 Test: 0.09695
Epoch: 5120 Train: 0.20394 Test: 0.20683
Epoch: 5200 Train: 0.16962 Test: 0.18972
Epoch: 5280 Train: 0.09079 Test: 0.07495
Epoch: 5360 Train: 0.20474 Test: 0.22831
Epoch: 5440 Train: 0.16576 Test: 0.24081
Epoch: 5520 Train: 0.45577 Test: 0.48815
Epoch: 5600 Train: 0.09198 Test: 0.11118
Epoch: 5680 Train: 0.18960 Test: 0.22627
Epoch: 5760 Train: 0.07761 Test: 0.11207
Epoch: 5840 Train: 0.05759 Test: 0.07589
Epoch: 5920 Train: 0.06435 Test: 0.07349
Epoch: 6000 Train: 0.08277 Test: 0.08711
Epoch: 6080 Train: 0.29739 Test: 0.33801
Epoch: 6160 Train: 0.13883 Test: 0.14361
Epoch: 6240 Train: 0.06352 Test: 0.07328
Epoch: 6320 Train: 0.04645 Test: 0.06220
Epoch: 6400 Train: 0.08529 Test: 0.10842
Epoch: 6480 Train: 0.06607 Test: 0.08194
Epoch: 6560 Train: 0.02867 Test: 0.04727
Epoch: 6640 Train: 0.04602 Test: 0.06791
Epoch: 6720 Train: 0.02748 Test: 0.04444
Epoch: 6800 Train: 0.07588 Test: 0.09319
Epoch: 6880 Train: 0.09242 Test: 0.12499
Epoch: 6960 Train: 0.16120 Test: 0.14018
Epoch: 7040 Train: 0.39681 Test: 0.32190
Epoch: 7120 Train: 0.04137 Test: 0.04279
Epoch: 7200 Train: 0.12822 Test: 0.16100
Epoch: 7280 Train: 0.07844 Test: 0.08702
Epoch: 7360 Train: 0.03149 Test: 0.05014
Epoch: 7440 Train: 0.02691 Test: 0.04434
Epoch: 7520 Train: 0.05153 Test: 0.06307
Epoch: 7600 Train: 0.10163 Test: 0.12527
Epoch: 7680 Train: 0.09465 Test: 0.09873
Epoch: 7760 Train: 0.02336 Test: 0.04384
Epoch: 7840 Train: 0.14339 Test: 0.13950
Epoch: 7920 Train: 0.02999 Test: 0.04719
Epoch: 7999 Train: 0.04752 Test: 0.07322
Training Loss: tensor(0.0475)
Test Loss: tensor(0.0732)
Learned LE: [ 0.884847   -0.01984864 -5.549759  ]
True LE: [  0.8511634    0.01776594 -14.549056  ]
Relative Error: [1.8054309  1.6941022  1.5997549  1.6033301  1.7746409  1.9856805
 2.0280766  1.9771159  2.109932   2.5446675  2.8350825  2.5266652
 2.1864161  2.1684363  2.1381967  2.0371284  2.1210344  1.9983507
 1.7807835  2.076962   2.6368089  3.5178776  4.287208   4.746643
 5.0556397  5.322905   5.4851017  5.427485   5.1857233  4.9903336
 5.680483   6.359332   6.885281   7.2109995  7.2860994  7.1572194
 6.8991184  6.593409   6.5042205  6.713351   7.026305   7.277237
 7.371481   7.1075106  6.9606977  6.492737   5.87474    4.7001114
 3.6957927  3.004338   2.602774   2.2866015  2.0196986  1.9544945
 2.123764   2.3736055  2.3950002  2.1853127  2.0058012  1.8591022
 1.7319634  1.6185367  1.5193577  1.4277521  1.3333051  1.2815988
 1.3183903  1.4774448  1.7023454  1.7719477  1.6701841  1.7208593
 2.1379578  2.4784539  2.150519   1.770011   1.7984351  1.8411951
 1.994206   1.9822214  1.2796928  1.4557946  1.9606037  2.6790068
 3.569582   4.356247   5.045309   5.6456594  6.0657625  6.160073
 5.9389243  5.6266403  5.2184267  4.9196     5.4366803  5.8955526
 6.2300963  6.2883177  6.167517   5.9818463  5.734073   5.6028376
 5.6642795  5.9474535  6.134208   6.1597967  5.8419185  5.749689
 5.4296637  4.905117   3.7561324  2.8998144  2.3342154  1.9996198
 1.7357574  1.4741635  1.3978138  1.5821155  1.8738372  1.91506
 1.7129103  1.5602953  1.435888   1.3351996  1.2347931  1.1463032
 1.0773547  1.0115632  0.99930435 1.04233    1.124574   1.2850044
 1.4222808  1.350751   1.2755996  1.5833546  2.1012473  1.9697425
 1.4092971  1.3654108  1.630497   2.2446995  1.7621233  0.98883295
 1.1689612  1.8551501  2.6428833  3.3804297  4.103985   4.961851
 5.796046   6.383945   6.508556   6.18004    5.7636213  5.341164
 4.9770703  4.5112514  4.7907696  4.9388523  5.039357   5.0500817
 4.9831333  4.9231205  4.7558384  4.647206   4.762589   4.9583445
 5.0051427  4.682382   4.4798183  4.3710318  4.0588884  3.1926193
 2.4323723  1.9024551  1.5819699  1.3529211  1.0769866  0.9090394
 1.0004604  1.3092095  1.4533019  1.2473139  1.1195683  1.0686452
 1.0256472  0.9284625  0.80834997 0.75350934 0.71351516 0.7083234
 0.7491377  0.7937595  0.8330504  0.93353915 1.0161831  0.93486047
 0.99501383 1.5613909  1.975827   1.4074656  0.8299106  1.3086628
 2.2675226  1.8535508  1.1435652  1.0233351  1.7108843  2.4521983
 3.1481397  3.8105521  4.552477   5.4117346  6.06468    6.337209
 6.1950045  5.808169   5.3151703  4.9105363  4.5587516  3.918839
 3.9275684  3.80886    3.691858   3.7869024  3.943873   3.9012947
 3.687416   3.5369844  3.6301374  3.7751794  3.6374156  3.3209455
 3.3196156  3.1349823  2.7732909  2.1589105  1.6602257  1.2727389
 1.1247386  0.9663961  0.78672385 0.6861085  0.8087688  1.0674744
 0.93985933 0.7458639  0.81891984 0.88482845 0.8447692  0.6327343
 0.5001447  0.46363184 0.43982422 0.43822116 0.47632387 0.51171416
 0.51295966 0.55797774 0.7064282  0.73453814 0.8542195  1.4514816
 1.7393589  0.84069985 0.5647362  1.6662413  1.6774211  1.1912093
 0.9176353  1.4411875  2.0245488  2.637386   3.2185793  3.780153
 4.4705205  5.300944   5.853304   5.8803277  5.49941    5.2144885
 4.7031813  4.3024087  3.964867   3.381079   2.7661893  2.791892
 2.6375377  2.5733948  2.7563627  2.7719877  2.5117831  2.3157866
 2.2805595  2.4368064  2.3063116  2.0949695  2.259455   2.0693243
 1.8463701  1.4490845  1.0880725  0.7949995  0.7476242  0.7560619
 0.7232937  0.58675164 0.6152304  0.7776169  0.5238166  0.4618392
 0.6267497  0.72394603 0.678685   0.46290672 0.37919542 0.34668723
 0.2735807  0.22422476 0.23477443 0.27184004 0.27969337 0.2898281
 0.50462013 0.7333565  0.66824526 0.8114338  1.2030458  0.6763826
 0.43788245 1.1800153  0.66113883 0.6164833  0.9622326  1.523262
 2.0095718  2.565369   2.9194963  3.494945   3.9817069  4.1818023
 4.2906876  4.354695   3.987278   3.573351   3.4587305  3.2406359
 3.0634131  2.764147   2.2981887  1.7504014  1.7666135  1.6020544
 1.4622333  1.4973917  1.4954592  1.2019607  1.0364618  1.0623639
 1.2027026  0.96445817 1.028852   1.2587942  1.1533831  0.953846
 0.7132605  0.5148039  0.44597542 0.5841981  0.5884039  0.46376228
 0.39752108 0.54649955 0.4206627  0.33015913 0.40090322 0.44162652
 0.4542247  0.33255967 0.27777562 0.319796   0.3187366  0.2573676
 0.1618576  0.14811754 0.18912996 0.24555938 0.34460887 0.5424335
 0.6214579  0.3204731  0.35469106 0.7115468  0.43716565 0.4209688
 0.258074   0.7591109  0.9946376  1.4183707 ]
