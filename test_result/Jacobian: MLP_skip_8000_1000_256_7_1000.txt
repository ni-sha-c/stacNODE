time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 256
n_layers: 7
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 103.31%, model saved.
Epoch: 0 Train: 60430.35156 Test: 3856.01587
Epoch 80: New minimal relative error: 93.16%, model saved.
Epoch: 80 Train: 17233.09570 Test: 1645.49756
Epoch: 160 Train: 15828.80078 Test: 1373.56311
Epoch 240: New minimal relative error: 59.71%, model saved.
Epoch: 240 Train: 15348.65234 Test: 1503.10461
Epoch: 320 Train: 15195.24316 Test: 1244.51794
Epoch: 400 Train: 13552.32324 Test: 1245.22827
Epoch: 480 Train: 15623.90234 Test: 1250.18433
Epoch: 560 Train: 13724.35742 Test: 1428.71204
Epoch: 640 Train: 15252.98730 Test: 1303.93872
Epoch: 720 Train: 13946.53125 Test: 1185.84473
Epoch: 800 Train: 15390.66992 Test: 1349.03345
Epoch: 880 Train: 16670.26758 Test: 1346.59839
Epoch: 960 Train: 14056.87988 Test: 1223.36609
Epoch: 1040 Train: 13004.84766 Test: 1013.27539
Epoch: 1120 Train: 12725.69141 Test: 920.22369
Epoch: 1200 Train: 11123.80762 Test: 757.32318
Epoch: 1280 Train: 9759.02344 Test: 672.42755
Epoch: 1360 Train: 9389.71289 Test: 738.56427
Epoch: 1440 Train: 9059.24609 Test: 860.34241
Epoch: 1520 Train: 7405.33447 Test: 390.97488
Epoch: 1600 Train: 6016.16455 Test: 257.32758
Epoch 1680: New minimal relative error: 16.12%, model saved.
Epoch: 1680 Train: 4122.30176 Test: 296.63846
Epoch: 1760 Train: 2940.86572 Test: 201.61545
Epoch: 1840 Train: 1451.32117 Test: 27.20682
Epoch: 1920 Train: 1220.59485 Test: 33.52753
Epoch: 2000 Train: 959.44830 Test: 12.70982
Epoch: 2080 Train: 813.60938 Test: 10.10567
Epoch: 2160 Train: 773.16278 Test: 8.67320
Epoch: 2240 Train: 673.15015 Test: 11.21893
Epoch: 2320 Train: 615.58728 Test: 6.08392
Epoch: 2400 Train: 518.18860 Test: 4.84773
Epoch: 2480 Train: 522.25085 Test: 6.87781
Epoch: 2560 Train: 463.66309 Test: 5.40039
Epoch: 2640 Train: 449.77972 Test: 3.87098
Epoch: 2720 Train: 410.15829 Test: 3.83604
Epoch 2800: New minimal relative error: 12.43%, model saved.
Epoch: 2800 Train: 376.30710 Test: 3.02875
Epoch 2880: New minimal relative error: 8.23%, model saved.
Epoch: 2880 Train: 342.37173 Test: 3.41637
Epoch: 2960 Train: 312.48941 Test: 2.36520
Epoch: 3040 Train: 284.76956 Test: 6.31292
Epoch: 3120 Train: 275.02075 Test: 1.82191
Epoch: 3200 Train: 292.90216 Test: 2.87106
Epoch: 3280 Train: 263.82605 Test: 1.70868
Epoch: 3360 Train: 258.33310 Test: 6.15396
Epoch: 3440 Train: 236.25392 Test: 1.35603
Epoch: 3520 Train: 211.48328 Test: 1.24684
Epoch 3600: New minimal relative error: 6.61%, model saved.
Epoch: 3600 Train: 203.17445 Test: 1.25031
Epoch: 3680 Train: 200.75055 Test: 1.60219
Epoch: 3760 Train: 192.04689 Test: 1.34400
Epoch: 3840 Train: 199.07852 Test: 1.67794
Epoch: 3920 Train: 187.18434 Test: 1.26798
Epoch: 4000 Train: 197.97350 Test: 2.30603
Epoch: 4080 Train: 191.37083 Test: 1.32893
Epoch 4160: New minimal relative error: 5.08%, model saved.
Epoch: 4160 Train: 200.94931 Test: 1.31754
Epoch: 4240 Train: 191.49850 Test: 1.27684
Epoch 4320: New minimal relative error: 4.44%, model saved.
Epoch: 4320 Train: 195.18272 Test: 1.26224
Epoch: 4400 Train: 203.87904 Test: 1.30246
Epoch: 4480 Train: 190.35637 Test: 1.20910
Epoch: 4560 Train: 184.09450 Test: 1.24441
Epoch: 4640 Train: 183.56595 Test: 1.31877
Epoch: 4720 Train: 195.47766 Test: 1.39139
Epoch: 4800 Train: 194.58035 Test: 1.33343
Epoch: 4880 Train: 200.87715 Test: 1.54734
Epoch: 4960 Train: 212.57654 Test: 2.81733
Epoch: 5040 Train: 199.60574 Test: 1.55922
Epoch: 5120 Train: 194.97240 Test: 1.62842
Epoch: 5200 Train: 186.23518 Test: 1.41160
Epoch: 5280 Train: 185.41017 Test: 1.49115
Epoch 5360: New minimal relative error: 3.66%, model saved.
Epoch: 5360 Train: 176.11610 Test: 1.29387
Epoch: 5440 Train: 161.84517 Test: 1.02285
Epoch: 5520 Train: 169.64998 Test: 1.08704
Epoch: 5600 Train: 157.23930 Test: 3.14195
Epoch: 5680 Train: 158.97699 Test: 1.02316
Epoch: 5760 Train: 146.12018 Test: 0.86977
Epoch: 5840 Train: 142.94383 Test: 0.91445
Epoch: 5920 Train: 136.02184 Test: 0.77745
Epoch: 6000 Train: 138.07367 Test: 0.77623
Epoch: 6080 Train: 130.90166 Test: 0.79031
Epoch: 6160 Train: 128.26488 Test: 0.97670
Epoch: 6240 Train: 128.50288 Test: 1.27351
Epoch 6320: New minimal relative error: 2.03%, model saved.
Epoch: 6320 Train: 128.80959 Test: 0.66132
Epoch: 6400 Train: 129.40369 Test: 3.19968
Epoch: 6480 Train: 129.62619 Test: 0.70701
Epoch: 6560 Train: 123.44008 Test: 0.69271
Epoch: 6640 Train: 122.10297 Test: 0.67451
Epoch: 6720 Train: 120.18965 Test: 0.62095
Epoch: 6800 Train: 118.27993 Test: 0.71477
Epoch: 6880 Train: 116.18030 Test: 0.74345
Epoch: 6960 Train: 113.78983 Test: 0.65769
Epoch: 7040 Train: 116.07681 Test: 0.72817
Epoch: 7120 Train: 111.93913 Test: 0.67177
Epoch: 7200 Train: 115.08288 Test: 0.74898
Epoch: 7280 Train: 118.58609 Test: 0.78214
Epoch: 7360 Train: 120.49593 Test: 0.78872
Epoch: 7440 Train: 123.28333 Test: 1.19277
Epoch: 7520 Train: 118.84029 Test: 0.86069
Epoch: 7600 Train: 121.79063 Test: 0.76672
Epoch: 7680 Train: 116.95547 Test: 0.83183
Epoch: 7760 Train: 119.78334 Test: 0.75005
Epoch: 7840 Train: 114.80982 Test: 0.69669
Epoch: 7920 Train: 113.95136 Test: 0.63446
Epoch: 7999 Train: 132.50844 Test: 0.91418
Training Loss: tensor(132.5084)
Test Loss: tensor(0.9142)
Learned LE: [ 8.4746128e-01  1.1086186e-02 -1.4507848e+01]
True LE: [ 8.7024242e-01  3.4231590e-03 -1.4543162e+01]
Relative Error: [ 8.90894    8.594935   8.269306   7.9942465  7.7373633  7.5808945
  7.5295954  8.064361   8.408506   8.59468    8.439014   8.239293
  8.092088   7.925848   7.787741   8.151014   8.521145   8.863502
  9.065592   8.974038   8.74032    8.371478   7.9113626  7.4022474
  6.88851    6.404081   5.952312   5.5056896  4.9151354  4.2268543
  3.960833   4.008367   4.2752724  4.831579   5.368425   6.1284585
  6.8238473  7.4869146  8.465505   9.513209  10.570872  11.4285755
 12.067987  12.515683  12.776448  12.923858  13.161996  13.22363
 13.117022  12.847538  12.429834  11.888668  11.251785  10.549809
 10.290758  10.363786  10.496428  10.422269  10.089922   9.353794
  8.652086   8.082283   7.605337   7.3542943  7.0899725  6.8834753
  6.6759086  6.5828166  6.6858144  7.1561203  7.604506   7.843531
  7.7867413  7.637038   7.586568   7.481735   7.4126863  7.6413155
  8.042103   8.4303055  8.618843   8.503311   8.235241   7.8220263
  7.295267   6.7102814  6.1574736  5.853303   5.489093   4.9532156
  4.4420114  3.821322   3.3321407  3.043046   3.241192   3.8227954
  4.4392476  5.18151    5.9833117  6.68664    7.640237   8.8005705
  9.906825  10.7945385 11.490994  11.962928  12.287852  12.564693
 12.819634  12.888039  12.730837  12.369618  11.8246975 11.121725
 10.389046   9.740272   9.48821    9.3851185  9.408122   9.42306
  9.136577   8.391833   7.717368   7.1103563  6.4812055  6.0615325
  5.8861046  5.732559   5.589922   5.5494323  5.7830734  6.1874638
  6.7389054  7.078829   7.148794   7.0841947  7.159086   7.084342
  7.0296974  7.0783334  7.544997   7.9826756  8.209911   8.091345
  7.7821956  7.322401   6.7527413  6.129716   5.824137   5.437406
  5.016413   4.457397   4.0495963  3.4167268  2.8749971  2.5624743
  2.375629   2.787788   3.499045   4.2385926  5.1480465  5.924082
  6.8073444  8.045721   9.210188  10.0837    10.807059  11.406234
 11.813578  12.144061  12.445615  12.536344  12.34373   11.913233
 11.296782  10.52187    9.689517   8.991629   8.661461   8.462257
  8.3712435  8.515553   8.45918    7.8402743  7.01784    6.3696766
  5.7148094  5.114183   4.6565337  4.549266   4.470815   4.480145
  4.7912607  5.2433677  5.8001375  6.3857093  6.6923275  6.7070475
  6.780027   6.810588   6.824553   6.6131473  7.0292635  7.514486
  7.767586   7.686089   7.4148626  6.907871   6.2903814  5.9100018
  5.5329657  5.0772243  4.5761395  4.06795    3.6326718  3.1017566
  2.4798021  2.044252   1.8803623  2.0852835  2.5647929  3.305503
  4.240544   5.1022525  5.9960437  7.23122    8.313222   9.270441
 10.200383  10.886207  11.317529  11.716192  12.0228405 12.077579
 11.881118  11.442438  10.762447   9.927427   9.139623   8.328223
  7.76911    7.7085576  7.7330866  7.9295506  8.081953   7.5802712
  6.718488   5.8699284  5.030041   4.410925   3.9326603  3.4962938
  3.3397503  3.3768106  3.6880507  4.2479057  4.898198   5.680423
  6.162035   6.275968   6.3602133  6.5224314  6.6215277  6.522302
  6.6723876  7.0948205  7.3449     7.3094263  7.051991   6.6060753
  6.010208   5.7498     5.3267455  4.786529   4.214375   3.7341774
  3.3021834  2.9215274  2.2120755  1.6517642  1.3912765  1.6209718
  2.0741613  2.4852405  3.3235588  4.2492576  5.306976   6.232841
  7.335284   8.477725   9.538527  10.321658  10.825125  11.305477
 11.55069   11.505812  11.241773  10.783589  10.164564   9.286306
  8.562986   7.758501   7.2627506  7.07974    7.1618276  7.3031096
  7.4181023  7.2915964  6.468889   5.472461   4.6868777  3.9644547
  3.272048   2.8703275  2.5158887  2.369766   2.509637   3.2566779
  4.00074    4.8071575  5.4858274  5.7694817  5.952132   6.179975
  6.3712096  6.4273596  6.271153   6.793103   7.041435   6.9958873
  6.765882   6.3369193  5.943978   5.669627   5.2147174  4.622032
  3.9581087  3.4274566  3.100148   2.6992686  2.2454603  1.530858
  1.0205395  1.065225   1.5507101  2.1336854  2.5772908  3.3801107
  4.455877   5.19885    6.289701   7.497357   8.540067   9.418602
 10.058732  10.572481  10.867425  10.870697  10.599523  10.109839
  9.434776   8.628712   7.9608636  7.1930676  6.7636266  6.606494
  6.544297   6.550736   6.6286564  6.720604   6.2298207  5.227875
  4.271388   3.6453266  3.1088076  2.5545406  2.1652489  1.8728335
  1.6653419  2.1807027  2.9823477  3.746075   4.573733   5.12434
  5.439788   5.766313   6.0263653  6.2100353  6.033278   6.2209015
  6.6333876  6.7711496  6.535296   6.164669   5.906226   5.672291
  5.220005   4.5990076  3.8782704  3.2456703]
