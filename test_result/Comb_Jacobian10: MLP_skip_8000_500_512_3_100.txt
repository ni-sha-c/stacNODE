time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 100
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 98.52%, model saved.
Epoch: 0 Train: 9178.38086 Test: 4097.96338
Epoch: 80 Train: 2506.40820 Test: 1130.29321
Epoch: 160 Train: 2102.53394 Test: 816.17047
Epoch: 240 Train: 1004.88635 Test: 300.59464
Epoch: 320 Train: 477.23279 Test: 137.28030
Epoch: 400 Train: 280.43735 Test: 69.27345
Epoch 480: New minimal relative error: 28.96%, model saved.
Epoch: 480 Train: 166.68050 Test: 18.22860
Epoch 560: New minimal relative error: 8.09%, model saved.
Epoch: 560 Train: 119.63674 Test: 10.63181
Epoch: 640 Train: 96.85049 Test: 14.25432
Epoch: 720 Train: 89.60306 Test: 7.42479
Epoch 800: New minimal relative error: 2.59%, model saved.
Epoch: 800 Train: 64.93755 Test: 4.12821
Epoch: 880 Train: 60.84511 Test: 3.84637
Epoch: 960 Train: 55.42884 Test: 13.26227
Epoch: 1040 Train: 42.52292 Test: 2.64667
Epoch: 1120 Train: 40.52501 Test: 2.17398
Epoch: 1200 Train: 34.60915 Test: 1.52234
Epoch: 1280 Train: 32.92973 Test: 1.39686
Epoch: 1360 Train: 29.27561 Test: 1.63078
Epoch: 1440 Train: 27.55271 Test: 1.08063
Epoch 1520: New minimal relative error: 2.33%, model saved.
Epoch: 1520 Train: 28.22013 Test: 1.18379
Epoch: 1600 Train: 75.86267 Test: 17.35542
Epoch: 1680 Train: 24.55165 Test: 0.85610
Epoch: 1760 Train: 22.66105 Test: 0.73293
Epoch: 1840 Train: 22.21842 Test: 1.17673
Epoch: 1920 Train: 25.55428 Test: 5.98160
Epoch: 2000 Train: 31.49125 Test: 12.36415
Epoch: 2080 Train: 18.29767 Test: 0.54481
Epoch: 2160 Train: 17.96632 Test: 0.53439
Epoch: 2240 Train: 18.57073 Test: 0.58424
Epoch: 2320 Train: 19.89404 Test: 4.15287
Epoch: 2400 Train: 15.91751 Test: 0.54150
Epoch: 2480 Train: 16.15724 Test: 0.44596
Epoch: 2560 Train: 17.37959 Test: 2.11008
Epoch: 2640 Train: 14.51871 Test: 0.47805
Epoch: 2720 Train: 14.45956 Test: 1.04223
Epoch: 2800 Train: 13.50521 Test: 0.43102
Epoch: 2880 Train: 15.74707 Test: 0.38100
Epoch: 2960 Train: 13.06009 Test: 0.28051
Epoch: 3040 Train: 12.97825 Test: 0.30787
Epoch: 3120 Train: 19.36624 Test: 8.82670
Epoch: 3200 Train: 11.80810 Test: 0.24381
Epoch: 3280 Train: 13.42320 Test: 0.55819
Epoch 3360: New minimal relative error: 1.60%, model saved.
Epoch: 3360 Train: 11.78463 Test: 0.23990
Epoch: 3440 Train: 12.76585 Test: 0.49553
Epoch: 3520 Train: 10.91273 Test: 0.20955
Epoch: 3600 Train: 11.39408 Test: 0.52557
Epoch: 3680 Train: 10.40567 Test: 0.20639
Epoch: 3760 Train: 10.21971 Test: 0.40505
Epoch: 3840 Train: 9.83226 Test: 0.24039
Epoch: 3920 Train: 9.71322 Test: 0.19990
Epoch: 4000 Train: 9.48542 Test: 0.22815
Epoch: 4080 Train: 9.79821 Test: 0.23613
Epoch: 4160 Train: 8.94406 Test: 0.16442
Epoch: 4240 Train: 9.01510 Test: 0.16810
Epoch: 4320 Train: 8.67773 Test: 0.15089
Epoch: 4400 Train: 9.03114 Test: 0.20775
Epoch: 4480 Train: 8.35149 Test: 0.14137
Epoch: 4560 Train: 8.49150 Test: 0.27776
Epoch: 4640 Train: 8.02351 Test: 0.12665
Epoch: 4720 Train: 8.60058 Test: 0.14502
Epoch: 4800 Train: 8.00078 Test: 0.12869
Epoch: 4880 Train: 8.16033 Test: 0.13939
Epoch: 4960 Train: 8.29774 Test: 0.59652
Epoch 5040: New minimal relative error: 1.26%, model saved.
Epoch: 5040 Train: 7.76931 Test: 0.12313
Epoch: 5120 Train: 7.78441 Test: 0.23943
Epoch: 5200 Train: 9.71310 Test: 2.37087
Epoch: 5280 Train: 7.73811 Test: 0.12315
Epoch: 5360 Train: 7.84352 Test: 0.28745
Epoch: 5440 Train: 7.52594 Test: 0.14779
Epoch: 5520 Train: 7.57584 Test: 0.29203
Epoch: 5600 Train: 7.49616 Test: 0.19481
Epoch: 5680 Train: 7.57869 Test: 0.14434
Epoch 5760: New minimal relative error: 0.96%, model saved.
Epoch: 5760 Train: 7.24458 Test: 0.11041
Epoch: 5840 Train: 7.50515 Test: 0.52943
Epoch: 5920 Train: 7.27208 Test: 0.10722
Epoch: 6000 Train: 7.06003 Test: 0.13131
Epoch: 6080 Train: 7.22636 Test: 0.38653
Epoch: 6160 Train: 7.63824 Test: 0.62916
Epoch: 6240 Train: 7.04957 Test: 0.27960
Epoch: 6320 Train: 6.81170 Test: 0.11601
Epoch: 6400 Train: 6.84980 Test: 0.23920
Epoch: 6480 Train: 6.70402 Test: 0.09138
Epoch: 6560 Train: 7.18627 Test: 0.59706
Epoch: 6640 Train: 6.46908 Test: 0.08374
Epoch: 6720 Train: 6.37734 Test: 0.08820
Epoch: 6800 Train: 6.44009 Test: 0.14109
Epoch: 6880 Train: 6.79975 Test: 0.68249
Epoch: 6960 Train: 6.16652 Test: 0.08267
Epoch: 7040 Train: 6.16761 Test: 0.19331
Epoch: 7120 Train: 5.96819 Test: 0.10241
Epoch: 7200 Train: 6.03231 Test: 0.10644
Epoch: 7280 Train: 6.04417 Test: 0.10741
Epoch: 7360 Train: 5.96383 Test: 0.08197
Epoch: 7440 Train: 5.92005 Test: 0.07049
Epoch: 7520 Train: 5.80423 Test: 0.09926
Epoch: 7600 Train: 5.84142 Test: 0.07694
Epoch: 7680 Train: 5.81194 Test: 0.10329
Epoch: 7760 Train: 5.82734 Test: 0.06515
Epoch: 7840 Train: 5.78342 Test: 0.07588
Epoch: 7920 Train: 5.71931 Test: 0.06378
Epoch: 7999 Train: 5.83973 Test: 0.06011
Training Loss: tensor(5.8397)
Test Loss: tensor(0.0601)
Learned LE: [ 9.017479e-01 -9.198304e-03 -1.455813e+01]
True LE: [ 8.8240355e-01  1.2684742e-02 -1.4568612e+01]
Relative Error: [2.7991862  2.380364   2.0944793  1.3807788  1.0115312  1.1787101
 1.2127738  1.3858452  1.6618795  2.0433314  2.0879824  2.1174762
 2.39536    2.5551524  2.7316883  2.7796626  2.619638   2.9461195
 3.3971057  3.6468606  3.5895069  3.4682512  3.226824   2.8778055
 2.8038356  2.695463   2.5695822  2.4475174  2.5041125  2.48979
 2.1162274  1.5866545  0.9263739  0.45058584 0.18624973 0.51124936
 1.1026762  1.5176979  1.799573   1.7631902  1.7264951  1.4486458
 1.1496502  1.1923708  1.1631868  1.0852765  0.5248391  0.39638522
 0.769773   1.0577705  1.2041152  1.2889854  1.4145076  1.7633666
 2.4440923  3.479756   4.0753174  4.3286805  3.8200624  3.5393505
 3.30591    2.7373288  2.0011454  1.518165   1.3293307  1.0861603
 0.6080136  0.68715584 0.7881212  0.7318787  0.9173862  1.2916771
 1.6033007  1.8614196  2.0457294  2.4425611  2.6466727  2.4264483
 2.2669826  2.395223   2.7932792  3.111933   3.1440825  3.0864036
 2.8748345  2.5094895  2.4061525  2.2655845  2.0547845  1.928553
 2.0562992  2.232546   2.096698   1.5618068  0.86271566 0.30038095
 0.37324572 0.87555546 0.9582685  1.4514303  1.6182641  1.6530689
 1.6640699  1.405522   1.0721102  1.0880456  1.2302799  1.1144708
 0.88731533 0.4538781  0.6371699  0.9273375  1.0982542  1.1765299
 1.2453272  1.5758183  2.2756252  3.0570385  3.585259   3.869572
 3.7409413  3.5210903  2.9113324  2.096651   1.4281888  0.9201535
 0.58600104 0.5555818  0.55668813 0.4974323  0.5627859  0.6541397
 0.76082826 0.7880524  0.99891126 1.3919113  1.7646208  2.0676064
 2.5075858  2.1294208  1.8984047  1.9316884  2.3137188  2.5866973
 2.7048888  2.752944   2.614671   2.2467637  2.0160358  1.8721864
 1.6710327  1.5336413  1.6624031  1.8436782  1.958003   1.6046189
 0.94481206 0.34192547 0.5780635  1.1336881  0.96185327 1.3732678
 1.5193734  1.4724376  1.4868304  1.326412   0.9863362  0.9559469
 1.1847037  1.2367221  1.0865884  0.71857226 0.5982198  0.82160366
 0.98198485 1.0672122  1.1857218  1.5108281  2.1661477  2.8398812
 3.0719242  3.3096318  3.3915098  3.4613137  2.7355447  1.901615
 1.1996785  0.5581776  0.12167223 0.30188823 0.5265564  0.6531338
 0.49399635 0.78869563 0.8634382  0.82047004 0.73431826 0.8811633
 1.3417635  1.7605418  2.069889   1.9331204  1.5181965  1.5139058
 1.788909   2.1356196  2.232122   2.3921523  2.3654897  2.082744
 1.6637365  1.476163   1.2486153  1.2470006  1.3647541  1.5310366
 1.6293361  1.623266   1.0378761  0.40964916 0.62165844 1.1404818
 1.2309078  1.1363411  1.3616381  1.379748   1.3722451  1.2632412
 1.0062134  0.83834875 1.0119798  1.2501689  1.1669661  0.8944717
 0.64538467 0.6341647  0.75677645 0.87379164 1.0600164  1.488478
 2.1462831  2.5198607  2.7773283  3.0231125  2.9019449  3.138721
 2.6033387  2.0176797  1.2381799  0.5958946  0.16456418 0.47243917
 0.65900624 0.9204914  0.6454497  0.7731997  0.8980556  0.98291385
 0.8513765  0.7403648  0.86986405 1.324825   1.6600307  1.610397
 1.2559723  1.2024087  1.3209578  1.6662551  1.9026239  2.078345
 2.1547983  1.9747195  1.5125638  1.2476689  0.8802422  0.7407852
 1.0478085  1.2894619  1.3681517  1.3650259  1.2221003  0.5875315
 0.49421582 1.1467265  1.4297352  1.1708279  1.0985059  1.2544705
 1.3529899  1.2808571  1.0453618  0.75027597 0.8845733  1.0921545
 1.245144   1.0917736  0.74199885 0.5313855  0.5978558  0.7228635
 0.96295446 1.3858614  1.9922276  2.2721193  2.4159517  2.8287168
 2.7642279  2.7605932  2.3687992  2.0633485  1.5910937  0.9078159
 0.31618643 0.5085531  0.7319579  0.7934593  0.87406325 0.62933254
 0.8413137  0.8723678  0.96849936 0.8545175  0.7005543  0.80932873
 1.1854364  1.2661774  1.0528443  1.0001426  1.0102879  1.2199643
 1.5264881  1.7791433  1.9502931  1.8674891  1.5341436  1.0706404
 0.7352934  0.2924146  0.5209006  0.92537373 1.1558863  1.17671
 1.0902066  0.87268436 0.29287156 0.92388916 1.3011982  1.4245011
 1.1171389  1.0618274  1.2152512  1.1970375  1.0741705  0.8300051
 0.7600742  0.9427907  1.2019616  1.2296709  0.91519856 0.53561985
 0.57668227 0.58582395 0.6937657  0.93042254 1.4720396  1.870841
 2.0288742  2.323141   2.403321   2.4044893  1.9882562  1.6665416
 1.4485387  1.1338556  0.7441218  0.29002857 0.6521869  0.79126817
 0.8249349  0.75847846 0.5820258  0.7100683  0.72436583 0.8106761
 0.7159245  0.58415294 0.6145485  0.88258934 0.8333188  0.7946069
 0.6998157  0.92504036 1.1040928  1.3147802  1.5790492  1.6615669
 1.4765673  1.07547    0.7027983  0.3763579 ]
