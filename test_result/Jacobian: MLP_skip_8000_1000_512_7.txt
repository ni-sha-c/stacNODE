time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 1000
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 7
reg_param: 3000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 106.58%, model saved.
Epoch: 0 Train: 170316.00000 Test: 4249.41797
Epoch: 80 Train: 44461.06250 Test: 1988.14880
Epoch: 160 Train: 40225.43750 Test: 1236.54785
Epoch: 240 Train: 36548.90625 Test: 2030.29272
Epoch: 320 Train: 44416.04297 Test: 1741.31555
Epoch: 400 Train: 36693.17188 Test: 1954.01868
Epoch: 480 Train: 37658.42188 Test: 1304.13306
Epoch 560: New minimal relative error: 59.45%, model saved.
Epoch: 560 Train: 36082.84766 Test: 1241.03564
Epoch: 640 Train: 38793.42578 Test: 2533.70386
Epoch: 720 Train: 39984.64844 Test: 1504.33813
Epoch: 800 Train: 41723.45703 Test: 1455.40967
Epoch: 880 Train: 34966.03906 Test: 1387.36414
Epoch: 960 Train: 37707.50391 Test: 1532.98352
Epoch: 1040 Train: 42387.85938 Test: 1591.23706
Epoch: 1120 Train: 40411.35547 Test: 1484.45752
Epoch: 1200 Train: 39538.92578 Test: 1467.85571
Epoch 1280: New minimal relative error: 56.63%, model saved.
Epoch: 1280 Train: 37204.44922 Test: 1417.12854
Epoch: 1360 Train: 40581.37500 Test: 1489.96399
Epoch: 1440 Train: 37617.44141 Test: 1390.52197
Epoch: 1520 Train: 36425.03906 Test: 1485.87390
Epoch: 1600 Train: 36771.05078 Test: 1397.46753
Epoch: 1680 Train: 39117.53516 Test: 1382.39722
Epoch: 1760 Train: 34993.40625 Test: 1333.98743
Epoch: 1840 Train: 36035.68750 Test: 1235.82092
Epoch: 1920 Train: 34038.05859 Test: 1258.16199
Epoch: 2000 Train: 34292.81641 Test: 1148.74512
Epoch: 2080 Train: 34208.70703 Test: 1231.26062
Epoch: 2160 Train: 33578.80469 Test: 1288.67249
Epoch: 2240 Train: 34183.23047 Test: 1177.88843
Epoch: 2320 Train: 33786.55859 Test: 1200.77234
Epoch 2400: New minimal relative error: 48.14%, model saved.
Epoch: 2400 Train: 33761.22266 Test: 1160.02625
Epoch: 2480 Train: 30882.47852 Test: 1110.10608
Epoch: 2560 Train: 30765.58594 Test: 1071.36804
Epoch: 2640 Train: 33820.15625 Test: 1116.41553
Epoch: 2720 Train: 31576.67578 Test: 1099.94592
Epoch: 2800 Train: 30294.56055 Test: 1038.09106
Epoch: 2880 Train: 29776.18945 Test: 931.61151
Epoch: 2960 Train: 29465.79688 Test: 1001.59949
Epoch: 3040 Train: 29636.98633 Test: 842.38300
Epoch: 3120 Train: 28894.94336 Test: 910.93573
Epoch: 3200 Train: 28889.95703 Test: 975.04315
Epoch: 3280 Train: 27329.77734 Test: 982.89990
Epoch: 3360 Train: 27575.18359 Test: 786.95288
Epoch: 3440 Train: 24869.05078 Test: 690.82843
Epoch: 3520 Train: 25049.23438 Test: 828.19360
Epoch: 3600 Train: 25283.98242 Test: 584.83911
Epoch: 3680 Train: 21709.26172 Test: 583.39020
Epoch: 3760 Train: 20441.52148 Test: 481.29736
Epoch: 3840 Train: 19646.06055 Test: 406.06757
Epoch: 3920 Train: 13920.85938 Test: 201.09497
Epoch: 4000 Train: 7620.83057 Test: 90.30820
Epoch: 4080 Train: 3674.08740 Test: 31.84090
Epoch 4160: New minimal relative error: 7.96%, model saved.
Epoch: 4160 Train: 2134.92065 Test: 8.36205
Epoch 4240: New minimal relative error: 7.31%, model saved.
Epoch: 4240 Train: 1746.72620 Test: 6.56701
Epoch: 4320 Train: 1502.87732 Test: 4.41674
Epoch: 4400 Train: 1295.85022 Test: 9.79017
Epoch: 4480 Train: 1211.29346 Test: 8.24171
Epoch: 4560 Train: 1147.30200 Test: 5.92660
Epoch: 4640 Train: 1124.50488 Test: 4.34758
Epoch: 4720 Train: 964.57251 Test: 9.93781
Epoch 4800: New minimal relative error: 7.09%, model saved.
Epoch: 4800 Train: 1022.91962 Test: 4.80639
Epoch: 4880 Train: 1028.13293 Test: 4.03627
Epoch: 4960 Train: 943.85498 Test: 3.06470
Epoch: 5040 Train: 975.43268 Test: 6.29155
Epoch: 5120 Train: 940.70404 Test: 4.64561
Epoch: 5200 Train: 824.30560 Test: 7.35952
Epoch: 5280 Train: 879.14508 Test: 3.29890
Epoch: 5360 Train: 858.57965 Test: 3.69913
Epoch: 5440 Train: 867.29236 Test: 5.69454
Epoch 5520: New minimal relative error: 5.94%, model saved.
Epoch: 5520 Train: 762.09149 Test: 3.54446
Epoch: 5600 Train: 712.13550 Test: 1.99222
Epoch: 5680 Train: 719.44708 Test: 1.65605
Epoch: 5760 Train: 660.51117 Test: 1.51960
Epoch: 5840 Train: 694.38263 Test: 2.42081
Epoch: 5920 Train: 659.70160 Test: 4.07846
Epoch 6000: New minimal relative error: 4.23%, model saved.
Epoch: 6000 Train: 547.54425 Test: 0.88499
Epoch: 6080 Train: 577.73511 Test: 1.49042
Epoch: 6160 Train: 532.57269 Test: 1.19314
Epoch 6240: New minimal relative error: 3.79%, model saved.
Epoch: 6240 Train: 511.23251 Test: 1.09788
Epoch: 6320 Train: 558.57855 Test: 2.26228
Epoch: 6400 Train: 522.52216 Test: 1.49970
Epoch 6480: New minimal relative error: 3.30%, model saved.
Epoch: 6480 Train: 529.63013 Test: 1.64606
Epoch: 6560 Train: 617.62457 Test: 3.43493
Epoch: 6640 Train: 570.74268 Test: 2.90758
Epoch: 6720 Train: 550.03510 Test: 3.11514
Epoch: 6800 Train: 523.80627 Test: 2.92866
Epoch: 6880 Train: 491.37781 Test: 2.44116
Epoch: 6960 Train: 488.95566 Test: 1.00242
Epoch: 7040 Train: 495.95471 Test: 0.77045
Epoch: 7120 Train: 506.75592 Test: 1.65740
Epoch: 7200 Train: 475.34973 Test: 0.63794
Epoch: 7280 Train: 455.54169 Test: 0.93262
Epoch: 7360 Train: 441.87732 Test: 0.81580
Epoch: 7440 Train: 429.53519 Test: 0.67532
Epoch 7520: New minimal relative error: 3.23%, model saved.
Epoch: 7520 Train: 428.91837 Test: 0.94599
Epoch: 7600 Train: 424.90472 Test: 0.96163
Epoch: 7680 Train: 419.10965 Test: 1.90695
Epoch: 7760 Train: 413.59015 Test: 3.62642
Epoch: 7840 Train: 426.80588 Test: 0.99275
Epoch: 7920 Train: 428.41443 Test: 1.45609
Epoch: 7999 Train: 410.06851 Test: 1.33124
Training Loss: tensor(410.0685)
Test Loss: tensor(1.3312)
Learned LE: [  0.9230042   -0.06454192 -14.491045  ]
True LE: [ 8.4647202e-01  9.3637789e-03 -1.4530106e+01]
Relative Error: [4.0108438  4.1708937  4.3593826  4.46691    4.556688   4.6838427
 4.879495   5.0723696  5.2944264  5.308439   4.741839   4.2777495
 4.0338883  3.6708884  2.937582   2.2645535  1.9710405  1.8325753
 1.7917341  1.7810795  1.7211858  1.771201   1.7472495  1.6433775
 1.4918826  1.4455827  1.3335103  1.5211475  1.9512358  2.5068665
 3.2226067  3.8984246  4.2501473  4.501439   4.580008   4.689565
 4.9964266  5.3637624  5.680684   5.9452133  5.9647317  5.819909
 5.451433   4.8824224  4.1912103  3.7942781  3.4867194  3.050328
 2.7250197  2.5776653  2.4298851  2.3407981  2.0968795  1.8302383
 1.6215928  1.8167194  2.334772   2.6862986  2.969654   3.1016748
 3.3161473  3.531697   3.637524   3.7939384  3.9509614  3.8190582
 3.8747072  4.047847   4.2635317  4.4962125  4.745719   4.9609613
 4.823454   4.229049   3.8566747  3.2685597  2.6279917  2.0881588
 1.7353784  1.5573086  1.4897603  1.4949441  1.4398878  1.4606887
 1.471985   1.4071589  1.2720925  1.1852574  1.0764792  1.2651061
 1.7379091  2.1646662  2.7823513  3.5875485  4.262123   4.495108
 4.533511   4.598912   4.874089   5.013548   5.2238913  5.4986844
 5.582146   5.453907   5.158415   4.6190147  3.9258366  3.4652007
 3.2339904  2.8206632  2.4369996  2.2838392  2.1300616  2.151346
 1.9772809  1.6731048  1.5372607  1.8144176  2.4136155  2.7182055
 2.8823507  3.0014203  3.0181975  3.1371403  3.2730546  3.3970714
 3.3945575  3.2816403  3.1776183  3.3065205  3.4795887  3.6989849
 3.9750137  4.2605414  4.4965825  4.36259    3.7535288  2.9444597
 2.2744105  1.8887872  1.5487696  1.2787272  1.1662784  1.215156
 1.2347962  1.217426   1.2834145  1.2041306  1.0618398  1.016807
 0.8826946  0.99826956 1.4083035  1.8919371  2.5104527  3.353576
 4.160343   4.415978   4.4481673  4.453585   4.7085137  4.835429
 4.9119363  4.971214   5.1434813  5.0586987  4.798625   4.277897
 3.740926   3.181069   2.9888403  2.6688826  2.2035408  2.0228338
 1.8305359  1.8601263  1.7822756  1.544457   1.4931403  1.8133519
 2.3797803  2.7060754  2.7868688  2.786509   2.7610247  2.7467875
 2.8402908  2.8748674  2.801172   2.6231816  2.4986935  2.5770392
 2.7719631  3.0006857  3.230744   3.4642181  3.6283858  3.8990703
 3.7457948  2.8169773  2.0020792  1.7170687  1.4917997  1.1034195
 0.90028274 0.9631879  1.0805843  1.0744284  1.1910243  1.1132636
 0.90224594 0.7948313  0.7618948  0.7879009  1.0740001  1.6892986
 2.384814   3.034175   3.837396   4.3127394  4.2991953  4.263398
 4.4484015  4.5989823  4.691564   4.585093   4.5917063  4.591211
 4.3568993  3.9403915  3.5786386  3.1194506  2.7627158  2.5410419
 2.0842242  1.7496665  1.5467786  1.5667118  1.4913692  1.4244771
 1.4736125  1.8532984  2.3925343  2.6389155  2.610872   2.5776246
 2.5949118  2.4311028  2.369354   2.269224   2.145694   2.0455544
 2.037543   2.0800908  2.2538812  2.4857557  2.6841927  2.873767
 2.944333   3.0179958  2.9827688  2.8421984  2.0195513  1.6501659
 1.3804207  1.0715604  0.81488293 0.7911914  1.0003877  0.9439788
 1.1203977  1.0561559  0.7934557  0.6110309  0.57129973 0.66402227
 0.766677   1.4064001  2.1526735  2.7037456  3.3949685  4.057376
 4.144825   4.02587    4.074118   4.2478366  4.388824   4.306419
 4.143406   3.9885309  3.877086   3.5942037  3.3976521  3.0488636
 2.524234   2.350329   1.9683017  1.5462351  1.3657037  1.3763047
 1.3055171  1.3042327  1.3674852  1.6798431  2.2264042  2.6041014
 2.5137856  2.420906   2.4379544  2.215891   1.8766005  1.6846267
 1.6763257  1.6227589  1.6742958  1.7810845  1.9026835  2.1074338
 2.300648   2.4830773  2.5249383  2.3782742  2.0480516  2.2133899
 2.0486398  1.7447793  1.4007747  1.0912093  0.8422289  0.64312494
 0.8664578  0.9128946  1.0127449  1.0071859  0.80830026 0.54815
 0.46786565 0.5663036  0.718724   1.1621954  1.7626574  2.4103458
 2.9142463  3.670623   4.0768614  3.9236438  3.7300653  3.795798
 3.9596803  3.9302294  3.8745277  3.4373772  3.3213816  3.216279
 3.1611905  2.9297774  2.5039625  2.1003764  1.7715706  1.3556801
 1.2046707  1.1325828  1.0607636  1.163485   1.3057747  1.6009433
 1.9524608  2.28303    2.2514987  2.2661319  2.2918563  1.9849062
 1.496919   1.2895323  1.2880383  1.3261398  1.4114963  1.5709078
 1.6992252  1.8843505  2.0693076  2.2256563  2.303525   2.076498
 1.5285532  1.2326285  1.5413933  1.733525   1.5609986  1.2610029
 0.881097   0.65810996 0.68404967 0.8325033  0.8514548  0.9156156
 0.7552409  0.53281075 0.43067178 0.5081467 ]
