time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 1000
num_test: 1000
num_val: 0
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.2
n_hidden: 128
n_layers: 5
reg_param: 500
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 32.611907959 Test: 31.260463715
Epoch 0: New minimal relative error: 31.26%, model saved.
Epoch: 100 Train: 5.371098518 Test: 5.443285942
Epoch 100: New minimal relative error: 5.44%, model saved.
Epoch: 200 Train: 5.478117943 Test: 5.515575886
Epoch: 300 Train: 5.351089001 Test: 5.402088165
Epoch 300: New minimal relative error: 5.40%, model saved.
Epoch: 400 Train: 5.306194305 Test: 5.386105537
Epoch 400: New minimal relative error: 5.39%, model saved.
Epoch: 500 Train: 5.195416451 Test: 5.252308846
Epoch 500: New minimal relative error: 5.25%, model saved.
Epoch: 600 Train: 5.228433609 Test: 5.293119907
Epoch: 700 Train: 5.122090816 Test: 5.167924881
Epoch 700: New minimal relative error: 5.17%, model saved.
Epoch: 800 Train: 5.106048584 Test: 5.218001366
Epoch: 900 Train: 5.096482754 Test: 5.216594696
Epoch: 1000 Train: 5.230478287 Test: 5.321949005
Epoch: 1100 Train: 5.320845604 Test: 5.422745705
Epoch: 1200 Train: 5.234342575 Test: 5.422458649
Epoch: 1300 Train: 5.189900398 Test: 5.292720795
Epoch: 1400 Train: 5.106180191 Test: 5.405781746
Epoch: 1500 Train: 5.046809196 Test: 5.209985733
Epoch: 1600 Train: 5.136453629 Test: 5.279602051
Epoch: 1700 Train: 5.116564274 Test: 5.262346268
Epoch: 1800 Train: 5.138710976 Test: 5.304971218
Epoch: 1900 Train: 5.154831886 Test: 5.287696838
Epoch: 2000 Train: 5.205914021 Test: 5.337298393
Epoch: 2100 Train: 5.211443901 Test: 5.289631844
Epoch: 2200 Train: 5.178966522 Test: 5.289604187
Epoch: 2300 Train: 5.172533989 Test: 5.287655830
Epoch: 2400 Train: 5.190621853 Test: 5.311322689
Epoch: 2500 Train: 5.183633804 Test: 5.300912857
Epoch: 2600 Train: 5.186397552 Test: 5.305088043
Epoch: 2700 Train: 5.181861401 Test: 5.298592567
Epoch: 2800 Train: 5.227814674 Test: 5.345821381
Epoch: 2900 Train: 5.238972664 Test: 5.335712910
Epoch: 3000 Train: 5.223645210 Test: 5.346911430
Epoch: 3100 Train: 5.240404129 Test: 5.338597298
Epoch: 3200 Train: 5.175734997 Test: 5.271611691
Epoch: 3300 Train: 5.122297764 Test: 5.292321205
Epoch: 3400 Train: 5.160671234 Test: 5.352237701
Epoch: 3500 Train: 5.179047585 Test: 5.315848351
Epoch: 3600 Train: 5.184509277 Test: 5.307521343
Epoch: 3700 Train: 5.167284012 Test: 5.344495773
Epoch: 3800 Train: 5.196905136 Test: 5.320716858
Epoch: 3900 Train: 5.201882362 Test: 5.333380699
Epoch: 4000 Train: 5.212746620 Test: 5.349472046
Epoch: 4100 Train: 5.216932297 Test: 5.364607811
Epoch: 4200 Train: 5.230414391 Test: 5.432767868
Epoch: 4300 Train: 5.242066383 Test: 5.474639416
Epoch: 4400 Train: 5.232061386 Test: 5.462028980
Epoch: 4500 Train: 5.236270905 Test: 5.463947773
Epoch: 4600 Train: 5.235413074 Test: 5.460158348
Epoch: 4700 Train: 5.233839512 Test: 5.459875107
Epoch: 4800 Train: 5.241767883 Test: 5.355117798
Epoch: 4900 Train: 5.254753113 Test: 5.371684074
Epoch: 5000 Train: 5.253037453 Test: 5.396726131
Epoch: 5100 Train: 5.257791519 Test: 5.420886040
Epoch: 5200 Train: 5.263667107 Test: 5.442188263
Epoch: 5300 Train: 5.262946129 Test: 5.444880486
Epoch: 5400 Train: 5.252360821 Test: 5.443005562
Epoch: 5500 Train: 5.238475800 Test: 5.437973976
Epoch: 5600 Train: 5.233291149 Test: 5.453972816
Epoch: 5700 Train: 5.241196632 Test: 5.446045399
Epoch: 5800 Train: 5.242430210 Test: 5.402578354
Epoch: 5900 Train: 5.241831303 Test: 5.394211769
Epoch: 6000 Train: 5.242728233 Test: 5.395174980
Epoch: 6100 Train: 5.236678123 Test: 5.395935059
Epoch: 6200 Train: 5.239270210 Test: 5.399407387
Epoch: 6300 Train: 5.241709709 Test: 5.425701618
Epoch: 6400 Train: 5.247155190 Test: 5.492394924
Epoch: 6500 Train: 5.215213776 Test: 5.413393974
Epoch: 6600 Train: 5.257815838 Test: 5.375928879
Epoch: 6700 Train: 5.235776901 Test: 5.368728638
Epoch: 6800 Train: 5.241700172 Test: 5.390003204
Epoch: 6900 Train: 5.181990623 Test: 5.364223480
Epoch: 7000 Train: 5.211896896 Test: 5.368884087
Epoch: 7100 Train: 5.220469475 Test: 5.315651894
Epoch: 7200 Train: 5.228155136 Test: 5.312112808
Epoch: 7300 Train: 5.185842037 Test: 5.267568588
Epoch: 7400 Train: 5.196609497 Test: 5.276076317
Epoch: 7500 Train: 5.199841022 Test: 5.293543816
Epoch: 7600 Train: 5.198886871 Test: 5.289592743
Epoch: 7700 Train: 5.187936783 Test: 5.289180756
Epoch: 7800 Train: 5.190395355 Test: 5.291810036
Epoch: 7900 Train: 5.190556526 Test: 5.290523529
Epoch: 8000 Train: 5.190691471 Test: 5.297751904
Epoch: 8100 Train: 5.187610626 Test: 5.305139542
Epoch: 8200 Train: 5.190408707 Test: 5.314927101
Epoch: 8300 Train: 5.193998814 Test: 5.314315319
Epoch: 8400 Train: 5.209387302 Test: 5.320514679
Epoch: 8500 Train: 5.197854042 Test: 5.303486824
Epoch: 8600 Train: 5.198776245 Test: 5.300305367
Epoch: 8700 Train: 5.201685905 Test: 5.313487053
Epoch: 8800 Train: 5.199865341 Test: 5.318196297
Epoch: 8900 Train: 5.202573299 Test: 5.320849895
Epoch: 9000 Train: 5.208862782 Test: 5.322187424
Epoch: 9100 Train: 5.208887100 Test: 5.330308914
Epoch: 9200 Train: 5.208862305 Test: 5.330962658
Epoch: 9300 Train: 5.208546638 Test: 5.330535889
Epoch: 9400 Train: 5.206943512 Test: 5.335486412
Epoch: 9500 Train: 5.218994141 Test: 5.335511208
Epoch: 9600 Train: 5.220738411 Test: 5.331939697
Epoch: 9700 Train: 5.219135284 Test: 5.336256504
Epoch: 9800 Train: 5.217926025 Test: 5.334527016
Epoch: 9900 Train: 5.220368862 Test: 5.332700729
Epoch: 9999 Train: 5.220516205 Test: 5.329543591
Training Loss: tensor(5.2205)
Test Loss: tensor(5.3295)
True Mean x: tensor(3.3019, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(3.3370, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3662, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(0.0024, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0052)
Jacobian term Test Loss: tensor(0.0053)
Learned LE: [0.9677065  0.35889274]
True LE: tensor([ 0.6932, -0.7017], dtype=torch.float64)
