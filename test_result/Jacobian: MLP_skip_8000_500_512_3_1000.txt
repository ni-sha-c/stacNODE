time_step: 0.01
lr: 0.001
weight_decay: 0.0005
num_epoch: 8000
num_train: 10000
num_test: 6000
num_trans: 500
loss_type: Jacobian
dyn_sys: lorenz
model_type: MLP_skip
n_hidden: 512
n_layers: 3
reg_param: 1000
optim_name: AdamW
train_dir: ../plot/Vector_field/train_MLPskip_Jac/
Epoch 0: New minimal relative error: 104.86%, model saved.
Epoch: 0 Train: 59782.46094 Test: 4198.34375
Epoch: 80 Train: 15974.45117 Test: 1648.79248
Epoch: 160 Train: 14339.86914 Test: 1463.12842
Epoch 240: New minimal relative error: 85.56%, model saved.
Epoch: 240 Train: 14107.97363 Test: 1334.18848
Epoch: 320 Train: 15856.20312 Test: 1350.96814
Epoch: 400 Train: 13641.89160 Test: 1414.97424
Epoch 480: New minimal relative error: 81.60%, model saved.
Epoch: 480 Train: 13965.09961 Test: 1413.18079
Epoch 560: New minimal relative error: 75.88%, model saved.
Epoch: 560 Train: 15845.43359 Test: 1455.58655
Epoch 640: New minimal relative error: 61.80%, model saved.
Epoch: 640 Train: 11827.63086 Test: 1034.12231
Epoch: 720 Train: 12994.89844 Test: 964.90338
Epoch: 800 Train: 9949.70020 Test: 669.63513
Epoch: 880 Train: 8179.12793 Test: 517.84772
Epoch: 960 Train: 5959.74316 Test: 289.51926
Epoch: 1040 Train: 4152.45117 Test: 135.94891
Epoch 1120: New minimal relative error: 32.51%, model saved.
Epoch: 1120 Train: 1850.34045 Test: 41.50927
Epoch 1200: New minimal relative error: 21.89%, model saved.
Epoch: 1200 Train: 1241.07629 Test: 26.94559
Epoch: 1280 Train: 942.34119 Test: 16.23469
Epoch: 1360 Train: 727.10870 Test: 7.17413
Epoch 1440: New minimal relative error: 16.42%, model saved.
Epoch: 1440 Train: 713.81134 Test: 8.76825
Epoch 1520: New minimal relative error: 11.01%, model saved.
Epoch: 1520 Train: 769.28680 Test: 10.36261
Epoch 1600: New minimal relative error: 7.33%, model saved.
Epoch: 1600 Train: 600.74768 Test: 5.37743
Epoch: 1680 Train: 508.77307 Test: 3.81224
Epoch: 1760 Train: 491.54474 Test: 3.37506
Epoch: 1840 Train: 466.80038 Test: 3.65725
Epoch: 1920 Train: 435.38257 Test: 3.81910
Epoch 2000: New minimal relative error: 7.26%, model saved.
Epoch: 2000 Train: 400.40671 Test: 2.64757
Epoch: 2080 Train: 357.67099 Test: 2.26400
Epoch: 2160 Train: 319.14224 Test: 1.66569
Epoch: 2240 Train: 400.96558 Test: 5.25512
Epoch: 2320 Train: 406.99811 Test: 5.25928
Epoch: 2400 Train: 325.57727 Test: 3.08138
Epoch: 2480 Train: 342.70444 Test: 3.70818
Epoch: 2560 Train: 324.60776 Test: 2.32205
Epoch: 2640 Train: 320.90079 Test: 4.19461
Epoch 2720: New minimal relative error: 6.91%, model saved.
Epoch: 2720 Train: 274.91000 Test: 1.97228
Epoch: 2800 Train: 253.67043 Test: 1.45717
Epoch: 2880 Train: 238.22546 Test: 1.23230
Epoch: 2960 Train: 239.19106 Test: 1.75200
Epoch: 3040 Train: 239.35745 Test: 1.47247
Epoch: 3120 Train: 348.85379 Test: 5.41350
Epoch: 3200 Train: 276.19351 Test: 3.28159
Epoch: 3280 Train: 251.34154 Test: 1.79206
Epoch: 3360 Train: 224.92848 Test: 2.63208
Epoch: 3440 Train: 216.86153 Test: 4.73933
Epoch: 3520 Train: 259.69473 Test: 2.04789
Epoch: 3600 Train: 253.35739 Test: 2.79246
Epoch 3680: New minimal relative error: 6.18%, model saved.
Epoch: 3680 Train: 239.42009 Test: 1.60357
Epoch: 3760 Train: 226.51112 Test: 9.28104
Epoch: 3840 Train: 231.89392 Test: 1.31693
Epoch 3920: New minimal relative error: 3.28%, model saved.
Epoch: 3920 Train: 210.63016 Test: 0.93534
Epoch: 4000 Train: 178.25447 Test: 0.58594
Epoch: 4080 Train: 174.10780 Test: 0.51945
Epoch: 4160 Train: 166.38359 Test: 1.54009
Epoch: 4240 Train: 162.74153 Test: 0.52663
Epoch: 4320 Train: 163.71082 Test: 0.62685
Epoch: 4400 Train: 159.02170 Test: 0.59522
Epoch: 4480 Train: 167.14624 Test: 0.85806
Epoch: 4560 Train: 161.76915 Test: 1.02237
Epoch: 4640 Train: 165.70802 Test: 0.84766
Epoch: 4720 Train: 162.84184 Test: 1.26657
Epoch: 4800 Train: 159.71591 Test: 0.91277
Epoch: 4880 Train: 156.30066 Test: 1.00415
Epoch: 4960 Train: 138.97311 Test: 0.74239
Epoch: 5040 Train: 146.03423 Test: 2.81717
Epoch: 5120 Train: 144.64398 Test: 1.02029
Epoch: 5200 Train: 143.20674 Test: 1.14924
Epoch: 5280 Train: 188.71075 Test: 1.29564
Epoch: 5360 Train: 164.55481 Test: 1.15571
Epoch: 5440 Train: 211.52261 Test: 3.79638
Epoch: 5520 Train: 153.81000 Test: 0.72056
Epoch: 5600 Train: 143.98537 Test: 0.70556
Epoch: 5680 Train: 188.24086 Test: 2.59301
Epoch: 5760 Train: 163.28302 Test: 1.62187
Epoch: 5840 Train: 167.35223 Test: 1.35175
Epoch: 5920 Train: 140.42383 Test: 0.56773
Epoch: 6000 Train: 143.68727 Test: 0.66654
Epoch: 6080 Train: 139.57898 Test: 0.58930
Epoch: 6160 Train: 154.01907 Test: 1.11716
Epoch: 6240 Train: 147.81421 Test: 0.83294
Epoch: 6320 Train: 152.60725 Test: 0.79573
Epoch: 6400 Train: 153.82765 Test: 1.33928
Epoch: 6480 Train: 150.52122 Test: 1.24479
Epoch: 6560 Train: 147.59286 Test: 1.41054
Epoch: 6640 Train: 141.62985 Test: 0.75468
Epoch: 6720 Train: 142.65347 Test: 0.81833
Epoch: 6800 Train: 134.02747 Test: 1.02724
Epoch: 6880 Train: 130.54897 Test: 1.00835
Epoch: 6960 Train: 124.66119 Test: 1.08015
Epoch: 7040 Train: 109.32588 Test: 0.46493
Epoch: 7120 Train: 120.93941 Test: 0.65804
Epoch: 7200 Train: 112.35513 Test: 0.52004
Epoch: 7280 Train: 113.32654 Test: 0.60993
Epoch: 7360 Train: 104.35593 Test: 0.44088
Epoch: 7440 Train: 105.65668 Test: 0.44687
Epoch: 7520 Train: 97.90318 Test: 0.35423
Epoch: 7600 Train: 96.29497 Test: 0.39189
Epoch: 7680 Train: 94.55399 Test: 0.43973
Epoch: 7760 Train: 93.03088 Test: 0.44101
Epoch: 7840 Train: 98.89880 Test: 0.56451
Epoch: 7920 Train: 99.70729 Test: 0.52246
Epoch: 7999 Train: 111.64046 Test: 0.78360
Training Loss: tensor(111.6405)
Test Loss: tensor(0.7836)
Learned LE: [  0.85423106  -0.01487439 -14.532097  ]
True LE: [ 8.8197899e-01  2.1636169e-03 -1.4556835e+01]
Relative Error: [ 4.0269485  4.7077413  5.6586957  6.7761602  7.998416   9.3850975
 10.914154  12.533519  13.469696  13.850546  14.219321  14.678128
 14.940307  14.482172  13.67636   12.883995  11.943007  10.963161
 10.177118   9.676343   8.939016   8.3861885  7.679329   7.3098526
  7.2857933  7.4080544  7.474707   7.9405985  8.63195    9.558323
 10.217084  11.191714  12.4610195 13.99834   14.57941   14.806402
 14.944559  15.0047035 15.163335  15.351396  15.413572  15.3513775
 14.54543   13.688368  12.864736  12.096443  11.177984  10.000936
  8.727235   7.7712784  7.134334   6.382236   5.73314    5.4732203
  5.623503   6.2397184  5.5640554  4.432058   3.74442    3.4406955
  3.1027906  3.0915108  3.3847656  3.9146848  4.7071867  5.7613826
  7.087037   8.5415    10.120318  11.509475  12.515648  12.747524
 12.99651   13.409227  13.920429  13.424915  12.486362  11.642108
 11.003464  10.190613   9.369824   8.713388   7.85278    7.3135266
  6.6200075  6.154746   6.024728   6.1131287  6.076779   6.448229
  7.236074   8.193149   9.161849  10.099749  11.363539  12.9128065
 13.336375  13.514906  13.979308  14.095579  14.289917  14.456373
 14.571672  14.682097  13.984913  13.130233  12.274495  11.459957
 10.480446   9.26278    8.000267   6.9833956  6.3003206  5.759194
  5.0870824  4.773104   4.86992    5.384813   5.130782   4.005846
  3.2757375  3.0485168  2.71556    2.6429794  2.8205955  3.3143218
  3.8670049  4.7512555  6.0948353  7.64069    9.279452  10.29283
 11.185819  11.403309  11.658023  11.974576  12.365322  12.122929
 11.538598  10.643047   9.916158   9.425355   8.78119    7.9245
  6.9841394  6.317148   5.775797   5.1590877  4.8960633  5.0056424
  4.8136754  5.0565     5.7160983  6.7846956  7.994286   9.036334
 10.254253  11.776093  12.211132  12.369463  12.781133  13.277827
 13.505472  13.693893  13.851036  14.086851  13.627704  12.782476
 11.891462  11.01877   10.012138   8.741387   7.4979944  6.4108243
  5.742474   5.392977   4.7128596  4.336821   4.2221713  4.5374594
  4.8911304  3.7108984  2.9540145  2.7145987  2.5565135  2.4575508
  2.556968   2.7705817  3.245737   4.033378   5.1173825  6.5765514
  8.047874   8.945971   9.901439  10.209274  10.47204   10.739318
 10.959232  10.70787   10.1485815  9.736351   9.092836   8.4825
  8.170705   7.407121   6.377493   5.5347514  5.0370207  4.377041
  3.959228   3.909195   3.7186286  3.7617862  4.245085   5.176184
  6.5548887  8.011737   9.173042  10.656314  11.272137  11.394242
 11.724529  12.350923  12.69989   12.838054  13.114701  13.530214
 13.475564  12.654273  11.732841  10.794168   9.802163   8.451721
  7.2497687  6.088893   5.424996   5.111743   4.5598173  4.0483494
  3.6713078  3.780448   4.399464   3.5872207  2.7504086  2.4298708
  2.4548898  2.3355837  2.4234111  2.5653162  2.9007652  3.5073233
  4.382315   5.45016    6.6930704  7.602717   8.625559   9.10357
  9.38942    9.690834   9.717032   9.402706   8.94392    8.436067
  8.107678   7.8232603  7.326778   6.9008     6.0508895  5.0388284
  4.336045   3.9190106  3.2861445  3.0437458  2.8566155  2.5754373
  2.8027928  3.5728772  4.8271937  6.4534397  8.013921   9.438536
 10.449006  10.509687  10.817547  11.012209  11.555703  11.795054
 11.992895  12.45177   12.935013  12.751487  11.81876   10.82074
  9.859467   8.418548   7.198183   6.0410123  5.2841263  4.9146986
  4.597246   3.8878613  3.3145885  3.0847232  3.471309   3.6118019
  2.6793306  2.2221656  2.2767708  2.2347498  2.2798986  2.3514202
  2.652825   3.1905723  3.9192617  4.5700097  5.483548   6.2485476
  7.2797747  8.038118   8.352977   8.707194   8.736308   8.337819
  8.032037   7.4635296  6.9905357  6.714646   6.778382   6.2362013
  5.6409965  4.935424   3.9997957  3.428934   2.9720817  2.4896328
  2.3182116  1.8541431  1.7396331  2.1646848  3.0685697  4.453241
  6.361531   8.097224   9.722657   9.708539   9.72728    9.838807
 10.206016  10.882654  10.990134  11.414521  11.94794   12.245911
 11.772375  11.141208  10.085764   8.734861   7.3654027  6.2432504
  5.3122253  4.8319893  4.6363072  3.8698845  3.144631   2.6258063
  2.6055765  3.1227088  2.7469273  2.033576   1.897896   2.150301
  2.1349351  2.2017446  2.4748082  2.9314702  3.489779   4.024204
  4.642384   5.0748734  5.8771515  6.9172115  7.291577   7.68751
  7.871196   7.625538   7.166301   6.786638   6.252375   5.801573
  5.597474   5.6499915  5.1732297  4.5994143  4.1199856  3.223971
  2.8451881  2.392394   1.9267824  1.6805772]
