time_step: 1.0
lr: 0.001
weight_decay: 0.0005
num_epoch: 10000
num_train: 5000
num_test: 5000
num_val: 3000
num_trans: 0
loss_type: Jacobian
dyn_sys: baker
model_type: MLP_skip
s: 0.0
n_hidden: 256
n_layers: 3
reg_param: 100.0
threshold: 0.0
optim_name: AdamW
Epoch: 0 Train: 198.515838623 Test: 5.306666851
Epoch 0: New minimal relative error: 5.31%, model saved.
Epoch: 100 Train: 18.770481110 Test: 10.798072815
Epoch: 200 Train: 14.024881363 Test: 10.532871246
Epoch: 300 Train: 12.460380554 Test: 10.342411995
Epoch: 400 Train: 12.404911995 Test: 10.551791191
Epoch: 500 Train: 14.063944817 Test: 10.813433647
Epoch: 600 Train: 12.664391518 Test: 10.779846191
Epoch: 700 Train: 13.163321495 Test: 9.901743889
Epoch: 800 Train: 13.233839035 Test: 10.135315895
Epoch: 900 Train: 12.686462402 Test: 10.160087585
Epoch: 1000 Train: 12.818372726 Test: 10.571904182
Epoch: 1100 Train: 12.399769783 Test: 10.511221886
Epoch: 1200 Train: 12.376386642 Test: 10.225539207
Epoch: 1300 Train: 12.258179665 Test: 10.240602493
Epoch: 1400 Train: 12.436388969 Test: 10.508033752
Epoch: 1500 Train: 12.303065300 Test: 10.636753082
Epoch: 1600 Train: 12.505503654 Test: 10.650246620
Epoch: 1700 Train: 12.241833687 Test: 10.486819267
Epoch: 1800 Train: 14.531368256 Test: 9.752911568
Epoch: 1900 Train: 12.886045456 Test: 10.587629318
Epoch: 2000 Train: 12.749732971 Test: 10.595488548
Epoch: 2100 Train: 12.866028786 Test: 10.649123192
Epoch: 2200 Train: 12.882266045 Test: 10.718221664
Epoch: 2300 Train: 12.732443810 Test: 10.752538681
Epoch: 2400 Train: 12.662776947 Test: 10.663138390
Epoch: 2500 Train: 12.488213539 Test: 10.536162376
Epoch: 2600 Train: 12.697212219 Test: 10.641580582
Epoch: 2700 Train: 12.562005043 Test: 10.496314049
Epoch: 2800 Train: 12.875975609 Test: 10.643170357
Epoch: 2900 Train: 13.262751579 Test: 10.474252701
Epoch: 3000 Train: 12.671053886 Test: 10.279234886
Epoch: 3100 Train: 12.564173698 Test: 10.151472092
Epoch: 3200 Train: 13.151577950 Test: 10.046640396
Epoch: 3300 Train: 13.497949600 Test: 10.293535233
Epoch: 3400 Train: 13.273922920 Test: 9.907045364
Epoch: 3500 Train: 13.817985535 Test: 10.081033707
Epoch: 3600 Train: 13.837223053 Test: 10.035142899
Epoch: 3700 Train: 14.095177650 Test: 10.021285057
Epoch: 3800 Train: 14.303858757 Test: 10.044185638
Epoch: 3900 Train: 14.460186958 Test: 9.976447105
Epoch: 4000 Train: 14.242229462 Test: 9.804238319
Epoch: 4100 Train: 14.060765266 Test: 9.881719589
Epoch: 4200 Train: 14.207748413 Test: 9.686284065
Epoch: 4300 Train: 14.291889191 Test: 9.752502441
Epoch: 4400 Train: 13.873161316 Test: 9.829665184
Epoch: 4500 Train: 13.447012901 Test: 10.003419876
Epoch: 4600 Train: 13.078660965 Test: 10.189764977
Epoch: 4700 Train: 13.168868065 Test: 10.317657471
Epoch: 4800 Train: 13.564264297 Test: 10.386658669
Epoch: 4900 Train: 13.718893051 Test: 10.404249191
Epoch: 5000 Train: 14.015330315 Test: 10.238389015
Epoch: 5100 Train: 13.637361526 Test: 9.741132736
Epoch: 5200 Train: 13.959000587 Test: 9.492198944
Epoch: 5300 Train: 14.836145401 Test: 9.487667084
Epoch: 5400 Train: 14.941537857 Test: 9.498394012
Epoch: 5500 Train: 14.934986115 Test: 9.555535316
Epoch: 5600 Train: 14.793213844 Test: 9.635512352
Epoch: 5700 Train: 14.567521095 Test: 9.734629631
Epoch: 5800 Train: 14.045980453 Test: 9.807648659
Epoch: 5900 Train: 14.134057999 Test: 9.853490829
Epoch: 6000 Train: 14.223670006 Test: 9.855358124
Epoch: 6100 Train: 15.190033913 Test: 9.845928192
Epoch: 6200 Train: 15.880458832 Test: 9.874279976
Epoch: 6300 Train: 16.256681442 Test: 9.915741920
Epoch: 6400 Train: 16.145885468 Test: 9.940618515
Epoch: 6500 Train: 16.259521484 Test: 10.002474785
Epoch: 6600 Train: 16.994216919 Test: 10.013082504
Epoch: 6700 Train: 17.971029282 Test: 9.980273247
Epoch: 6800 Train: 19.288883209 Test: 9.941390038
Epoch: 6900 Train: 20.498430252 Test: 9.863643646
Epoch: 7000 Train: 21.440353394 Test: 9.805099487
Epoch: 7100 Train: 21.907749176 Test: 9.727187157
Epoch: 7200 Train: 21.842004776 Test: 9.698424339
Epoch: 7300 Train: 21.190490723 Test: 9.746901512
Epoch: 7400 Train: 20.057167053 Test: 9.849761009
Epoch: 7500 Train: 19.116859436 Test: 9.878253937
Epoch: 7600 Train: 18.311643600 Test: 9.856149673
Epoch: 7700 Train: 17.243072510 Test: 9.805446625
Epoch: 7800 Train: 16.253444672 Test: 9.777880669
Epoch: 7900 Train: 16.806602478 Test: 9.799551010
Epoch: 8000 Train: 17.150339127 Test: 9.790854454
Epoch: 8100 Train: 17.333173752 Test: 9.776350975
Epoch: 8200 Train: 17.319747925 Test: 9.784387589
Epoch: 8300 Train: 16.820858002 Test: 9.798049927
Epoch: 8400 Train: 16.457717896 Test: 9.777153015
Epoch: 8500 Train: 16.187814713 Test: 9.780218124
Epoch: 8600 Train: 15.937328339 Test: 9.778969765
Epoch: 8700 Train: 15.772624969 Test: 9.766874313
Epoch: 8800 Train: 15.725375175 Test: 9.773715019
Epoch: 8900 Train: 15.756326675 Test: 9.805426598
Epoch: 9000 Train: 15.712633133 Test: 9.834775925
Epoch: 9100 Train: 15.561609268 Test: 9.840375900
Epoch: 9200 Train: 15.626892090 Test: 9.851252556
Epoch: 9300 Train: 15.608184814 Test: 9.865990639
Epoch: 9400 Train: 15.514810562 Test: 9.887211800
Epoch: 9500 Train: 15.433785439 Test: 9.917554855
Epoch: 9600 Train: 15.290946007 Test: 9.920752525
Epoch: 9700 Train: 15.199363708 Test: 9.937676430
Epoch: 9800 Train: 15.133708954 Test: 9.952917099
Epoch: 9900 Train: 15.036188126 Test: 9.972976685
Epoch: 9999 Train: 14.924207687 Test: 9.976866722
Training Loss: tensor(14.9242)
Test Loss: tensor(9.9769)
True Mean x: tensor(3.1718, device='cuda:0', grad_fn=<MeanBackward0>)
Learned Mean x: tensor(-0.0916, device='cuda:0', grad_fn=<MeanBackward0>)
True Var x: tensor(3.3282, device='cuda:0', grad_fn=<VarBackward0>)
Learned Var x: tensor(4.1311e-05, device='cuda:0', grad_fn=<VarBackward0>)
Jacobian term Training Loss: tensor(0.0490)
Jacobian term Test Loss: tensor(0.0496)
Learned LE: [ 0.5003747  -0.02984391]
True LE: tensor([ 0.6931, -0.6931], dtype=torch.float64)
